{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a6e231",
   "metadata": {},
   "source": [
    "# MRI based brain tumor IDH classification with MONAI (3D multiparametric MRI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514708e4",
   "metadata": {},
   "source": [
    "This tutorial shows how to construct a training workflow of binary classification task.  \n",
    "And it contains below features:\n",
    "1. Transforms for Monai dictionary format data.\n",
    "2. Define a new transform according MONAI transform API.\n",
    "3. Load Nifti image with metadata, load a list of images and stack them.\n",
    "5. 3D Voxel DynUNet model, Dice loss, cross entropy loss function for IDH classification task.\n",
    "6. Deterministic training for reproducibility.\n",
    "\n",
    "The Brain tumor dataset can be downloaded from \n",
    "https://ipp.cbica.upenn.edu/ and  http://medicaldecathlon.com/.  \n",
    "\n",
    "Target: IDH classification based on whole brain, tumour core, whole tumor, and enhancing tumor from MRI \n",
    "Modality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  \n",
    "training: 135 3D MRI \\\n",
    "validation:  \\\n",
    "testing: Not revealed\n",
    "\n",
    "Source: BRATS 2020/2021 datasets.  \n",
    "Challenge: RSNA-MICCAI Brain Tumor Radiogenomic Classification\n",
    "\n",
    "Below figure shows image patches with the tumor sub-regions that are annotated in the different modalities (top left) and the final labels for the whole dataset (right). (Figure taken from the [BraTS IEEE TMI paper](https://ieeexplore.ieee.org/document/6975210/))  \n",
    "![image](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7283692/6975210/6975210-fig-3-source-large.gif)\n",
    "\n",
    "The image patches show from left to right:\n",
    "1. the whole tumor (yellow) visible in T2-FLAIR (Fig.A).\n",
    "2. the tumor core (red) visible in T2 (Fig.B).\n",
    "3. the enhancing tumor structures (light blue) visible in T1Gd, surrounding the cystic/necrotic components of the core (green) (Fig. C).\n",
    "4. The segmentations are used to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139e9994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.9.0\n",
      "Numpy version: 1.22.3\n",
      "Pytorch version: 1.10.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: af0e0e9f757558d144b655c63afcea3a4e0a06f5\n",
      "MONAI __file__: /home/mmiv-ml/anaconda3/envs/sa_tumorseg22/lib/python3.9/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.9\n",
      "Nibabel version: 4.0.1\n",
      "scikit-image version: 0.19.3\n",
      "Pillow version: 9.0.1\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.11.2\n",
      "tqdm version: 4.64.0\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.1\n",
      "pandas version: 1.4.2\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import gc\n",
    "import logging\n",
    "import copy\n",
    "import pdb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.networks.nets import DynUNet, EfficientNetBN, DenseNet121, SegResNet, SegResNetVAE, AttentionUnet\n",
    "from monai.data import CacheDataset, Dataset, DataLoader, ThreadDataLoader, list_data_collate\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    CastToTyped,\n",
    "    Compose, \n",
    "    CropForegroundd,\n",
    "    ResizeWithPadOrCrop,\n",
    "    ResizeWithPadOrCropd,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    Resized,\n",
    "    EnsureChannelFirstd, \n",
    "    Orientationd,\n",
    "    LoadImaged,\n",
    "    CopyItemsd,\n",
    "    NormalizeIntensity,\n",
    "    HistogramNormalize,\n",
    "    NormalizeIntensityd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandCropByLabelClassesd,\n",
    "    RandAffined,\n",
    "    RandFlipd,\n",
    "    Flipd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandGibbsNoised,\n",
    "    RandStdShiftIntensityd,\n",
    "    RandScaleIntensityd,\n",
    "    RandZoomd, \n",
    "    SpatialCrop, \n",
    "    SpatialPadd, \n",
    "    MapTransform,\n",
    "    CastToType,\n",
    "    ToTensord,\n",
    "    AddChanneld,\n",
    "    MapTransform,\n",
    "    Orientationd,\n",
    "    ScaleIntensityd,\n",
    "    ScaleIntensity,\n",
    "    ScaleIntensityRangePercentilesd,\n",
    "    KeepLargestConnectedComponentd,\n",
    "    KeepLargestConnectedComponent,\n",
    "    ScaleIntensityRange,\n",
    "    RandShiftIntensityd,\n",
    "    RandAdjustContrastd,\n",
    "    AdjustContrastd,\n",
    "    Rotated,\n",
    "    ToNumpyd,\n",
    "    ToDeviced,\n",
    "    EnsureType,\n",
    "    EnsureTyped,\n",
    "    DataStatsd,\n",
    ")\n",
    "\n",
    "from monai.config import KeysCollection\n",
    "from monai.transforms.compose import MapTransform, Randomizable\n",
    "from collections.abc import Iterable\n",
    "from typing import Any, Dict, Hashable, Mapping, Optional, Sequence, Tuple, Union\n",
    "from monai.utils import set_determinism\n",
    "from monai.utils import (\n",
    "    ensure_tuple,\n",
    "    ensure_tuple_rep,\n",
    "    ensure_tuple_size,\n",
    ")\n",
    "\n",
    "from monai.optimizers import LearningRateFinder\n",
    "\n",
    "from monai.transforms.compose import MapTransform\n",
    "from monai.transforms.utils import generate_spatial_bounding_box\n",
    "from skimage.transform import resize\n",
    "from monai.losses import DiceCELoss, DiceLoss\n",
    "from monai.utils import set_determinism\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "\n",
    "from monai.metrics import DiceMetric, ROCAUCMetric, HausdorffDistanceMetric\n",
    "from monai.data import decollate_batch\n",
    "import glob\n",
    "import monai\n",
    "from monai.metrics import compute_meandice\n",
    "import random\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from typing import Sequence, Optional\n",
    "import ipywidgets as widgets\n",
    "from itertools import compress\n",
    "import SimpleITK as sitk\n",
    "import torchio as tio\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, accuracy_score, recall_score, \\\n",
    "accuracy_score, precision_score, f1_score, make_scorer,balanced_accuracy_score \n",
    "\n",
    "from monai.utils import ensure_tuple_rep\n",
    "from monai.networks.layers.factories import Conv, Dropout, Norm, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from ranger21 import Ranger21\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itkwidgets import view\n",
    "import random\n",
    "monai.config.print_config()\n",
    "#from sliding_window_inference_classes import sliding_window_inference_classes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7965822f",
   "metadata": {},
   "source": [
    "# Pipelines implemented here\n",
    "#[image](ProposedArchImgPath =250x250)\\\n",
    "<img src=\"assets/ProposedIDHClass.png\" align=\"left\" width=\"1024\" height=\"1800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78284a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf8e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_THREADS =2\n",
    "sitk.ProcessObject.SetGlobalDefaultNumberOfThreads(MAX_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b41351c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 18 06:16:28 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.172.01   Driver Version: 450.172.01   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    43W / 300W |    108MiB / 32505MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    54W / 300W |   7832MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    65W / 300W |   3271MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   48C    P0   113W / 300W |   9188MiB / 32508MiB |     28%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2917      G   /usr/lib/xorg/Xorg                 86MiB |\n",
      "|    0   N/A  N/A      3284      G   /usr/bin/gnome-shell               16MiB |\n",
      "|    1   N/A  N/A      2917      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A      2917      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A    343254      C   .../sa_tumorseg22/bin/python     1645MiB |\n",
      "|    2   N/A  N/A    343456      C   .../sa_tumorseg22/bin/python     1613MiB |\n",
      "|    3   N/A  N/A      2917      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A    329702      C   .../sa_tumorseg22/bin/python     9175MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "seeds = 40961024\n",
    "set_determinism(seed=seeds)\n",
    "##np.random.seed(seeds) np random seed does not work here\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a770ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#patch_size = (128, 128, 128)\n",
    "spacing = (1.0, 1.0, 1.0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"3\"\n",
    "device = torch.device('cuda:0')\n",
    "deviceName = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b83c3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "data_rpath = '/home/mmiv-ml/data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e239e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BraTS2021</th>\n",
       "      <th>t1wPath</th>\n",
       "      <th>t1cwPath</th>\n",
       "      <th>t1cw_N4CorrectPath</th>\n",
       "      <th>t2wPath</th>\n",
       "      <th>t2w_N4CorrectPath</th>\n",
       "      <th>flairPath</th>\n",
       "      <th>segPath</th>\n",
       "      <th>brain_maskPath</th>\n",
       "      <th>brain_mask_ch2Path</th>\n",
       "      <th>...</th>\n",
       "      <th>ET_CoordX</th>\n",
       "      <th>ET_CoordY</th>\n",
       "      <th>ET_CoordZ</th>\n",
       "      <th>ED_CoordX</th>\n",
       "      <th>ED_CoordY</th>\n",
       "      <th>ED_CoordZ</th>\n",
       "      <th>NEC_CoordX</th>\n",
       "      <th>NEC_CoordY</th>\n",
       "      <th>NEC_CoordZ</th>\n",
       "      <th>is_merged_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BraTS2021_00140</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1ce_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t2_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_00140/ROI_BraTS2021_00140.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_00140/BraTS2021_00140_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>168.685087</td>\n",
       "      <td>167.653671</td>\n",
       "      <td>79.886450</td>\n",
       "      <td>162.346647</td>\n",
       "      <td>173.396768</td>\n",
       "      <td>87.441763</td>\n",
       "      <td>168.083333</td>\n",
       "      <td>167.200000</td>\n",
       "      <td>78.066667</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BraTS2021_01283</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1ce_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t2_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01283/ROI_BraTS2021_01283.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01283/BraTS2021_01283_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>145.484701</td>\n",
       "      <td>134.678620</td>\n",
       "      <td>59.585174</td>\n",
       "      <td>152.096980</td>\n",
       "      <td>146.947874</td>\n",
       "      <td>73.214571</td>\n",
       "      <td>147.219848</td>\n",
       "      <td>134.146249</td>\n",
       "      <td>59.135090</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BraTS2021_01528</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1ce_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t2_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01528/ROI_BraTS2021_01528.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01528/BraTS2021_01528_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>77.531023</td>\n",
       "      <td>144.899230</td>\n",
       "      <td>82.371416</td>\n",
       "      <td>94.469503</td>\n",
       "      <td>140.150948</td>\n",
       "      <td>66.994481</td>\n",
       "      <td>71.698179</td>\n",
       "      <td>136.327992</td>\n",
       "      <td>62.269723</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BraTS2021_01503</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1ce_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t2_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01503/ROI_BraTS2021_01503.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01503/BraTS2021_01503_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>110.542553</td>\n",
       "      <td>73.074468</td>\n",
       "      <td>70.808511</td>\n",
       "      <td>107.090113</td>\n",
       "      <td>82.676138</td>\n",
       "      <td>76.029439</td>\n",
       "      <td>105.099771</td>\n",
       "      <td>65.077985</td>\n",
       "      <td>76.992437</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BraTS2021_01453</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1ce_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t2_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01453/ROI_BraTS2021_01453.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01453/BraTS2021_01453_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>86.031397</td>\n",
       "      <td>128.011381</td>\n",
       "      <td>67.940149</td>\n",
       "      <td>81.830275</td>\n",
       "      <td>119.381631</td>\n",
       "      <td>64.750845</td>\n",
       "      <td>83.705329</td>\n",
       "      <td>127.626959</td>\n",
       "      <td>65.589342</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t1Gd_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t2_afterN4Correct.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-651/LGG-651_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-651/LGG-651_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.500422</td>\n",
       "      <td>115.393242</td>\n",
       "      <td>68.151900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t1Gd_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t2_afterN4Correct.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-658/LGG-658_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-658/LGG-658_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136.202745</td>\n",
       "      <td>166.640807</td>\n",
       "      <td>107.448810</td>\n",
       "      <td>140.095694</td>\n",
       "      <td>173.392344</td>\n",
       "      <td>97.712919</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t1Gd_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t2_afterN4Correct.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-659/LGG-659_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-659/LGG-659_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.580447</td>\n",
       "      <td>105.934087</td>\n",
       "      <td>122.249243</td>\n",
       "      <td>130.666667</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t1Gd_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t2_afterN4Correct.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-660/LGG-660_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-660/LGG-660_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.208202</td>\n",
       "      <td>185.948429</td>\n",
       "      <td>73.406706</td>\n",
       "      <td>88.338498</td>\n",
       "      <td>194.035995</td>\n",
       "      <td>76.130393</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t1Gd_afterN4Correct.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t2_afterN4Correct.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-766/LGG-766_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-766/LGG-766_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145.203497</td>\n",
       "      <td>93.599896</td>\n",
       "      <td>80.739451</td>\n",
       "      <td>144.431587</td>\n",
       "      <td>91.927287</td>\n",
       "      <td>82.874902</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>368 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           BraTS2021  \\\n",
       "0    BraTS2021_00140   \n",
       "1    BraTS2021_01283   \n",
       "2    BraTS2021_01528   \n",
       "3    BraTS2021_01503   \n",
       "4    BraTS2021_01453   \n",
       "..               ...   \n",
       "363              NaN   \n",
       "364              NaN   \n",
       "365              NaN   \n",
       "366              NaN   \n",
       "367              NaN   \n",
       "\n",
       "                                                                                               t1wPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1.nii.gz   \n",
       "..                                                                                                 ...   \n",
       "363                                                                                                NaN   \n",
       "364                                                                                                NaN   \n",
       "365                                                                                                NaN   \n",
       "366                                                                                                NaN   \n",
       "367                                                                                                NaN   \n",
       "\n",
       "                                                                                                t1cwPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1ce.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1ce.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1ce.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1ce.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1ce.nii.gz   \n",
       "..                                                                                                   ...   \n",
       "363     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t1Gd.nii.gz   \n",
       "364     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t1Gd.nii.gz   \n",
       "365     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t1Gd.nii.gz   \n",
       "366     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t1Gd.nii.gz   \n",
       "367     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t1Gd.nii.gz   \n",
       "\n",
       "                                                                                                     t1cw_N4CorrectPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1ce_afterN4Correct.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1ce_afterN4Correct.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1ce_afterN4Correct.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1ce_afterN4Correct.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1ce_afterN4Correct.nii.gz   \n",
       "..                                                                                                                  ...   \n",
       "363     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t1Gd_afterN4Correct.nii.gz   \n",
       "364     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t1Gd_afterN4Correct.nii.gz   \n",
       "365     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t1Gd_afterN4Correct.nii.gz   \n",
       "366     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t1Gd_afterN4Correct.nii.gz   \n",
       "367     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t1Gd_afterN4Correct.nii.gz   \n",
       "\n",
       "                                                                                               t2wPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t2.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t2.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t2.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t2.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t2.nii.gz   \n",
       "..                                                                                                 ...   \n",
       "363     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t2.nii.gz   \n",
       "364     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t2.nii.gz   \n",
       "365     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t2.nii.gz   \n",
       "366     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t2.nii.gz   \n",
       "367     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t2.nii.gz   \n",
       "\n",
       "                                                                                                    t2w_N4CorrectPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t2_afterN4Correct.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t2_afterN4Correct.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t2_afterN4Correct.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t2_afterN4Correct.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t2_afterN4Correct.nii.gz   \n",
       "..                                                                                                                ...   \n",
       "363     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t2_afterN4Correct.nii.gz   \n",
       "364     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t2_afterN4Correct.nii.gz   \n",
       "365     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t2_afterN4Correct.nii.gz   \n",
       "366     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t2_afterN4Correct.nii.gz   \n",
       "367     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t2_afterN4Correct.nii.gz   \n",
       "\n",
       "                                                                                                flairPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_flair.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_flair.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_flair.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_flair.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_flair.nii.gz   \n",
       "..                                                                                                    ...   \n",
       "363                                                                                                   NaN   \n",
       "364                                                                                                   NaN   \n",
       "365                                                                                                   NaN   \n",
       "366                                                                                                   NaN   \n",
       "367                                                                                                   NaN   \n",
       "\n",
       "                                                                                                segPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_seg.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_seg.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_seg.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_seg.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_seg.nii.gz   \n",
       "..                                                                                                  ...   \n",
       "363            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-651/LGG-651_pred.nii.gz   \n",
       "364            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-658/LGG-658_pred.nii.gz   \n",
       "365            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-659/LGG-659_pred.nii.gz   \n",
       "366            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-660/LGG-660_pred.nii.gz   \n",
       "367            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-766/LGG-766_pred.nii.gz   \n",
       "\n",
       "                                                                 brain_maskPath  \\\n",
       "0    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_00140/ROI_BraTS2021_00140.nii.gz   \n",
       "1    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01283/ROI_BraTS2021_01283.nii.gz   \n",
       "2    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01528/ROI_BraTS2021_01528.nii.gz   \n",
       "3    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01503/ROI_BraTS2021_01503.nii.gz   \n",
       "4    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01453/ROI_BraTS2021_01453.nii.gz   \n",
       "..                                                                          ...   \n",
       "363                                                                         NaN   \n",
       "364                                                                         NaN   \n",
       "365                                                                         NaN   \n",
       "366                                                                         NaN   \n",
       "367                                                                         NaN   \n",
       "\n",
       "                                                                                         brain_mask_ch2Path  \\\n",
       "0    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_00140/BraTS2021_00140_BrainROIT1cwx2.nii.gz   \n",
       "1    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01283/BraTS2021_01283_BrainROIT1cwx2.nii.gz   \n",
       "2    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01528/BraTS2021_01528_BrainROIT1cwx2.nii.gz   \n",
       "3    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01503/BraTS2021_01503_BrainROIT1cwx2.nii.gz   \n",
       "4    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01453/BraTS2021_01453_BrainROIT1cwx2.nii.gz   \n",
       "..                                                                                                      ...   \n",
       "363      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-651/LGG-651_BrainROIT1cwx2.nii.gz   \n",
       "364      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-658/LGG-658_BrainROIT1cwx2.nii.gz   \n",
       "365      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-659/LGG-659_BrainROIT1cwx2.nii.gz   \n",
       "366      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-660/LGG-660_BrainROIT1cwx2.nii.gz   \n",
       "367      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-766/LGG-766_BrainROIT1cwx2.nii.gz   \n",
       "\n",
       "     ...   ET_CoordX   ET_CoordY  ET_CoordZ   ED_CoordX   ED_CoordY  \\\n",
       "0    ...  168.685087  167.653671  79.886450  162.346647  173.396768   \n",
       "1    ...  145.484701  134.678620  59.585174  152.096980  146.947874   \n",
       "2    ...   77.531023  144.899230  82.371416   94.469503  140.150948   \n",
       "3    ...  110.542553   73.074468  70.808511  107.090113   82.676138   \n",
       "4    ...   86.031397  128.011381  67.940149   81.830275  119.381631   \n",
       "..   ...         ...         ...        ...         ...         ...   \n",
       "363  ...         NaN         NaN        NaN  150.500422  115.393242   \n",
       "364  ...         NaN         NaN        NaN  136.202745  166.640807   \n",
       "365  ...         NaN         NaN        NaN  131.580447  105.934087   \n",
       "366  ...         NaN         NaN        NaN   88.208202  185.948429   \n",
       "367  ...         NaN         NaN        NaN  145.203497   93.599896   \n",
       "\n",
       "      ED_CoordZ  NEC_CoordX  NEC_CoordY  NEC_CoordZ  is_merged_3  \n",
       "0     87.441763  168.083333  167.200000   78.066667         both  \n",
       "1     73.214571  147.219848  134.146249   59.135090         both  \n",
       "2     66.994481   71.698179  136.327992   62.269723         both  \n",
       "3     76.029439  105.099771   65.077985   76.992437         both  \n",
       "4     64.750845   83.705329  127.626959   65.589342         both  \n",
       "..          ...         ...         ...         ...          ...  \n",
       "363   68.151900         NaN         NaN         NaN         both  \n",
       "364  107.448810  140.095694  173.392344   97.712919         both  \n",
       "365  122.249243  130.666667  116.000000  123.000000         both  \n",
       "366   73.406706   88.338498  194.035995   76.130393         both  \n",
       "367   80.739451  144.431587   91.927287   82.874902         both  \n",
       "\n",
       "[368 rows x 52 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BraTS20Subjectsp1q19WithMetaDF  = pd.read_csv('assets/BraTS_TCGA_LGG_GBM_LGG_1p19qDFMoreMeta_N4CorrectLatDF.csv')\n",
    "BraTS20Subjectsp1q19WithMetaDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8522f1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36bc9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d5fcdc3",
   "metadata": {},
   "source": [
    "## Cearing a list of dictionaries in order to feed into Monai's Dataset\n",
    "Keys:\n",
    "- ***image:*** T1, T1c, T2, and flair image\n",
    "- ***label:*** Segmented mask GT\n",
    "- ***brain_mask:*** Whole brain area (brain area=1 and Non brain area=0)\n",
    "- ***IDH_value:*** IDH class corresponding to the subject/images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62a705",
   "metadata": {},
   "source": [
    "### Creating/extracting 3 splits for cross validaion (3 cross validaion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f4402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_df(BraTS20SubjectsIDHWithMetaDF):\n",
    "    \n",
    "    \n",
    "    BraTS20SubjectsIDHTrainDCT = {}\n",
    "    BraTS20SubjectsIDHValDCT = {}\n",
    "    BraTS20SubjectsIDHTestDCT = {}\n",
    "    \n",
    "    aDCT = {'fold0':[1, 2, 3], 'fold1':[2, 3, 1], 'fold3': [3, 1, 2]}\n",
    "    \n",
    "    for indx, (akey, aval) in enumerate(aDCT.items()):\n",
    "        \n",
    "    \n",
    "        BraTS20SubjectsIDHWithMetaDFTrain = BraTS20SubjectsIDHWithMetaDF.loc[BraTS20SubjectsIDHWithMetaDF['CV_group']==aval[0]]\n",
    "        BraTS20SubjectsIDHWithMetaDFVal = BraTS20SubjectsIDHWithMetaDF.loc[BraTS20SubjectsIDHWithMetaDF['CV_group']==aval[1]]\n",
    "        BraTS20SubjectsIDHWithMetaDFTest = BraTS20SubjectsIDHWithMetaDF.loc[BraTS20SubjectsIDHWithMetaDF['CV_group']==aval[2]]\n",
    "\n",
    "        train_files = [{'image': (image_nameT1ce, image_nameT2), 'label': label_name, 'brain_mask':brain_mask, 'IDH_label':np.array(IDH_label_name).astype(np.float32)} \n",
    "                       for image_nameT1ce, image_nameT2, label_name, brain_mask, IDH_label_name \n",
    "                       in zip(BraTS20SubjectsIDHWithMetaDFTrain['t1cwPath'], BraTS20SubjectsIDHWithMetaDFTrain['t2wPath'], BraTS20SubjectsIDHWithMetaDFTrain['segPath'], \\\n",
    "                              BraTS20SubjectsIDHWithMetaDFTrain['brain_mask_ch2Path'], BraTS20SubjectsIDHWithMetaDFTrain['1p19q_co_deletion_bin'].values)]\n",
    "        \n",
    "        val_files =[{'image': (image_nameT1ce, image_nameT2), 'label': label_name, 'brain_mask':brain_mask, 'IDH_label':np.array(IDH_label_name).astype(np.float32)} \n",
    "                    for image_nameT1ce, image_nameT2, label_name, brain_mask, IDH_label_name \n",
    "                    in zip(BraTS20SubjectsIDHWithMetaDFVal['t1cwPath'], BraTS20SubjectsIDHWithMetaDFVal['t2wPath'],BraTS20SubjectsIDHWithMetaDFVal['segPath'],\\\n",
    "                           BraTS20SubjectsIDHWithMetaDFVal['brain_mask_ch2Path'], BraTS20SubjectsIDHWithMetaDFVal['1p19q_co_deletion_bin'].values)]\n",
    "        \n",
    "        test_files = [{'image': (image_nameT1ce, image_nameT2), 'label': label_name, 'brain_mask':brain_mask, 'IDH_label':np.array(IDH_label_name).astype(np.float32)} \n",
    "                      for image_nameT1ce, image_nameT2, label_name, brain_mask, IDH_label_name \n",
    "                      in zip(BraTS20SubjectsIDHWithMetaDFTest['t1cwPath'], BraTS20SubjectsIDHWithMetaDFTest['t2wPath'], BraTS20SubjectsIDHWithMetaDFTest['segPath'], \\\n",
    "                             BraTS20SubjectsIDHWithMetaDFTest['brain_mask_ch2Path'], BraTS20SubjectsIDHWithMetaDFTest['1p19q_co_deletion_bin'].values)]\n",
    "        \n",
    "        \n",
    "        BraTS20SubjectsIDHTrainDCT[f'fold{indx}'] = copy.deepcopy(train_files)\n",
    "        BraTS20SubjectsIDHValDCT[f'fold{indx}'] = copy.deepcopy(val_files)\n",
    "        BraTS20SubjectsIDHTestDCT[f'fold{indx}'] = copy.deepcopy(test_files)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return BraTS20SubjectsIDHTrainDCT, BraTS20SubjectsIDHValDCT, BraTS20SubjectsIDHTestDCT\n",
    "        \n",
    "        \n",
    "        \n",
    "BraTS20SubjectsIDHTrainDCT, BraTS20SubjectsIDHValDCT, BraTS20SubjectsIDHTestDCT =  get_train_val_test_df(BraTS20Subjectsp1q19WithMetaDF)    \n",
    "        \n",
    "        \n",
    "\n",
    "# train_files_image = [(image_nameT1, image_nameT1ce, image_nameT2, image_nameFl) \n",
    "#                      for image_nameT1,image_nameT1ce, image_nameT2, image_nameFl \n",
    "#                      in zip(dfTrainLbl['t1wPath'], dfTrainLbl['t1cwPath'], dfTrainLbl['T2wPath'], dfTrainLbl['FlairPath'])]\n",
    "# train_files_label = dfTrainLbl['segPath'].tolist()\n",
    "# train_files_brain_mask = dfTrainLbl['brain_maskPath'].tolist()\n",
    "# train_files_IDH_label = dfTrainLbl['IDH_value'].values.ravel().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c293c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62977b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_splits = 3\n",
    "# #train_index = np.linspace(0, train_features.shape[0]-1, num = train_features.shape[0], dtype = np.uint16, endpoint=True)\n",
    "# #partition_data = monai.data.utils.partition_dataset_classes(train_index, train_labels.values.ravel().tolist(), shuffle=True, num_partitions=n_splits) \n",
    "# #partition_data = monai.data.utils.partition_dataset_classes(train_files, dfTrainLbl['IDH_value'].values.ravel().tolist(), shuffle=True, num_partitions=n_splits)\n",
    "# partition_data = monai.data.partition_dataset_classes(train_files, BraTS20SubjectsIDHWithMetaDF['IDH_value'].values.ravel().tolist(), shuffle=True, num_partitions=n_splits)\n",
    "# print(len(partition_data), len(partition_data[0]), len(partition_data[1]), len(partition_data[2]))\n",
    "\n",
    "\n",
    "# # val_folds = {}\n",
    "# # train_folds = {}\n",
    "# # flds = np.linspace(0, n_splits, num=n_splits, dtype = np.int8)\n",
    "# # for cfold in range(n_splits):\n",
    "# #     not_cfold = np.delete(flds, cfold)\n",
    "# #     val_folds[cfold] = partition_data[cfold]\n",
    "# # #     train_folds[cfold] = \n",
    "# # # sub_flds = flds[..., ~0]   \n",
    "# # # sub_flds\n",
    "\n",
    "# val_folds = {}\n",
    "# train_folds = {}\n",
    "# flds = np.linspace(0, n_splits, num=n_splits, dtype = np.uint8)\n",
    "# for cfold in range(n_splits):\n",
    "#     #val_folds[f\"fold{cfold}\"] = train_features.values[partition_data[cfold],:]\n",
    "#     #train_folds[f\"fold{cfold}\"] = np.delete(train_features.values, partition_data[cfold], axis=0)\n",
    "#     #not_cfold = np.delete(flds, cfold)\n",
    "    \n",
    "#     val_folds[f\"fold{cfold}\"] = copy.deepcopy(partition_data[cfold])\n",
    "#     val_folds[f\"fold{cfold}_IDH_label\"] = copy.deepcopy([adct['IDH_label'].item() for adct in partition_data[cfold]])\n",
    "#     train_folds_masks = [1]*n_splits\n",
    "#     train_folds_masks[cfold] = 0\n",
    "#     partition_data_non_cfold = list()\n",
    "#     for aDctLstitem in compress(partition_data, train_folds_masks):\n",
    "#         partition_data_non_cfold.extend(aDctLstitem)\n",
    "        \n",
    "        \n",
    "#     train_folds[f\"fold{cfold}\"] = copy.deepcopy(partition_data_non_cfold)\n",
    "#     train_folds[f\"fold{cfold}_IDH_label\"] = copy.deepcopy([adct['IDH_label'].item() for adct in partition_data_non_cfold])\n",
    "\n",
    "# for i in range(n_splits):\n",
    "#     print('val: ', len(val_folds[f'fold{i}']), 'train: ', len(train_folds[f'fold{i}']), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cbdb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_folds[\"fold0\"]), len(train_files)\n",
    "\n",
    "# for i_cv in range(n_splits):\n",
    "#     print('Training classes\\n')\n",
    "#     print(np.unique([train_folds[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(train_folds[f'fold{i_cv}']))], return_counts = True))\n",
    "#     print('\\nValidation classes\\n')\n",
    "#     print(np.unique([val_folds[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(val_folds[f'fold{i_cv}']))], return_counts = True))\n",
    "#     print('#'*4, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42836e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classes\n",
      "\n",
      "(array([0., 1.]), array([79, 43]))\n",
      "\n",
      "Validation classes\n",
      "\n",
      "(array([0., 1.]), array([80, 44]))\n",
      "\n",
      "Testing classes\n",
      "\n",
      "(array([0., 1.]), array([79, 43]))\n",
      "######################################## \n",
      "\n",
      "\n",
      "Training classes\n",
      "\n",
      "(array([0., 1.]), array([80, 44]))\n",
      "\n",
      "Validation classes\n",
      "\n",
      "(array([0., 1.]), array([79, 43]))\n",
      "\n",
      "Testing classes\n",
      "\n",
      "(array([0., 1.]), array([79, 43]))\n",
      "######################################## \n",
      "\n",
      "\n",
      "Training classes\n",
      "\n",
      "(array([0., 1.]), array([79, 43]))\n",
      "\n",
      "Validation classes\n",
      "\n",
      "(array([0., 1.]), array([79, 43]))\n",
      "\n",
      "Testing classes\n",
      "\n",
      "(array([0., 1.]), array([80, 44]))\n",
      "######################################## \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_splits = 3\n",
    "dfFolds = BraTS20SubjectsIDHTrainDCT\n",
    "for i_cv in range(n_splits):\n",
    "    print('Training classes\\n')\n",
    "    print(np.unique([BraTS20SubjectsIDHTrainDCT[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(BraTS20SubjectsIDHTrainDCT[f'fold{i_cv}']))], return_counts = True))\n",
    "    print('\\nValidation classes\\n')\n",
    "    print(np.unique([BraTS20SubjectsIDHValDCT[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(BraTS20SubjectsIDHValDCT[f'fold{i_cv}']))], return_counts = True))\n",
    "    #print('#'*4, '\\n')\n",
    "    print('\\nTesting classes\\n')\n",
    "    print(np.unique([BraTS20SubjectsIDHTestDCT[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(BraTS20SubjectsIDHTestDCT[f'fold{i_cv}']))], return_counts = True))\n",
    "    print('#'*40, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa202c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6151e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_folds['fold0'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c55bf0",
   "metadata": {},
   "source": [
    "***HistogramStandardization***\n",
    "\n",
    "Implementing histogram standardization from [torchIO](https://github.com/fepegar/torchio) library\n",
    "\n",
    "Bases: [torchio.transforms.preprocessing.intensity.normalization_transform.NormalizationTransform](https://torchio.readthedocs.io/transforms/preprocessing.html#torchio.transforms.preprocessing.intensity.NormalizationTransform)\n",
    "\n",
    "Perform histogram standardization of intensity values.\n",
    "\n",
    "Implementation of [New variants of a method of MRI scale standardization](https://ieeexplore.ieee.org/document/836373).\n",
    "\n",
    "We can visit in [torchio.transforms.HistogramStandardization.train()]((https://torchio.readthedocs.io/transforms/preprocessing.html#torchio.transforms.HistogramStandardization.train)) for more details.\n",
    "\n",
    "PARAMETERS\n",
    "landmarks â€“ Dictionary (or path to a PyTorch file with .pt or .pth extension in which a dictionary has been saved) whose keys are image names in the subject and values are NumPy arrays or paths to NumPy arrays defining the landmarks after training with [torchio.transforms.HistogramStandardization.train()](https://torchio.readthedocs.io/transforms/preprocessing.html#torchio.transforms.HistogramStandardization.train).\n",
    "\n",
    "Here, ***save_dir*** is a path where the trained histogram files for four channels (T1w, T1cw, T2w, and Flair), and trained model's weights will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea226c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t1cw': '/home/mmiv-ml/saruarlive/IDHRadiogenomics2022/assets/histeq_t1cw_DynUnetCommon_OnlyBrats21_Full.npy',\n",
       " 't2w': '/home/mmiv-ml/saruarlive/IDHRadiogenomics2022/assets/histeq_t2w_DynUnetCommon_OnlyBrats21_Full.npy'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_prefix = 'DynUnetCommon_OnlyBrats21_Full'\n",
    "hist_save_dir = '/home/mmiv-ml/saruarlive/IDHRadiogenomics2022/assets'\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "hiseq_t1cnpyfile =  os.path.join(hist_save_dir, f\"histeq_t1cw_{file_prefix}.npy\")\n",
    "t1cw_landmarks = (hiseq_t1cnpyfile if os.path.isfile(hiseq_t1cnpyfile) else \\\n",
    "                  tio.HistogramStandardization.train(image_t1cwpaths, output_path = hiseq_t1cnpyfile))\n",
    "\n",
    "\n",
    "\n",
    "hiseq_t2npyfile = os.path.join(hist_save_dir, f\"histeq_t2w_{file_prefix}.npy\")\n",
    "t2w_landmarks = (hiseq_t2npyfile if os.path.isfile(hiseq_t2npyfile) else \\\n",
    "                 tio.HistogramStandardization.train(image_t2wpaths, output_path = hiseq_t2npyfile))\n",
    "\n",
    "\n",
    "landmarks_dict = {'t1cw': t1cw_landmarks, 't2w': t2w_landmarks}\n",
    "#tio_landmarktransform = tio.HistogramStandardization(landmarks_dict)\n",
    "landmarks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db77f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = 'AttnDynUNet_BratsTCGA_1p19q_3CV_2ChnlsMorePatch_OnlyWSampler_Infer1PatchSWIRngr21_2nRatioclass_HistStand'\n",
    "#file_prefix = 'AttnDynUNet_BratsTCGA_1p19q_3CV_2ChnlsMorePatch_WeightSampler_Infer1PatchSWIRngr21_2nRatioclass_HistStand'\n",
    "savedirname = 'DynUNetVariants_TCGA'\n",
    "save_dir = os.path.join('/raid/brats2021/pthTCGA_1p19q_CoDeletion', savedirname)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b9341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e374456",
   "metadata": {},
   "source": [
    "## Classes for Monai/Pytorch compose class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf822f1",
   "metadata": {},
   "source": [
    "A class to rearrange label mask array as \n",
    "- [0, :, :, :] = the multi class mask (class labels: 0 (background), 1, 2, and 4)\n",
    "- [1, :, :, :] = the whole tumor mask (class labels: 0 (background), and 1)\\\n",
    "Not using here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90218d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelPlusWT(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            \n",
    "            d[key]=np.squeeze(d[key], axis = 0) # Converting 1, H, W, D to H, W, D\n",
    "            result.append(d[key])\n",
    "\n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            result.append(\n",
    "                np.logical_or(\n",
    "                    np.logical_or(d[key] == 2, d[key] == 4), d[key] == 1\n",
    "                )\n",
    "            )\n",
    "            ## merge label 1 and label 4 to construct TC\n",
    "            #result.append(np.logical_or(d[key] == 1, d[key] == 4))\n",
    "            ## label 4 is ET\n",
    "            #result.append(d[key] == 4)\n",
    "            d[key] = np.stack(result, axis=0).astype(np.uint8)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b8cd0",
   "metadata": {},
   "source": [
    "#### Define a new transform to convert brain tumor labels\n",
    "Here we convert the multi-classes labels into multi-labels segmentation task in One-Hot format.\\\n",
    "Not using here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5af3ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            \n",
    "            d[key]=np.squeeze(d[key], axis = 0) # Converting 1, H, W, D to H, W, D\n",
    "\n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            result.append(\n",
    "                np.logical_or(\n",
    "                    np.logical_or(d[key] == 2, d[key] == 4), d[key] == 1\n",
    "                )\n",
    "            )\n",
    "            # merge label 1 and label 4 to construct TC\n",
    "            result.append(np.logical_or(d[key] == 1, d[key] == 4))\n",
    "            # label 4 is ET\n",
    "            result.append(d[key] == 4)\n",
    "            d[key] = np.stack(result, axis=0).astype(np.float32)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5d5b76",
   "metadata": {},
   "source": [
    "#### A class to add new key having the tumor mask (GT) to the existing data dictionary\n",
    "The new key, ***label_mask*** will have the same dimension (size: 4,x,x,x) with image array (size: 4,x,x,x)\\\n",
    "Using in ***compose*** class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9070307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToIDHLabel2WTd(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, IDH_label_key:str = 'IDH_label') -> None:\n",
    "\n",
    "        super().__init__(keys)\n",
    "        self.IDH_label_key = IDH_label_key\n",
    "       \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            #WT = np.logical_or(np.logical_or(d[key] == 2, d[key] == 4), d[key] == 1).astype(np.uint8)\n",
    "            result = []\n",
    "            WT = np.squeeze(d[key], axis = 0)\n",
    "            if d[self.IDH_label_key].item() == 1:\n",
    "                WT=np.multiply(WT, 2)\n",
    "                #WT = 2*WT\n",
    "            \n",
    "            result.append(WT==1)\n",
    "            result.append(WT==2)\n",
    "            \n",
    "            d[key] = np.stack(result, axis = 0).astype(np.float32)\n",
    "            \n",
    "    \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9fb399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convert2WTd(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            output_classes = 2\n",
    "            \n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            WT = np.logical_or(np.logical_or(d[key] == 2, d[key] == 4), d[key] == 1).astype(np.float32)\n",
    "            d[f'{key}'] = WT\n",
    "         \n",
    "            WT = np.expand_dims(ndimage.binary_dilation(np.squeeze(WT, axis=0), iterations=2), axis = 0)\n",
    "            #WT = np.stack(tuple([ndimage.binary_dilation((np.squeeze(WT, axis = 0)==_k).astype(WT.dtype), iterations=5).astype(WT.dtype) for _k in range(output_classes)]), axis = 0)\n",
    "            d[f'{key}_mask'] = WT\n",
    "            d[f'{key}_mask_meta_dict'] = copy.deepcopy(d[f\"{key}_meta_dict\"])\n",
    "            \n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31ff846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialCropWTCOMd(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, keys: KeysCollection, roi_size, COM_label_key:str = 'label_mask') -> None:\n",
    "\n",
    "        super().__init__(keys)\n",
    "        self.COM_label_key = COM_label_key\n",
    "        self.roi_size = roi_size\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            Coms = np.array([ndimage.measurements.center_of_mass(lbl) for lbl in list(d[self.COM_label_key])])\n",
    "            Coms[np.isnan(Coms)] = 70\n",
    "            Coms=Coms[0].astype(np.uint16).tolist()\n",
    "        \n",
    "            sc_com= SpatialCrop(roi_center= Coms, roi_size=self.roi_size)\n",
    "            d[key] = sc_com(d[key])\n",
    "                \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27e08750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatLabelBrainmaskd(MapTransform):\n",
    "    \"\"\"\n",
    "          we do not need labels as it is a generative problem\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, image_key = 'image', label_key = 'label', \n",
    "                 brain_mask_key = 'brain_mask') -> None:\n",
    "        \n",
    "        super().__init__(keys)\n",
    "        self.brain_mask_key = brain_mask_key\n",
    "        self.image_key = image_key\n",
    "        self.label_key = label_key\n",
    "    \n",
    "    \n",
    "    def __call__(self, data):\n",
    "     \n",
    "        d = dict(data)\n",
    "        #d[self.image_key] = np.concatenate((d[self.image_key], d[self.label_key], d[self.brain_mask_key][0:1]), axis = 0)\n",
    "        d[self.image_key] = np.concatenate((d[self.image_key], d[self.label_key][0:1]), axis = 0)\n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770073b6",
   "metadata": {},
   "source": [
    "### Implementing channelwise histogram normalization\n",
    "(Not using here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "768c111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistogramNormalizeChannelWised(MapTransform):\n",
    "    \"\"\"\n",
    "          we do not need labels as it is a generative problem\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, brain_mask_key = 'brain_mask', min=0, max=255) -> None:\n",
    "        \n",
    "        super().__init__(keys)\n",
    "        self.brain_mask_key = brain_mask_key\n",
    "        self.histnorms = HistogramNormalize(num_bins=256, min=min, max=max)\n",
    "    \n",
    "    \n",
    "    def __call__(self, data):\n",
    "     \n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            nchnl = d[key].shape[0]\n",
    "            for ch in range(nchnl):\n",
    "                d[key][ch] = self.histnorms(d[key][ch], d[self.brain_mask_key][ch])\n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8791a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class adapter_tio2monai(MapTransform):\n",
    "    \"\"\"\n",
    "    # wrapper for tio affine transformation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        #keys: KeysCollection,\n",
    "        mode = 'train',\n",
    "        tiofn=None,\n",
    "        **mykwargs,\n",
    "    ) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "           Wrapper from torchio to monai\n",
    "        \"\"\"\n",
    "        \n",
    "        #super().__init__(keys)\n",
    "        #super().__init__(**mykwargs)\n",
    "        self.tiofn = tiofn(**mykwargs)\n",
    "        self.mode = mode\n",
    "        self.mykwargs = mykwargs\n",
    "        \n",
    "    \n",
    "    def __call__(self, data: Mapping[Hashable, np.ndarray]) -> Dict[Hashable, np.ndarray]:\n",
    "        d = dict(data)\n",
    "        \n",
    "        if self.mode =='train':\n",
    "            subject = tio.Subject(\n",
    "                image=tio.ScalarImage(tensor=d[\"image\"], affine = d['image_meta_dict']['affine']),  # this class is new\n",
    "                label=tio.LabelMap(tensor=d[\"label\"], affine = d['label_meta_dict']['affine']),\n",
    "                brain_mask = tio.LabelMap(tensor=d[\"brain_mask\"], affine = d['brain_mask_meta_dict']['affine'])\n",
    "            )\n",
    "            transformed = self.tiofn(subject)\n",
    "            d[\"image\"] = transformed[\"image\"].numpy()\n",
    "            d[\"label\"] = transformed[\"label\"].numpy()\n",
    "            d[\"brain_mask\"] = transformed[\"brain_mask\"].numpy()\n",
    "            d[\"image_meta_dict\"]['affine'] = transformed[\"image\"].affine.copy()\n",
    "            d[\"label_meta_dict\"]['affine'] = transformed[\"label\"].affine.copy()\n",
    "            d[\"brain_mask_meta_dict\"]['affine'] = transformed[\"brain_mask\"].affine.copy()\n",
    "            \n",
    "            \n",
    "        elif self.mode =='infer':\n",
    "            \n",
    "            subject = tio.Subject(\n",
    "                image=tio.ScalarImage(tensor=d[\"image\"], affine = d['image_meta_dict']['affine']),  # this class is new\n",
    "                brain_mask = tio.LabelMap(tensor=d[\"brain_mask\"], affine = d['brain_mask_meta_dict']['affine'])\n",
    "            )\n",
    "            \n",
    "            transformed = self.tiofn(subject)\n",
    "            d[\"image\"] = transformed[\"image\"].numpy()\n",
    "            d[\"brain_mask\"] = transformed[\"brain_mask\"].numpy()\n",
    "            d[\"image_meta_dict\"]['affine'] = transformed[\"image\"].affine.copy()\n",
    "            d[\"brain_mask_meta_dict\"]['affine'] = transformed[\"brain_mask\"].affine.copy()\n",
    "        \n",
    "        else:\n",
    "            print('Please select mode either train or infer')\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2eb6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class adapter_tioChannelWise2monai(MapTransform):\n",
    "    \"\"\"\n",
    "    # wrapper for tio affine transformation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        #keys: KeysCollection,\n",
    "        mode = 'train',\n",
    "        tiofn=None,\n",
    "        **mykwargs,\n",
    "    ) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "           Wrapper from torchio to monai\n",
    "        \"\"\"\n",
    "        \n",
    "        #super().__init__(keys)\n",
    "        #super().__init__(**mykwargs)\n",
    "        self.tiofn = tiofn(**mykwargs)\n",
    "        self.mode = mode\n",
    "        self.mykwargs = mykwargs\n",
    "        \n",
    "    \n",
    "    def __call__(self, data: Mapping[Hashable, np.ndarray]) -> Dict[Hashable, np.ndarray]:\n",
    "        d = dict(data)\n",
    "        if self.mode =='train':\n",
    "            subject = tio.Subject(\n",
    "               \n",
    "                t1cw=tio.ScalarImage(tensor=d[\"image\"][0:1,...], affine = d['image_meta_dict']['affine']),\n",
    "                t2w=tio.ScalarImage(tensor=d[\"image\"][1:2,...], affine = d['image_meta_dict']['affine']),\n",
    "               \n",
    "                label=tio.LabelMap(tensor=d[\"label\"], affine = d['label_meta_dict']['affine']),\n",
    "                brain_mask = tio.LabelMap(tensor=d[\"brain_mask\"], affine = d['brain_mask_meta_dict']['affine'])\n",
    "            )\n",
    "            transformed = self.tiofn(subject)\n",
    "            d[\"image\"] = np.concatenate([transformed[\"t1cw\"].numpy(), transformed[\"t2w\"].numpy()], axis = 0)\n",
    "            d[\"label\"] = transformed[\"label\"].numpy()\n",
    "            d[\"brain_mask\"] = transformed[\"brain_mask\"].numpy()\n",
    "            d[\"image_meta_dict\"]['affine'] = transformed[\"t1cw\"].affine.copy()\n",
    "            d[\"label_meta_dict\"]['affine'] = transformed[\"label\"].affine.copy()\n",
    "            d[\"brain_mask_meta_dict\"]['affine'] = transformed[\"brain_mask\"].affine.copy()\n",
    "            \n",
    "        \n",
    "        elif self.mode =='infer':\n",
    "            \n",
    "            subject = tio.Subject(\n",
    "                t1cw=tio.ScalarImage(tensor=d[\"image\"][0:1,...], affine = d['image_meta_dict']['affine']),\n",
    "                t2w=tio.ScalarImage(tensor=d[\"image\"][1:2,...], affine = d['image_meta_dict']['affine']),\n",
    "                brain_mask = tio.LabelMap(tensor=d[\"brain_mask\"], affine = d['brain_mask_meta_dict']['affine'])\n",
    "            )\n",
    "            \n",
    "            transformed = self.tiofn(subject)\n",
    "            d[\"image\"] = np.concatenate([transformed[\"t1cw\"].numpy(), transformed[\"t2w\"].numpy()], axis = 0)\n",
    "    \n",
    "            d[\"brain_mask\"] = transformed[\"brain_mask\"].numpy()\n",
    "            d[\"image_meta_dict\"]['affine'] = transformed[\"t1cw\"].affine.copy()\n",
    "            d[\"brain_mask_meta_dict\"]['affine'] = transformed[\"brain_mask\"].affine.copy()\n",
    "        \n",
    "        else:\n",
    "            print('Please select mode either train or infer')\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a83983",
   "metadata": {},
   "source": [
    "### Defining traning and validation transforms\n",
    "\n",
    "- Training transform includes:\n",
    "    - LoadImaged\n",
    "    - EnsureChannelFirstd\n",
    "    - HistogramNormalizeChannelWised: Histogram normalization channel wise (custom class defined aboove)\n",
    "    - NormalizeIntensityd\n",
    "    - RandRotate90d\n",
    "    - RandZoomd\n",
    "    - ConvertToIDHLabel2WTd (custom class defined above)\n",
    "    - CropForegroundd: Cropping foreground based on the whole tumor mask (WT GT)\n",
    "    - RandCropByPosNegLabeld: Randomly cropping 8 patches based on 3: 1 (WT : non tumor tissus) ratio\n",
    "    - RandGaussianNoised\n",
    "    - RandStdShiftIntensityd\n",
    "    - RandFlipd\n",
    "    \n",
    "- validation transform includes:\n",
    "    - LoadImaged\n",
    "    - EnsureChannelFirstd\n",
    "    - HistogramNormalizeChannelWised: Histogram normalization channel wise (custom class defined aboove)\n",
    "    - NormalizeIntensityd\n",
    "    - ConvertToIDHLabel2WTd (custom class defined above)\n",
    "    \n",
    "Most of transfroms are implemented using [Monai](https://docs.monai.io/en/latest/transforms.html#dictionary-transforms) library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3843738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75230460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_foreground(x):\n",
    "    # threshold at not equal to 0\n",
    "    return x == 1\n",
    "\n",
    "\n",
    "#Resized(keys=keys[0:-1], spatial_size=patch_size, mode = ('area','nearest','nearest')),\n",
    "\n",
    "# ConvertToMultiChannelBasedOnBratsClassesd(keys = ['label']),\n",
    "# ConcatLabelBrainmaskd(keys = None, image_key = 'image', label_key = 'label', brain_mask_key = 'brain_mask'),\n",
    "# CropForegroundd(keys=keys[0:-1], source_key=\"brain_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord'),\n",
    "# SpatialPadd(keys=keys[0:-1], spatial_size=patch_size),\n",
    "\n",
    "# RandGaussianSmoothd(\n",
    "#     keys=[\"image\"],\n",
    "#     sigma_x=(0.5, 1.15),\n",
    "#     sigma_y=(0.5, 1.15),\n",
    "#     sigma_z=(0.5, 1.15),\n",
    "#     prob=0.3,\n",
    "# ),\n",
    "\n",
    "# RandScaleIntensityd(keys=[\"image\"], factors=0.3, prob=0.3),\n",
    "# RandGibbsNoised(keys=[\"image\"], prob=0.3, alpha=(0.1, 0.5), as_tensor_output=False),\n",
    "\n",
    "          \n",
    "# DataStatsd(keys=keys[0:-1], prefix=\"Data\", data_type=True, data_shape=True, value_range=True, data_value=False),\n",
    "#DataStatsd(keys=keysExt[0:-1], prefix=\"Data\", data_type=True, data_shape=True, value_range=True, data_value=False),\n",
    "\n",
    "\n",
    "def get_task_transforms(patch_size, task='train', pos_sample_num=1, neg_sample_num=1, num_samples=1, num_classes = 2, cratio = [1, 3]):\n",
    "    \n",
    "    #spatial_size=(30, 30, 30)\n",
    "    orig_img_size = (240, 240, 155)\n",
    "\n",
    "    if task=='train':\n",
    "        keys = [\"image\", 'label', 'brain_mask', 'IDH_label']\n",
    "        keysExt = [\"image\", 'label', 'brain_mask', 'label_mask', 'IDH_label']\n",
    "        \n",
    "        all_transform = [\n",
    "            \n",
    "            LoadImaged(keys=keys[0:-1], reader = \"NibabelReader\"),\n",
    "            EnsureChannelFirstd(keys=keys[0:-1]),\n",
    "            adapter_tioChannelWise2monai(tiofn = tio.HistogramStandardization, mode = 'train', landmarks = landmarks_dict),\n",
    "            NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
    "            #HistogramNormalizeChannelWised(keys = ['image'], brain_mask_key = 'brain_mask', min = 1, max = 65535),\n",
    "            \n",
    "            adapter_tio2monai(tiofn = tio.OneOf, transforms={tio.RandomAffine(scales=(0.9, 1.2),degrees=15, isotropic=True): 0.6, \\\n",
    "                                            tio.RandomElasticDeformation(): 0.4}, p = 0.4), #p = 0.4\n",
    "            #adapter_tio2monai(tiofn = tio.RandomAffine, scales=(0.9, 1.2), degrees=15, isotropic=True, p = 0.2), \n",
    "            \n",
    "\n",
    "            \n",
    "            #ConvertToIDHLabel2WTd(keys = [\"label\"]),\n",
    "            CopyItemsd(keys=[\"label\"], names=[\"label_mask\"], times=1),\n",
    "            Convert2WTd(keys = [\"label\"]),\n",
    "            ConvertToIDHLabel2WTd(keys = [\"label\"], IDH_label_key = 'IDH_label'),\n",
    "            CropForegroundd(keys=keysExt[0:-1], source_key=\"brain_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord'),\n",
    "            #Spacingd(keys = keysExt[0:-1], pixdim=(1.25, 1.25, 1.25), mode = ('bilinear','nearest', 'nearest', 'nearest')),\n",
    "            RandZoomd(\n",
    "                keys=keysExt[0:-1],\n",
    "                min_zoom=0.9,\n",
    "                max_zoom=1.1,\n",
    "                mode=(\"trilinear\", \"nearest\", \"nearest\", \"nearest\"),\n",
    "                align_corners=(True, None, None, None),\n",
    "                prob=0.15,\n",
    "            ),\n",
    "           \n",
    "            #ResizeWithPadOrCropd(keys = keysExt[0:-1], spatial_size = (128, 160, 128)),\n",
    "            RandGaussianNoised(keys=[\"image\"], std=0.01, prob=0.15),\n",
    "            RandGaussianSmoothd(\n",
    "                keys=[\"image\"],\n",
    "                sigma_x=(0.5, 1.15),\n",
    "                sigma_y=(0.5, 1.15),\n",
    "                sigma_z=(0.5, 1.15),\n",
    "                prob=0.15,\n",
    "            ),\n",
    "            RandStdShiftIntensityd(keys = [\"image\"], factors=0.3, nonzero=True, channel_wise=True, prob=0.15), \n",
    "            RandScaleIntensityd(keys=[\"image\"], factors=0.3, prob=0.15),\n",
    "            RandGibbsNoised(keys=[\"image\"], prob=0.15, alpha=(0.1, 0.5), as_tensor_output=False),\n",
    "            RandFlipd(keys=keysExt[0:-1], prob=0.5, spatial_axis=0),\n",
    "            RandFlipd(keys=keysExt[0:-1], prob=0.5, spatial_axis=1),\n",
    "            RandFlipd(keys=keysExt[0:-1], prob=0.5, spatial_axis=2),\n",
    "                        \n",
    "            CropForegroundd(keys=keysExt[0:-1], source_key=\"label_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord', margin=2),\n",
    "            #ResizeWithPadOrCropd(keys = keysExt[0:-1], spatial_size = patch_size),\n",
    "            SpatialPadd(keys = keysExt[0:-1], spatial_size = patch_size),\n",
    "#             RandCropByLabelClassesd(\n",
    "#                 keys=keysExt[0:-1],            \n",
    "#                 label_key = \"label_mask\",\n",
    "#                 spatial_size = patch_size,    \n",
    "#                 ratios= cratio,\n",
    "#                 num_classes=num_classes,              \n",
    "#                 num_samples=num_samples,\n",
    "#                 image_key=\"brain_mask\",\n",
    "#                 image_threshold=0.0,\n",
    "#                 #allow_smaller = True,\n",
    "#             ),\n",
    "            \n",
    "            RandCropByPosNegLabeld(\n",
    "                keys=keysExt[0:-1],\n",
    "                label_key=\"label_mask\",\n",
    "                spatial_size=patch_size,\n",
    "                pos=pos_sample_num,\n",
    "                neg=neg_sample_num,\n",
    "                num_samples=num_samples,\n",
    "                image_key=\"brain_mask\",\n",
    "                image_threshold=0.,\n",
    "            ),\n",
    "                        \n",
    "            #SpatialCropWTCOMd(keys=keysExt[0:-1], roi_size=patch_size, COM_label_key = \"label_mask\"),\n",
    "            SpatialPadd(keys = keysExt[0:-1], spatial_size = patch_size),\n",
    "        \n",
    "            #CastToTyped(keys=keysExt, dtype=(np.float32, np.uint8, np.uint8, np.uint8, np.float32)),\n",
    "            CastToTyped(keys=keysExt, dtype=(np.float32, np.float32, np.float32, np.float32, np.float32)),\n",
    "            #dtype = (torch.float32, torch.float32, torch.float32, torch.float32, torch.float32)\n",
    "            #ToTensord(keys=keysExt),\n",
    "\n",
    "#             #EnsureTyped(keys=keys, data_type = \"tensor\"),\n",
    "#             #ToDeviced(keys = keys, device = deviceName),\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif task=='validation':\n",
    "    \n",
    "        keys = [\"image\", 'label', 'brain_mask', 'IDH_label']\n",
    "        keysExt = [\"image\", 'label', 'brain_mask', 'label_mask', 'IDH_label']\n",
    "        all_transform = [\n",
    "            \n",
    "            LoadImaged(keys=keys[0:-1], reader = \"NibabelReader\"),\n",
    "            EnsureChannelFirstd(keys=keys[0:-1]),\n",
    "            adapter_tioChannelWise2monai(tiofn = tio.HistogramStandardization, mode = 'train', landmarks = landmarks_dict),\n",
    "            #HistogramNormalizeChannelWised(keys = ['image'], brain_mask_key = 'brain_mask', min = 1, max = 65535),\n",
    "            NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
    "            #ConvertToIDHLabel2WTd(keys = [\"label\"]),\n",
    "           \n",
    "            CopyItemsd(keys=[\"label\"], names=[\"label_mask\"], times=1),\n",
    "            Convert2WTd(keys = [\"label\"]),\n",
    "            ConvertToIDHLabel2WTd(keys = [\"label\"], IDH_label_key = 'IDH_label'),\n",
    "            CropForegroundd(keys=keysExt[0:-1], source_key=\"brain_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord'),\n",
    "            \n",
    "            #Spacingd(keys = keysExt[0:-1], pixdim=(1.25, 1.25, 1.25), mode = ('bilinear','nearest', 'nearest', 'nearest')),\n",
    "            #ResizeWithPadOrCropd(keys = keysExt[0:-1], spatial_size = (128, 160, 128)),\n",
    "            CropForegroundd(keys=keysExt[0:-1], source_key=\"label_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord', margin=2), \n",
    "            #SpatialCropWTCOMd(keys=keysExt[0:-1], roi_size=patch_size, COM_label_key = \"label_mask\"),\n",
    "            SpatialPadd(keys = keysExt[0:-1], spatial_size = patch_size),\n",
    "#             RandCropByPosNegLabeld(\n",
    "#                 keys=keysExt[0:-1],\n",
    "#                 label_key=\"label_mask\",\n",
    "#                 spatial_size=patch_size,\n",
    "#                 pos=pos_sample_num,\n",
    "#                 neg=neg_sample_num,\n",
    "#                 num_samples=num_samples,\n",
    "#                 image_key=\"brain_mask\",\n",
    "#                 image_threshold=0,\n",
    "#             ),\n",
    "        \n",
    "#             RandCropByLabelClassesd(\n",
    "#                 keys=keysExt[0:-1],            \n",
    "#                 label_key = \"label_mask\",\n",
    "#                 spatial_size = patch_size,    \n",
    "#                 ratios= cratio,\n",
    "#                 num_classes=num_classes,              \n",
    "#                 num_samples=num_samples,\n",
    "#                 image_key=\"brain_mask\",\n",
    "#                 image_threshold=0.0,\n",
    "#                 #allow_smaller = True,\n",
    "#             ),\n",
    "\n",
    "            #CastToTyped(keys=keysExt, dtype=(np.float32, np.uint8, np.uint8, np.uint8, np.float32)),\n",
    "            CastToTyped(keys=keysExt, dtype=(np.float32, np.float32, np.float32, np.float32, np.float32)),\n",
    "            #ToTensord(keys=keysExt, dtype=(torch.float32, torch.float32, torch.float32, torch.float32, torch.float32)),\n",
    "            #EnsureTyped(keys=keys, data_type = \"tensor\"),\n",
    "            #ToDeviced(keys = keys, device = deviceName),\n",
    "        ]\n",
    "        \n",
    "    else:\n",
    "        print('print task either train or validation here')\n",
    "\n",
    "\n",
    "    return Compose(all_transform)\n",
    "\n",
    "# def create_cachedir(cache_dir):\n",
    "#     if not os.path.exists(cache_dir):\n",
    "#         os.makedirs(cache_dir)\n",
    "#     return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955b08b",
   "metadata": {},
   "source": [
    "### Section for visual inspection and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db754eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 11)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#patch_size=(128, 160, 128)\n",
    "#patch_size=(64, 80, 64)\n",
    "patch_size=(32, 32, 32)\n",
    "train_transforms = get_task_transforms(patch_size, task='train', pos_sample_num=3, neg_sample_num=1, num_samples=16, cratio = [1, 3])\n",
    "val_transforms = get_task_transforms(patch_size, task='validation', pos_sample_num=1, neg_sample_num=1, num_samples=1, cratio = [1, 3])\n",
    "len(train_transforms), len(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4eaea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6bb4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investi_files = BraTS20SubjectsIDHTrainDCT['fold0']\n",
    "# all_train_dataset = monai.data.Dataset(data=investi_files, transform=train_transforms)\n",
    "# all_train_dataset[0][0]['label_meta_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a62bc769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_patch = all_train_dataset[15][3]\n",
    "# view(image = sub_patch['image'][0], label_image = sub_patch['label_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c2463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "751e5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vpatch = all_train_dataset[0][14]\n",
    "# view(image = vpatch['image'][1], label_image = vpatch['label'][1])\n",
    "\n",
    "\n",
    "#np.unique(all_train_dataset[0]['label'][1], return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be05f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view(image = vpatch['image'][1], label_image = vpatch['label_mask'][0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8129dc9",
   "metadata": {},
   "source": [
    "investi_files = BraTS20SubjectsIDHTrainDCT['fold0']\n",
    "#all_train_dataset = Dataset(data=investi_files, transform=val_transforms)\n",
    "all_train_dataset = monai.data.Dataset(data=investi_files, transform=train_transforms)\n",
    "print(all_train_dataset[0][0]['image'].shape)\n",
    "for kj in all_train_dataset:\n",
    "    #asub = kj\n",
    "    num_samples=15\n",
    "    pindx = random.choice(np.arange(num_samples, dtype = np.uint8))\n",
    "    kj = kj[pindx]\n",
    "    print('IDH_label:', kj['IDH_label'])\n",
    "    #print(kj['label'].shape, kj['label'].unique(return_counts = True))\n",
    "    #print(kj['label_mask'].shape, kj['label_mask'].unique(return_counts = True))\n",
    "    print(kj['label'].shape, np.unique(kj['label'], return_counts = True))\n",
    "    print(kj['label_mask'].shape, np.unique(kj['label_mask'], return_counts = True))\n",
    "    #unque = kj['label_mask'].unique(return_counts = True)[1]\n",
    "    #print('\\n Background ratio:', unque[0]/unque.sum(), ' Tumor(=1): ',  unque[1]/unque.sum())\n",
    "    print('#'*100)\n",
    "#print(asub['image'].shape)\n",
    "# print(asub['image'][0].min(),asub['image'][0].max(), asub['image'][0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e514332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "035c530a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#investifiles = copy.deepcopy(BraTS20SubjectsIDHTrainDCT[\"fold0\"])\n",
    "\n",
    "# for i in range(len(investifiles)):\n",
    "#     investifiles[i]['IDH_label'] = investifiles[i]['IDH_label'].astype(np.float32) \n",
    "# all_train_dataset = monai.data.Dataset(data=investifiles, transform=train_transforms)\n",
    "#all_train_dataset[10][3]['IDH_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "482fae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cfold in tqdm(range(len(BraTS20SubjectsIDHTrainDCT))):\n",
    "#     all_train_dataset = monai.data.Dataset(data=copy.deepcopy(BraTS20SubjectsIDHTrainDCT[f\"fold{cfold}\"]), transform=train_transforms)\n",
    "#     dls = monai.data.DataLoader(all_train_dataset, batch_size=8, shuffle=False, collate_fn=list_data_collate)\n",
    "#     # abatch = next(iter(dls))\n",
    "#     # print(abatch['image'].shape)\n",
    "#     # print(abatch['label'].shape)\n",
    "#     for epoch in range(5):\n",
    "#         for abatch in dls:\n",
    "#             print(abatch['image'].shape)\n",
    "#             print(abatch['label'].shape)\n",
    "#             print(abatch['IDH_label'].shape)\n",
    "#             print(abatch['IDH_label'], '\\n', '###'*10, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416663a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aaf6d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#few_train_dataset = Dataset(data=train_files[0:10], transform=train_transforms)\n",
    "# asub = few_train_dataset[5] \n",
    "# view(image = asub['image'][3].cpu(), label_image = asub['label_mask'][0].cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d6856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a7e5a74",
   "metadata": {},
   "source": [
    "### Few investigation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d175af55",
   "metadata": {},
   "source": [
    "afold_train_dataset = monai.data.Dataset(data=BraTS20SubjectsIDHTrainDCT['fold0'], transform=train_transforms)\n",
    "#train_folds['fold0_IDH_label']\n",
    "i_cv = 2\n",
    "uval, ucnt = np.unique([BraTS20SubjectsIDHTrainDCT[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(BraTS20SubjectsIDHTrainDCT[f'fold{i_cv}']))], return_counts = True)\n",
    "weight = 1./(ucnt/ucnt.min())\n",
    "#weight = np.array([0.55, 0.45])\n",
    "sample_weights = np.array([weight[int(t)] for t in [BraTS20SubjectsIDHTrainDCT[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(BraTS20SubjectsIDHTrainDCT[f'fold{i_cv}']))]])\n",
    "sample_weights = torch.from_numpy(sample_weights)\n",
    "weight, ucnt, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42135b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "666f5fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyInstWLogitLoss(nn.Module):\n",
    "    def __init__(self, is_smooth=False, label_smoothing = 0.1):\n",
    "        super().__init__()\n",
    "        self.is_smooth = is_smooth\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "       \n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        if self.is_smooth == True:\n",
    "            y_true = y_true.float() * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "\n",
    "        y_true=y_true.type_as(y_pred)   ### y_pred, and y_true should be same size and same data type\n",
    "        \n",
    "        \n",
    "        #deviceidx = y_pred.get_device()\n",
    "        #device = torch.device('cpu') if deviceidx == -1 else torch.device(f'cuda:{deviceidx}')\n",
    "        #loss = F.binary_cross_entropy_with_logits(y_pred.to(device), y_true.to(device), pos_weight = weight.to(device))  ##pos_weight = weight \n",
    "        loss = F.binary_cross_entropy_with_logits(y_pred, y_true) \n",
    "        return loss\n",
    "\n",
    "\n",
    "# class DeepDiceCELogitInstLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         #self.volweight = torch.softmax(torch.tensor([0.12, 0.33, 0.55]), dim = 0)\n",
    "#         self.dice = DiceLoss(include_background=False, to_onehot_y=True, softmax=True, squared_pred=True, batch = False)  # reduction = \"none\", batch = True\n",
    "#         #self.smcross_entropy = CrossEntropyInstLoss()  ### was none torch.Tensor([0.66, 0.33, 1]), torch.tensor(self.volweight)\n",
    "\n",
    "#     def forward(self, y_pred, y_true):\n",
    "        \n",
    "#         y_true = y_true.unsqueeze(dim=0).expand(y_pred.shape[1],-1,-1,-1,-1, -1)\n",
    "#         #return sum([0.5 ** i * ((self.dice(p, l)) + self.smcross_entropy(p, l)) \\\n",
    "#         #            for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "#         return sum([0.5 ** i * self.dice(p, l) for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeepDiceCELogitInstLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.volweight = torch.softmax(torch.tensor([0.12, 0.33, 0.55]), dim = 0)\n",
    "        self.dice = DiceLoss(to_onehot_y=False, sigmoid=True, squared_pred=True, batch = True)  # reduction = \"none\", False\n",
    "        self.logitcross_entropy = CrossEntropyInstWLogitLoss()  ### was none torch.Tensor([0.66, 0.33, 1]), torch.tensor(self.volweight)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_true = y_true.unsqueeze(dim=0).expand(y_pred.shape[1],-1,-1,-1,-1, -1)\n",
    "        return sum([0.5 ** i * ((self.dice(p, l)) + self.logitcross_entropy(p, l)) \\\n",
    "                    for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "    \n",
    "loss_function = DeepDiceCELogitInstLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "459876e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6450)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxp = torch.randint(0,4,size=(6,3, 128, 128, 128))\n",
    "#pred = [torch.randn(6,3,8,8,6), torch.randn(6,3,8,8,6), torch.randn(6,3,8,8,6)]\n",
    "pred = torch.stack([torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128)], dim=1)\n",
    "loss_function(pred, xxp.float())\n",
    "#loss_function(pred, xxp.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae98c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6db301b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#afold_train_dataset[200]['IDH_label'], afold_train_dataset[200]['label'].unique(return_counts = True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "414ab7d9",
   "metadata": {},
   "source": [
    "#sampler = WeightedRandomSampler(sample_weights, num_samples= len(samples_weight))\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples= len(sample_weights), replacement=True)\n",
    "\n",
    "afold_train_loader = monai.data.DataLoader(afold_train_dataset, batch_size=32, sampler=sampler)\n",
    "#afold_train_loader = torch.utils.data.DataLoader(afold_train_dataset, batch_size=32, sampler=sampler)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9375ae2a",
   "metadata": {},
   "source": [
    "for ibatch in afold_train_loader:\n",
    "    ibatch_IDH = ibatch['IDH_label']\n",
    "    print(torch.eq(ibatch_IDH, 0).sum(), torch.eq(ibatch_IDH, 1).sum())\n",
    "    print('#'*50)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2d94191",
   "metadata": {},
   "source": [
    "numDataPoints = 1000\n",
    "data_dim = 5\n",
    "bs = 100\n",
    "\n",
    "# Create dummy data with class imbalance 9 to 1\n",
    "data = torch.FloatTensor(numDataPoints, data_dim)\n",
    "target = np.hstack((np.zeros(int(numDataPoints * 0.9), dtype=np.int32),\n",
    "                    np.ones(int(numDataPoints * 0.1), dtype=np.int32)))\n",
    "\n",
    "print('target train 0/1: {}/{}'.format(len(np.where(target == 0)[0]), len(np.where(target == 1)[0])))\n",
    "\n",
    "class_sample_count = np.array(\n",
    "    [len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in target])\n",
    "samples_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab10786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714c06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "063b223a",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6406748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBasicBlock(nn.Module):\n",
    "\n",
    "        \n",
    "    def __init__(self, input_channels, output_channels,\n",
    "         conv_op=nn.Conv3d, conv_kwargs=None,\n",
    "         norm_op=nn.BatchNorm3d, norm_op_kwargs=None,\n",
    "         dropout_op=nn.Dropout3d, dropout_op_kwargs=None,\n",
    "         nonlin=nn.LeakyReLU, nonlin_kwargs=None):\n",
    "        super(DownBasicBlock, self).__init__()\n",
    "        \n",
    "        if nonlin_kwargs is None:\n",
    "            nonlin_kwargs = {'negative_slope': 1e-2, 'inplace': True}\n",
    "        if dropout_op_kwargs is None:\n",
    "            dropout_op_kwargs = {'p': 0.0, 'inplace': True}\n",
    "        if norm_op_kwargs is None:\n",
    "            norm_op_kwargs = {'eps': 1e-5, 'affine': True, 'momentum': 0.1}\n",
    "        if conv_kwargs is None:\n",
    "            conv_kwargs3x3_0 = {'kernel_size': 3, 'stride': 2, 'padding': 1, 'dilation': 1, 'bias': True}\n",
    "            conv_kwargs3x3_1 = {'kernel_size': 3, 'stride': 1, 'padding': 1, 'dilation': 1, 'bias': True}\n",
    "        else:\n",
    "            conv_kwargs3x3_0 = conv_kwargs\n",
    "            conv_kwargs3x3_1 = {'kernel_size': 3, 'stride': 1, 'padding': 1, 'dilation': 1, 'bias': True}\n",
    "            \n",
    "            \n",
    "\n",
    "        self.nonlin = nonlin\n",
    "        self.nonlin_kwargs = nonlin_kwargs\n",
    "\n",
    "        self.dropout_op = dropout_op\n",
    "        self.dropout_op_kwargs = dropout_op_kwargs\n",
    "        \n",
    "        self.conv_op = conv_op\n",
    "        self.conv_kwargs3x3_0 = conv_kwargs3x3_0\n",
    "        self.conv_kwargs3x3_1 = conv_kwargs3x3_1\n",
    "        self.conv_kwargs1x1 = {'kernel_size': 1, 'stride': 1, 'padding': 0, 'dilation': 1, 'bias': True}\n",
    "        \n",
    "        self.norm_op = norm_op\n",
    "        self.norm_op_kwargs = norm_op_kwargs\n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv3x3_0 = self.conv_op(input_channels, output_channels, **self.conv_kwargs3x3_0)\n",
    "        self.instnorm3x3_0 = self.norm_op(output_channels, **self.norm_op_kwargs)\n",
    "        \n",
    "        self.conv3x3_1 = self.conv_op(output_channels, output_channels, **self.conv_kwargs3x3_1)\n",
    "        self.instnorm3x3_1 = self.norm_op(output_channels, **self.norm_op_kwargs)\n",
    "        \n",
    "        #self.conv1x1 = self.conv_op(input_channels, output_channels, **self.conv_kwargs1x1)\n",
    "        #self.instnorm1x1 = self.norm_op(output_channels, **self.norm_op_kwargs)\n",
    "        \n",
    "        if self.dropout_op is not None and self.dropout_op_kwargs['p'] is not None and self.dropout_op_kwargs['p'] > 0:\n",
    "            self.dropout = self.dropout_op(**self.dropout_op_kwargs)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        \n",
    "        self.lrelu = self.nonlin(**self.nonlin_kwargs)\n",
    "  \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv3x3_0(x)\n",
    "        out = self.instnorm3x3_0(out)\n",
    "        out = self.lrelu(out)\n",
    "\n",
    "        out = self.conv3x3_1(out)\n",
    "        out = self.instnorm3x3_1(out)\n",
    "        out = self.lrelu(out)\n",
    "        #print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "221bc5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBasicBlock(nn.Module):\n",
    "\n",
    "        \n",
    "    def __init__(self, input_channels, output_channels,\n",
    "         conv_op=nn.Conv3d, conv_kwargs=None,\n",
    "         norm_op=nn.BatchNorm3d, norm_op_kwargs=None,\n",
    "         dropout_op=nn.Dropout3d, dropout_op_kwargs=None,\n",
    "         nonlin=nn.LeakyReLU, nonlin_kwargs=None):\n",
    "        super(UpBasicBlock, self).__init__()\n",
    "        \n",
    "        if nonlin_kwargs is None:\n",
    "            nonlin_kwargs = {'negative_slope': 1e-2, 'inplace': True}\n",
    "        if dropout_op_kwargs is None:\n",
    "            dropout_op_kwargs = {'p': 0.0, 'inplace': True}\n",
    "        if norm_op_kwargs is None:\n",
    "            norm_op_kwargs = {'eps': 1e-5, 'affine': True, 'momentum': 0.1}\n",
    "        if conv_kwargs is None:\n",
    "            conv_kwargs3x3_0 = {'kernel_size': 3, 'stride': 1, 'padding': 1, 'dilation': 1, 'bias': True}\n",
    "            conv_kwargs3x3_1 = {'kernel_size': 3, 'stride': 1, 'padding': 1, 'dilation': 1, 'bias': True}\n",
    "        else:\n",
    "            conv_kwargs3x3_0 = conv_kwargs\n",
    "            conv_kwargs3x3_1 = {'kernel_size': 3, 'stride': 1, 'padding': 1, 'dilation': 1, 'bias': True}\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        self.nonlin = nonlin\n",
    "        self.nonlin_kwargs = nonlin_kwargs\n",
    "\n",
    "        self.dropout_op = dropout_op\n",
    "        self.dropout_op_kwargs = dropout_op_kwargs\n",
    "        \n",
    "        self.conv_op = conv_op\n",
    "        self.conv_kwargs3x3_0 = conv_kwargs3x3_0\n",
    "        self.conv_kwargs3x3_1 = conv_kwargs3x3_1\n",
    "        self.conv_kwargs1x1 = {'kernel_size': 1, 'stride': 1, 'padding': 0, 'dilation': 1, 'bias': True}\n",
    "        \n",
    "        self.norm_op = norm_op\n",
    "        self.norm_op_kwargs = norm_op_kwargs\n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv3x3_0 = self.conv_op(input_channels, output_channels, **self.conv_kwargs3x3_0)\n",
    "        self.instnorm3x3_0 = self.norm_op(output_channels, **self.norm_op_kwargs)\n",
    "        \n",
    "        self.conv3x3_1 = self.conv_op(output_channels, output_channels, **self.conv_kwargs3x3_1)\n",
    "        self.instnorm3x3_1 = self.norm_op(output_channels, **self.norm_op_kwargs)\n",
    "        \n",
    "        #self.conv1x1 = self.conv_op(input_channels, output_channels, **self.conv_kwargs1x1)\n",
    "        #self.instnorm1x1 = self.norm_op(output_channels, **self.norm_op_kwargs)\n",
    "        \n",
    "        if self.dropout_op is not None and self.dropout_op_kwargs['p'] is not None and self.dropout_op_kwargs['p'] > 0:\n",
    "            self.dropout = self.dropout_op(**self.dropout_op_kwargs)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        \n",
    "        self.lrelu = self.nonlin(**self.nonlin_kwargs)\n",
    "  \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv3x3_0(x)\n",
    "        out = self.instnorm3x3_0(out)\n",
    "        out = self.lrelu(out)\n",
    "\n",
    "        out = self.conv3x3_1(out)\n",
    "        out = self.instnorm3x3_1(out)\n",
    "        out = self.lrelu(out)\n",
    "        #print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b69e1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDropoutNormNonlin(nn.Module):\n",
    "    \"\"\"\n",
    "    fixes a bug in ConvDropoutNormNonlin where lrelu was used regardless of nonlin. Bad.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_channels, output_channels,\n",
    "                 conv_op=nn.Conv3d, conv_kwargs=None,\n",
    "                 norm_op=nn.BatchNorm3d, norm_op_kwargs=None,\n",
    "                 dropout_op=nn.Dropout3d, dropout_op_kwargs=None,\n",
    "                 nonlin=nn.LeakyReLU, nonlin_kwargs=None):\n",
    "        super(ConvDropoutNormNonlin, self).__init__()\n",
    "        if nonlin_kwargs is None:\n",
    "            nonlin_kwargs = {'negative_slope': 1e-2, 'inplace': True}\n",
    "        if dropout_op_kwargs is None:\n",
    "            dropout_op_kwargs = {'p': 0.0, 'inplace': True}\n",
    "        if norm_op_kwargs is None:\n",
    "            norm_op_kwargs = {'eps': 1e-5, 'affine': True, 'momentum': 0.1}\n",
    "        if conv_kwargs is None:\n",
    "            conv_kwargs = {'kernel_size': 3, 'stride': 1, 'padding': 1, 'dilation': 1, 'bias': True}\n",
    "\n",
    "        self.nonlin_kwargs = nonlin_kwargs\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout_op = dropout_op\n",
    "        self.dropout_op_kwargs = dropout_op_kwargs\n",
    "        self.norm_op_kwargs = norm_op_kwargs\n",
    "        self.conv_kwargs = conv_kwargs\n",
    "        self.conv_op = conv_op\n",
    "        self.norm_op = norm_op\n",
    "\n",
    "        self.conv = self.conv_op(input_channels, output_channels, **self.conv_kwargs)\n",
    "        if self.dropout_op is not None and self.dropout_op_kwargs['p'] is not None and self.dropout_op_kwargs[\n",
    "            'p'] > 0:\n",
    "            self.dropout = self.dropout_op(**self.dropout_op_kwargs)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        self.instnorm = self.norm_op(output_channels, **self.norm_op_kwargs)\n",
    "        self.lrelu = self.nonlin(**self.nonlin_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        return self.lrelu(self.instnorm(x))\n",
    "    \n",
    "\n",
    "class ConvDropoutNonlinNorm(ConvDropoutNormNonlin):\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        return self.instnorm(self.lrelu(x))\n",
    "\n",
    "\n",
    "class ConvNonlinSeg(nn.Module):\n",
    "    \"\"\"\n",
    "    fixes a bug in ConvDropoutNormNonlin where lrelu was used regardless of nonlin. Bad.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_channels, output_channels,\n",
    "                 conv_op=nn.Conv3d, conv_kwargs=None,\n",
    "                 nonlin=nn.LeakyReLU, nonlin_kwargs=None):\n",
    "        super(ConvNonlinSeg, self).__init__()\n",
    "        if nonlin_kwargs is None:\n",
    "            nonlin_kwargs = {'negative_slope': 1e-2, 'inplace': True}\n",
    "\n",
    "        if conv_kwargs is None:\n",
    "            conv_kwargs = {'kernel_size': 1, 'stride': 1, 'padding': 0, 'dilation': 1, 'bias': False}\n",
    "\n",
    "        self.nonlin_kwargs = nonlin_kwargs\n",
    "        self.nonlin = nonlin\n",
    "        self.conv_kwargs = conv_kwargs\n",
    "        self.conv_op = conv_op\n",
    "\n",
    "\n",
    "        self.conv = self.conv_op(input_channels, output_channels, **self.conv_kwargs)\n",
    "\n",
    "        #self.lrelu = self.nonlin(**self.nonlin_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        #x = self.conv(self.lrelu(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "faf09235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UEncoder(nn.Module):\n",
    "    def __init__(self, num_input_channels, encodFilters, norm_op=nn.InstanceNorm3d):\n",
    "        super(UEncoder, self).__init__()\n",
    "        \n",
    "        \n",
    "        init_contexts_conv_kwargs = {'kernel_size': 3, 'stride': 1, 'padding': 1, 'dilation': 1, 'bias': True}        \n",
    "                \n",
    "        \"\"\" First block \"\"\"\n",
    "        \n",
    "        self.context0_encod =  DownBasicBlock(num_input_channels, encodFilters[0], conv_kwargs = init_contexts_conv_kwargs, norm_op=norm_op) \n",
    "        \n",
    "        \n",
    "        self.context1_encod =  DownBasicBlock(encodFilters[0], encodFilters[1], norm_op=norm_op) \n",
    "  \n",
    "        \n",
    "        self.context2_encod =  DownBasicBlock(encodFilters[1], encodFilters[2], norm_op=norm_op) \n",
    "             \n",
    "        self.context3_encod =  DownBasicBlock(encodFilters[2], encodFilters[3], norm_op=norm_op) \n",
    "      \n",
    "        self.context4_encod =  DownBasicBlock(encodFilters[3], encodFilters[4], norm_op=norm_op) \n",
    "      \n",
    "        #self.context5_encod =  DownBasicBlock(encodFilters[4], encodFilters[5], norm_op=norm_op)\n",
    "        \n",
    "        #self.context6_encod =  DownBasicBlock(encodFilters[5], encodFilters[6], norm_op=norm_op)\n",
    "        #self.reduced_pool = nn.MaxPool3d(3, stride=2, padding = 1)\n",
    "       \n",
    "                    \n",
    "                                                              \n",
    "    def forward(self, ax):\n",
    "        \n",
    "        ax = self.context0_encod(ax)\n",
    "        axdecod0 = ax\n",
    "        #ax=self.reduced_pool(ax)\n",
    "        \n",
    "    \n",
    "        ax = self.context1_encod(ax)\n",
    "        axdecod1 = ax\n",
    "        #ax=self.reduced_pool(ax)\n",
    "        \n",
    "        ax = self.context2_encod(ax)\n",
    "        axdecod2 = ax\n",
    "        #ax=self.reduced_pool(ax)\n",
    "        \n",
    "        ax = self.context3_encod(ax)\n",
    "        axdecod3 = ax\n",
    "        #ax=self.reduced_pool(ax)\n",
    "        \n",
    "        ax = self.context4_encod(ax)\n",
    "        #axdecod4 = ax\n",
    "        #ax=self.reduced_pool(ax)\n",
    "        \n",
    "        #ax = self.context5_encod(ax)\n",
    "        #axdecod5 = ax\n",
    "        \n",
    "        #ax = self.context6_encod(ax)\n",
    "\n",
    "        return ax, axdecod3, axdecod2, axdecod1, axdecod0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02402260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_block(nn.Module):\n",
    "    \n",
    "    def __init__(self, F_g, F_l, F_int,\n",
    "         conv_op=nn.Conv3d, conv_kwargs=None,\n",
    "         norm_op=nn.InstanceNorm3d, norm_op_kwargs=None,\n",
    "         nonlin=nn.LeakyReLU, nonlin_kwargs=None):\n",
    "        #norm_op=nn.BatchNorm3d\n",
    "        super(Attention_block, self).__init__()\n",
    "        \n",
    "        if nonlin_kwargs is None:\n",
    "            nonlin_kwargs = {'negative_slope': 1e-2, 'inplace': True}\n",
    "        if norm_op_kwargs is None:\n",
    "            norm_op_kwargs = {'eps': 1e-5, 'affine': True, 'momentum': 0.1}\n",
    "        if conv_kwargs is None:\n",
    "            conv_kwargs1x1 = {'kernel_size': 1, 'stride': 1, 'padding': 0, 'dilation': 1, 'bias': True}\n",
    "            \n",
    "        self.W_g = nn.Sequential(\n",
    "            conv_op(F_g, F_int, **conv_kwargs1x1),\n",
    "            norm_op(F_int, **norm_op_kwargs)\n",
    "            )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            conv_op(F_l, F_int, **conv_kwargs1x1),\n",
    "            norm_op(F_int, **norm_op_kwargs)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            conv_op(F_int, 1, **conv_kwargs1x1),\n",
    "            norm_op(1, **norm_op_kwargs),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.lrelu = nonlin(**nonlin_kwargs)\n",
    "        \n",
    "    def forward(self, gA, xA):\n",
    "        gA1 = self.W_g(gA)\n",
    "        xA1 = self.W_x(xA)\n",
    "        psi = self.lrelu(gA1+xA1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return xA*psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16524d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynUOneEncodAttn(nn.Module):\n",
    "    def __init__(self, num_classes=4, num_input_channels=4, base_filters=32, dropout_p=0.0,\n",
    "                 final_nonlin=None, leakiness=1e-2, conv_bias=True, inst_norm_affine=True,\n",
    "                 lrelu_inplace=True, do_ds=True):\n",
    "        super(DynUOneEncodAttn, self).__init__()\n",
    "\n",
    "        self.do_ds = do_ds\n",
    "        self.lrelu_inplace = lrelu_inplace\n",
    "        self.inst_norm_affine = inst_norm_affine\n",
    "        self.conv_bias = conv_bias\n",
    "        self.leakiness = leakiness\n",
    "        self.final_nonlin = final_nonlin\n",
    "        norm_op = nn.BatchNorm3d\n",
    "\n",
    "        \n",
    "        nonsymetry_loc_upTrans_kwargs = {'kernel_size': (2, 3, 2), 'stride': (2, 1, 2), 'padding': 0, 'dilation': 1, 'bias': False}\n",
    "        loc_upTrans_kwargs = {'kernel_size': 2, 'stride': 2, 'padding': 0, 'dilation': 1, 'bias': False}\n",
    "        loc_conv_kwargs = {'kernel_size': 3, 'stride': 1, 'padding': 1, 'dilation': 1, 'bias': True}\n",
    "        loc_seg_conv_kwargs = {'kernel_size': 1, 'stride': 1, 'padding': 0, 'dilation': 1, 'bias': False}\n",
    "        \n",
    "        \n",
    "        \n",
    "        dropout_op_kwargs = {'p': 0.0, 'inplace': True}\n",
    "        \n",
    "        #encodFilters = [base_filters, 32, 64, 128, 160, 160]\n",
    "        encodFilters = [base_filters, 64, 128, 256, 320]\n",
    "        #encodFilters = [base_filters, 128, 256, 384, 512]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.get_skip_encodcontext_mtch4 = UEncoder(num_input_channels = num_input_channels, encodFilters = encodFilters, norm_op=norm_op)\n",
    "#        self.get_skip_encodcontext_mt0 = UEncoder(num_input_channels = num_input_channels-3, encodFilters = encodFilters, norm_op=norm_op)\n",
    "#         self.get_skip_encodcontext_mt1 = UEncoder(num_input_channels = num_input_channels-1, encodFilters = encodFilters)\n",
    "#         self.get_skip_encodcontext_mt2 = UEncoder(num_input_channels = num_input_channels-1, encodFilters = encodFilters)\n",
    "#         self.get_skip_encodcontext_mt3 = UEncoder(num_input_channels = num_input_channels-1, encodFilters = encodFilters)\n",
    "        \n",
    "        ch_mult = 1\n",
    "        #loc_upTrans_kwargs['kernel_size'], loc_upTrans_kwargs['stride'], bias = loc_upTrans_kwargs['bias']\n",
    "        self.upTrans1 =  nn.ConvTranspose3d(encodFilters[-1]*ch_mult, encodFilters[-2]*ch_mult, **loc_upTrans_kwargs)\n",
    "        self.att1 = Attention_block(encodFilters[-2]*ch_mult, encodFilters[-2]*ch_mult, encodFilters[-3]*ch_mult, norm_op=norm_op)\n",
    "        self.uploc1 = UpBasicBlock((encodFilters[-2]+encodFilters[-2])*ch_mult, encodFilters[-2]*ch_mult, norm_op=norm_op)\n",
    "        \n",
    "        self.upTrans2 =  nn.ConvTranspose3d(encodFilters[-2]*ch_mult, encodFilters[-3]*ch_mult, **loc_upTrans_kwargs)\n",
    "        self.att2 = Attention_block(encodFilters[-3]*ch_mult, encodFilters[-3]*ch_mult, encodFilters[-4]*ch_mult, norm_op=norm_op)\n",
    "        self.uploc2 = UpBasicBlock((encodFilters[-3]+encodFilters[-3])*ch_mult, encodFilters[-3]*ch_mult, norm_op=norm_op)\n",
    "        self.loc2_seg = ConvNonlinSeg(encodFilters[-3]*ch_mult, num_classes, conv_kwargs = loc_seg_conv_kwargs)\n",
    "        \n",
    "        self.upTrans3 =  nn.ConvTranspose3d(encodFilters[-3]*ch_mult, encodFilters[-4]*ch_mult, **loc_upTrans_kwargs) \n",
    "        self.att3 = Attention_block(encodFilters[-4]*ch_mult, encodFilters[-4]*ch_mult, encodFilters[-5]*ch_mult, norm_op=norm_op)\n",
    "        self.uploc3 = UpBasicBlock((encodFilters[-4]+encodFilters[-4])*ch_mult, encodFilters[-4]*ch_mult, norm_op=norm_op)\n",
    "        self.loc3_seg = ConvNonlinSeg(encodFilters[-4]*ch_mult, num_classes, conv_kwargs = loc_seg_conv_kwargs)\n",
    "        \n",
    "        \n",
    "        self.upTrans4 =  nn.ConvTranspose3d(encodFilters[-4]*ch_mult, encodFilters[-5]*ch_mult, **loc_upTrans_kwargs)\n",
    "        self.att4 = Attention_block(encodFilters[-5]*ch_mult, encodFilters[-5]*ch_mult, encodFilters[-5]*ch_mult//2, norm_op=norm_op)\n",
    "        self.uploc4 = UpBasicBlock((encodFilters[-5]+encodFilters[-5])*ch_mult, encodFilters[-5]*ch_mult, norm_op=norm_op)\n",
    "        self.loc4_seg = ConvNonlinSeg(encodFilters[-5]*ch_mult, num_classes, conv_kwargs = loc_seg_conv_kwargs)\n",
    "        \n",
    "#         self.upTrans5 =  nn.ConvTranspose3d(encodFilters[-5]*ch_mult, encodFilters[-6]*ch_mult, **loc_upTrans_kwargs)\n",
    "#         self.att5 = Attention_block(encodFilters[-6]*ch_mult, encodFilters[-6]*ch_mult, encodFilters[-6]*ch_mult//2, norm_op=norm_op)\n",
    "#         self.uploc5 = UpBasicBlock((encodFilters[-6]+encodFilters[-6])*ch_mult, encodFilters[-6]*ch_mult, norm_op=norm_op)\n",
    "#         self.loc5_seg = ConvNonlinSeg(encodFilters[-6]*ch_mult, num_classes, conv_kwargs = loc_seg_conv_kwargs)\n",
    "        \n",
    "        \n",
    "#         self.upTrans6 =  nn.ConvTranspose3d(encodFilters[-6]*ch_mult, encodFilters[-7]*ch_mult, **loc_upTrans_kwargs)\n",
    "#         self.att6 = Attention_block(encodFilters[-7]*ch_mult, encodFilters[-7]*ch_mult, (encodFilters[-7]*ch_mult)//2, norm_op=norm_op)\n",
    "#         self.uploc6 = UpBasicBlock((encodFilters[-7]+encodFilters[-7])*ch_mult, encodFilters[-7]*ch_mult, norm_op=norm_op)\n",
    "#         self.loc6_seg = ConvNonlinSeg(encodFilters[-7]*ch_mult, num_classes, conv_kwargs = loc_seg_conv_kwargs)\n",
    "        \n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(torch.as_tensor(m.weight), a=0.01)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(torch.as_tensor(m.weight), 1)\n",
    "                nn.init.constant_(torch.as_tensor(m.bias), 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(torch.as_tensor(m.weight), 1)\n",
    "                nn.init.constant_(torch.as_tensor(m.bias), 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(torch.as_tensor(m.bias), 0)\n",
    "                \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        seg_outputs = []\n",
    "        size = list(x.shape[2:])\n",
    "\n",
    "        \n",
    "        xyzh, xdecod3, xdecod2, xdecod1, xdecod0 = self.get_skip_encodcontext_mtch4(x)\n",
    "\n",
    "        \n",
    "        ##########################\n",
    "        ###Strating of up block ###\n",
    "        \n",
    "        xyzh = self.upTrans1(xyzh)\n",
    "        xdecod3 = self.att1(gA=xyzh, xA= xdecod3)\n",
    "        xyzh = torch.cat([xyzh, xdecod3], dim=1)\n",
    "        xyzh = self.uploc1(xyzh) \n",
    "        \n",
    "        xyzh = self.upTrans2(xyzh)\n",
    "        xdecod2 = self.att2(gA=xyzh, xA= xdecod2)\n",
    "        xyzh = torch.cat([xyzh, xdecod2], dim=1)\n",
    "        xyzh = self.uploc2(xyzh)\n",
    "        seg_outputs.append(F.interpolate(self.loc2_seg(xyzh), size = size))\n",
    "        \n",
    "        xyzh = self.upTrans3(xyzh)\n",
    "        xdecod1 = self.att3(gA=xyzh, xA= xdecod1)\n",
    "        xyzh = torch.cat([xyzh, xdecod1], dim=1)\n",
    "        xyzh = self.uploc3(xyzh)\n",
    "        seg_outputs.append(F.interpolate(self.loc3_seg(xyzh), size = size))\n",
    "        \n",
    "        xyzh = self.upTrans4(xyzh)\n",
    "        xdecod0 = self.att4(gA=xyzh, xA= xdecod0)\n",
    "        xyzh = torch.cat([xyzh,xdecod0], dim=1)\n",
    "        xyzh = self.uploc4(xyzh)\n",
    "        seg_outputs.append(F.interpolate(self.loc4_seg(xyzh), size = size))\n",
    "        \n",
    "        \n",
    "        if self.training:\n",
    "            return torch.stack([seg_outputs[-1], seg_outputs[-2], seg_outputs[-3]], dim=1)\n",
    "        else:\n",
    "            return seg_outputs[-1]\n",
    "        \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ab7d971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "model = DynUOneEncodAttn(num_classes=2, num_input_channels=2, base_filters=32).to(device)\n",
    "inps = torch.randn(2, 2, 32, 32, 32).to(device)\n",
    "with torch.cuda.amp.autocast():\n",
    "    outs = model(inps)\n",
    "print(outs.shape)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7d77769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid/brats2021/pthBraTS2020_IDHGenomics/TwoEncodUNetVariants_TCGA/AttnDynUNet_BratsTCGA_HistStand_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8220_epoch287.pth'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_model_save_dir = os.path.join('/raid/brats2021/pthBraTS2020_IDHGenomics/TwoEncodUNetVariants_TCGA')\n",
    "\n",
    "transfer_mode_DCTList = {'fold0': glob.glob(f'{transfer_model_save_dir}/AttnDynUNet_BratsTCGA_HistStand_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8220_epoch287.pt*'),\\\n",
    "               'fold1':glob.glob(f'{transfer_model_save_dir}/AttnDynUNet_BratsTCGA_HistStand_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8581_epoch296.pth*'),\\\n",
    "               'fold2':glob.glob(f'{transfer_model_save_dir}/AttnDynUNet_BratsTCGA_HistStand_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold2_0.8866_epoch256.pth*')}\n",
    "transfer_modelPath = transfer_mode_DCTList['fold0'][0]\n",
    "transfer_modelPath"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b4ed883",
   "metadata": {},
   "source": [
    "current_model_dict = model.state_dict()\n",
    "loaded_state_dict = torch.load(transfer_mode_DCTList[\"fold0\"][0], map_location=device)\n",
    "new_state_dict={k:v if v.size()==current_model_dict[k].size() else current_model_dict[k] for k,v in zip(current_model_dict.keys(), loaded_state_dict.values())}\n",
    "model.load_state_dict(new_state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443320ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f7a9474",
   "metadata": {},
   "source": [
    "## Custom editing of SegResNetVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5f62e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]] \n",
      " [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]]\n",
      "strides length 5\n",
      "Filters: [64, 96, 128, 192, 256]\n",
      "Strides: [2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "def get_kernels_strides(sizes, spacings):\n",
    "    #sizes, spacings = patch_size[task_id], spacing[task_id]\n",
    "    strides, kernels = [], []\n",
    "\n",
    "    while True:\n",
    "        spacing_ratio = [sp / min(spacings) for sp in spacings]\n",
    "        stride = [\n",
    "            2 if ratio <= 2 and size >= 8 else 1\n",
    "            for (ratio, size) in zip(spacing_ratio, sizes)\n",
    "        ]\n",
    "        kernel = [3 if ratio <= 2 else 1 for ratio in spacing_ratio]\n",
    "        if all(s == 1 for s in stride):\n",
    "            break\n",
    "        sizes = [i / j for i, j in zip(sizes, stride)]\n",
    "        spacings = [i * j for i, j in zip(spacings, stride)]\n",
    "        kernels.append(kernel)\n",
    "        strides.append(stride)\n",
    "    strides.insert(0, len(spacings) * [1])\n",
    "    kernels.append(len(spacings) * [3])\n",
    "    return kernels, strides\n",
    "#task_id = \"01\"\n",
    "kernels, strides = get_kernels_strides(patch_size, spacing)\n",
    "kernels.append([3, 3, 3])\n",
    "strides.append([2, 2, 2])\n",
    "\n",
    "print(kernels,'\\n', strides)\n",
    "\n",
    "print('strides length', len(strides))\n",
    "#filters = [32, 64, 128, 256, 320, 384, 512][: len(strides)]\n",
    "filters = [64, 96, 128, 192, 256, 384, 512, 768, 1024][: len(strides)]\n",
    "print(\"Filters:\", filters)\n",
    "strides = [s[0] for s in strides[1:]]\n",
    "print(\"Strides:\", strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b5facff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernels, strides = get_kernels_strides((128, 128, 128), spacing)\n",
    "# #kernels, strides\n",
    "# kernels = [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 6, 3]]\n",
    "# strides = [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1fc19fa",
   "metadata": {},
   "source": [
    "model = DynUNet(    \n",
    "    spatial_dims=3,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    kernel_size=kernels,\n",
    "    strides=strides,\n",
    "    filters = filters,\n",
    "    upsample_kernel_size=strides[1:],\n",
    "    norm_name=\"batch\",\n",
    "    deep_supervision=True,\n",
    "    res_block=True,\n",
    "    deep_supr_num=2,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a134845",
   "metadata": {},
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3563f56",
   "metadata": {},
   "source": [
    "inps = torch.randn(3, 4, 32, 32, 32).to(device)\n",
    "x = model(inps)\n",
    "# # model\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2f43bbf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "58e63791",
   "metadata": {},
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (4, 32, 32, 32))\n",
    "# # inps = torch.randn(3, 4, 48, 64, 48)\n",
    "# # litConv = nn.Conv3d(4, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "# # litConv(inps).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc9873",
   "metadata": {},
   "source": [
    "### Defining loss functions\n",
    "- ***CrossEntropyLogitLoss*** Cross entropy logit loss from [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)\n",
    "- ***DiceCELoss*** Dice + Cross entropy loss from Monai\n",
    "https://docs.monai.io/en/latest/losses.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e727065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyInstWLogitLoss(nn.Module):\n",
    "    def __init__(self, is_smooth=False, label_smoothing = 0.1):\n",
    "        super().__init__()\n",
    "        self.is_smooth = is_smooth\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "       \n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        if self.is_smooth == True:\n",
    "            y_true = y_true.float() * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "\n",
    "        y_true=y_true.type_as(y_pred)   ### y_pred, and y_true should be same size and same data type\n",
    "        \n",
    "        \n",
    "        #deviceidx = y_pred.get_device()\n",
    "        #device = torch.device('cpu') if deviceidx == -1 else torch.device(f'cuda:{deviceidx}')\n",
    "        #loss = F.binary_cross_entropy_with_logits(y_pred.to(device), y_true.to(device), pos_weight = weight.to(device))  ##pos_weight = weight \n",
    "        loss = F.binary_cross_entropy_with_logits(y_pred, y_true) \n",
    "        return loss\n",
    "\n",
    "\n",
    "# class DeepDiceCELogitInstLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         #self.volweight = torch.softmax(torch.tensor([0.12, 0.33, 0.55]), dim = 0)\n",
    "#         self.dice = DiceLoss(include_background=False, to_onehot_y=True, softmax=True, squared_pred=True, batch = False)  # reduction = \"none\", batch = True\n",
    "#         #self.smcross_entropy = CrossEntropyInstLoss()  ### was none torch.Tensor([0.66, 0.33, 1]), torch.tensor(self.volweight)\n",
    "\n",
    "#     def forward(self, y_pred, y_true):\n",
    "        \n",
    "#         y_true = y_true.unsqueeze(dim=0).expand(y_pred.shape[1],-1,-1,-1,-1, -1)\n",
    "#         #return sum([0.5 ** i * ((self.dice(p, l)) + self.smcross_entropy(p, l)) \\\n",
    "#         #            for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "#         return sum([0.5 ** i * self.dice(p, l) for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeepDiceCELogitInstLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.volweight = torch.softmax(torch.tensor([0.12, 0.33, 0.55]), dim = 0)\n",
    "        self.dice = DiceLoss(to_onehot_y=False, sigmoid=True, squared_pred=True, batch = True)  # reduction = \"none\", False\n",
    "        self.logitcross_entropy = CrossEntropyInstWLogitLoss()  ### was none torch.Tensor([0.66, 0.33, 1]), torch.tensor(self.volweight)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_true = y_true.unsqueeze(dim=0).expand(y_pred.shape[1],-1,-1,-1,-1, -1)\n",
    "        return sum([0.5 ** i * ((self.dice(p, l)) + self.logitcross_entropy(p, l)) \\\n",
    "                    for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "    \n",
    "loss_function = DeepDiceCELogitInstLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b2346f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6449)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxp = torch.randint(0,4,size=(6,3, 128, 128, 128))\n",
    "#pred = [torch.randn(6,3,8,8,6), torch.randn(6,3,8,8,6), torch.randn(6,3,8,8,6)]\n",
    "pred = torch.stack([torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128)], dim=1)\n",
    "loss_function(pred, xxp.float())\n",
    "#loss_function(pred, xxp.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad5c53c",
   "metadata": {},
   "source": [
    "#### A function to create ***cache_dir*** to save transformed outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b65a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAndcreate_cachedir(cache_dir):\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    else:\n",
    "        #print(\"Pass\")\n",
    "        shutil.rmtree(cache_dir)\n",
    "        os.makedirs(cache_dir)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54491d69",
   "metadata": {},
   "source": [
    "### Pytorch training loop\n",
    "\n",
    "Following functionalities are added\n",
    "\n",
    "- Implemeting learning rate finder\n",
    "- Defining Ranger21 optimizer (learning rate scheduler attached to it)\n",
    "- Implementing mixed precision (AMP)\n",
    "- Saving the model weights based on the performance on validation data\n",
    "- Executing 5 fold cross validation (CV) training/validation pipeline, saving a few best model's weights at each fold\n",
    "- Defining train_dataset/train_loader, and val_dataset/val_loader at each fold\n",
    "- Defining a CNN based classification model (DenseNet, EfficientNet, etc.) at each fold to make sure that all accumulated gradients get vanished\n",
    "\n",
    "The key variables which are used here\n",
    "- ***val_cache_dir:*** The path where the transformed ouputs of validaion files will be cached/saved\n",
    "- ***train_cache_dir:*** The path where the transformed ouputs of training files will be cached/saved\n",
    "- ***max_epochs:*** Total number of epochs\n",
    "- ***save_dir:*** The path to save the checkpoints/weights of the model\n",
    "- ***file_prefix:*** The text file where loss/accuracy is recoded like\n",
    "\n",
    "```\n",
    "current fold: 0 current epoch: 1, acc_metric: 0.4579 accuracy: 0.5085, f1score: 0.5085 epoch 1 average training loss: 0.7250 average validation loss: 0.7128 \n",
    "current fold: 0 current epoch: 2, acc_metric: 0.4876 accuracy: 0.5424, f1score: 0.5424 epoch 2 average training loss: 0.6961 average validation loss: 0.6940 \n",
    "current fold: 0 current epoch: 3, acc_metric: 0.4870 accuracy: 0.4915, f1score: 0.4915 epoch 3 average training loss: 0.6862 average validation loss: 0.6965 \n",
    "current fold: 0 current epoch: 4, acc_metric: 0.4882 accuracy: 0.5593, f1score: 0.5593 epoch 4 average training loss: 0.6715 average validation loss: 0.6885 \n",
    "current fold: 0 current epoch: 5, acc_metric: 0.5927 accuracy: 0.5593, f1score: 0.5593 epoch 5 average training loss: 0.6555 average validation loss: 0.6826 \n",
    "```\n",
    "\n",
    "- ***val_interval*** Epoch interval to investivate the model's performance on validation data. If the current performance is better than in previous epochs, the model's weights will be saved\n",
    "- ***key_metric_n_saved*** The number of model checkpoints we want to save. It it is set as 5, top 5 checkpoints/weights will be saved in 5 different ***.pth*** files  \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be097b90",
   "metadata": {},
   "source": [
    "    \n",
    "def getbatch_segclass(x):\n",
    "    xdvc = x.device\n",
    "    x_batchlist = torch.unbind(x, dim = 0)\n",
    "    xbatchclass = []\n",
    "    for xc in x_batchlist:\n",
    "        x_chlist = torch.unbind(xc, dim = 0)\n",
    "        xclassNoList = []\n",
    "        xvalueList = []\n",
    "        for x_i in x_chlist:\n",
    "\n",
    "            xv, xc = torch.unique(x_i, return_counts  = True)\n",
    "\n",
    "            if xc.shape[0]==1:\n",
    "                xclassNoList.append(xc[0].item())\n",
    "                xvalueList.append(xv[0].item())\n",
    "            elif: xc.shape[0]==2:\n",
    "                    if torch.any(torch.eq(xv, 1)):\n",
    "                        xclassNoList.append(xc[1].item())\n",
    "                        xvalueList.append(xv[1].item())\n",
    "                    else:\n",
    "                        print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "            else:\n",
    "                print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "\n",
    "\n",
    "\n",
    "        if torch.any(torch.eq(torch.tensor(xvalueList), 1)):  \n",
    "            xclass = torch.argmax(torch.tensor(xclassNoList))\n",
    "        else:\n",
    "            '''If all uniques class values are 0, we are assigning nan values as a class'''\n",
    "            xclass = torch.tensor(float('NaN'))\n",
    "\n",
    "\n",
    "        xbatchclass.append(xclass.item())\n",
    "\n",
    "\n",
    "    xbatchclass = torch.tensor(xbatchclass)\n",
    "    return torch.mode(xbatchclass.to(xdvc))[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "293d26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#***Executed pipeline***\\\n",
    "#<img src=\"assets/ProposedIDHClass.png\" align=\"left\" width=\"1024\" height=\"1800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38e96ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segclass(x, dim = 1):\n",
    "    xdvc = x.device\n",
    "    x_chlist = torch.unbind(x, dim = dim)\n",
    "    xclassNoList = []\n",
    "    xvalueList = []\n",
    "\n",
    "    for x_i in x_chlist:\n",
    "\n",
    "        xv, xc = torch.unique(x_i, return_counts  = True)\n",
    "\n",
    "        if xc.shape[0]==1:\n",
    "            if xv==0:\n",
    "                xclassNoList.append(-1)\n",
    "                xvalueList.append(xv[0].item())\n",
    "            elif xv==1:\n",
    "                xclassNoList.append(xc[0].item())\n",
    "                xvalueList.append(xv[0].item())\n",
    "            else:\n",
    "                print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "\n",
    "\n",
    "        elif xc.shape[0]==2:\n",
    "                if torch.any(torch.eq(xv, 1)):\n",
    "                    xclassNoList.append(xc[1].item())\n",
    "                    xvalueList.append(xv[1].item())\n",
    "                else:\n",
    "                    print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "        else:\n",
    "            print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "\n",
    "    #pdb.set_trace()\n",
    "    #if torch.any(torch.eq(torch.tensor(xvalueList), 1)):\n",
    "\n",
    "    if xclassNoList[0]!=xclassNoList[1]: \n",
    "        xclass = torch.argmax(torch.tensor(xclassNoList).to(xdvc))\n",
    "    else:\n",
    "        xclass = torch.tensor(float('NaN')).to(xdvc)\n",
    "\n",
    "    #else:\n",
    "        '''If all uniques class values are 0, we are assigning nan values as a class'''\n",
    "    #    xclass = torch.tensor(float('NaN')).to(xdvc)\n",
    "\n",
    "\n",
    "    return xclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d203e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbatch_segclass(x):\n",
    "    xdvc = x.device\n",
    "    x_batchlist = torch.unbind(x, dim = 0)\n",
    "    xbatchclass = []\n",
    "    for xc in x_batchlist:\n",
    "        xcclass = get_segclass(xc, dim = 0)            \n",
    "        xbatchclass.append(xcclass.item())\n",
    "\n",
    "\n",
    "    xbatchclass = torch.tensor(xbatchclass)\n",
    "    if torch.all(torch.isnan(xbatchclass))==True:\n",
    "\n",
    "            return torch.tensor(float('NaN')).to(xdvc)\n",
    "\n",
    "    else:\n",
    "\n",
    "        num_xbatchnanvalues = torch.isnan(xbatchclass).sum().item()\n",
    "        not_xbatchnanmask = torch.logical_not(torch.isnan(xbatchclass))\n",
    "        xbatchclass = xbatchclass[not_xbatchnanmask]\n",
    "\n",
    "        xclassVal_01, xclassCnt_01 =xbatchclass.unique(return_counts = True)\n",
    "\n",
    "        if xclassCnt_01.shape[0]==1:\n",
    "            return xclassVal_01[0].to(xdvc)\n",
    "\n",
    "\n",
    "        if xclassCnt_01.shape[0]==2:\n",
    "            if xclassCnt_01[0]!=xclassCnt_01[1]:\n",
    "                ''' xclassCnt_01 will always be two values converting [7, 8] to 1; [8, 7] to 0'''\n",
    "                return torch.argmax(xclassCnt_01).to(xdvc)  \n",
    "\n",
    "            else:\n",
    "\n",
    "                return torch.tensor(float('NaN')).to(xdvc)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36f6bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binarize_tensor(x, dim=1):\n",
    "    x_chlist = torch.unbind(x, dim = dim)\n",
    "    bin_x = torch.zeros_like(x_chlist[0])\n",
    "    for x_i in x_chlist:\n",
    "        bin_x = torch.logical_or(x_i, bin_x)\n",
    "    return bin_x.unsqueeze(dim=dim).to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb63c8",
   "metadata": {},
   "source": [
    "### Starting training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb34e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_files, train_files_IDH_label, val_files, val_files_IDH_label, batch_size = 2, epochs = 10, find_lr=False, cfold = 0, transfer_modelPath=None):\n",
    "    \n",
    "\n",
    "    #     model = DenseNet201(spatial_dims=2, in_channels=3,\n",
    "    #                        out_channels=num_classes, pretrained=True).to(device)\n",
    "\n",
    "    # create spatial 3D\n",
    "    #model = MultiDenseNet(spatial_dims=3, in_channelsList=(4, 1, 1, 1, 1), out_channels=2, block_config = (6, 12, 24, 16)).to(device)\n",
    "    #model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=4, out_channels=1).to(device)\n",
    "    #model = monai.networks.nets.DenseNet264(spatial_dims=3, in_channels=4, out_channels=1, init_features=64, growth_rate=32, block_config=(6, 12, 64, 48)).to(device)\n",
    "    #patch_size=(64, 80, 64)\n",
    "    \n",
    "    num_classes = 2\n",
    "    model = DynUOneEncodAttn(num_classes=num_classes, num_input_channels=2, base_filters=32).to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    auc_metric = ROCAUCMetric()\n",
    "    \n",
    "\n",
    "    #train_files, train_files_IDH_label, val_files, val_files_IDH_label = train_files[:48], train_files_IDH_label[:48], val_files[:16], val_files_IDH_label[:16]\n",
    "\n",
    "    \"\"\"\n",
    "    Block for using Monai's caching mechanishm for faster training\n",
    "    \"\"\"\n",
    "    \n",
    "    file_prefixfold = file_prefix  ##or file_prefix f\"{file_prefix}_fold{cfold}\" if saving cv file\n",
    "    data_rpath = '/home/mmiv-ml/data'\n",
    "    train_cache_dir = os.path.join(data_rpath,f'cachingDataset/{file_prefixfold}/train')    \n",
    "    val_cache_dir = os.path.join(data_rpath,f'cachingDataset/{file_prefixfold}/val')\n",
    "    \n",
    "    is_done_train = removeAndcreate_cachedir(train_cache_dir)\n",
    "    is_done_val = removeAndcreate_cachedir(val_cache_dir)\n",
    "    \n",
    "\n",
    "    n_train_cache_n_trans = len(train_transforms) #15\n",
    "    n_val_cache_n_trans = len(val_transforms)\n",
    "    \n",
    "     # create a training data loader\n",
    "\n",
    "    train_dataset = monai.data.CacheNTransDataset(data=train_files, transform=train_transforms,\\\n",
    "                                               cache_n_trans = n_train_cache_n_trans, cache_dir = train_cache_dir)\n",
    "    \n",
    "    \n",
    "    \n",
    "#    train_dataset = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "    \n",
    "#     #train_folds['fold0_IDH_label']\n",
    "\n",
    "    train_files_IDH_labels = np.array([id_lbl['IDH_label'].item() for id_lbl in train_files])\n",
    "    uval, ucnt = np.unique(train_files_IDH_labels, return_counts=True)\n",
    "    weight = 1./(ucnt/ucnt.min())\n",
    "    #weight = 1./ucnt\n",
    "    #weight = np.array([0.55, 0.45])\n",
    "    sample_weights = np.array([weight[int(t)] for t in train_files_IDH_labels])\n",
    "    sample_weights = torch.from_numpy(sample_weights)\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples= len(sample_weights), replacement=True)\n",
    "\n",
    "\n",
    "    #train_dataset = Dataset(data=train_files, transform=train_transforms)\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    #train_dataset = CacheDataset(data=train_files, transform=train_transforms, cache_rate = 1.0, num_workers=8)\n",
    "    #train_loader = ThreadDataLoader(train_dataset, num_workers=0, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #shiffle = False, sampler = sampler, shuffle=True doesnot work with patch, num_workers=2\n",
    "    train_loader = monai.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=list_data_collate, sampler = sampler) \n",
    "    \n",
    "\n",
    "    \n",
    "    # create a validation data loader\n",
    "    \n",
    "    #val_dataset = monai.data.Dataset(data=val_files, transform=val_transforms)\n",
    "    #val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    #val_dataset = CacheDataset(data=val_files, transform=val_transforms, cache_rate = 1.0, num_workers=5)\n",
    "    \n",
    "    val_dataset = monai.data.CacheNTransDataset(data=val_files, transform=val_transforms,\\\n",
    "                                            cache_n_trans = n_val_cache_n_trans, cache_dir = val_cache_dir)\n",
    "    #val_loader = ThreadDataLoader(val_dataset, num_workers=0, batch_size=1)\n",
    "    val_loader = monai.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True) ##\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for ibatch in train_loader:\n",
    "#         ibatch_IDH = ibatch['IDH_label']\n",
    "#         print(torch.eq(ibatch_IDH, 0).sum(), torch.eq(ibatch_IDH, 1).sum())\n",
    "#         print('#'*50)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    just initialising some basic steps\n",
    "    \"\"\"\n",
    "    \n",
    "    max_epochs = epochs\n",
    "    find_lr=False\n",
    "    \n",
    "    ### Calling the loss function ***CrossEntropyPlusMSELoss**,and optimizer   \n",
    "    #loss_function = nn.CrossEntropyLoss()\n",
    "    #loss_function=nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=1e-03, weight_decay=1e-07)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-05, weight_decay = 1e-4)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=1e-03, momentum= 0.99, nesterov=True)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), 1e-03, weight_decay=1e-04)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    \n",
    "    max_lr_init = 1e-04\n",
    "    \"\"\"\n",
    "     ###################### Block for LR finder from pytorch ignite ########################\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if find_lr:\n",
    "\n",
    "        def prepare_batch(batch, device=None, non_blocking=False):\n",
    "            return _prepare_batch((batch['image'], batch['IDH_label'].long()), device, non_blocking)\n",
    "\n",
    "        #trainer = create_supervised_trainer(model, optimizer, loss_function, device, non_blocking=False, prepare_batch=prepare_batch)\n",
    "        def train_step(engine, batch):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            x, y = batch['image'].to(device), batch['IDH_label'].to(device)  #non_blocking=True\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_pred = model(x)\n",
    "                loss4lr = loss_function(y_pred, y)\n",
    "                \n",
    "            scaler.scale(loss4lr).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            return loss4lr.item()\n",
    "\n",
    "        trainer = Engine(train_step)\n",
    "\n",
    "        ProgressBar(persist=True).attach(trainer, output_transform=lambda x: {\"batch loss\": x})\n",
    "        lr_finder = FastaiLRFinder()\n",
    "        to_save={'model': model, 'optimizer': optimizer}\n",
    "        num_iter = 100  #2*len(train_loader)\n",
    "        run_epochs = int(np.ceil(num_iter/len(train_loader)))\n",
    "        with lr_finder.attach(trainer, to_save, end_lr=10, num_iter=num_iter, diverge_th=1.5) as trainer_with_lr_finder:    ####diverge_th=1.5\n",
    "\n",
    "            trainer_with_lr_finder.run(train_loader, max_epochs=run_epochs)  #max_epochs=run_epochs or 5\n",
    "\n",
    "        ax = lr_finder.plot()\n",
    "        plt.show()\n",
    "        \n",
    "        max_lr = lr_finder.lr_suggestion() if lr_finder.lr_suggestion()<5e-03 else max_lr_init\n",
    "        #max_lr = lr_finder.lr_suggestion() ##max_lr/10 i guess not needed, ignite does itself\n",
    "        print(f'Suggested learning rate by LR finder for this fold: {lr_finder.lr_suggestion()}')\n",
    "        \n",
    "    else:\n",
    "        max_lr = max_lr_init\n",
    "        \n",
    "    #max_lr_slice = 1e01*max_lr if max_lr<5e-03 else 1e-02\n",
    "    #max_lr_slice = 1e-01*max_lr if max_lr<1e-05 else max_lr\n",
    "    \n",
    "    ''' Transfer learning section '''\n",
    "    if transfer_modelPath is not None:\n",
    "        current_model_dict = model.state_dict()\n",
    "        loaded_state_dict = torch.load(transfer_modelPath, map_location=device)\n",
    "        new_state_dict={k:v if v.size()==current_model_dict[k].size() else current_model_dict[k] for k,v in zip(current_model_dict.keys(), loaded_state_dict.values())}\n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    ### defining learning rate scheduler\n",
    "    \n",
    "    \"\"\"\n",
    "    #steps_per_epoch=len(train_loader)\n",
    "    #optimizer.param_groups[0]['lr'] = max_lr #*1e-01\n",
    "    #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr_slice, steps_per_epoch=len(train_loader), epochs=max_epochs)\n",
    "    #scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: (1 - epoch / max_epochs) ** 0.9)\n",
    "    \n",
    "    #max_lr = 1e-3   \n",
    "    optimizer = Ranger21(model.parameters(), lr = max_lr, num_epochs = epochs, num_batches_per_epoch = len(train_loader))\n",
    "    \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "     ###################### Block for native pytorch training loop and  ########################\n",
    "    \"\"\"\n",
    "\n",
    "    key_metric_n_saved = 2   ### Usually I keep it 5\n",
    "    save_last = False \n",
    "    dispformat_specs = '.4f'\n",
    "\n",
    "        \n",
    "#     file_prefix = 'ConvEffNet_Brats21_5CV'\n",
    "#     savedirname = 'ConvEffNet_Brats21'\n",
    "#     save_dir = os.path.join('/raid/brats2021/pthBraTS2021Radiogenomics', savedirname)\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "\n",
    "    logsfile_path = f\"{save_dir}/Logs_{file_prefix}.txt\"\n",
    "\n",
    "\n",
    "    epoch_num = max_epochs #  max_epochs\n",
    "    val_interval = 1\n",
    "    valstep = 0\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = list()\n",
    "    metric_values = list()\n",
    "\n",
    "\n",
    "    numsiters = len(train_files) // train_loader.batch_size\n",
    "\n",
    "    first_batch = monai.utils.misc.first(train_loader)\n",
    "        \n",
    "    \n",
    "    #post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)  ### num_classes=num_classes\n",
    "    #post_label = AsDiscrete(to_onehot=num_classes) ###num_classes=num_classes\n",
    "    #dice_metric = monai.metrics.DiceMetric(include_background=False, reduction='mean', get_not_nans=False)\n",
    "    \n",
    "    \n",
    "    dice_metric = monai.metrics.DiceMetric(include_background=True, reduction='mean', get_not_nans=False)\n",
    "    dice_metric_batch = monai.metrics.DiceMetric(include_background=True, reduction='mean_batch', get_not_nans=False)\n",
    "    post_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])  \n",
    "    \n",
    "    def one_hot_permute(x):\n",
    "        return F.one_hot(x.squeeze(dim=0).long(), num_classes=num_classes).permute(3, 0, 1, 2)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{epoch_num}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.\n",
    "        stepiter = 0\n",
    "        for batch_data in train_loader:\n",
    "            \n",
    "            stepiter += 1\n",
    "            inputs, labels, IDH_labels= (\n",
    "                batch_data['image'].to(device),\n",
    "                batch_data['label'].to(device),\n",
    "                batch_data['IDH_label'].to(device),\n",
    "            )\n",
    "            \n",
    "          \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "\n",
    "                # compute output\n",
    "                outputs  = model(inputs)\n",
    "                loss = loss_function(outputs, labels) \n",
    "\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"{stepiter}/{numsiters}, train_loss: {loss.item():.4f}\")\n",
    "\n",
    "            #scheduler.step() \n",
    "            \n",
    "        epoch_loss /= stepiter\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        \n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "                y = torch.tensor([], dtype=torch.long, device=device)\n",
    "                val_losses = torch.tensor([], dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "                for val_data in val_loader:\n",
    "\n",
    "                    val_inputs, val_labels, val_IDH_labels = (\n",
    "                        val_data['image'].to(device),\n",
    "                        val_data['label'].to(device),\n",
    "                        val_data['IDH_label'].to(device),\n",
    "                    )\n",
    "                \n",
    "                    roi_size = patch_size #(32, 32, 32)\n",
    "                    sw_batch_size = 8\n",
    "                    val_overlap = 0.5\n",
    "                    mode=\"gaussian\"\n",
    "                            \n",
    "                    \n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        \n",
    "                        val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device) \n",
    "                        #val_outputs = model(val_inputs)\n",
    "                        val_ce_loss = loss_function(val_outputs.unsqueeze(dim=1), val_labels)\n",
    "\n",
    "                    val_losses = torch.cat([val_losses, val_ce_loss.view(1)], dim = 0)\n",
    "                    val_outputs = torch.stack([post_pred(i) for i in torch.unbind(val_outputs, dim = 0)], dim = 0)\n",
    "                    \n",
    "                    \n",
    "                    #val_labels2hot = torch.stack([one_hot_permute(i) for i in torch.unbind(val_labels, dim = 0)], dim = 0)\n",
    "\n",
    "                    \n",
    "                    val_labels_bin = get_binarize_tensor(val_labels, dim=1)\n",
    "                    val_outputs_bin = get_binarize_tensor(val_outputs, dim=1)\n",
    "                    \n",
    "                    dice_metric(y_pred=val_outputs_bin, y=val_labels_bin)\n",
    "                    \n",
    "                    \n",
    "                    klcc = KeepLargestConnectedComponent(applied_labels = [0, 1])  ##is_onehot=True\n",
    "                    #val_labels = klcc(val_labels.squeeze(dim=0)).unsqueeze(dim=0)\n",
    "                    #val_outputs = klcc(val_outputs.squeeze(dim=0)).unsqueeze(dim=0)\n",
    "                    val_outputs = torch.stack([klcc(i) for i in torch.unbind(val_outputs, dim = 0)], dim = 0)\n",
    "                \n",
    "                    val_label4mSeg_C = get_segclass(val_outputs, dim = 1)\n",
    "                    #val_label4mSeg_C = getbatch_segclass(val_outputs)\n",
    "                    #val_surv_labels = val_surv_labels.squeeze(dim=1)  ###Squeezing from B, 1 to B if needed\n",
    "                    y_pred = torch.cat([y_pred, val_label4mSeg_C.view(1)], dim=0)\n",
    "                \n",
    "                    #pdb.set_trace()    \n",
    "                    val_IDH_labels = torch.mode(val_IDH_labels)[0].view(1)           \n",
    "                    y = torch.cat([y, val_IDH_labels], dim=0)\n",
    "\n",
    "                mdice_value = dice_metric.aggregate()\n",
    "                dice_metric.reset()\n",
    "                \n",
    "                \n",
    "                \n",
    "                y_pred, y = y_pred.cpu(), y.cpu()\n",
    "                \n",
    "                if torch.all(torch.isnan(y_pred))==True:\n",
    "                    \n",
    "                    auc_result, accscore, f1score = np.nan, np.nan, np.nan\n",
    "                    #print('acc_metric#', np.nan, ', auc#', np.nan, ', f1#', np.nan, '\\n')\n",
    "                \n",
    "                else:\n",
    "\n",
    "                    num_nanvalues = torch.isnan(y_pred).sum().item()\n",
    "                    not_nanmask = torch.logical_not(torch.isnan(y_pred))\n",
    "                    y = y[not_nanmask]\n",
    "                    y_pred = y_pred[not_nanmask]\n",
    "                    \n",
    "                    \n",
    "                    acc_value = torch.eq(y_pred, y)\n",
    "                    acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "\n",
    "                    '''auc metric'''\n",
    "                    auc_metric(y_pred, y)\n",
    "                    auc_result = auc_metric.aggregate()\n",
    "                    auc_metric.reset()\n",
    "                    \n",
    "                    '''balanced accuracy and f1 score'''\n",
    "                    accscore = balanced_accuracy_score(y, y_pred)\n",
    "                    f1score = f1_score(y, y_pred, average='micro')\n",
    "                    #print('acc_metric#', acc_metric, ', auc#', auc_result, ', f1#', f1score, '\\n')\n",
    "                \n",
    "\n",
    "                del y, y_pred\n",
    "                \n",
    "            \n",
    "                epoch_val_losses=torch.mean(val_losses).detach().cpu().item()\n",
    "                #metric = auc_result\n",
    "                mdice_value = mdice_value.item()\n",
    "                #metric = mdice_value\n",
    "                metric = (mdice_value+auc_result)/2\n",
    "                metric= -1.0 if np.isnan(metric) else metric\n",
    "                metric_values.append(metric) ######List of over number of epochs\n",
    "                printstring = \"Best PMetric\"\n",
    "                \n",
    "\n",
    "                with open(logsfile_path, 'a') as file:\n",
    "                    file.write(\n",
    "                        f\"current fold: {cfold} current epoch: {epoch + 1} dice_score: {mdice_value:^{dispformat_specs}} acc_metric: {auc_result:^{dispformat_specs}}\" \n",
    "                        f\" accuracy: {accscore:^{dispformat_specs}}, f1score: {f1score:^{dispformat_specs}}\"\n",
    "                        f\" epoch {epoch + 1} average training loss: {epoch_loss:^{dispformat_specs}} average validation loss: {epoch_val_losses:^{dispformat_specs}} \\n\"\n",
    "\n",
    "                    )\n",
    "\n",
    "                if valstep < key_metric_n_saved:\n",
    "                    \n",
    "                    torch.save(model.state_dict(), os.path.join(save_dir, f\"{file_prefix}_Fold{cfold}_{metric:^{dispformat_specs}}_epoch{epoch + 1}.pth\"))\n",
    "                    print(\n",
    "                        f\"current fold: {cfold} current epoch: {epoch + 1} dice_score: {mdice_value:^{dispformat_specs}} acc_metric: {auc_result:^{dispformat_specs}}\" \n",
    "                        f\" accuracy: {accscore:^{dispformat_specs}}, f1score: {f1score:^{dispformat_specs}}\"\n",
    "                        f\" epoch {epoch + 1} average training loss: {epoch_loss:^{dispformat_specs}} average validation loss: {epoch_val_losses:^{dispformat_specs}}\"\n",
    "                        \n",
    "                    )\n",
    "\n",
    "                else:\n",
    "\n",
    "                    #sortmetric_values = sorted(metric_values[:-1], reverse=True)  ###Higher loss needs to be deleted, so sorting is reversed\n",
    "                    sortmetric_values = sorted(metric_values[:-1], reverse=False)  \n",
    "\n",
    "                    if metric>=sortmetric_values[-key_metric_n_saved]:\n",
    "                        savegood_metric = metric\n",
    "                        good_metric_epoch = epoch + 1\n",
    "\n",
    "                        #if os.path.exists(f\"{save_dir}/{file_prefix}_{sortmetric_values[-key_metric_n_saved]:.4f}.pth\"):\n",
    "                        #    os.remove(f\"{save_dir}/{file_prefix}_{sortmetric_values[-key_metric_n_saved]:.4f}.pth\")\n",
    "                        #else:\n",
    "                        #    print(\"The file does not exist\")\n",
    "                        \n",
    "                        \n",
    "                        glblist = glob.glob(f\"{save_dir}/{file_prefix}_Fold{cfold}_{sortmetric_values[-key_metric_n_saved]:^{dispformat_specs}}_*\")\n",
    "\n",
    "                        if not glblist:\n",
    "                            print(\"The file does not exist\")\n",
    "                        else:\n",
    "                            os.remove(glblist[0])\n",
    "\n",
    "\n",
    "                        torch.save(model.state_dict(), os.path.join(save_dir, f\"{file_prefix}_Fold{cfold}_{metric:^{dispformat_specs}}_epoch{epoch + 1}.pth\"))\n",
    "                        print(\"saved new best metric model\")\n",
    "                        print(\n",
    "                            f\"current fold: {cfold} current epoch: {epoch + 1} validation loss: {epoch_val_losses:^{dispformat_specs}}\"\n",
    "                            f\" dice_score: {mdice_value:^{dispformat_specs}} acc_metric: {auc_result:^{dispformat_specs}}\"\n",
    "                            f\" accuracy: {accscore:^{dispformat_specs}}, f1score: {f1score:^{dispformat_specs}}\"\n",
    "                            f\"\\n saved {printstring}: {savegood_metric:^{dispformat_specs}} at epoch: {good_metric_epoch}\"\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        f\"current fold: {cfold} current epoch: {epoch + 1} validation loss: {epoch_val_losses:^{dispformat_specs}}\"\n",
    "                        f\" dice_score: {mdice_value:^{dispformat_specs}} acc_metric: {auc_result:^{dispformat_specs}}\"\n",
    "                        f\" accuracy: {accscore:^{dispformat_specs}}, f1score: {f1score:^{dispformat_specs}}\"\n",
    "\n",
    "                        #pass\n",
    "\n",
    "                valstep += 1\n",
    "        ####Saving last epoch\n",
    "        if epoch==epoch_num-1:\n",
    "            if save_last:\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, f\"{file_prefix}_Fold{cfold}_{metric:^{dispformat_specs}}_last_epoch{epoch + 1}.pth\"))\n",
    "                \n",
    "            #break\n",
    "            \n",
    "    # Free up GPU memory after training\n",
    "    model = None\n",
    "    train_loader, val_loader = None, None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01cde9",
   "metadata": {},
   "source": [
    "### Loop to execute n_splits=3 fold cross validation\n",
    "if the model is trained and the checkpoints are saved already, just setting the start_training flag as false, to run remaining part of the programs of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "18e0e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f197444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 Bacth Investigation, minimum batch size 2\n",
      "Ranger21 optimizer ready with following settings:\n",
      "\n",
      "Core optimizer = AdamW\n",
      "Learning rate of 0.0001\n",
      "\n",
      "Important - num_epochs of training = ** 500 epochs **\n",
      "please confirm this is correct or warmup and warmdown will be off\n",
      "\n",
      "Warm-up: linear warmup, over 2000 iterations\n",
      "\n",
      "Lookahead active, merging every 5 steps, with blend factor of 0.5\n",
      "Norm Loss active, factor = 0.0001\n",
      "Stable weight decay of 0.0001\n",
      "Gradient Centralization = On\n",
      "\n",
      "Adaptive Gradient Clipping = True\n",
      "\tclipping value of 0.01\n",
      "\tsteps for clipping = 0.001\n",
      "\n",
      "Warm-down: Linear warmdown, starting at 72.0%, iteration 5760 of 8000\n",
      "warm down will decay until 3e-05 lr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/500\n",
      "params size saved\n",
      "total param groups = 1\n",
      "total params in groups = 127\n",
      "1/15, train_loss: 2.3467\n",
      "2/15, train_loss: 2.3148\n",
      "3/15, train_loss: 2.3971\n",
      "4/15, train_loss: 2.4228\n",
      "5/15, train_loss: 2.3888\n",
      "6/15, train_loss: 2.3938\n",
      "7/15, train_loss: 2.3265\n",
      "8/15, train_loss: 2.5200\n",
      "9/15, train_loss: 2.4417\n",
      "10/15, train_loss: 2.3919\n",
      "11/15, train_loss: 2.3344\n",
      "12/15, train_loss: 2.3825\n",
      "13/15, train_loss: 2.3998\n",
      "14/15, train_loss: 2.3051\n",
      "15/15, train_loss: 2.4035\n",
      "16/15, train_loss: 2.6343\n",
      "epoch 1 average loss: 2.4002\n",
      "current fold: 0 current epoch: 1 dice_score: 0.2498 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452 epoch 1 average training loss: 2.4002 average validation loss: 1.4906\n",
      "----------\n",
      "epoch 2/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/15, train_loss: 2.4009\n",
      "2/15, train_loss: 2.3650\n",
      "3/15, train_loss: 2.4075\n",
      "4/15, train_loss: 2.4076\n",
      "5/15, train_loss: 2.3416\n",
      "6/15, train_loss: 2.4060\n",
      "7/15, train_loss: 2.3838\n",
      "8/15, train_loss: 2.3662\n",
      "9/15, train_loss: 2.3141\n",
      "10/15, train_loss: 2.3788\n",
      "11/15, train_loss: 2.4303\n",
      "12/15, train_loss: 2.4165\n",
      "13/15, train_loss: 2.4256\n",
      "14/15, train_loss: 2.3809\n",
      "15/15, train_loss: 2.3708\n",
      "16/15, train_loss: 2.6479\n",
      "epoch 2 average loss: 2.4027\n",
      "current fold: 0 current epoch: 2 dice_score: 0.2622 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452 epoch 2 average training loss: 2.4027 average validation loss: 1.5187\n",
      "----------\n",
      "epoch 3/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/15, train_loss: 2.3660\n",
      "2/15, train_loss: 2.3183\n",
      "3/15, train_loss: 2.3532\n",
      "4/15, train_loss: 2.5381\n",
      "5/15, train_loss: 2.5044\n",
      "6/15, train_loss: 2.3710\n",
      "7/15, train_loss: 2.3839\n",
      "8/15, train_loss: 2.3963\n",
      "9/15, train_loss: 2.3859\n",
      "10/15, train_loss: 2.3144\n",
      "11/15, train_loss: 2.3940\n",
      "12/15, train_loss: 2.5596\n",
      "13/15, train_loss: 2.3898\n",
      "14/15, train_loss: 2.4273\n",
      "15/15, train_loss: 2.5166\n",
      "16/15, train_loss: 2.5658\n",
      "epoch 3 average loss: 2.4240\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 3 validation loss: 1.5304 dice_score: 0.2644 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.3822 at epoch: 3\n",
      "----------\n",
      "epoch 4/500\n",
      "1/15, train_loss: 2.3172\n",
      "2/15, train_loss: 2.3909\n",
      "3/15, train_loss: 2.3041\n",
      "4/15, train_loss: 2.3464\n",
      "5/15, train_loss: 2.3386\n",
      "6/15, train_loss: 2.3496\n",
      "7/15, train_loss: 2.3899\n",
      "8/15, train_loss: 2.3459\n",
      "9/15, train_loss: 2.4054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/15, train_loss: 2.3429\n",
      "11/15, train_loss: 2.4045\n",
      "12/15, train_loss: 2.3420\n",
      "13/15, train_loss: 2.3930\n",
      "14/15, train_loss: 2.3444\n",
      "15/15, train_loss: 2.2762\n",
      "16/15, train_loss: 2.2794\n",
      "epoch 4 average loss: 2.3482\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 4 validation loss: 1.5246 dice_score: 0.2644 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.3822 at epoch: 4\n",
      "----------\n",
      "epoch 5/500\n",
      "1/15, train_loss: 2.3607\n",
      "2/15, train_loss: 2.3692\n",
      "3/15, train_loss: 2.6039\n",
      "4/15, train_loss: 2.3434\n",
      "5/15, train_loss: 2.3870\n",
      "6/15, train_loss: 2.3207\n",
      "7/15, train_loss: 2.3608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/15, train_loss: 2.2324\n",
      "9/15, train_loss: 2.2608\n",
      "10/15, train_loss: 2.3381\n",
      "11/15, train_loss: 2.3655\n",
      "12/15, train_loss: 2.3367\n",
      "13/15, train_loss: 2.2700\n",
      "14/15, train_loss: 2.3739\n",
      "15/15, train_loss: 2.2920\n",
      "16/15, train_loss: 2.6878\n",
      "epoch 5 average loss: 2.3689\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 5 validation loss: 1.5235 dice_score: 0.2724 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.3862 at epoch: 5\n",
      "----------\n",
      "epoch 6/500\n",
      "1/15, train_loss: 2.3210\n",
      "2/15, train_loss: 2.2620\n",
      "3/15, train_loss: 2.3050\n",
      "4/15, train_loss: 2.2990\n",
      "5/15, train_loss: 2.2764\n",
      "6/15, train_loss: 2.3684\n",
      "7/15, train_loss: 2.3971\n",
      "8/15, train_loss: 2.3639\n",
      "9/15, train_loss: 2.3812\n",
      "10/15, train_loss: 2.3420\n",
      "11/15, train_loss: 2.3372\n",
      "12/15, train_loss: 2.2948\n",
      "13/15, train_loss: 2.2361\n",
      "14/15, train_loss: 2.3263\n",
      "15/15, train_loss: 2.3407\n",
      "16/15, train_loss: 2.4961\n",
      "epoch 6 average loss: 2.3342\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 6 validation loss: 1.5077 dice_score: 0.2794 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.3897 at epoch: 6\n",
      "----------\n",
      "epoch 7/500\n",
      "1/15, train_loss: 2.2932\n",
      "2/15, train_loss: 2.3181\n",
      "3/15, train_loss: 2.3649\n",
      "4/15, train_loss: 2.2681\n",
      "5/15, train_loss: 2.2528\n",
      "6/15, train_loss: 2.2749\n",
      "7/15, train_loss: 2.2807\n",
      "8/15, train_loss: 2.3050\n",
      "9/15, train_loss: 2.2753\n",
      "10/15, train_loss: 2.3594\n",
      "11/15, train_loss: 2.2369\n",
      "12/15, train_loss: 2.4045\n",
      "13/15, train_loss: 2.2505\n",
      "14/15, train_loss: 2.4686\n",
      "15/15, train_loss: 2.2920\n",
      "16/15, train_loss: 2.1485\n",
      "epoch 7 average loss: 2.2996\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 7 validation loss: 1.4989 dice_score: 0.2848 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.3924 at epoch: 7\n",
      "----------\n",
      "epoch 8/500\n",
      "1/15, train_loss: 2.2561\n",
      "2/15, train_loss: 2.2563\n",
      "3/15, train_loss: 2.3709\n",
      "4/15, train_loss: 2.4359\n",
      "5/15, train_loss: 2.2658\n",
      "6/15, train_loss: 2.3792\n",
      "7/15, train_loss: 2.3201\n",
      "8/15, train_loss: 2.1663\n",
      "9/15, train_loss: 2.3180\n",
      "10/15, train_loss: 2.2941\n",
      "11/15, train_loss: 2.3191\n",
      "12/15, train_loss: 2.2979\n",
      "13/15, train_loss: 2.2843\n",
      "14/15, train_loss: 2.2950\n",
      "15/15, train_loss: 2.2774\n",
      "16/15, train_loss: 2.3310\n",
      "epoch 8 average loss: 2.3042\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 8 validation loss: 1.4830 dice_score: 0.2969 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.3985 at epoch: 8\n",
      "----------\n",
      "epoch 9/500\n",
      "1/15, train_loss: 2.3378\n",
      "2/15, train_loss: 2.3258\n",
      "3/15, train_loss: 2.2995\n",
      "4/15, train_loss: 2.1970\n",
      "5/15, train_loss: 2.2754\n",
      "6/15, train_loss: 2.2139\n",
      "7/15, train_loss: 2.2920\n",
      "8/15, train_loss: 2.2670\n",
      "9/15, train_loss: 2.2696\n",
      "10/15, train_loss: 2.2986\n",
      "11/15, train_loss: 2.2697\n",
      "12/15, train_loss: 2.2656\n",
      "13/15, train_loss: 2.3175\n",
      "14/15, train_loss: 2.2104\n",
      "15/15, train_loss: 2.3100\n",
      "16/15, train_loss: 2.5311\n",
      "epoch 9 average loss: 2.2926\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 9 validation loss: 1.4777 dice_score: 0.3054 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.4027 at epoch: 9\n",
      "----------\n",
      "epoch 10/500\n",
      "1/15, train_loss: 2.3974\n",
      "2/15, train_loss: 2.3264\n",
      "3/15, train_loss: 2.1689\n",
      "4/15, train_loss: 2.2950\n",
      "5/15, train_loss: 2.2838\n",
      "6/15, train_loss: 2.1581\n",
      "7/15, train_loss: 2.2805\n",
      "8/15, train_loss: 2.4281\n",
      "9/15, train_loss: 2.4709\n",
      "10/15, train_loss: 2.2707\n",
      "11/15, train_loss: 2.2280\n",
      "12/15, train_loss: 2.1642\n",
      "13/15, train_loss: 2.2789\n",
      "14/15, train_loss: 2.2482\n",
      "15/15, train_loss: 2.2150\n",
      "16/15, train_loss: 2.1140\n",
      "epoch 10 average loss: 2.2705\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 10 validation loss: 1.4618 dice_score: 0.3149 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.4074 at epoch: 10\n",
      "----------\n",
      "epoch 11/500\n",
      "1/15, train_loss: 2.2069\n",
      "2/15, train_loss: 2.2615\n",
      "3/15, train_loss: 2.2018\n",
      "4/15, train_loss: 2.1421\n",
      "5/15, train_loss: 2.1650\n",
      "6/15, train_loss: 2.2432\n",
      "7/15, train_loss: 2.2001\n",
      "8/15, train_loss: 2.1945\n",
      "9/15, train_loss: 2.2545\n",
      "10/15, train_loss: 2.1074\n",
      "11/15, train_loss: 2.1389\n",
      "12/15, train_loss: 2.2071\n",
      "13/15, train_loss: 2.1789\n",
      "14/15, train_loss: 2.1728\n",
      "15/15, train_loss: 2.3909\n",
      "16/15, train_loss: 2.5529\n",
      "epoch 11 average loss: 2.2262\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 11 validation loss: 1.4466 dice_score: 0.3289 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.4145 at epoch: 11\n",
      "----------\n",
      "epoch 12/500\n",
      "1/15, train_loss: 2.3460\n",
      "2/15, train_loss: 2.1910\n",
      "3/15, train_loss: 2.2225\n",
      "4/15, train_loss: 2.2004\n",
      "5/15, train_loss: 2.1588\n",
      "6/15, train_loss: 2.2714\n",
      "7/15, train_loss: 2.2833\n",
      "8/15, train_loss: 2.1862\n",
      "9/15, train_loss: 2.1561\n",
      "10/15, train_loss: 2.2006\n",
      "11/15, train_loss: 2.1040\n",
      "12/15, train_loss: 2.2167\n",
      "13/15, train_loss: 2.1553\n",
      "14/15, train_loss: 2.0696\n",
      "15/15, train_loss: 2.2224\n",
      "16/15, train_loss: 2.5743\n",
      "epoch 12 average loss: 2.2224\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 12 validation loss: 1.4330 dice_score: 0.3481 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.4240 at epoch: 12\n",
      "----------\n",
      "epoch 13/500\n",
      "1/15, train_loss: 2.2254\n",
      "2/15, train_loss: 2.1287\n",
      "3/15, train_loss: 2.3779\n",
      "4/15, train_loss: 2.2488\n",
      "5/15, train_loss: 2.1970\n",
      "6/15, train_loss: 2.1030\n",
      "7/15, train_loss: 2.1288\n",
      "8/15, train_loss: 2.0776\n",
      "9/15, train_loss: 2.1139\n",
      "10/15, train_loss: 2.2268\n",
      "11/15, train_loss: 2.1836\n",
      "12/15, train_loss: 2.1695\n",
      "13/15, train_loss: 2.0832\n",
      "14/15, train_loss: 2.2996\n",
      "15/15, train_loss: 2.1782\n",
      "16/15, train_loss: 2.3070\n",
      "epoch 13 average loss: 2.1906\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 13 validation loss: 1.4117 dice_score: 0.3675 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.4338 at epoch: 13\n",
      "----------\n",
      "epoch 14/500\n",
      "1/15, train_loss: 2.1642\n",
      "2/15, train_loss: 2.1384\n",
      "3/15, train_loss: 2.0968\n",
      "4/15, train_loss: 2.1803\n",
      "5/15, train_loss: 2.1756\n",
      "6/15, train_loss: 2.0035\n",
      "7/15, train_loss: 2.1535\n",
      "8/15, train_loss: 2.1215\n",
      "9/15, train_loss: 2.1486\n",
      "10/15, train_loss: 2.0723\n",
      "11/15, train_loss: 2.1904\n",
      "12/15, train_loss: 2.0259\n",
      "13/15, train_loss: 2.3267\n",
      "14/15, train_loss: 2.2679\n",
      "15/15, train_loss: 2.0516\n",
      "16/15, train_loss: 2.5237\n",
      "epoch 14 average loss: 2.1651\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 14 validation loss: 1.3968 dice_score: 0.3856 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.4428 at epoch: 14\n",
      "----------\n",
      "epoch 15/500\n",
      "1/15, train_loss: 2.0984\n",
      "2/15, train_loss: 2.0928\n",
      "3/15, train_loss: 2.0841\n",
      "4/15, train_loss: 2.0530\n",
      "5/15, train_loss: 2.0115\n",
      "6/15, train_loss: 2.1409\n",
      "7/15, train_loss: 2.1616\n",
      "8/15, train_loss: 2.0318\n",
      "9/15, train_loss: 2.0785\n",
      "10/15, train_loss: 2.2289\n",
      "11/15, train_loss: 2.0660\n",
      "12/15, train_loss: 2.1204\n",
      "13/15, train_loss: 2.1186\n",
      "14/15, train_loss: 2.0296\n",
      "15/15, train_loss: 2.0055\n",
      "16/15, train_loss: 2.1807\n",
      "epoch 15 average loss: 2.0939\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 15 validation loss: 1.3752 dice_score: 0.4080 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.4540 at epoch: 15\n",
      "----------\n",
      "epoch 16/500\n",
      "1/15, train_loss: 2.1181\n",
      "2/15, train_loss: 2.0603\n",
      "3/15, train_loss: 2.2234\n",
      "4/15, train_loss: 2.0580\n",
      "5/15, train_loss: 2.0331\n",
      "6/15, train_loss: 2.0930\n",
      "7/15, train_loss: 2.1397\n",
      "8/15, train_loss: 2.1384\n",
      "9/15, train_loss: 2.0509\n",
      "10/15, train_loss: 2.0734\n",
      "11/15, train_loss: 2.0728\n",
      "12/15, train_loss: 2.0627\n",
      "13/15, train_loss: 1.9838\n",
      "14/15, train_loss: 2.0556\n",
      "15/15, train_loss: 2.1861\n",
      "16/15, train_loss: 1.8224\n",
      "epoch 16 average loss: 2.0732\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 16 validation loss: 1.3504 dice_score: 0.4323 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.4661 at epoch: 16\n",
      "----------\n",
      "epoch 17/500\n",
      "1/15, train_loss: 2.0413\n",
      "2/15, train_loss: 2.0431\n",
      "3/15, train_loss: 2.0172\n",
      "4/15, train_loss: 1.9970\n",
      "5/15, train_loss: 1.9195\n",
      "6/15, train_loss: 1.9844\n",
      "7/15, train_loss: 2.0758\n",
      "8/15, train_loss: 2.3090\n",
      "9/15, train_loss: 2.2011\n",
      "10/15, train_loss: 1.9147\n",
      "11/15, train_loss: 2.1094\n",
      "12/15, train_loss: 2.0396\n",
      "13/15, train_loss: 1.9345\n",
      "14/15, train_loss: 2.0817\n",
      "15/15, train_loss: 1.9818\n",
      "16/15, train_loss: 2.4630\n",
      "epoch 17 average loss: 2.0696\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 17 validation loss: 1.3271 dice_score: 0.4576 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.6452\n",
      " saved Best PMetric: 0.4788 at epoch: 17\n",
      "----------\n",
      "epoch 18/500\n",
      "1/15, train_loss: 2.1505\n",
      "2/15, train_loss: 2.0429\n",
      "3/15, train_loss: 2.0097\n",
      "4/15, train_loss: 2.0115\n",
      "5/15, train_loss: 2.0950\n",
      "6/15, train_loss: 1.9353\n",
      "7/15, train_loss: 2.0863\n",
      "8/15, train_loss: 2.0262\n",
      "9/15, train_loss: 1.9265\n",
      "10/15, train_loss: 2.0795\n",
      "11/15, train_loss: 2.0381\n",
      "12/15, train_loss: 2.0535\n",
      "13/15, train_loss: 1.8475\n",
      "14/15, train_loss: 2.0442\n",
      "15/15, train_loss: 2.0368\n",
      "16/15, train_loss: 2.2908\n",
      "epoch 18 average loss: 2.0421\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 18 validation loss: 1.3033 dice_score: 0.4881 acc_metric: 0.4938 accuracy: 0.4938, f1score: 0.6371\n",
      " saved Best PMetric: 0.4909 at epoch: 18\n",
      "----------\n",
      "epoch 19/500\n",
      "1/15, train_loss: 1.8876\n",
      "2/15, train_loss: 2.1235\n",
      "3/15, train_loss: 1.9765\n",
      "4/15, train_loss: 1.8783\n",
      "5/15, train_loss: 1.9941\n",
      "6/15, train_loss: 2.1234\n",
      "7/15, train_loss: 2.4897\n",
      "8/15, train_loss: 1.9849\n",
      "9/15, train_loss: 2.0039\n",
      "10/15, train_loss: 1.9100\n",
      "11/15, train_loss: 1.8485\n",
      "12/15, train_loss: 1.9390\n",
      "13/15, train_loss: 2.2055\n",
      "14/15, train_loss: 1.9595\n",
      "15/15, train_loss: 2.1038\n",
      "16/15, train_loss: 2.0556\n",
      "epoch 19 average loss: 2.0302\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 19 validation loss: 1.2878 dice_score: 0.5075 acc_metric: 0.4938 accuracy: 0.4938, f1score: 0.6371\n",
      " saved Best PMetric: 0.5006 at epoch: 19\n",
      "----------\n",
      "epoch 20/500\n",
      "1/15, train_loss: 1.8650\n",
      "2/15, train_loss: 1.8835\n",
      "3/15, train_loss: 2.0434\n",
      "4/15, train_loss: 1.9512\n",
      "5/15, train_loss: 2.0920\n",
      "6/15, train_loss: 1.9125\n",
      "7/15, train_loss: 2.0838\n",
      "8/15, train_loss: 1.8406\n",
      "9/15, train_loss: 2.4613\n",
      "10/15, train_loss: 1.8333\n",
      "11/15, train_loss: 1.9475\n",
      "12/15, train_loss: 1.8853\n",
      "13/15, train_loss: 1.9146\n",
      "14/15, train_loss: 1.9751\n",
      "15/15, train_loss: 2.0550\n",
      "16/15, train_loss: 1.8140\n",
      "epoch 20 average loss: 1.9724\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 20 validation loss: 1.2684 dice_score: 0.5279 acc_metric: 0.4938 accuracy: 0.4938, f1score: 0.6371\n",
      " saved Best PMetric: 0.5108 at epoch: 20\n",
      "----------\n",
      "epoch 21/500\n",
      "1/15, train_loss: 2.0219\n",
      "2/15, train_loss: 1.8997\n",
      "3/15, train_loss: 2.0086\n",
      "4/15, train_loss: 1.8489\n",
      "5/15, train_loss: 2.2221\n",
      "6/15, train_loss: 2.1001\n",
      "7/15, train_loss: 2.0804\n",
      "8/15, train_loss: 1.7894\n",
      "9/15, train_loss: 1.7949\n",
      "10/15, train_loss: 1.8508\n",
      "11/15, train_loss: 2.1274\n",
      "12/15, train_loss: 1.8631\n",
      "13/15, train_loss: 1.9551\n",
      "14/15, train_loss: 1.9992\n",
      "15/15, train_loss: 1.9495\n",
      "16/15, train_loss: 2.4631\n",
      "epoch 21 average loss: 1.9984\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 21 validation loss: 1.2457 dice_score: 0.5455 acc_metric: 0.4813 accuracy: 0.4813, f1score: 0.6210\n",
      " saved Best PMetric: 0.5134 at epoch: 21\n",
      "----------\n",
      "epoch 22/500\n",
      "1/15, train_loss: 1.9182\n",
      "2/15, train_loss: 1.7578\n",
      "3/15, train_loss: 1.8703\n",
      "4/15, train_loss: 1.8693\n",
      "5/15, train_loss: 1.8940\n",
      "6/15, train_loss: 1.8724\n",
      "7/15, train_loss: 1.9368\n",
      "8/15, train_loss: 1.8596\n",
      "9/15, train_loss: 1.9552\n",
      "10/15, train_loss: 1.9750\n",
      "11/15, train_loss: 1.7992\n",
      "12/15, train_loss: 2.1872\n",
      "13/15, train_loss: 1.8015\n",
      "14/15, train_loss: 1.8167\n",
      "15/15, train_loss: 1.9571\n",
      "16/15, train_loss: 2.4713\n",
      "epoch 22 average loss: 1.9338\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 22 validation loss: 1.2295 dice_score: 0.5625 acc_metric: 0.4813 accuracy: 0.4813, f1score: 0.6210\n",
      " saved Best PMetric: 0.5219 at epoch: 22\n",
      "----------\n",
      "epoch 23/500\n",
      "1/15, train_loss: 2.0165\n",
      "2/15, train_loss: 2.0154\n",
      "3/15, train_loss: 1.9889\n",
      "4/15, train_loss: 1.8770\n",
      "5/15, train_loss: 1.9791\n",
      "6/15, train_loss: 1.7182\n",
      "7/15, train_loss: 1.7955\n",
      "8/15, train_loss: 1.8554\n",
      "9/15, train_loss: 1.9134\n",
      "10/15, train_loss: 1.9242\n",
      "11/15, train_loss: 1.9810\n",
      "12/15, train_loss: 1.8205\n",
      "13/15, train_loss: 1.7930\n",
      "14/15, train_loss: 1.9321\n",
      "15/15, train_loss: 1.8284\n",
      "16/15, train_loss: 2.1605\n",
      "epoch 23 average loss: 1.9124\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 23 validation loss: 1.2090 dice_score: 0.5791 acc_metric: 0.4864 accuracy: 0.4864, f1score: 0.6210\n",
      " saved Best PMetric: 0.5327 at epoch: 23\n",
      "----------\n",
      "epoch 24/500\n",
      "1/15, train_loss: 1.7043\n",
      "2/15, train_loss: 1.6975\n",
      "3/15, train_loss: 1.9198\n",
      "4/15, train_loss: 1.9346\n",
      "5/15, train_loss: 1.8960\n",
      "6/15, train_loss: 1.9115\n",
      "7/15, train_loss: 1.9217\n",
      "8/15, train_loss: 1.7201\n",
      "9/15, train_loss: 1.8802\n",
      "10/15, train_loss: 1.7868\n",
      "11/15, train_loss: 1.8358\n",
      "12/15, train_loss: 1.9514\n",
      "13/15, train_loss: 1.8357\n",
      "14/15, train_loss: 1.8946\n",
      "15/15, train_loss: 1.7378\n",
      "16/15, train_loss: 2.1583\n",
      "epoch 24 average loss: 1.8616\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 24 validation loss: 1.1795 dice_score: 0.5939 acc_metric: 0.4739 accuracy: 0.4739, f1score: 0.6048\n",
      " saved Best PMetric: 0.5339 at epoch: 24\n",
      "----------\n",
      "epoch 25/500\n",
      "1/15, train_loss: 1.8426\n",
      "2/15, train_loss: 1.7678\n",
      "3/15, train_loss: 1.8310\n",
      "4/15, train_loss: 1.8119\n",
      "5/15, train_loss: 1.7924\n",
      "6/15, train_loss: 1.7757\n",
      "7/15, train_loss: 2.1783\n",
      "8/15, train_loss: 2.1271\n",
      "9/15, train_loss: 1.8944\n",
      "10/15, train_loss: 1.8152\n",
      "11/15, train_loss: 1.7327\n",
      "12/15, train_loss: 1.8338\n",
      "13/15, train_loss: 1.8015\n",
      "14/15, train_loss: 1.7460\n",
      "15/15, train_loss: 1.8520\n",
      "16/15, train_loss: 2.4295\n",
      "epoch 25 average loss: 1.8895\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 25 validation loss: 1.1869 dice_score: 0.6062 acc_metric: 0.4739 accuracy: 0.4739, f1score: 0.6048\n",
      " saved Best PMetric: 0.5401 at epoch: 25\n",
      "----------\n",
      "epoch 26/500\n",
      "1/15, train_loss: 1.6711\n",
      "2/15, train_loss: 1.6629\n",
      "3/15, train_loss: 1.9170\n",
      "4/15, train_loss: 1.8385\n",
      "5/15, train_loss: 1.8034\n",
      "6/15, train_loss: 2.0714\n",
      "7/15, train_loss: 1.8306\n",
      "8/15, train_loss: 1.8812\n",
      "9/15, train_loss: 1.8343\n",
      "10/15, train_loss: 1.7418\n",
      "11/15, train_loss: 1.7096\n",
      "12/15, train_loss: 1.7586\n",
      "13/15, train_loss: 1.7633\n",
      "14/15, train_loss: 1.9829\n",
      "15/15, train_loss: 1.7693\n",
      "16/15, train_loss: 1.7489\n",
      "epoch 26 average loss: 1.8115\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 26 validation loss: 1.1738 dice_score: 0.6181 acc_metric: 0.4739 accuracy: 0.4739, f1score: 0.6048\n",
      " saved Best PMetric: 0.5460 at epoch: 26\n",
      "----------\n",
      "epoch 27/500\n",
      "1/15, train_loss: 1.7836\n",
      "2/15, train_loss: 1.7630\n",
      "3/15, train_loss: 1.8226\n",
      "4/15, train_loss: 1.7294\n",
      "5/15, train_loss: 1.8495\n",
      "6/15, train_loss: 1.9508\n",
      "7/15, train_loss: 1.6991\n",
      "8/15, train_loss: 1.8540\n",
      "9/15, train_loss: 1.9392\n",
      "10/15, train_loss: 2.0159\n",
      "11/15, train_loss: 1.9329\n",
      "12/15, train_loss: 1.8849\n",
      "13/15, train_loss: 1.8742\n",
      "14/15, train_loss: 1.7934\n",
      "15/15, train_loss: 1.7520\n",
      "16/15, train_loss: 1.5948\n",
      "epoch 27 average loss: 1.8275\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 27 validation loss: 1.1639 dice_score: 0.6268 acc_metric: 0.4852 accuracy: 0.4852, f1score: 0.6129\n",
      " saved Best PMetric: 0.5560 at epoch: 27\n",
      "----------\n",
      "epoch 28/500\n",
      "1/15, train_loss: 1.7419\n",
      "2/15, train_loss: 1.7138\n",
      "3/15, train_loss: 1.6642\n",
      "4/15, train_loss: 1.7374\n",
      "5/15, train_loss: 1.5758\n",
      "6/15, train_loss: 1.7980\n",
      "7/15, train_loss: 1.6583\n",
      "8/15, train_loss: 1.8416\n",
      "9/15, train_loss: 1.7207\n",
      "10/15, train_loss: 1.8385\n",
      "11/15, train_loss: 1.6688\n",
      "12/15, train_loss: 1.5876\n",
      "13/15, train_loss: 1.6813\n",
      "14/15, train_loss: 1.8798\n",
      "15/15, train_loss: 1.6673\n",
      "16/15, train_loss: 1.6624\n",
      "epoch 28 average loss: 1.7148\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 28 validation loss: 1.1507 dice_score: 0.6396 acc_metric: 0.5420 accuracy: 0.5420, f1score: 0.6532\n",
      " saved Best PMetric: 0.5908 at epoch: 28\n",
      "----------\n",
      "epoch 29/500\n",
      "1/15, train_loss: 2.1619\n",
      "2/15, train_loss: 1.8792\n",
      "3/15, train_loss: 1.6571\n",
      "4/15, train_loss: 1.7588\n",
      "5/15, train_loss: 2.0937\n",
      "6/15, train_loss: 1.9354\n",
      "7/15, train_loss: 1.8065\n",
      "8/15, train_loss: 1.7816\n",
      "9/15, train_loss: 1.6935\n",
      "10/15, train_loss: 1.7800\n",
      "11/15, train_loss: 2.0701\n",
      "12/15, train_loss: 1.6055\n",
      "13/15, train_loss: 1.7609\n",
      "14/15, train_loss: 1.7892\n",
      "15/15, train_loss: 1.7922\n",
      "16/15, train_loss: 1.7511\n",
      "epoch 29 average loss: 1.8323\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 29 validation loss: 1.1361 dice_score: 0.6475 acc_metric: 0.5750 accuracy: 0.5750, f1score: 0.6694\n",
      " saved Best PMetric: 0.6112 at epoch: 29\n",
      "----------\n",
      "epoch 30/500\n",
      "1/15, train_loss: 1.6428\n",
      "2/15, train_loss: 1.7020\n",
      "3/15, train_loss: 1.6263\n",
      "4/15, train_loss: 1.8042\n",
      "5/15, train_loss: 1.6076\n",
      "6/15, train_loss: 1.6173\n",
      "7/15, train_loss: 1.4958\n",
      "8/15, train_loss: 1.6551\n",
      "9/15, train_loss: 1.5488\n",
      "10/15, train_loss: 1.9452\n",
      "11/15, train_loss: 1.5835\n",
      "12/15, train_loss: 1.6955\n",
      "13/15, train_loss: 1.8147\n",
      "14/15, train_loss: 1.7267\n",
      "15/15, train_loss: 1.7027\n",
      "16/15, train_loss: 1.9765\n",
      "epoch 30 average loss: 1.6965\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 30 validation loss: 1.1427 dice_score: 0.6519 acc_metric: 0.5699 accuracy: 0.5699, f1score: 0.6694\n",
      " saved Best PMetric: 0.6109 at epoch: 30\n",
      "----------\n",
      "epoch 31/500\n",
      "1/15, train_loss: 1.6912\n",
      "2/15, train_loss: 1.7312\n",
      "3/15, train_loss: 1.8132\n",
      "4/15, train_loss: 1.5064\n",
      "5/15, train_loss: 1.5230\n",
      "6/15, train_loss: 1.6972\n",
      "7/15, train_loss: 1.4964\n",
      "8/15, train_loss: 1.5857\n",
      "9/15, train_loss: 1.7419\n",
      "10/15, train_loss: 1.6092\n",
      "11/15, train_loss: 1.6715\n",
      "12/15, train_loss: 1.6524\n",
      "13/15, train_loss: 1.8119\n",
      "14/15, train_loss: 1.6859\n",
      "15/15, train_loss: 1.8771\n",
      "16/15, train_loss: 1.9560\n",
      "epoch 31 average loss: 1.6906\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 31 validation loss: 1.1364 dice_score: 0.6558 acc_metric: 0.6091 accuracy: 0.6091, f1score: 0.6935\n",
      " saved Best PMetric: 0.6325 at epoch: 31\n",
      "----------\n",
      "epoch 32/500\n",
      "1/15, train_loss: 1.8692\n",
      "2/15, train_loss: 1.4805\n",
      "3/15, train_loss: 1.5933\n",
      "4/15, train_loss: 1.5248\n",
      "5/15, train_loss: 1.5404\n",
      "6/15, train_loss: 1.5373\n",
      "7/15, train_loss: 1.5873\n",
      "8/15, train_loss: 1.5430\n",
      "9/15, train_loss: 1.5425\n",
      "10/15, train_loss: 1.8743\n",
      "11/15, train_loss: 1.8177\n",
      "12/15, train_loss: 1.6515\n",
      "13/15, train_loss: 1.6855\n",
      "14/15, train_loss: 1.6989\n",
      "15/15, train_loss: 1.4777\n",
      "16/15, train_loss: 2.4500\n",
      "epoch 32 average loss: 1.6796\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 32 validation loss: 1.1609 dice_score: 0.6533 acc_metric: 0.6483 accuracy: 0.6483, f1score: 0.7177\n",
      " saved Best PMetric: 0.6508 at epoch: 32\n",
      "----------\n",
      "epoch 33/500\n",
      "1/15, train_loss: 1.6842\n",
      "2/15, train_loss: 1.6349\n",
      "3/15, train_loss: 1.6551\n",
      "4/15, train_loss: 1.5440\n",
      "5/15, train_loss: 1.5215\n",
      "6/15, train_loss: 1.6870\n",
      "7/15, train_loss: 1.4169\n",
      "8/15, train_loss: 1.4543\n",
      "9/15, train_loss: 1.5038\n",
      "10/15, train_loss: 1.7360\n",
      "11/15, train_loss: 1.5690\n",
      "12/15, train_loss: 1.4602\n",
      "13/15, train_loss: 1.3925\n",
      "14/15, train_loss: 1.3810\n",
      "15/15, train_loss: 1.6340\n",
      "16/15, train_loss: 1.4369\n",
      "epoch 33 average loss: 1.5444\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 33 validation loss: 1.1162 dice_score: 0.6720 acc_metric: 0.6653 accuracy: 0.6653, f1score: 0.6935\n",
      " saved Best PMetric: 0.6687 at epoch: 33\n",
      "----------\n",
      "epoch 34/500\n",
      "1/15, train_loss: 1.5653\n",
      "2/15, train_loss: 1.4875\n",
      "3/15, train_loss: 1.5874\n",
      "4/15, train_loss: 1.5016\n",
      "5/15, train_loss: 1.5237\n",
      "6/15, train_loss: 1.5184\n",
      "7/15, train_loss: 1.5422\n",
      "8/15, train_loss: 1.9114\n",
      "9/15, train_loss: 1.4861\n",
      "10/15, train_loss: 1.9189\n",
      "11/15, train_loss: 1.4663\n",
      "12/15, train_loss: 2.1343\n",
      "13/15, train_loss: 1.6914\n",
      "14/15, train_loss: 1.6308\n",
      "15/15, train_loss: 1.7231\n",
      "16/15, train_loss: 2.1508\n",
      "epoch 34 average loss: 1.6774\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 34 validation loss: 1.1275 dice_score: 0.6724 acc_metric: 0.6790 accuracy: 0.6790, f1score: 0.7177\n",
      " saved Best PMetric: 0.6757 at epoch: 34\n",
      "----------\n",
      "epoch 35/500\n",
      "1/15, train_loss: 1.7620\n",
      "2/15, train_loss: 1.3971\n",
      "3/15, train_loss: 1.5320\n",
      "4/15, train_loss: 1.5945\n",
      "5/15, train_loss: 1.5116\n",
      "6/15, train_loss: 1.3987\n",
      "7/15, train_loss: 1.4854\n",
      "8/15, train_loss: 1.6303\n",
      "9/15, train_loss: 1.3590\n",
      "10/15, train_loss: 1.6431\n",
      "11/15, train_loss: 1.6537\n",
      "12/15, train_loss: 1.4298\n",
      "13/15, train_loss: 1.4464\n",
      "14/15, train_loss: 1.7345\n",
      "15/15, train_loss: 1.4868\n",
      "16/15, train_loss: 2.1919\n",
      "epoch 35 average loss: 1.5785\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 35 validation loss: 1.1090 dice_score: 0.6800 acc_metric: 0.6818 accuracy: 0.6818, f1score: 0.7016\n",
      " saved Best PMetric: 0.6809 at epoch: 35\n",
      "----------\n",
      "epoch 36/500\n",
      "1/15, train_loss: 1.4330\n",
      "2/15, train_loss: 1.6736\n",
      "3/15, train_loss: 1.5929\n",
      "4/15, train_loss: 1.4639\n",
      "5/15, train_loss: 1.5534\n",
      "6/15, train_loss: 1.4792\n",
      "7/15, train_loss: 2.0517\n",
      "8/15, train_loss: 1.3130\n",
      "9/15, train_loss: 1.6201\n",
      "10/15, train_loss: 1.3493\n",
      "11/15, train_loss: 1.2818\n",
      "12/15, train_loss: 1.5252\n",
      "13/15, train_loss: 1.3324\n",
      "14/15, train_loss: 1.3931\n",
      "15/15, train_loss: 1.4471\n",
      "16/15, train_loss: 1.3065\n",
      "epoch 36 average loss: 1.4885\n",
      "----------\n",
      "epoch 37/500\n",
      "1/15, train_loss: 1.3156\n",
      "2/15, train_loss: 1.4960\n",
      "3/15, train_loss: 1.3553\n",
      "4/15, train_loss: 1.7345\n",
      "5/15, train_loss: 1.3327\n",
      "6/15, train_loss: 1.7064\n",
      "7/15, train_loss: 1.6516\n",
      "8/15, train_loss: 2.4680\n",
      "9/15, train_loss: 1.5437\n",
      "10/15, train_loss: 1.4658\n",
      "11/15, train_loss: 1.2707\n",
      "12/15, train_loss: 1.4089\n",
      "13/15, train_loss: 1.5123\n",
      "14/15, train_loss: 1.2676\n",
      "15/15, train_loss: 1.4016\n",
      "16/15, train_loss: 1.3257\n",
      "epoch 37 average loss: 1.5160\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 37 validation loss: 1.1234 dice_score: 0.6825 acc_metric: 0.7193 accuracy: 0.7193, f1score: 0.7500\n",
      " saved Best PMetric: 0.7009 at epoch: 37\n",
      "----------\n",
      "epoch 38/500\n",
      "1/15, train_loss: 1.6131\n",
      "2/15, train_loss: 1.6096\n",
      "3/15, train_loss: 1.2531\n",
      "4/15, train_loss: 1.4368\n",
      "5/15, train_loss: 1.4104\n",
      "6/15, train_loss: 1.3106\n",
      "7/15, train_loss: 1.7478\n",
      "8/15, train_loss: 1.3806\n",
      "9/15, train_loss: 1.4093\n",
      "10/15, train_loss: 1.4617\n",
      "11/15, train_loss: 1.2886\n",
      "12/15, train_loss: 1.4950\n",
      "13/15, train_loss: 1.4086\n",
      "14/15, train_loss: 1.3264\n",
      "15/15, train_loss: 1.3893\n",
      "16/15, train_loss: 2.4935\n",
      "epoch 38 average loss: 1.5022\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 38 validation loss: 1.1075 dice_score: 0.6872 acc_metric: 0.7108 accuracy: 0.7108, f1score: 0.7258\n",
      " saved Best PMetric: 0.6990 at epoch: 38\n",
      "----------\n",
      "epoch 39/500\n",
      "1/15, train_loss: 2.1175\n",
      "2/15, train_loss: 1.4715\n",
      "3/15, train_loss: 1.3953\n",
      "4/15, train_loss: 1.4604\n",
      "5/15, train_loss: 1.1664\n",
      "6/15, train_loss: 1.2029\n",
      "7/15, train_loss: 1.2678\n",
      "8/15, train_loss: 1.4956\n",
      "9/15, train_loss: 1.5098\n",
      "10/15, train_loss: 1.2487\n",
      "11/15, train_loss: 1.3044\n",
      "12/15, train_loss: 1.6352\n",
      "13/15, train_loss: 1.2488\n",
      "14/15, train_loss: 1.3027\n",
      "15/15, train_loss: 1.1926\n",
      "16/15, train_loss: 1.1852\n",
      "epoch 39 average loss: 1.3878\n",
      "----------\n",
      "epoch 40/500\n",
      "1/15, train_loss: 1.4998\n",
      "2/15, train_loss: 1.1446\n",
      "3/15, train_loss: 1.4796\n",
      "4/15, train_loss: 1.2669\n",
      "5/15, train_loss: 1.4738\n",
      "6/15, train_loss: 1.3590\n",
      "7/15, train_loss: 1.4302\n",
      "8/15, train_loss: 1.2782\n",
      "9/15, train_loss: 1.4748\n",
      "10/15, train_loss: 1.2108\n",
      "11/15, train_loss: 1.3057\n",
      "12/15, train_loss: 1.3521\n",
      "13/15, train_loss: 1.1452\n",
      "14/15, train_loss: 1.2102\n",
      "15/15, train_loss: 1.4634\n",
      "16/15, train_loss: 1.6718\n",
      "epoch 40 average loss: 1.3604\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 40 validation loss: 1.1230 dice_score: 0.6927 acc_metric: 0.7057 accuracy: 0.7057, f1score: 0.7258\n",
      " saved Best PMetric: 0.6992 at epoch: 40\n",
      "----------\n",
      "epoch 41/500\n",
      "1/15, train_loss: 1.1001\n",
      "2/15, train_loss: 1.5126\n",
      "3/15, train_loss: 1.4290\n",
      "4/15, train_loss: 1.6276\n",
      "5/15, train_loss: 1.4399\n",
      "6/15, train_loss: 1.3762\n",
      "7/15, train_loss: 1.1475\n",
      "8/15, train_loss: 1.1697\n",
      "9/15, train_loss: 1.5613\n",
      "10/15, train_loss: 1.4075\n",
      "11/15, train_loss: 1.1976\n",
      "12/15, train_loss: 1.3079\n",
      "13/15, train_loss: 1.5428\n",
      "14/15, train_loss: 1.2683\n",
      "15/15, train_loss: 1.3244\n",
      "16/15, train_loss: 2.1501\n",
      "epoch 41 average loss: 1.4102\n",
      "----------\n",
      "epoch 42/500\n",
      "1/15, train_loss: 1.5238\n",
      "2/15, train_loss: 1.1495\n",
      "3/15, train_loss: 1.1013\n",
      "4/15, train_loss: 1.1169\n",
      "5/15, train_loss: 1.1343\n",
      "6/15, train_loss: 1.1921\n",
      "7/15, train_loss: 1.3474\n",
      "8/15, train_loss: 1.1452\n",
      "9/15, train_loss: 1.0022\n",
      "10/15, train_loss: 1.2771\n",
      "11/15, train_loss: 1.0489\n",
      "12/15, train_loss: 1.1456\n",
      "13/15, train_loss: 1.2990\n",
      "14/15, train_loss: 1.1586\n",
      "15/15, train_loss: 1.3028\n",
      "16/15, train_loss: 2.5616\n",
      "epoch 42 average loss: 1.2816\n",
      "----------\n",
      "epoch 43/500\n",
      "1/15, train_loss: 1.0137\n",
      "2/15, train_loss: 1.2467\n",
      "3/15, train_loss: 1.1985\n",
      "4/15, train_loss: 1.1279\n",
      "5/15, train_loss: 1.1960\n",
      "6/15, train_loss: 1.1589\n",
      "7/15, train_loss: 1.0943\n",
      "8/15, train_loss: 1.1889\n",
      "9/15, train_loss: 1.3564\n",
      "10/15, train_loss: 1.4066\n",
      "11/15, train_loss: 1.1048\n",
      "12/15, train_loss: 1.6927\n",
      "13/15, train_loss: 1.2277\n",
      "14/15, train_loss: 1.0488\n",
      "15/15, train_loss: 1.7860\n",
      "16/15, train_loss: 1.2430\n",
      "epoch 43 average loss: 1.2557\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 43 validation loss: 1.0927 dice_score: 0.7096 acc_metric: 0.7057 accuracy: 0.7057, f1score: 0.7258\n",
      " saved Best PMetric: 0.7077 at epoch: 43\n",
      "----------\n",
      "epoch 44/500\n",
      "1/15, train_loss: 1.0420\n",
      "2/15, train_loss: 1.2478\n",
      "3/15, train_loss: 1.0360\n",
      "4/15, train_loss: 1.2578\n",
      "5/15, train_loss: 1.2366\n",
      "6/15, train_loss: 1.4658\n",
      "7/15, train_loss: 1.1544\n",
      "8/15, train_loss: 1.3403\n",
      "9/15, train_loss: 1.2036\n",
      "10/15, train_loss: 1.2865\n",
      "11/15, train_loss: 1.1876\n",
      "12/15, train_loss: 1.3204\n",
      "13/15, train_loss: 1.3236\n",
      "14/15, train_loss: 1.4780\n",
      "15/15, train_loss: 1.1721\n",
      "16/15, train_loss: 2.8909\n",
      "epoch 44 average loss: 1.3527\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 44 validation loss: 1.1080 dice_score: 0.7114 acc_metric: 0.7347 accuracy: 0.7347, f1score: 0.7500\n",
      " saved Best PMetric: 0.7230 at epoch: 44\n",
      "----------\n",
      "epoch 45/500\n",
      "1/15, train_loss: 1.3035\n",
      "2/15, train_loss: 1.0697\n",
      "3/15, train_loss: 1.8526\n",
      "4/15, train_loss: 1.2676\n",
      "5/15, train_loss: 1.0857\n",
      "6/15, train_loss: 1.0446\n",
      "7/15, train_loss: 1.2528\n",
      "8/15, train_loss: 1.4510\n",
      "9/15, train_loss: 1.1769\n",
      "10/15, train_loss: 1.8901\n",
      "11/15, train_loss: 1.2189\n",
      "12/15, train_loss: 1.0409\n",
      "13/15, train_loss: 1.0123\n",
      "14/15, train_loss: 1.1437\n",
      "15/15, train_loss: 1.3353\n",
      "16/15, train_loss: 1.4166\n",
      "epoch 45 average loss: 1.2851\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 45 validation loss: 1.0997 dice_score: 0.7150 acc_metric: 0.7193 accuracy: 0.7193, f1score: 0.7500\n",
      " saved Best PMetric: 0.7172 at epoch: 45\n",
      "----------\n",
      "epoch 46/500\n",
      "1/15, train_loss: 1.0597\n",
      "2/15, train_loss: 1.2625\n",
      "3/15, train_loss: 1.2720\n",
      "4/15, train_loss: 1.1113\n",
      "5/15, train_loss: 1.3591\n",
      "6/15, train_loss: 1.5818\n",
      "7/15, train_loss: 1.2928\n",
      "8/15, train_loss: 1.6462\n",
      "9/15, train_loss: 1.6717\n",
      "10/15, train_loss: 1.0453\n",
      "11/15, train_loss: 1.0081\n",
      "12/15, train_loss: 1.5035\n",
      "13/15, train_loss: 1.1286\n",
      "14/15, train_loss: 0.9906\n",
      "15/15, train_loss: 1.0612\n",
      "16/15, train_loss: 2.2776\n",
      "epoch 46 average loss: 1.3295\n",
      "----------\n",
      "epoch 47/500\n",
      "1/15, train_loss: 1.6703\n",
      "2/15, train_loss: 1.2711\n",
      "3/15, train_loss: 2.5914\n",
      "4/15, train_loss: 1.1187\n",
      "5/15, train_loss: 1.3552\n",
      "6/15, train_loss: 1.0388\n",
      "7/15, train_loss: 1.1373\n",
      "8/15, train_loss: 1.0422\n",
      "9/15, train_loss: 1.0444\n",
      "10/15, train_loss: 1.3193\n",
      "11/15, train_loss: 1.2430\n",
      "12/15, train_loss: 2.2667\n",
      "13/15, train_loss: 1.6400\n",
      "14/15, train_loss: 1.1769\n",
      "15/15, train_loss: 1.1714\n",
      "16/15, train_loss: 1.5119\n",
      "epoch 47 average loss: 1.4124\n",
      "----------\n",
      "epoch 48/500\n",
      "1/15, train_loss: 1.5608\n",
      "2/15, train_loss: 0.9724\n",
      "3/15, train_loss: 1.0337\n",
      "4/15, train_loss: 1.1431\n",
      "5/15, train_loss: 1.2144\n",
      "6/15, train_loss: 1.3540\n",
      "7/15, train_loss: 1.5068\n",
      "8/15, train_loss: 0.9576\n",
      "9/15, train_loss: 1.0756\n",
      "10/15, train_loss: 2.0750\n",
      "11/15, train_loss: 1.1567\n",
      "12/15, train_loss: 1.4248\n",
      "13/15, train_loss: 0.9851\n",
      "14/15, train_loss: 1.2770\n",
      "15/15, train_loss: 1.7217\n",
      "16/15, train_loss: 2.5395\n",
      "epoch 48 average loss: 1.3749\n",
      "----------\n",
      "epoch 49/500\n",
      "1/15, train_loss: 0.9451\n",
      "2/15, train_loss: 1.1329\n",
      "3/15, train_loss: 1.4608\n",
      "4/15, train_loss: 1.3597\n",
      "5/15, train_loss: 1.1397\n",
      "6/15, train_loss: 1.2743\n",
      "7/15, train_loss: 1.0575\n",
      "8/15, train_loss: 1.0333\n",
      "9/15, train_loss: 0.9823\n",
      "10/15, train_loss: 0.9952\n",
      "11/15, train_loss: 1.0276\n",
      "12/15, train_loss: 1.0721\n",
      "13/15, train_loss: 0.9431\n",
      "14/15, train_loss: 0.9390\n",
      "15/15, train_loss: 1.0081\n",
      "16/15, train_loss: 2.1493\n",
      "epoch 49 average loss: 1.1575\n",
      "----------\n",
      "epoch 50/500\n",
      "1/15, train_loss: 1.2918\n",
      "2/15, train_loss: 1.2261\n",
      "3/15, train_loss: 1.1706\n",
      "4/15, train_loss: 1.2332\n",
      "5/15, train_loss: 0.8791\n",
      "6/15, train_loss: 0.8980\n",
      "7/15, train_loss: 1.2020\n",
      "8/15, train_loss: 0.9672\n",
      "9/15, train_loss: 1.0877\n",
      "10/15, train_loss: 0.9370\n",
      "11/15, train_loss: 1.1680\n",
      "12/15, train_loss: 1.1102\n",
      "13/15, train_loss: 0.9313\n",
      "14/15, train_loss: 1.4728\n",
      "15/15, train_loss: 0.9923\n",
      "16/15, train_loss: 1.0783\n",
      "epoch 50 average loss: 1.1029\n",
      "----------\n",
      "epoch 51/500\n",
      "1/15, train_loss: 0.9942\n",
      "2/15, train_loss: 0.9355\n",
      "3/15, train_loss: 1.0754\n",
      "4/15, train_loss: 1.1740\n",
      "5/15, train_loss: 1.0000\n",
      "6/15, train_loss: 1.0751\n",
      "7/15, train_loss: 0.9210\n",
      "8/15, train_loss: 1.4203\n",
      "9/15, train_loss: 1.1069\n",
      "10/15, train_loss: 0.9868\n",
      "11/15, train_loss: 1.0207\n",
      "12/15, train_loss: 0.9530\n",
      "13/15, train_loss: 1.1866\n",
      "14/15, train_loss: 1.0586\n",
      "15/15, train_loss: 1.7204\n",
      "16/15, train_loss: 1.2405\n",
      "epoch 51 average loss: 1.1168\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 51 validation loss: 1.0918 dice_score: 0.7368 acc_metric: 0.7040 accuracy: 0.7040, f1score: 0.7500\n",
      " saved Best PMetric: 0.7204 at epoch: 51\n",
      "----------\n",
      "epoch 52/500\n",
      "1/15, train_loss: 0.8978\n",
      "2/15, train_loss: 1.4872\n",
      "3/15, train_loss: 1.3251\n",
      "4/15, train_loss: 1.0181\n",
      "5/15, train_loss: 1.1604\n",
      "6/15, train_loss: 1.1481\n",
      "7/15, train_loss: 1.0838\n",
      "8/15, train_loss: 1.0098\n",
      "9/15, train_loss: 1.0867\n",
      "10/15, train_loss: 0.8855\n",
      "11/15, train_loss: 0.9637\n",
      "12/15, train_loss: 0.9742\n",
      "13/15, train_loss: 1.0498\n",
      "14/15, train_loss: 1.0725\n",
      "15/15, train_loss: 1.2991\n",
      "16/15, train_loss: 1.2319\n",
      "epoch 52 average loss: 1.1059\n",
      "----------\n",
      "epoch 53/500\n",
      "1/15, train_loss: 1.0915\n",
      "2/15, train_loss: 0.8922\n",
      "3/15, train_loss: 1.5278\n",
      "4/15, train_loss: 1.0267\n",
      "5/15, train_loss: 1.0324\n",
      "6/15, train_loss: 0.9356\n",
      "7/15, train_loss: 0.9790\n",
      "8/15, train_loss: 1.0106\n",
      "9/15, train_loss: 0.9075\n",
      "10/15, train_loss: 0.9078\n",
      "11/15, train_loss: 1.0738\n",
      "12/15, train_loss: 0.8928\n",
      "13/15, train_loss: 1.2450\n",
      "14/15, train_loss: 0.7768\n",
      "15/15, train_loss: 1.1322\n",
      "16/15, train_loss: 0.8749\n",
      "epoch 53 average loss: 1.0192\n",
      "----------\n",
      "epoch 54/500\n",
      "1/15, train_loss: 1.0261\n",
      "2/15, train_loss: 1.1216\n",
      "3/15, train_loss: 1.2477\n",
      "4/15, train_loss: 1.4766\n",
      "5/15, train_loss: 1.3161\n",
      "6/15, train_loss: 0.8051\n",
      "7/15, train_loss: 0.8929\n",
      "8/15, train_loss: 1.0472\n",
      "9/15, train_loss: 1.1204\n",
      "10/15, train_loss: 1.2365\n",
      "11/15, train_loss: 0.9098\n",
      "12/15, train_loss: 1.2190\n",
      "13/15, train_loss: 0.8162\n",
      "14/15, train_loss: 1.5551\n",
      "15/15, train_loss: 1.8593\n",
      "16/15, train_loss: 2.2249\n",
      "epoch 54 average loss: 1.2422\n",
      "----------\n",
      "epoch 55/500\n",
      "1/15, train_loss: 0.7862\n",
      "2/15, train_loss: 1.1657\n",
      "3/15, train_loss: 1.0106\n",
      "4/15, train_loss: 1.6295\n",
      "5/15, train_loss: 0.8831\n",
      "6/15, train_loss: 0.8872\n",
      "7/15, train_loss: 1.0413\n",
      "8/15, train_loss: 0.8451\n",
      "9/15, train_loss: 0.9259\n",
      "10/15, train_loss: 0.9664\n",
      "11/15, train_loss: 0.8414\n",
      "12/15, train_loss: 0.8212\n",
      "13/15, train_loss: 0.8511\n",
      "14/15, train_loss: 0.9704\n",
      "15/15, train_loss: 0.8576\n",
      "16/15, train_loss: 2.7047\n",
      "epoch 55 average loss: 1.0742\n",
      "----------\n",
      "epoch 56/500\n",
      "1/15, train_loss: 0.8745\n",
      "2/15, train_loss: 0.7671\n",
      "3/15, train_loss: 1.1054\n",
      "4/15, train_loss: 2.2525\n",
      "5/15, train_loss: 0.8251\n",
      "6/15, train_loss: 0.7565\n",
      "7/15, train_loss: 0.8280\n",
      "8/15, train_loss: 1.1516\n",
      "9/15, train_loss: 0.7726\n",
      "10/15, train_loss: 0.8556\n",
      "11/15, train_loss: 0.8996\n",
      "12/15, train_loss: 1.4200\n",
      "13/15, train_loss: 1.2860\n",
      "14/15, train_loss: 1.2340\n",
      "15/15, train_loss: 2.2931\n",
      "16/15, train_loss: 2.2974\n",
      "epoch 56 average loss: 1.2262\n",
      "----------\n",
      "epoch 57/500\n",
      "1/15, train_loss: 1.0518\n",
      "2/15, train_loss: 1.1200\n",
      "3/15, train_loss: 0.8400\n",
      "4/15, train_loss: 1.2348\n",
      "5/15, train_loss: 0.7414\n",
      "6/15, train_loss: 0.8138\n",
      "7/15, train_loss: 0.8288\n",
      "8/15, train_loss: 0.8159\n",
      "9/15, train_loss: 0.8455\n",
      "10/15, train_loss: 0.9134\n",
      "11/15, train_loss: 0.7136\n",
      "12/15, train_loss: 1.7793\n",
      "13/15, train_loss: 0.7577\n",
      "14/15, train_loss: 0.8010\n",
      "15/15, train_loss: 0.8845\n",
      "16/15, train_loss: 2.2425\n",
      "epoch 57 average loss: 1.0240\n",
      "----------\n",
      "epoch 58/500\n",
      "1/15, train_loss: 1.9890\n",
      "2/15, train_loss: 1.2213\n",
      "3/15, train_loss: 0.8247\n",
      "4/15, train_loss: 0.9573\n",
      "5/15, train_loss: 0.9151\n",
      "6/15, train_loss: 0.7654\n",
      "7/15, train_loss: 0.9383\n",
      "8/15, train_loss: 0.8784\n",
      "9/15, train_loss: 1.0504\n",
      "10/15, train_loss: 0.7645\n",
      "11/15, train_loss: 1.5979\n",
      "12/15, train_loss: 0.8234\n",
      "13/15, train_loss: 1.6368\n",
      "14/15, train_loss: 0.8474\n",
      "15/15, train_loss: 0.9603\n",
      "16/15, train_loss: 1.0033\n",
      "epoch 58 average loss: 1.0733\n",
      "----------\n",
      "epoch 59/500\n",
      "1/15, train_loss: 1.2802\n",
      "2/15, train_loss: 0.9533\n",
      "3/15, train_loss: 0.8288\n",
      "4/15, train_loss: 1.3465\n",
      "5/15, train_loss: 1.0199\n",
      "6/15, train_loss: 0.9766\n",
      "7/15, train_loss: 1.4406\n",
      "8/15, train_loss: 0.7779\n",
      "9/15, train_loss: 0.8762\n",
      "10/15, train_loss: 0.9054\n",
      "11/15, train_loss: 0.7130\n",
      "12/15, train_loss: 0.8629\n",
      "13/15, train_loss: 1.1167\n",
      "14/15, train_loss: 1.3720\n",
      "15/15, train_loss: 0.8711\n",
      "16/15, train_loss: 2.3263\n",
      "epoch 59 average loss: 1.1042\n",
      "----------\n",
      "epoch 60/500\n",
      "1/15, train_loss: 0.9457\n",
      "2/15, train_loss: 1.4085\n",
      "3/15, train_loss: 0.9536\n",
      "4/15, train_loss: 1.0630\n",
      "5/15, train_loss: 0.9098\n",
      "6/15, train_loss: 0.7907\n",
      "7/15, train_loss: 1.0869\n",
      "8/15, train_loss: 0.7809\n",
      "9/15, train_loss: 0.9115\n",
      "10/15, train_loss: 0.9448\n",
      "11/15, train_loss: 1.4206\n",
      "12/15, train_loss: 0.8356\n",
      "13/15, train_loss: 1.0016\n",
      "14/15, train_loss: 0.6964\n",
      "15/15, train_loss: 0.7874\n",
      "16/15, train_loss: 2.5273\n",
      "epoch 60 average loss: 1.0665\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 60 validation loss: 1.0663 dice_score: 0.7682 acc_metric: 0.7278 accuracy: 0.7278, f1score: 0.7742\n",
      " saved Best PMetric: 0.7480 at epoch: 60\n",
      "----------\n",
      "epoch 61/500\n",
      "1/15, train_loss: 0.7757\n",
      "2/15, train_loss: 2.2830\n",
      "3/15, train_loss: 1.0049\n",
      "4/15, train_loss: 0.6552\n",
      "5/15, train_loss: 0.7424\n",
      "6/15, train_loss: 0.8888\n",
      "7/15, train_loss: 0.8963\n",
      "8/15, train_loss: 1.1077\n",
      "9/15, train_loss: 0.8134\n",
      "10/15, train_loss: 0.8896\n",
      "11/15, train_loss: 0.8068\n",
      "12/15, train_loss: 2.5709\n",
      "13/15, train_loss: 0.8150\n",
      "14/15, train_loss: 1.3085\n",
      "15/15, train_loss: 0.8408\n",
      "16/15, train_loss: 0.7010\n",
      "epoch 61 average loss: 1.0688\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 61 validation loss: 1.0637 dice_score: 0.7680 acc_metric: 0.6875 accuracy: 0.6875, f1score: 0.7419\n",
      " saved Best PMetric: 0.7277 at epoch: 61\n",
      "----------\n",
      "epoch 62/500\n",
      "1/15, train_loss: 0.7603\n",
      "2/15, train_loss: 1.2199\n",
      "3/15, train_loss: 0.7522\n",
      "4/15, train_loss: 1.2995\n",
      "5/15, train_loss: 0.9057\n",
      "6/15, train_loss: 0.7360\n",
      "7/15, train_loss: 0.8506\n",
      "8/15, train_loss: 0.8734\n",
      "9/15, train_loss: 1.1976\n",
      "10/15, train_loss: 0.8067\n",
      "11/15, train_loss: 0.8654\n",
      "12/15, train_loss: 1.8265\n",
      "13/15, train_loss: 0.9664\n",
      "14/15, train_loss: 0.9647\n",
      "15/15, train_loss: 1.6068\n",
      "16/15, train_loss: 0.6867\n",
      "epoch 62 average loss: 1.0199\n",
      "----------\n",
      "epoch 63/500\n",
      "1/15, train_loss: 0.9868\n",
      "2/15, train_loss: 0.8024\n",
      "3/15, train_loss: 1.1667\n",
      "4/15, train_loss: 0.9794\n",
      "5/15, train_loss: 1.1624\n",
      "6/15, train_loss: 0.8408\n",
      "7/15, train_loss: 2.3692\n",
      "8/15, train_loss: 0.7122\n",
      "9/15, train_loss: 0.7812\n",
      "10/15, train_loss: 0.7856\n",
      "11/15, train_loss: 0.8512\n",
      "12/15, train_loss: 0.8705\n",
      "13/15, train_loss: 0.9475\n",
      "14/15, train_loss: 0.7367\n",
      "15/15, train_loss: 0.7646\n",
      "16/15, train_loss: 2.7678\n",
      "epoch 63 average loss: 1.0953\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 63 validation loss: 1.0330 dice_score: 0.7716 acc_metric: 0.7330 accuracy: 0.7330, f1score: 0.7742\n",
      " saved Best PMetric: 0.7523 at epoch: 63\n",
      "----------\n",
      "epoch 64/500\n",
      "1/15, train_loss: 0.8151\n",
      "2/15, train_loss: 0.9152\n",
      "3/15, train_loss: 1.1287\n",
      "4/15, train_loss: 1.2196\n",
      "5/15, train_loss: 1.4920\n",
      "6/15, train_loss: 0.9396\n",
      "7/15, train_loss: 0.8747\n",
      "8/15, train_loss: 0.9142\n",
      "9/15, train_loss: 0.8848\n",
      "10/15, train_loss: 0.8523\n",
      "11/15, train_loss: 0.8890\n",
      "12/15, train_loss: 0.8568\n",
      "13/15, train_loss: 1.4905\n",
      "14/15, train_loss: 0.8752\n",
      "15/15, train_loss: 0.7825\n",
      "16/15, train_loss: 2.6057\n",
      "epoch 64 average loss: 1.0960\n",
      "----------\n",
      "epoch 65/500\n",
      "1/15, train_loss: 0.9139\n",
      "2/15, train_loss: 0.7213\n",
      "3/15, train_loss: 0.6651\n",
      "4/15, train_loss: 0.8746\n",
      "5/15, train_loss: 0.6870\n",
      "6/15, train_loss: 0.9562\n",
      "7/15, train_loss: 0.7992\n",
      "8/15, train_loss: 1.1516\n",
      "9/15, train_loss: 0.7277\n",
      "10/15, train_loss: 1.4714\n",
      "11/15, train_loss: 1.3897\n",
      "12/15, train_loss: 0.8899\n",
      "13/15, train_loss: 0.7034\n",
      "14/15, train_loss: 1.3877\n",
      "15/15, train_loss: 0.9958\n",
      "16/15, train_loss: 2.7224\n",
      "epoch 65 average loss: 1.0661\n",
      "----------\n",
      "epoch 66/500\n",
      "1/15, train_loss: 0.7549\n",
      "2/15, train_loss: 0.8539\n",
      "3/15, train_loss: 1.2882\n",
      "4/15, train_loss: 0.8283\n",
      "5/15, train_loss: 1.0653\n",
      "6/15, train_loss: 1.0084\n",
      "7/15, train_loss: 0.8127\n",
      "8/15, train_loss: 1.2117\n",
      "9/15, train_loss: 0.8572\n",
      "10/15, train_loss: 1.0686\n",
      "11/15, train_loss: 0.7088\n",
      "12/15, train_loss: 0.8128\n",
      "13/15, train_loss: 0.8377\n",
      "14/15, train_loss: 0.7007\n",
      "15/15, train_loss: 0.7901\n",
      "16/15, train_loss: 1.3128\n",
      "epoch 66 average loss: 0.9320\n",
      "----------\n",
      "epoch 67/500\n",
      "1/15, train_loss: 1.0377\n",
      "2/15, train_loss: 1.2012\n",
      "3/15, train_loss: 1.1720\n",
      "4/15, train_loss: 1.2603\n",
      "5/15, train_loss: 0.7960\n",
      "6/15, train_loss: 0.9916\n",
      "7/15, train_loss: 0.7967\n",
      "8/15, train_loss: 0.8994\n",
      "9/15, train_loss: 0.9056\n",
      "10/15, train_loss: 0.7835\n",
      "11/15, train_loss: 0.7599\n",
      "12/15, train_loss: 0.9544\n",
      "13/15, train_loss: 0.6909\n",
      "14/15, train_loss: 0.8426\n",
      "15/15, train_loss: 0.7131\n",
      "16/15, train_loss: 0.6550\n",
      "epoch 67 average loss: 0.9037\n",
      "----------\n",
      "epoch 68/500\n",
      "1/15, train_loss: 1.0558\n",
      "2/15, train_loss: 0.6587\n",
      "3/15, train_loss: 1.4462\n",
      "4/15, train_loss: 0.9347\n",
      "5/15, train_loss: 0.6898\n",
      "6/15, train_loss: 0.7287\n",
      "7/15, train_loss: 0.8156\n",
      "8/15, train_loss: 0.7091\n",
      "9/15, train_loss: 0.8536\n",
      "10/15, train_loss: 0.6469\n",
      "11/15, train_loss: 1.0559\n",
      "12/15, train_loss: 0.9060\n",
      "13/15, train_loss: 0.7088\n",
      "14/15, train_loss: 0.7488\n",
      "15/15, train_loss: 0.6691\n",
      "16/15, train_loss: 2.6784\n",
      "epoch 68 average loss: 0.9566\n",
      "----------\n",
      "epoch 69/500\n",
      "1/15, train_loss: 0.7616\n",
      "2/15, train_loss: 1.7933\n",
      "3/15, train_loss: 0.6821\n",
      "4/15, train_loss: 0.8726\n",
      "5/15, train_loss: 0.7158\n",
      "6/15, train_loss: 0.6109\n",
      "7/15, train_loss: 0.7692\n",
      "8/15, train_loss: 0.6628\n",
      "9/15, train_loss: 0.7035\n",
      "10/15, train_loss: 0.8222\n",
      "11/15, train_loss: 1.0252\n",
      "12/15, train_loss: 0.8067\n",
      "13/15, train_loss: 0.6857\n",
      "14/15, train_loss: 0.9178\n",
      "15/15, train_loss: 0.9544\n",
      "16/15, train_loss: 2.1874\n",
      "epoch 69 average loss: 0.9357\n",
      "----------\n",
      "epoch 70/500\n",
      "1/15, train_loss: 0.6792\n",
      "2/15, train_loss: 0.7533\n",
      "3/15, train_loss: 0.6255\n",
      "4/15, train_loss: 1.3533\n",
      "5/15, train_loss: 0.7456\n",
      "6/15, train_loss: 0.8535\n",
      "7/15, train_loss: 0.7611\n",
      "8/15, train_loss: 0.7935\n",
      "9/15, train_loss: 0.9452\n",
      "10/15, train_loss: 0.6320\n",
      "11/15, train_loss: 0.7564\n",
      "12/15, train_loss: 0.6816\n",
      "13/15, train_loss: 0.8385\n",
      "14/15, train_loss: 0.9200\n",
      "15/15, train_loss: 0.6407\n",
      "16/15, train_loss: 1.3082\n",
      "epoch 70 average loss: 0.8305\n",
      "----------\n",
      "epoch 71/500\n",
      "1/15, train_loss: 1.6038\n",
      "2/15, train_loss: 0.7320\n",
      "3/15, train_loss: 0.7600\n",
      "4/15, train_loss: 0.6802\n",
      "5/15, train_loss: 1.0270\n",
      "6/15, train_loss: 0.7222\n",
      "7/15, train_loss: 0.7104\n",
      "8/15, train_loss: 1.2610\n",
      "9/15, train_loss: 0.8862\n",
      "10/15, train_loss: 1.0123\n",
      "11/15, train_loss: 0.7089\n",
      "12/15, train_loss: 0.8396\n",
      "13/15, train_loss: 0.7125\n",
      "14/15, train_loss: 1.0091\n",
      "15/15, train_loss: 1.2739\n",
      "16/15, train_loss: 2.3352\n",
      "epoch 71 average loss: 1.0171\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 71 validation loss: 1.0623 dice_score: 0.7993 acc_metric: 0.7017 accuracy: 0.7017, f1score: 0.7339\n",
      " saved Best PMetric: 0.7505 at epoch: 71\n",
      "----------\n",
      "epoch 72/500\n",
      "1/15, train_loss: 0.8091\n",
      "2/15, train_loss: 0.9172\n",
      "3/15, train_loss: 0.6476\n",
      "4/15, train_loss: 0.8461\n",
      "5/15, train_loss: 0.9447\n",
      "6/15, train_loss: 0.9572\n",
      "7/15, train_loss: 1.0377\n",
      "8/15, train_loss: 1.1154\n",
      "9/15, train_loss: 0.7314\n",
      "10/15, train_loss: 0.6962\n",
      "11/15, train_loss: 0.7347\n",
      "12/15, train_loss: 0.8311\n",
      "13/15, train_loss: 0.6025\n",
      "14/15, train_loss: 0.9583\n",
      "15/15, train_loss: 0.8854\n",
      "16/15, train_loss: 0.8781\n",
      "epoch 72 average loss: 0.8495\n",
      "----------\n",
      "epoch 73/500\n",
      "1/15, train_loss: 0.8578\n",
      "2/15, train_loss: 0.9533\n",
      "3/15, train_loss: 0.7853\n",
      "4/15, train_loss: 1.0596\n",
      "5/15, train_loss: 0.8741\n",
      "6/15, train_loss: 0.7479\n",
      "7/15, train_loss: 1.5707\n",
      "8/15, train_loss: 0.9215\n",
      "9/15, train_loss: 0.7501\n",
      "10/15, train_loss: 0.6155\n",
      "11/15, train_loss: 0.8213\n",
      "12/15, train_loss: 0.8691\n",
      "13/15, train_loss: 0.8774\n",
      "14/15, train_loss: 0.8133\n",
      "15/15, train_loss: 0.7298\n",
      "16/15, train_loss: 1.3170\n",
      "epoch 73 average loss: 0.9102\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 73 validation loss: 1.0299 dice_score: 0.8007 acc_metric: 0.7080 accuracy: 0.7080, f1score: 0.7419\n",
      " saved Best PMetric: 0.7543 at epoch: 73\n",
      "----------\n",
      "epoch 74/500\n",
      "1/15, train_loss: 0.6754\n",
      "2/15, train_loss: 0.6738\n",
      "3/15, train_loss: 1.0145\n",
      "4/15, train_loss: 0.7584\n",
      "5/15, train_loss: 0.9560\n",
      "6/15, train_loss: 0.8031\n",
      "7/15, train_loss: 1.0654\n",
      "8/15, train_loss: 0.6686\n",
      "9/15, train_loss: 0.7244\n",
      "10/15, train_loss: 0.7650\n",
      "11/15, train_loss: 0.9107\n",
      "12/15, train_loss: 0.7736\n",
      "13/15, train_loss: 0.8632\n",
      "14/15, train_loss: 0.6340\n",
      "15/15, train_loss: 0.6485\n",
      "16/15, train_loss: 2.6023\n",
      "epoch 74 average loss: 0.9086\n",
      "----------\n",
      "epoch 75/500\n",
      "1/15, train_loss: 0.9248\n",
      "2/15, train_loss: 0.6083\n",
      "3/15, train_loss: 0.6283\n",
      "4/15, train_loss: 0.6707\n",
      "5/15, train_loss: 0.6456\n",
      "6/15, train_loss: 0.7080\n",
      "7/15, train_loss: 0.7105\n",
      "8/15, train_loss: 0.5962\n",
      "9/15, train_loss: 0.7275\n",
      "10/15, train_loss: 0.7481\n",
      "11/15, train_loss: 0.7878\n",
      "12/15, train_loss: 0.7525\n",
      "13/15, train_loss: 0.6687\n",
      "14/15, train_loss: 1.0781\n",
      "15/15, train_loss: 0.8572\n",
      "16/15, train_loss: 1.9257\n",
      "epoch 75 average loss: 0.8149\n",
      "----------\n",
      "epoch 76/500\n",
      "1/15, train_loss: 0.6754\n",
      "2/15, train_loss: 0.6356\n",
      "3/15, train_loss: 0.8796\n",
      "4/15, train_loss: 2.1861\n",
      "5/15, train_loss: 0.6910\n",
      "6/15, train_loss: 0.5629\n",
      "7/15, train_loss: 0.7077\n",
      "8/15, train_loss: 0.7076\n",
      "9/15, train_loss: 0.8127\n",
      "10/15, train_loss: 0.7811\n",
      "11/15, train_loss: 0.9000\n",
      "12/15, train_loss: 0.7040\n",
      "13/15, train_loss: 1.0495\n",
      "14/15, train_loss: 0.8659\n",
      "15/15, train_loss: 0.7433\n",
      "16/15, train_loss: 0.5375\n",
      "epoch 76 average loss: 0.8400\n",
      "----------\n",
      "epoch 77/500\n",
      "1/15, train_loss: 0.6502\n",
      "2/15, train_loss: 1.5922\n",
      "3/15, train_loss: 0.8894\n",
      "4/15, train_loss: 0.6680\n",
      "5/15, train_loss: 0.6704\n",
      "6/15, train_loss: 0.5911\n",
      "7/15, train_loss: 0.7670\n",
      "8/15, train_loss: 0.6225\n",
      "9/15, train_loss: 0.8017\n",
      "10/15, train_loss: 0.7447\n",
      "11/15, train_loss: 0.5863\n",
      "12/15, train_loss: 0.6373\n",
      "13/15, train_loss: 2.0421\n",
      "14/15, train_loss: 1.4224\n",
      "15/15, train_loss: 0.6307\n",
      "16/15, train_loss: 2.4243\n",
      "epoch 77 average loss: 0.9838\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 77 validation loss: 1.0323 dice_score: 0.7979 acc_metric: 0.7159 accuracy: 0.7159, f1score: 0.7258\n",
      " saved Best PMetric: 0.7569 at epoch: 77\n",
      "----------\n",
      "epoch 78/500\n",
      "1/15, train_loss: 0.7489\n",
      "2/15, train_loss: 0.9370\n",
      "3/15, train_loss: 1.0832\n",
      "4/15, train_loss: 0.6857\n",
      "5/15, train_loss: 0.6945\n",
      "6/15, train_loss: 0.9219\n",
      "7/15, train_loss: 0.7793\n",
      "8/15, train_loss: 0.7596\n",
      "9/15, train_loss: 0.9723\n",
      "10/15, train_loss: 0.6087\n",
      "11/15, train_loss: 0.6461\n",
      "12/15, train_loss: 0.5657\n",
      "13/15, train_loss: 0.6475\n",
      "14/15, train_loss: 0.5603\n",
      "15/15, train_loss: 0.6825\n",
      "16/15, train_loss: 2.6949\n",
      "epoch 78 average loss: 0.8743\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 78 validation loss: 1.0033 dice_score: 0.8104 acc_metric: 0.7131 accuracy: 0.7131, f1score: 0.7419\n",
      " saved Best PMetric: 0.7617 at epoch: 78\n",
      "----------\n",
      "epoch 79/500\n",
      "1/15, train_loss: 0.8196\n",
      "2/15, train_loss: 0.8350\n",
      "3/15, train_loss: 0.6137\n",
      "4/15, train_loss: 0.7165\n",
      "5/15, train_loss: 0.8592\n",
      "6/15, train_loss: 0.7073\n",
      "7/15, train_loss: 0.8044\n",
      "8/15, train_loss: 0.5464\n",
      "9/15, train_loss: 1.0449\n",
      "10/15, train_loss: 0.9770\n",
      "11/15, train_loss: 0.5920\n",
      "12/15, train_loss: 0.6401\n",
      "13/15, train_loss: 0.5915\n",
      "14/15, train_loss: 0.6387\n",
      "15/15, train_loss: 0.6286\n",
      "16/15, train_loss: 2.5240\n",
      "epoch 79 average loss: 0.8462\n",
      "----------\n",
      "epoch 80/500\n",
      "1/15, train_loss: 0.7165\n",
      "2/15, train_loss: 0.7179\n",
      "3/15, train_loss: 0.6811\n",
      "4/15, train_loss: 0.9265\n",
      "5/15, train_loss: 0.8073\n",
      "6/15, train_loss: 0.7995\n",
      "7/15, train_loss: 0.6081\n",
      "8/15, train_loss: 0.6702\n",
      "9/15, train_loss: 0.8502\n",
      "10/15, train_loss: 0.5402\n",
      "11/15, train_loss: 0.7346\n",
      "12/15, train_loss: 0.9662\n",
      "13/15, train_loss: 0.5993\n",
      "14/15, train_loss: 1.2630\n",
      "15/15, train_loss: 0.7434\n",
      "16/15, train_loss: 2.3729\n",
      "epoch 80 average loss: 0.8748\n",
      "----------\n",
      "epoch 81/500\n",
      "1/15, train_loss: 1.0404\n",
      "2/15, train_loss: 0.5697\n",
      "3/15, train_loss: 0.7743\n",
      "4/15, train_loss: 0.6865\n",
      "5/15, train_loss: 0.6889\n",
      "6/15, train_loss: 0.9060\n",
      "7/15, train_loss: 0.7209\n",
      "8/15, train_loss: 0.6430\n",
      "9/15, train_loss: 1.1273\n",
      "10/15, train_loss: 0.9960\n",
      "11/15, train_loss: 0.6141\n",
      "12/15, train_loss: 0.6015\n",
      "13/15, train_loss: 0.8693\n",
      "14/15, train_loss: 0.8293\n",
      "15/15, train_loss: 0.9189\n",
      "16/15, train_loss: 0.5609\n",
      "epoch 81 average loss: 0.7842\n",
      "----------\n",
      "epoch 82/500\n",
      "1/15, train_loss: 0.5978\n",
      "2/15, train_loss: 0.8587\n",
      "3/15, train_loss: 0.5317\n",
      "4/15, train_loss: 1.0580\n",
      "5/15, train_loss: 0.7208\n",
      "6/15, train_loss: 1.2678\n",
      "7/15, train_loss: 0.6007\n",
      "8/15, train_loss: 0.7691\n",
      "9/15, train_loss: 0.8344\n",
      "10/15, train_loss: 0.6143\n",
      "11/15, train_loss: 0.6991\n",
      "12/15, train_loss: 0.8033\n",
      "13/15, train_loss: 0.9425\n",
      "14/15, train_loss: 0.8165\n",
      "15/15, train_loss: 0.5988\n",
      "16/15, train_loss: 2.8266\n",
      "epoch 82 average loss: 0.9088\n",
      "----------\n",
      "epoch 83/500\n",
      "1/15, train_loss: 0.8368\n",
      "2/15, train_loss: 1.1721\n",
      "3/15, train_loss: 1.0980\n",
      "4/15, train_loss: 0.5737\n",
      "5/15, train_loss: 0.7001\n",
      "6/15, train_loss: 2.4434\n",
      "7/15, train_loss: 0.7361\n",
      "8/15, train_loss: 0.6934\n",
      "9/15, train_loss: 0.5968\n",
      "10/15, train_loss: 0.6101\n",
      "11/15, train_loss: 0.6535\n",
      "12/15, train_loss: 0.5776\n",
      "13/15, train_loss: 0.6928\n",
      "14/15, train_loss: 0.8656\n",
      "15/15, train_loss: 0.8591\n",
      "16/15, train_loss: 2.7537\n",
      "epoch 83 average loss: 0.9914\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 83 validation loss: 1.0071 dice_score: 0.8071 acc_metric: 0.7102 accuracy: 0.7102, f1score: 0.7581\n",
      " saved Best PMetric: 0.7586 at epoch: 83\n",
      "----------\n",
      "epoch 84/500\n",
      "1/15, train_loss: 0.6137\n",
      "2/15, train_loss: 0.6085\n",
      "3/15, train_loss: 0.7224\n",
      "4/15, train_loss: 0.5861\n",
      "5/15, train_loss: 0.6864\n",
      "6/15, train_loss: 0.7144\n",
      "7/15, train_loss: 0.8936\n",
      "8/15, train_loss: 1.5107\n",
      "9/15, train_loss: 0.6655\n",
      "10/15, train_loss: 1.0795\n",
      "11/15, train_loss: 0.6737\n",
      "12/15, train_loss: 0.6815\n",
      "13/15, train_loss: 0.6774\n",
      "14/15, train_loss: 0.9507\n",
      "15/15, train_loss: 0.8714\n",
      "16/15, train_loss: 2.3355\n",
      "epoch 84 average loss: 0.8919\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 84 validation loss: 1.0084 dice_score: 0.8124 acc_metric: 0.7284 accuracy: 0.7284, f1score: 0.7419\n",
      " saved Best PMetric: 0.7704 at epoch: 84\n",
      "----------\n",
      "epoch 85/500\n",
      "1/15, train_loss: 0.8794\n",
      "2/15, train_loss: 0.5705\n",
      "3/15, train_loss: 0.6166\n",
      "4/15, train_loss: 0.7327\n",
      "5/15, train_loss: 0.5537\n",
      "6/15, train_loss: 0.8207\n",
      "7/15, train_loss: 0.5892\n",
      "8/15, train_loss: 0.5710\n",
      "9/15, train_loss: 1.1227\n",
      "10/15, train_loss: 0.6469\n",
      "11/15, train_loss: 0.6752\n",
      "12/15, train_loss: 0.5903\n",
      "13/15, train_loss: 1.1565\n",
      "14/15, train_loss: 0.5891\n",
      "15/15, train_loss: 0.5451\n",
      "16/15, train_loss: 2.9236\n",
      "epoch 85 average loss: 0.8490\n",
      "----------\n",
      "epoch 86/500\n",
      "1/15, train_loss: 0.5830\n",
      "2/15, train_loss: 0.5813\n",
      "3/15, train_loss: 0.5543\n",
      "4/15, train_loss: 0.7418\n",
      "5/15, train_loss: 0.6926\n",
      "6/15, train_loss: 0.5935\n",
      "7/15, train_loss: 0.7417\n",
      "8/15, train_loss: 0.7966\n",
      "9/15, train_loss: 1.0467\n",
      "10/15, train_loss: 0.6166\n",
      "11/15, train_loss: 0.5625\n",
      "12/15, train_loss: 0.8082\n",
      "13/15, train_loss: 0.7254\n",
      "14/15, train_loss: 1.0711\n",
      "15/15, train_loss: 0.6730\n",
      "16/15, train_loss: 1.0462\n",
      "epoch 86 average loss: 0.7397\n",
      "----------\n",
      "epoch 87/500\n",
      "1/15, train_loss: 0.6999\n",
      "2/15, train_loss: 1.1006\n",
      "3/15, train_loss: 2.5886\n",
      "4/15, train_loss: 0.7929\n",
      "5/15, train_loss: 0.5682\n",
      "6/15, train_loss: 0.5399\n",
      "7/15, train_loss: 0.5947\n",
      "8/15, train_loss: 0.5358\n",
      "9/15, train_loss: 0.6017\n",
      "10/15, train_loss: 0.5982\n",
      "11/15, train_loss: 0.5332\n",
      "12/15, train_loss: 0.6399\n",
      "13/15, train_loss: 0.5628\n",
      "14/15, train_loss: 0.6980\n",
      "15/15, train_loss: 0.6614\n",
      "16/15, train_loss: 0.5602\n",
      "epoch 87 average loss: 0.7672\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 87 validation loss: 0.9924 dice_score: 0.8041 acc_metric: 0.7307 accuracy: 0.7307, f1score: 0.7581\n",
      " saved Best PMetric: 0.7674 at epoch: 87\n",
      "----------\n",
      "epoch 88/500\n",
      "1/15, train_loss: 0.6544\n",
      "2/15, train_loss: 0.6089\n",
      "3/15, train_loss: 0.6640\n",
      "4/15, train_loss: 0.4943\n",
      "5/15, train_loss: 0.5961\n",
      "6/15, train_loss: 0.6657\n",
      "7/15, train_loss: 0.6511\n",
      "8/15, train_loss: 0.7957\n",
      "9/15, train_loss: 0.6696\n",
      "10/15, train_loss: 0.5484\n",
      "11/15, train_loss: 0.9399\n",
      "12/15, train_loss: 0.6586\n",
      "13/15, train_loss: 0.7014\n",
      "14/15, train_loss: 0.5596\n",
      "15/15, train_loss: 0.5155\n",
      "16/15, train_loss: 0.6919\n",
      "epoch 88 average loss: 0.6509\n",
      "----------\n",
      "epoch 89/500\n",
      "1/15, train_loss: 0.5164\n",
      "2/15, train_loss: 0.8679\n",
      "3/15, train_loss: 0.8699\n",
      "4/15, train_loss: 0.7202\n",
      "5/15, train_loss: 0.5713\n",
      "6/15, train_loss: 0.5261\n",
      "7/15, train_loss: 0.5471\n",
      "8/15, train_loss: 0.6115\n",
      "9/15, train_loss: 0.7292\n",
      "10/15, train_loss: 0.8536\n",
      "11/15, train_loss: 0.9941\n",
      "12/15, train_loss: 0.6952\n",
      "13/15, train_loss: 0.6006\n",
      "14/15, train_loss: 0.4891\n",
      "15/15, train_loss: 0.9334\n",
      "16/15, train_loss: 1.9759\n",
      "epoch 89 average loss: 0.7813\n",
      "----------\n",
      "epoch 90/500\n",
      "1/15, train_loss: 0.8305\n",
      "2/15, train_loss: 0.5284\n",
      "3/15, train_loss: 0.5325\n",
      "4/15, train_loss: 0.5586\n",
      "5/15, train_loss: 0.7053\n",
      "6/15, train_loss: 0.8861\n",
      "7/15, train_loss: 0.6016\n",
      "8/15, train_loss: 0.5131\n",
      "9/15, train_loss: 0.8630\n",
      "10/15, train_loss: 0.4977\n",
      "11/15, train_loss: 0.5782\n",
      "12/15, train_loss: 0.5526\n",
      "13/15, train_loss: 1.0490\n",
      "14/15, train_loss: 0.6339\n",
      "15/15, train_loss: 0.7206\n",
      "16/15, train_loss: 2.3390\n",
      "epoch 90 average loss: 0.7744\n",
      "----------\n",
      "epoch 91/500\n",
      "1/15, train_loss: 0.5759\n",
      "2/15, train_loss: 0.6134\n",
      "3/15, train_loss: 0.6049\n",
      "4/15, train_loss: 1.8255\n",
      "5/15, train_loss: 1.0707\n",
      "6/15, train_loss: 0.6620\n",
      "7/15, train_loss: 0.4736\n",
      "8/15, train_loss: 0.6137\n",
      "9/15, train_loss: 0.8409\n",
      "10/15, train_loss: 0.8828\n",
      "11/15, train_loss: 0.6065\n",
      "12/15, train_loss: 0.6527\n",
      "13/15, train_loss: 1.6774\n",
      "14/15, train_loss: 0.5663\n",
      "15/15, train_loss: 0.5169\n",
      "16/15, train_loss: 2.3999\n",
      "epoch 91 average loss: 0.9115\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 91 validation loss: 1.0028 dice_score: 0.8248 acc_metric: 0.7119 accuracy: 0.7119, f1score: 0.7339\n",
      " saved Best PMetric: 0.7683 at epoch: 91\n",
      "----------\n",
      "epoch 92/500\n",
      "1/15, train_loss: 0.9257\n",
      "2/15, train_loss: 0.6612\n",
      "3/15, train_loss: 0.5561\n",
      "4/15, train_loss: 0.7367\n",
      "5/15, train_loss: 0.6864\n",
      "6/15, train_loss: 0.5955\n",
      "7/15, train_loss: 0.9810\n",
      "8/15, train_loss: 0.5861\n",
      "9/15, train_loss: 0.7264\n",
      "10/15, train_loss: 0.4958\n",
      "11/15, train_loss: 0.5968\n",
      "12/15, train_loss: 0.5758\n",
      "13/15, train_loss: 2.7542\n",
      "14/15, train_loss: 0.7390\n",
      "15/15, train_loss: 0.7465\n",
      "16/15, train_loss: 1.4150\n",
      "epoch 92 average loss: 0.8611\n",
      "----------\n",
      "epoch 93/500\n",
      "1/15, train_loss: 0.5403\n",
      "2/15, train_loss: 0.7127\n",
      "3/15, train_loss: 0.5883\n",
      "4/15, train_loss: 0.5565\n",
      "5/15, train_loss: 0.5004\n",
      "6/15, train_loss: 0.9777\n",
      "7/15, train_loss: 1.1637\n",
      "8/15, train_loss: 0.8388\n",
      "9/15, train_loss: 0.5349\n",
      "10/15, train_loss: 0.8077\n",
      "11/15, train_loss: 0.5904\n",
      "12/15, train_loss: 0.8985\n",
      "13/15, train_loss: 0.6833\n",
      "14/15, train_loss: 0.6298\n",
      "15/15, train_loss: 0.5633\n",
      "16/15, train_loss: 2.8747\n",
      "epoch 93 average loss: 0.8413\n",
      "----------\n",
      "epoch 94/500\n",
      "1/15, train_loss: 0.6214\n",
      "2/15, train_loss: 0.5523\n",
      "3/15, train_loss: 0.5330\n",
      "4/15, train_loss: 0.5316\n",
      "5/15, train_loss: 0.5187\n",
      "6/15, train_loss: 0.7666\n",
      "7/15, train_loss: 0.5340\n",
      "8/15, train_loss: 0.7057\n",
      "9/15, train_loss: 0.8629\n",
      "10/15, train_loss: 1.4923\n",
      "11/15, train_loss: 0.4622\n",
      "12/15, train_loss: 0.6971\n",
      "13/15, train_loss: 0.7433\n",
      "14/15, train_loss: 0.5785\n",
      "15/15, train_loss: 0.5543\n",
      "16/15, train_loss: 2.4582\n",
      "epoch 94 average loss: 0.7883\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 94 validation loss: 0.9781 dice_score: 0.8097 acc_metric: 0.7307 accuracy: 0.7307, f1score: 0.7581\n",
      " saved Best PMetric: 0.7702 at epoch: 94\n",
      "----------\n",
      "epoch 95/500\n",
      "1/15, train_loss: 0.6107\n",
      "2/15, train_loss: 0.5039\n",
      "3/15, train_loss: 0.5256\n",
      "4/15, train_loss: 0.6986\n",
      "5/15, train_loss: 0.6378\n",
      "6/15, train_loss: 1.5746\n",
      "7/15, train_loss: 0.7008\n",
      "8/15, train_loss: 0.9772\n",
      "9/15, train_loss: 1.6469\n",
      "10/15, train_loss: 0.5184\n",
      "11/15, train_loss: 0.8220\n",
      "12/15, train_loss: 0.5988\n",
      "13/15, train_loss: 0.7394\n",
      "14/15, train_loss: 0.9254\n",
      "15/15, train_loss: 0.4470\n",
      "16/15, train_loss: 2.6897\n",
      "epoch 95 average loss: 0.9136\n",
      "----------\n",
      "epoch 96/500\n",
      "1/15, train_loss: 0.5114\n",
      "2/15, train_loss: 0.5989\n",
      "3/15, train_loss: 0.6137\n",
      "4/15, train_loss: 0.5837\n",
      "5/15, train_loss: 0.6136\n",
      "6/15, train_loss: 0.9034\n",
      "7/15, train_loss: 0.5041\n",
      "8/15, train_loss: 0.6374\n",
      "9/15, train_loss: 1.5214\n",
      "10/15, train_loss: 0.6869\n",
      "11/15, train_loss: 0.5140\n",
      "12/15, train_loss: 0.5406\n",
      "13/15, train_loss: 0.6350\n",
      "14/15, train_loss: 0.4770\n",
      "15/15, train_loss: 0.6779\n",
      "16/15, train_loss: 2.7558\n",
      "epoch 96 average loss: 0.7984\n",
      "----------\n",
      "epoch 97/500\n",
      "1/15, train_loss: 0.4633\n",
      "2/15, train_loss: 1.1096\n",
      "3/15, train_loss: 0.5691\n",
      "4/15, train_loss: 0.6158\n",
      "5/15, train_loss: 0.5100\n",
      "6/15, train_loss: 0.5272\n",
      "7/15, train_loss: 0.6779\n",
      "8/15, train_loss: 0.5382\n",
      "9/15, train_loss: 0.9148\n",
      "10/15, train_loss: 0.5937\n",
      "11/15, train_loss: 0.5175\n",
      "12/15, train_loss: 0.7784\n",
      "13/15, train_loss: 0.5524\n",
      "14/15, train_loss: 0.6346\n",
      "15/15, train_loss: 0.5897\n",
      "16/15, train_loss: 2.3861\n",
      "epoch 97 average loss: 0.7486\n",
      "----------\n",
      "epoch 98/500\n",
      "1/15, train_loss: 1.0210\n",
      "2/15, train_loss: 0.5870\n",
      "3/15, train_loss: 1.0158\n",
      "4/15, train_loss: 0.6432\n",
      "5/15, train_loss: 0.6902\n",
      "6/15, train_loss: 2.0318\n",
      "7/15, train_loss: 0.4564\n",
      "8/15, train_loss: 0.6443\n",
      "9/15, train_loss: 0.5705\n",
      "10/15, train_loss: 0.6096\n",
      "11/15, train_loss: 0.4956\n",
      "12/15, train_loss: 0.8526\n",
      "13/15, train_loss: 0.8055\n",
      "14/15, train_loss: 0.6529\n",
      "15/15, train_loss: 0.5856\n",
      "16/15, train_loss: 0.4757\n",
      "epoch 98 average loss: 0.7586\n",
      "----------\n",
      "epoch 99/500\n",
      "1/15, train_loss: 0.7801\n",
      "2/15, train_loss: 0.8813\n",
      "3/15, train_loss: 0.5309\n",
      "4/15, train_loss: 0.5273\n",
      "5/15, train_loss: 0.6891\n",
      "6/15, train_loss: 0.7186\n",
      "7/15, train_loss: 0.7964\n",
      "8/15, train_loss: 0.5000\n",
      "9/15, train_loss: 0.5094\n",
      "10/15, train_loss: 0.5283\n",
      "11/15, train_loss: 0.6031\n",
      "12/15, train_loss: 0.5546\n",
      "13/15, train_loss: 0.7263\n",
      "14/15, train_loss: 0.5682\n",
      "15/15, train_loss: 0.6027\n",
      "16/15, train_loss: 2.2753\n",
      "epoch 99 average loss: 0.7370\n",
      "----------\n",
      "epoch 100/500\n",
      "1/15, train_loss: 1.7630\n",
      "2/15, train_loss: 0.6386\n",
      "3/15, train_loss: 0.4498\n",
      "4/15, train_loss: 0.7056\n",
      "5/15, train_loss: 0.7168\n",
      "6/15, train_loss: 0.8991\n",
      "7/15, train_loss: 0.5006\n",
      "8/15, train_loss: 0.4648\n",
      "9/15, train_loss: 0.6734\n",
      "10/15, train_loss: 0.8323\n",
      "11/15, train_loss: 0.5402\n",
      "12/15, train_loss: 0.7052\n",
      "13/15, train_loss: 0.9596\n",
      "14/15, train_loss: 0.5191\n",
      "15/15, train_loss: 0.8755\n",
      "16/15, train_loss: 2.3714\n",
      "epoch 100 average loss: 0.8509\n",
      "----------\n",
      "epoch 101/500\n",
      "1/15, train_loss: 0.5447\n",
      "2/15, train_loss: 0.4958\n",
      "3/15, train_loss: 0.6553\n",
      "4/15, train_loss: 0.6303\n",
      "5/15, train_loss: 0.4682\n",
      "6/15, train_loss: 0.7182\n",
      "7/15, train_loss: 0.5132\n",
      "8/15, train_loss: 0.5754\n",
      "9/15, train_loss: 0.5736\n",
      "10/15, train_loss: 0.6095\n",
      "11/15, train_loss: 0.6667\n",
      "12/15, train_loss: 1.1914\n",
      "13/15, train_loss: 0.6287\n",
      "14/15, train_loss: 0.4645\n",
      "15/15, train_loss: 0.5565\n",
      "16/15, train_loss: 2.6312\n",
      "epoch 101 average loss: 0.7452\n",
      "----------\n",
      "epoch 102/500\n",
      "1/15, train_loss: 0.5940\n",
      "2/15, train_loss: 0.5100\n",
      "3/15, train_loss: 0.5146\n",
      "4/15, train_loss: 0.5115\n",
      "5/15, train_loss: 0.5293\n",
      "6/15, train_loss: 0.5937\n",
      "7/15, train_loss: 0.5214\n",
      "8/15, train_loss: 0.5985\n",
      "9/15, train_loss: 0.6065\n",
      "10/15, train_loss: 0.4793\n",
      "11/15, train_loss: 0.8708\n",
      "12/15, train_loss: 0.7958\n",
      "13/15, train_loss: 0.5361\n",
      "14/15, train_loss: 0.5953\n",
      "15/15, train_loss: 0.6396\n",
      "16/15, train_loss: 0.5249\n",
      "epoch 102 average loss: 0.5888\n",
      "----------\n",
      "epoch 103/500\n",
      "1/15, train_loss: 0.7188\n",
      "2/15, train_loss: 0.5818\n",
      "3/15, train_loss: 0.4664\n",
      "4/15, train_loss: 0.4796\n",
      "5/15, train_loss: 0.5185\n",
      "6/15, train_loss: 0.8109\n",
      "7/15, train_loss: 0.7022\n",
      "8/15, train_loss: 1.3442\n",
      "9/15, train_loss: 0.6865\n",
      "10/15, train_loss: 0.5988\n",
      "11/15, train_loss: 0.4980\n",
      "12/15, train_loss: 0.5947\n",
      "13/15, train_loss: 0.5717\n",
      "14/15, train_loss: 0.7420\n",
      "15/15, train_loss: 0.5134\n",
      "16/15, train_loss: 1.0051\n",
      "epoch 103 average loss: 0.6770\n",
      "----------\n",
      "epoch 104/500\n",
      "1/15, train_loss: 0.5334\n",
      "2/15, train_loss: 0.6489\n",
      "3/15, train_loss: 0.6760\n",
      "4/15, train_loss: 0.8525\n",
      "5/15, train_loss: 0.5354\n",
      "6/15, train_loss: 0.5629\n",
      "7/15, train_loss: 0.8005\n",
      "8/15, train_loss: 0.4900\n",
      "9/15, train_loss: 0.4572\n",
      "10/15, train_loss: 0.6397\n",
      "11/15, train_loss: 0.7698\n",
      "12/15, train_loss: 0.5925\n",
      "13/15, train_loss: 0.4586\n",
      "14/15, train_loss: 0.4524\n",
      "15/15, train_loss: 0.5114\n",
      "16/15, train_loss: 2.5561\n",
      "epoch 104 average loss: 0.7211\n",
      "----------\n",
      "epoch 105/500\n",
      "1/15, train_loss: 0.4784\n",
      "2/15, train_loss: 1.7416\n",
      "3/15, train_loss: 0.4700\n",
      "4/15, train_loss: 0.6579\n",
      "5/15, train_loss: 1.6525\n",
      "6/15, train_loss: 0.7501\n",
      "7/15, train_loss: 0.6478\n",
      "8/15, train_loss: 0.4762\n",
      "9/15, train_loss: 0.5350\n",
      "10/15, train_loss: 0.4793\n",
      "11/15, train_loss: 0.5691\n",
      "12/15, train_loss: 0.8157\n",
      "13/15, train_loss: 0.5570\n",
      "14/15, train_loss: 0.6886\n",
      "15/15, train_loss: 1.0256\n",
      "16/15, train_loss: 0.4610\n",
      "epoch 105 average loss: 0.7504\n",
      "----------\n",
      "epoch 106/500\n",
      "1/15, train_loss: 0.5237\n",
      "2/15, train_loss: 0.4462\n",
      "3/15, train_loss: 0.7610\n",
      "4/15, train_loss: 0.5796\n",
      "5/15, train_loss: 0.5073\n",
      "6/15, train_loss: 0.4576\n",
      "7/15, train_loss: 0.4602\n",
      "8/15, train_loss: 0.4816\n",
      "9/15, train_loss: 0.5374\n",
      "10/15, train_loss: 0.5037\n",
      "11/15, train_loss: 0.6359\n",
      "12/15, train_loss: 0.5053\n",
      "13/15, train_loss: 1.5914\n",
      "14/15, train_loss: 0.4682\n",
      "15/15, train_loss: 0.8224\n",
      "16/15, train_loss: 2.5611\n",
      "epoch 106 average loss: 0.7402\n",
      "----------\n",
      "epoch 107/500\n",
      "1/15, train_loss: 0.6944\n",
      "2/15, train_loss: 1.0841\n",
      "3/15, train_loss: 0.4230\n",
      "4/15, train_loss: 0.5139\n",
      "5/15, train_loss: 0.5607\n",
      "6/15, train_loss: 0.5662\n",
      "7/15, train_loss: 0.6067\n",
      "8/15, train_loss: 0.5853\n",
      "9/15, train_loss: 0.6006\n",
      "10/15, train_loss: 0.4342\n",
      "11/15, train_loss: 0.4750\n",
      "12/15, train_loss: 0.5532\n",
      "13/15, train_loss: 0.4972\n",
      "14/15, train_loss: 0.5197\n",
      "15/15, train_loss: 2.3500\n",
      "16/15, train_loss: 0.4252\n",
      "epoch 107 average loss: 0.6806\n",
      "----------\n",
      "epoch 108/500\n",
      "1/15, train_loss: 0.6723\n",
      "2/15, train_loss: 0.4982\n",
      "3/15, train_loss: 0.7250\n",
      "4/15, train_loss: 1.1021\n",
      "5/15, train_loss: 0.8717\n",
      "6/15, train_loss: 0.5506\n",
      "7/15, train_loss: 0.5077\n",
      "8/15, train_loss: 0.5028\n",
      "9/15, train_loss: 0.5253\n",
      "10/15, train_loss: 0.4829\n",
      "11/15, train_loss: 0.6416\n",
      "12/15, train_loss: 0.4352\n",
      "13/15, train_loss: 0.7181\n",
      "14/15, train_loss: 0.9831\n",
      "15/15, train_loss: 0.5097\n",
      "16/15, train_loss: 2.3709\n",
      "epoch 108 average loss: 0.7561\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 108 validation loss: 0.9637 dice_score: 0.8321 acc_metric: 0.7233 accuracy: 0.7233, f1score: 0.7419\n",
      " saved Best PMetric: 0.7777 at epoch: 108\n",
      "----------\n",
      "epoch 109/500\n",
      "1/15, train_loss: 0.4280\n",
      "2/15, train_loss: 0.5979\n",
      "3/15, train_loss: 0.5325\n",
      "4/15, train_loss: 0.6865\n",
      "5/15, train_loss: 0.7044\n",
      "6/15, train_loss: 0.5374\n",
      "7/15, train_loss: 0.7940\n",
      "8/15, train_loss: 0.5267\n",
      "9/15, train_loss: 0.4617\n",
      "10/15, train_loss: 0.6725\n",
      "11/15, train_loss: 0.4430\n",
      "12/15, train_loss: 0.6792\n",
      "13/15, train_loss: 0.4493\n",
      "14/15, train_loss: 0.9191\n",
      "15/15, train_loss: 0.9147\n",
      "16/15, train_loss: 0.3928\n",
      "epoch 109 average loss: 0.6087\n",
      "----------\n",
      "epoch 110/500\n",
      "1/15, train_loss: 0.7891\n",
      "2/15, train_loss: 0.4594\n",
      "3/15, train_loss: 0.7094\n",
      "4/15, train_loss: 0.7276\n",
      "5/15, train_loss: 0.5276\n",
      "6/15, train_loss: 0.6480\n",
      "7/15, train_loss: 0.6662\n",
      "8/15, train_loss: 0.4725\n",
      "9/15, train_loss: 0.7544\n",
      "10/15, train_loss: 0.5560\n",
      "11/15, train_loss: 0.6846\n",
      "12/15, train_loss: 0.5776\n",
      "13/15, train_loss: 0.5386\n",
      "14/15, train_loss: 0.4886\n",
      "15/15, train_loss: 0.4609\n",
      "16/15, train_loss: 2.8771\n",
      "epoch 110 average loss: 0.7461\n",
      "----------\n",
      "epoch 111/500\n",
      "1/15, train_loss: 0.4265\n",
      "2/15, train_loss: 0.4560\n",
      "3/15, train_loss: 0.6959\n",
      "4/15, train_loss: 0.5114\n",
      "5/15, train_loss: 1.0010\n",
      "6/15, train_loss: 0.5066\n",
      "7/15, train_loss: 0.4232\n",
      "8/15, train_loss: 0.7318\n",
      "9/15, train_loss: 0.3869\n",
      "10/15, train_loss: 0.5341\n",
      "11/15, train_loss: 0.4829\n",
      "12/15, train_loss: 0.4321\n",
      "13/15, train_loss: 0.5586\n",
      "14/15, train_loss: 0.5916\n",
      "15/15, train_loss: 1.5773\n",
      "16/15, train_loss: 0.4852\n",
      "epoch 111 average loss: 0.6126\n",
      "----------\n",
      "epoch 112/500\n",
      "1/15, train_loss: 0.5109\n",
      "2/15, train_loss: 0.5382\n",
      "3/15, train_loss: 0.5334\n",
      "4/15, train_loss: 0.5404\n",
      "5/15, train_loss: 0.4405\n",
      "6/15, train_loss: 0.6801\n",
      "7/15, train_loss: 1.1618\n",
      "8/15, train_loss: 0.4610\n",
      "9/15, train_loss: 0.5909\n",
      "10/15, train_loss: 0.4091\n",
      "11/15, train_loss: 0.5135\n",
      "12/15, train_loss: 0.7651\n",
      "13/15, train_loss: 0.3984\n",
      "14/15, train_loss: 0.8669\n",
      "15/15, train_loss: 0.7013\n",
      "16/15, train_loss: 2.3425\n",
      "epoch 112 average loss: 0.7159\n",
      "----------\n",
      "epoch 113/500\n",
      "1/15, train_loss: 0.5888\n",
      "2/15, train_loss: 0.7046\n",
      "3/15, train_loss: 0.6311\n",
      "4/15, train_loss: 0.5492\n",
      "5/15, train_loss: 0.4318\n",
      "6/15, train_loss: 0.9104\n",
      "7/15, train_loss: 0.7105\n",
      "8/15, train_loss: 0.5199\n",
      "9/15, train_loss: 0.6898\n",
      "10/15, train_loss: 0.8341\n",
      "11/15, train_loss: 0.4890\n",
      "12/15, train_loss: 0.4974\n",
      "13/15, train_loss: 0.7932\n",
      "14/15, train_loss: 0.5253\n",
      "15/15, train_loss: 0.5473\n",
      "16/15, train_loss: 0.4482\n",
      "epoch 113 average loss: 0.6169\n",
      "----------\n",
      "epoch 114/500\n",
      "1/15, train_loss: 0.4663\n",
      "2/15, train_loss: 0.7028\n",
      "3/15, train_loss: 0.4696\n",
      "4/15, train_loss: 0.5666\n",
      "5/15, train_loss: 0.5077\n",
      "6/15, train_loss: 0.4656\n",
      "7/15, train_loss: 0.4647\n",
      "8/15, train_loss: 0.4424\n",
      "9/15, train_loss: 0.5055\n",
      "10/15, train_loss: 0.4866\n",
      "11/15, train_loss: 0.4276\n",
      "12/15, train_loss: 0.5352\n",
      "13/15, train_loss: 0.4138\n",
      "14/15, train_loss: 0.4785\n",
      "15/15, train_loss: 1.1954\n",
      "16/15, train_loss: 2.4310\n",
      "epoch 114 average loss: 0.6600\n",
      "----------\n",
      "epoch 115/500\n",
      "1/15, train_loss: 0.5372\n",
      "2/15, train_loss: 0.4611\n",
      "3/15, train_loss: 0.4934\n",
      "4/15, train_loss: 0.5744\n",
      "5/15, train_loss: 0.4632\n",
      "6/15, train_loss: 0.6583\n",
      "7/15, train_loss: 0.4736\n",
      "8/15, train_loss: 1.3513\n",
      "9/15, train_loss: 0.4482\n",
      "10/15, train_loss: 0.5635\n",
      "11/15, train_loss: 0.5077\n",
      "12/15, train_loss: 0.4580\n",
      "13/15, train_loss: 0.5679\n",
      "14/15, train_loss: 1.1769\n",
      "15/15, train_loss: 0.5686\n",
      "16/15, train_loss: 2.4550\n",
      "epoch 115 average loss: 0.7349\n",
      "----------\n",
      "epoch 116/500\n",
      "1/15, train_loss: 0.4737\n",
      "2/15, train_loss: 0.4703\n",
      "3/15, train_loss: 0.4557\n",
      "4/15, train_loss: 0.4619\n",
      "5/15, train_loss: 0.5649\n",
      "6/15, train_loss: 0.4710\n",
      "7/15, train_loss: 0.5196\n",
      "8/15, train_loss: 0.6388\n",
      "9/15, train_loss: 0.4856\n",
      "10/15, train_loss: 0.6946\n",
      "11/15, train_loss: 0.4706\n",
      "12/15, train_loss: 0.5161\n",
      "13/15, train_loss: 0.8572\n",
      "14/15, train_loss: 0.5017\n",
      "15/15, train_loss: 0.4797\n",
      "16/15, train_loss: 0.5436\n",
      "epoch 116 average loss: 0.5378\n",
      "----------\n",
      "epoch 117/500\n",
      "1/15, train_loss: 0.4122\n",
      "2/15, train_loss: 0.5975\n",
      "3/15, train_loss: 0.4952\n",
      "4/15, train_loss: 1.2868\n",
      "5/15, train_loss: 0.5056\n",
      "6/15, train_loss: 0.6517\n",
      "7/15, train_loss: 1.1278\n",
      "8/15, train_loss: 0.4843\n",
      "9/15, train_loss: 0.4675\n",
      "10/15, train_loss: 0.4950\n",
      "11/15, train_loss: 0.4555\n",
      "12/15, train_loss: 0.5440\n",
      "13/15, train_loss: 0.8787\n",
      "14/15, train_loss: 1.2470\n",
      "15/15, train_loss: 0.5092\n",
      "16/15, train_loss: 0.3791\n",
      "epoch 117 average loss: 0.6586\n",
      "----------\n",
      "epoch 118/500\n",
      "1/15, train_loss: 0.7571\n",
      "2/15, train_loss: 0.4091\n",
      "3/15, train_loss: 0.6327\n",
      "4/15, train_loss: 0.8482\n",
      "5/15, train_loss: 0.4770\n",
      "6/15, train_loss: 0.5042\n",
      "7/15, train_loss: 0.6021\n",
      "8/15, train_loss: 0.5045\n",
      "9/15, train_loss: 0.5002\n",
      "10/15, train_loss: 0.4554\n",
      "11/15, train_loss: 0.7507\n",
      "12/15, train_loss: 0.9386\n",
      "13/15, train_loss: 0.4481\n",
      "14/15, train_loss: 0.5733\n",
      "15/15, train_loss: 0.3922\n",
      "16/15, train_loss: 2.8010\n",
      "epoch 118 average loss: 0.7247\n",
      "----------\n",
      "epoch 119/500\n",
      "1/15, train_loss: 0.4328\n",
      "2/15, train_loss: 0.4075\n",
      "3/15, train_loss: 0.7268\n",
      "4/15, train_loss: 0.7018\n",
      "5/15, train_loss: 0.5460\n",
      "6/15, train_loss: 0.5851\n",
      "7/15, train_loss: 0.6562\n",
      "8/15, train_loss: 0.4316\n",
      "9/15, train_loss: 0.4436\n",
      "10/15, train_loss: 0.5196\n",
      "11/15, train_loss: 0.5139\n",
      "12/15, train_loss: 0.7581\n",
      "13/15, train_loss: 0.4263\n",
      "14/15, train_loss: 0.6055\n",
      "15/15, train_loss: 0.6892\n",
      "16/15, train_loss: 0.4087\n",
      "epoch 119 average loss: 0.5533\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 119 validation loss: 0.9730 dice_score: 0.8328 acc_metric: 0.7216 accuracy: 0.7216, f1score: 0.7661\n",
      " saved Best PMetric: 0.7772 at epoch: 119\n",
      "----------\n",
      "epoch 120/500\n",
      "1/15, train_loss: 0.5375\n",
      "2/15, train_loss: 0.4567\n",
      "3/15, train_loss: 0.6726\n",
      "4/15, train_loss: 0.6955\n",
      "5/15, train_loss: 0.4928\n",
      "6/15, train_loss: 0.9569\n",
      "7/15, train_loss: 0.6197\n",
      "8/15, train_loss: 0.5521\n",
      "9/15, train_loss: 0.4992\n",
      "10/15, train_loss: 0.4900\n",
      "11/15, train_loss: 0.5338\n",
      "12/15, train_loss: 0.4741\n",
      "13/15, train_loss: 0.3917\n",
      "14/15, train_loss: 0.5987\n",
      "15/15, train_loss: 0.4223\n",
      "16/15, train_loss: 2.7492\n",
      "epoch 120 average loss: 0.6964\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 120 validation loss: 0.9717 dice_score: 0.8336 acc_metric: 0.7216 accuracy: 0.7216, f1score: 0.7661\n",
      " saved Best PMetric: 0.7776 at epoch: 120\n",
      "----------\n",
      "epoch 121/500\n",
      "1/15, train_loss: 0.4355\n",
      "2/15, train_loss: 0.6825\n",
      "3/15, train_loss: 0.4555\n",
      "4/15, train_loss: 0.4735\n",
      "5/15, train_loss: 0.5004\n",
      "6/15, train_loss: 0.4179\n",
      "7/15, train_loss: 0.3768\n",
      "8/15, train_loss: 0.5749\n",
      "9/15, train_loss: 0.9430\n",
      "10/15, train_loss: 0.4485\n",
      "11/15, train_loss: 0.3898\n",
      "12/15, train_loss: 0.4395\n",
      "13/15, train_loss: 0.6456\n",
      "14/15, train_loss: 0.6258\n",
      "15/15, train_loss: 0.4532\n",
      "16/15, train_loss: 0.3979\n",
      "epoch 121 average loss: 0.5163\n",
      "----------\n",
      "epoch 122/500\n",
      "1/15, train_loss: 0.4105\n",
      "2/15, train_loss: 0.5359\n",
      "3/15, train_loss: 0.7521\n",
      "4/15, train_loss: 0.4152\n",
      "5/15, train_loss: 0.6842\n",
      "6/15, train_loss: 1.8223\n",
      "7/15, train_loss: 0.5945\n",
      "8/15, train_loss: 0.4100\n",
      "9/15, train_loss: 0.8985\n",
      "10/15, train_loss: 0.4181\n",
      "11/15, train_loss: 0.5459\n",
      "12/15, train_loss: 1.0168\n",
      "13/15, train_loss: 0.4917\n",
      "14/15, train_loss: 0.5727\n",
      "15/15, train_loss: 0.5830\n",
      "16/15, train_loss: 2.6158\n",
      "epoch 122 average loss: 0.7980\n",
      "----------\n",
      "epoch 123/500\n",
      "1/15, train_loss: 0.4623\n",
      "2/15, train_loss: 0.4174\n",
      "3/15, train_loss: 0.4300\n",
      "4/15, train_loss: 1.5350\n",
      "5/15, train_loss: 0.4765\n",
      "6/15, train_loss: 0.5166\n",
      "7/15, train_loss: 1.0740\n",
      "8/15, train_loss: 0.4367\n",
      "9/15, train_loss: 0.6090\n",
      "10/15, train_loss: 0.5608\n",
      "11/15, train_loss: 0.5049\n",
      "12/15, train_loss: 0.4415\n",
      "13/15, train_loss: 0.5014\n",
      "14/15, train_loss: 0.5954\n",
      "15/15, train_loss: 0.4175\n",
      "16/15, train_loss: 2.4254\n",
      "epoch 123 average loss: 0.7128\n",
      "----------\n",
      "epoch 124/500\n",
      "1/15, train_loss: 0.4664\n",
      "2/15, train_loss: 0.5320\n",
      "3/15, train_loss: 0.4249\n",
      "4/15, train_loss: 0.4347\n",
      "5/15, train_loss: 0.4512\n",
      "6/15, train_loss: 0.4043\n",
      "7/15, train_loss: 0.5895\n",
      "8/15, train_loss: 0.5333\n",
      "9/15, train_loss: 0.3684\n",
      "10/15, train_loss: 0.5277\n",
      "11/15, train_loss: 0.4712\n",
      "12/15, train_loss: 0.5604\n",
      "13/15, train_loss: 1.1996\n",
      "14/15, train_loss: 0.5957\n",
      "15/15, train_loss: 0.5659\n",
      "16/15, train_loss: 2.2499\n",
      "epoch 124 average loss: 0.6484\n",
      "----------\n",
      "epoch 125/500\n",
      "1/15, train_loss: 0.5928\n",
      "2/15, train_loss: 1.0833\n",
      "3/15, train_loss: 0.4425\n",
      "4/15, train_loss: 0.8714\n",
      "5/15, train_loss: 0.4179\n",
      "6/15, train_loss: 0.4790\n",
      "7/15, train_loss: 0.4916\n",
      "8/15, train_loss: 0.4532\n",
      "9/15, train_loss: 0.7413\n",
      "10/15, train_loss: 0.3702\n",
      "11/15, train_loss: 0.4149\n",
      "12/15, train_loss: 0.4161\n",
      "13/15, train_loss: 0.3915\n",
      "14/15, train_loss: 0.6219\n",
      "15/15, train_loss: 0.3509\n",
      "16/15, train_loss: 2.2423\n",
      "epoch 125 average loss: 0.6488\n",
      "----------\n",
      "epoch 126/500\n",
      "1/15, train_loss: 0.4059\n",
      "\n",
      "** Ranger21 update = Warmup complete - lr set to 0.0001\n",
      "\n",
      "2/15, train_loss: 0.4165\n",
      "3/15, train_loss: 0.6122\n",
      "4/15, train_loss: 0.5724\n",
      "5/15, train_loss: 0.3973\n",
      "6/15, train_loss: 0.4895\n",
      "7/15, train_loss: 0.4335\n",
      "8/15, train_loss: 0.4739\n",
      "9/15, train_loss: 0.4650\n",
      "10/15, train_loss: 0.6198\n",
      "11/15, train_loss: 0.6843\n",
      "12/15, train_loss: 0.4145\n",
      "13/15, train_loss: 1.2471\n",
      "14/15, train_loss: 0.6219\n",
      "15/15, train_loss: 0.6593\n",
      "16/15, train_loss: 2.7503\n",
      "epoch 126 average loss: 0.7040\n",
      "----------\n",
      "epoch 127/500\n",
      "1/15, train_loss: 0.3898\n",
      "2/15, train_loss: 0.5231\n",
      "3/15, train_loss: 0.6012\n",
      "4/15, train_loss: 0.5233\n",
      "5/15, train_loss: 0.4220\n",
      "6/15, train_loss: 0.5769\n",
      "7/15, train_loss: 0.5007\n",
      "8/15, train_loss: 0.7220\n",
      "9/15, train_loss: 0.4596\n",
      "10/15, train_loss: 0.5113\n",
      "11/15, train_loss: 0.4430\n",
      "12/15, train_loss: 0.4633\n",
      "13/15, train_loss: 0.4389\n",
      "14/15, train_loss: 2.7722\n",
      "15/15, train_loss: 0.6496\n",
      "16/15, train_loss: 2.2025\n",
      "epoch 127 average loss: 0.7625\n",
      "----------\n",
      "epoch 128/500\n",
      "1/15, train_loss: 0.6969\n",
      "2/15, train_loss: 0.4339\n",
      "3/15, train_loss: 0.4357\n",
      "4/15, train_loss: 0.4695\n",
      "5/15, train_loss: 0.4862\n",
      "6/15, train_loss: 0.9119\n",
      "7/15, train_loss: 0.4413\n",
      "8/15, train_loss: 0.5342\n",
      "9/15, train_loss: 0.4405\n",
      "10/15, train_loss: 0.4214\n",
      "11/15, train_loss: 0.8164\n",
      "12/15, train_loss: 0.5809\n",
      "13/15, train_loss: 0.6430\n",
      "14/15, train_loss: 0.4480\n",
      "15/15, train_loss: 0.4291\n",
      "16/15, train_loss: 0.4751\n",
      "epoch 128 average loss: 0.5415\n",
      "----------\n",
      "epoch 129/500\n",
      "1/15, train_loss: 0.7856\n",
      "2/15, train_loss: 1.3231\n",
      "3/15, train_loss: 0.4386\n",
      "4/15, train_loss: 0.4073\n",
      "5/15, train_loss: 0.5011\n",
      "6/15, train_loss: 0.5575\n",
      "7/15, train_loss: 0.6650\n",
      "8/15, train_loss: 0.4331\n",
      "9/15, train_loss: 0.4331\n",
      "10/15, train_loss: 0.5020\n",
      "11/15, train_loss: 0.5873\n",
      "12/15, train_loss: 0.7583\n",
      "13/15, train_loss: 0.4564\n",
      "14/15, train_loss: 0.4595\n",
      "15/15, train_loss: 0.5226\n",
      "16/15, train_loss: 2.2875\n",
      "epoch 129 average loss: 0.6949\n",
      "----------\n",
      "epoch 130/500\n",
      "1/15, train_loss: 0.6013\n",
      "2/15, train_loss: 0.5232\n",
      "3/15, train_loss: 2.1674\n",
      "4/15, train_loss: 1.1516\n",
      "5/15, train_loss: 1.1297\n",
      "6/15, train_loss: 0.5732\n",
      "7/15, train_loss: 0.5463\n",
      "8/15, train_loss: 0.5869\n",
      "9/15, train_loss: 0.3935\n",
      "10/15, train_loss: 2.5739\n",
      "11/15, train_loss: 0.8228\n",
      "12/15, train_loss: 0.4944\n",
      "13/15, train_loss: 0.4623\n",
      "14/15, train_loss: 0.5787\n",
      "15/15, train_loss: 0.6015\n",
      "16/15, train_loss: 0.4878\n",
      "epoch 130 average loss: 0.8559\n",
      "----------\n",
      "epoch 131/500\n",
      "1/15, train_loss: 0.3721\n",
      "2/15, train_loss: 0.5435\n",
      "3/15, train_loss: 0.4509\n",
      "4/15, train_loss: 0.5061\n",
      "5/15, train_loss: 0.4341\n",
      "6/15, train_loss: 0.5634\n",
      "7/15, train_loss: 0.4043\n",
      "8/15, train_loss: 0.7158\n",
      "9/15, train_loss: 0.6536\n",
      "10/15, train_loss: 0.4644\n",
      "11/15, train_loss: 0.4207\n",
      "12/15, train_loss: 0.7355\n",
      "13/15, train_loss: 0.5572\n",
      "14/15, train_loss: 0.5311\n",
      "15/15, train_loss: 0.4931\n",
      "16/15, train_loss: 0.3679\n",
      "epoch 131 average loss: 0.5134\n",
      "----------\n",
      "epoch 132/500\n",
      "1/15, train_loss: 0.7519\n",
      "2/15, train_loss: 0.5491\n",
      "3/15, train_loss: 0.4352\n",
      "4/15, train_loss: 0.6180\n",
      "5/15, train_loss: 0.5233\n",
      "6/15, train_loss: 0.5457\n",
      "7/15, train_loss: 0.7141\n",
      "8/15, train_loss: 0.4860\n",
      "9/15, train_loss: 0.5234\n",
      "10/15, train_loss: 0.6307\n",
      "11/15, train_loss: 0.4976\n",
      "12/15, train_loss: 0.5278\n",
      "13/15, train_loss: 0.5189\n",
      "14/15, train_loss: 0.4705\n",
      "15/15, train_loss: 0.3834\n",
      "16/15, train_loss: 0.4406\n",
      "epoch 132 average loss: 0.5385\n",
      "----------\n",
      "epoch 133/500\n",
      "1/15, train_loss: 1.2179\n",
      "2/15, train_loss: 0.4165\n",
      "3/15, train_loss: 0.5355\n",
      "4/15, train_loss: 0.3913\n",
      "5/15, train_loss: 0.5582\n",
      "6/15, train_loss: 0.4493\n",
      "7/15, train_loss: 0.3522\n",
      "8/15, train_loss: 0.4235\n",
      "9/15, train_loss: 0.5682\n",
      "10/15, train_loss: 0.4492\n",
      "11/15, train_loss: 0.5113\n",
      "12/15, train_loss: 0.5225\n",
      "13/15, train_loss: 0.4584\n",
      "14/15, train_loss: 0.3877\n",
      "15/15, train_loss: 0.5526\n",
      "16/15, train_loss: 0.4216\n",
      "epoch 133 average loss: 0.5135\n",
      "----------\n",
      "epoch 134/500\n",
      "1/15, train_loss: 0.7228\n",
      "2/15, train_loss: 0.5156\n",
      "3/15, train_loss: 0.4330\n",
      "4/15, train_loss: 0.4110\n",
      "5/15, train_loss: 0.7862\n",
      "6/15, train_loss: 0.3583\n",
      "7/15, train_loss: 0.3997\n",
      "8/15, train_loss: 0.3709\n",
      "9/15, train_loss: 0.4420\n",
      "10/15, train_loss: 0.5952\n",
      "11/15, train_loss: 0.5426\n",
      "12/15, train_loss: 0.3861\n",
      "13/15, train_loss: 0.5184\n",
      "14/15, train_loss: 0.3414\n",
      "15/15, train_loss: 0.4654\n",
      "16/15, train_loss: 0.6101\n",
      "epoch 134 average loss: 0.4937\n",
      "----------\n",
      "epoch 135/500\n",
      "1/15, train_loss: 0.5591\n",
      "2/15, train_loss: 0.3950\n",
      "3/15, train_loss: 0.6130\n",
      "4/15, train_loss: 0.6002\n",
      "5/15, train_loss: 0.6919\n",
      "6/15, train_loss: 0.5038\n",
      "7/15, train_loss: 0.8907\n",
      "8/15, train_loss: 0.4140\n",
      "9/15, train_loss: 0.4593\n",
      "10/15, train_loss: 0.9208\n",
      "11/15, train_loss: 0.3752\n",
      "12/15, train_loss: 1.6101\n",
      "13/15, train_loss: 0.4782\n",
      "14/15, train_loss: 0.4654\n",
      "15/15, train_loss: 0.5173\n",
      "16/15, train_loss: 2.7095\n",
      "epoch 135 average loss: 0.7627\n",
      "----------\n",
      "epoch 136/500\n",
      "1/15, train_loss: 0.6639\n",
      "2/15, train_loss: 0.4170\n",
      "3/15, train_loss: 0.4896\n",
      "4/15, train_loss: 0.4271\n",
      "5/15, train_loss: 0.5761\n",
      "6/15, train_loss: 0.9411\n",
      "7/15, train_loss: 0.4048\n",
      "8/15, train_loss: 0.6163\n",
      "9/15, train_loss: 0.4460\n",
      "10/15, train_loss: 0.4929\n",
      "11/15, train_loss: 0.5888\n",
      "12/15, train_loss: 0.4284\n",
      "13/15, train_loss: 0.4143\n",
      "14/15, train_loss: 0.4183\n",
      "15/15, train_loss: 1.0711\n",
      "16/15, train_loss: 0.3944\n",
      "epoch 136 average loss: 0.5494\n",
      "----------\n",
      "epoch 137/500\n",
      "1/15, train_loss: 0.3844\n",
      "2/15, train_loss: 0.5093\n",
      "3/15, train_loss: 0.5129\n",
      "4/15, train_loss: 0.4842\n",
      "5/15, train_loss: 0.3882\n",
      "6/15, train_loss: 0.6040\n",
      "7/15, train_loss: 0.4273\n",
      "8/15, train_loss: 1.2824\n",
      "9/15, train_loss: 0.6101\n",
      "10/15, train_loss: 0.8164\n",
      "11/15, train_loss: 0.4398\n",
      "12/15, train_loss: 0.4355\n",
      "13/15, train_loss: 0.4206\n",
      "14/15, train_loss: 0.6143\n",
      "15/15, train_loss: 0.3881\n",
      "16/15, train_loss: 2.5532\n",
      "epoch 137 average loss: 0.6794\n",
      "----------\n",
      "epoch 138/500\n",
      "1/15, train_loss: 0.4989\n",
      "2/15, train_loss: 0.3825\n",
      "3/15, train_loss: 0.4424\n",
      "4/15, train_loss: 0.4889\n",
      "5/15, train_loss: 0.4563\n",
      "6/15, train_loss: 0.5220\n",
      "7/15, train_loss: 0.4255\n",
      "8/15, train_loss: 0.4609\n",
      "9/15, train_loss: 0.4414\n",
      "10/15, train_loss: 0.3929\n",
      "11/15, train_loss: 2.6771\n",
      "12/15, train_loss: 0.5165\n",
      "13/15, train_loss: 0.4584\n",
      "14/15, train_loss: 0.4477\n",
      "15/15, train_loss: 0.5118\n",
      "16/15, train_loss: 0.4279\n",
      "epoch 138 average loss: 0.5969\n",
      "----------\n",
      "epoch 139/500\n",
      "1/15, train_loss: 0.4304\n",
      "2/15, train_loss: 0.5927\n",
      "3/15, train_loss: 0.3851\n",
      "4/15, train_loss: 0.4651\n",
      "5/15, train_loss: 0.4671\n",
      "6/15, train_loss: 0.3865\n",
      "7/15, train_loss: 0.4202\n",
      "8/15, train_loss: 0.3873\n",
      "9/15, train_loss: 0.3891\n",
      "10/15, train_loss: 1.0054\n",
      "11/15, train_loss: 0.4395\n",
      "12/15, train_loss: 0.5132\n",
      "13/15, train_loss: 1.5312\n",
      "14/15, train_loss: 0.3803\n",
      "15/15, train_loss: 0.7070\n",
      "16/15, train_loss: 2.5693\n",
      "epoch 139 average loss: 0.6918\n",
      "----------\n",
      "epoch 140/500\n",
      "1/15, train_loss: 0.4605\n",
      "2/15, train_loss: 0.3644\n",
      "3/15, train_loss: 0.4567\n",
      "4/15, train_loss: 0.5445\n",
      "5/15, train_loss: 0.6176\n",
      "6/15, train_loss: 0.4677\n",
      "7/15, train_loss: 0.4372\n",
      "8/15, train_loss: 0.4045\n",
      "9/15, train_loss: 0.6963\n",
      "10/15, train_loss: 0.4197\n",
      "11/15, train_loss: 0.6174\n",
      "12/15, train_loss: 0.3583\n",
      "13/15, train_loss: 1.0139\n",
      "14/15, train_loss: 0.4226\n",
      "15/15, train_loss: 0.4288\n",
      "16/15, train_loss: 2.5796\n",
      "epoch 140 average loss: 0.6431\n",
      "----------\n",
      "epoch 141/500\n",
      "1/15, train_loss: 0.4592\n",
      "2/15, train_loss: 0.4245\n",
      "3/15, train_loss: 0.4426\n",
      "4/15, train_loss: 0.6822\n",
      "5/15, train_loss: 0.4457\n",
      "6/15, train_loss: 0.4128\n",
      "7/15, train_loss: 0.6052\n",
      "8/15, train_loss: 0.3859\n",
      "9/15, train_loss: 0.9369\n",
      "10/15, train_loss: 0.5629\n",
      "11/15, train_loss: 0.4056\n",
      "12/15, train_loss: 0.4184\n",
      "13/15, train_loss: 0.4854\n",
      "14/15, train_loss: 0.6846\n",
      "15/15, train_loss: 0.4101\n",
      "16/15, train_loss: 3.1103\n",
      "epoch 141 average loss: 0.6795\n",
      "----------\n",
      "epoch 142/500\n",
      "1/15, train_loss: 0.3802\n",
      "2/15, train_loss: 0.5659\n",
      "3/15, train_loss: 0.4357\n",
      "4/15, train_loss: 0.7830\n",
      "5/15, train_loss: 0.3997\n",
      "6/15, train_loss: 0.4328\n",
      "7/15, train_loss: 0.4235\n",
      "8/15, train_loss: 0.4528\n",
      "9/15, train_loss: 0.4967\n",
      "10/15, train_loss: 0.4656\n",
      "11/15, train_loss: 0.4787\n",
      "12/15, train_loss: 0.4655\n",
      "13/15, train_loss: 0.3341\n",
      "14/15, train_loss: 0.4695\n",
      "15/15, train_loss: 0.4923\n",
      "16/15, train_loss: 2.7100\n",
      "epoch 142 average loss: 0.6116\n",
      "----------\n",
      "epoch 143/500\n",
      "1/15, train_loss: 0.3852\n",
      "2/15, train_loss: 0.4556\n",
      "3/15, train_loss: 2.2951\n",
      "4/15, train_loss: 0.5375\n",
      "5/15, train_loss: 0.4416\n",
      "6/15, train_loss: 0.4011\n",
      "7/15, train_loss: 0.5228\n",
      "8/15, train_loss: 2.3821\n",
      "9/15, train_loss: 0.4207\n",
      "10/15, train_loss: 0.4715\n",
      "11/15, train_loss: 0.8171\n",
      "12/15, train_loss: 0.3292\n",
      "13/15, train_loss: 0.6179\n",
      "14/15, train_loss: 0.5614\n",
      "15/15, train_loss: 0.3824\n",
      "16/15, train_loss: 2.6690\n",
      "epoch 143 average loss: 0.8556\n",
      "----------\n",
      "epoch 144/500\n",
      "1/15, train_loss: 0.4044\n",
      "2/15, train_loss: 0.4248\n",
      "3/15, train_loss: 0.4053\n",
      "4/15, train_loss: 0.5790\n",
      "5/15, train_loss: 0.3964\n",
      "6/15, train_loss: 0.4201\n",
      "7/15, train_loss: 0.3636\n",
      "8/15, train_loss: 0.3785\n",
      "9/15, train_loss: 0.3761\n",
      "10/15, train_loss: 0.6465\n",
      "11/15, train_loss: 0.4294\n",
      "12/15, train_loss: 0.4712\n",
      "13/15, train_loss: 0.3977\n",
      "14/15, train_loss: 0.6402\n",
      "15/15, train_loss: 0.3944\n",
      "16/15, train_loss: 0.6974\n",
      "epoch 144 average loss: 0.4641\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 144 validation loss: 0.9672 dice_score: 0.8418 acc_metric: 0.7318 accuracy: 0.7318, f1score: 0.7661\n",
      " saved Best PMetric: 0.7868 at epoch: 144\n",
      "----------\n",
      "epoch 145/500\n",
      "1/15, train_loss: 0.4708\n",
      "2/15, train_loss: 0.3627\n",
      "3/15, train_loss: 0.8933\n",
      "4/15, train_loss: 0.4538\n",
      "5/15, train_loss: 0.5557\n",
      "6/15, train_loss: 0.4500\n",
      "7/15, train_loss: 0.3770\n",
      "8/15, train_loss: 0.3968\n",
      "9/15, train_loss: 0.4701\n",
      "10/15, train_loss: 0.4747\n",
      "11/15, train_loss: 0.4307\n",
      "12/15, train_loss: 0.5334\n",
      "13/15, train_loss: 0.4460\n",
      "14/15, train_loss: 0.4593\n",
      "15/15, train_loss: 0.4157\n",
      "16/15, train_loss: 2.7010\n",
      "epoch 145 average loss: 0.6182\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 145 validation loss: 0.9704 dice_score: 0.8446 acc_metric: 0.7108 accuracy: 0.7108, f1score: 0.7258\n",
      " saved Best PMetric: 0.7777 at epoch: 145\n",
      "----------\n",
      "epoch 146/500\n",
      "1/15, train_loss: 0.4388\n",
      "2/15, train_loss: 0.4512\n",
      "3/15, train_loss: 0.5172\n",
      "4/15, train_loss: 1.0484\n",
      "5/15, train_loss: 0.3921\n",
      "6/15, train_loss: 0.5009\n",
      "7/15, train_loss: 2.4480\n",
      "8/15, train_loss: 0.4768\n",
      "9/15, train_loss: 0.3532\n",
      "10/15, train_loss: 0.6255\n",
      "11/15, train_loss: 0.3690\n",
      "12/15, train_loss: 0.4305\n",
      "13/15, train_loss: 0.4260\n",
      "14/15, train_loss: 0.5418\n",
      "15/15, train_loss: 0.5742\n",
      "16/15, train_loss: 0.3781\n",
      "epoch 146 average loss: 0.6232\n",
      "----------\n",
      "epoch 147/500\n",
      "1/15, train_loss: 0.4076\n",
      "2/15, train_loss: 0.5275\n",
      "3/15, train_loss: 0.8568\n",
      "4/15, train_loss: 0.4382\n",
      "5/15, train_loss: 0.4287\n",
      "6/15, train_loss: 0.4174\n",
      "7/15, train_loss: 0.5178\n",
      "8/15, train_loss: 0.3873\n",
      "9/15, train_loss: 0.4458\n",
      "10/15, train_loss: 0.3661\n",
      "11/15, train_loss: 0.6100\n",
      "12/15, train_loss: 0.6484\n",
      "13/15, train_loss: 0.6992\n",
      "14/15, train_loss: 0.4623\n",
      "15/15, train_loss: 0.4131\n",
      "16/15, train_loss: 0.3097\n",
      "epoch 147 average loss: 0.4960\n",
      "----------\n",
      "epoch 148/500\n",
      "1/15, train_loss: 0.5712\n",
      "2/15, train_loss: 1.3622\n",
      "3/15, train_loss: 0.4777\n",
      "4/15, train_loss: 0.3970\n",
      "5/15, train_loss: 0.3527\n",
      "6/15, train_loss: 0.7690\n",
      "7/15, train_loss: 0.4183\n",
      "8/15, train_loss: 0.3880\n",
      "9/15, train_loss: 0.3461\n",
      "10/15, train_loss: 0.3952\n",
      "11/15, train_loss: 0.6852\n",
      "12/15, train_loss: 0.5499\n",
      "13/15, train_loss: 0.5490\n",
      "14/15, train_loss: 0.3224\n",
      "15/15, train_loss: 0.3444\n",
      "16/15, train_loss: 2.4078\n",
      "epoch 148 average loss: 0.6460\n",
      "----------\n",
      "epoch 149/500\n",
      "1/15, train_loss: 0.3678\n",
      "2/15, train_loss: 0.4667\n",
      "3/15, train_loss: 0.6909\n",
      "4/15, train_loss: 0.4757\n",
      "5/15, train_loss: 0.5058\n",
      "6/15, train_loss: 0.4843\n",
      "7/15, train_loss: 0.4834\n",
      "8/15, train_loss: 0.3658\n",
      "9/15, train_loss: 0.4694\n",
      "10/15, train_loss: 0.3844\n",
      "11/15, train_loss: 0.3200\n",
      "12/15, train_loss: 1.0513\n",
      "13/15, train_loss: 0.4299\n",
      "14/15, train_loss: 0.3613\n",
      "15/15, train_loss: 0.4200\n",
      "16/15, train_loss: 2.1931\n",
      "epoch 149 average loss: 0.5919\n",
      "----------\n",
      "epoch 150/500\n",
      "1/15, train_loss: 0.4982\n",
      "2/15, train_loss: 0.3680\n",
      "3/15, train_loss: 0.4392\n",
      "4/15, train_loss: 0.4291\n",
      "5/15, train_loss: 0.3451\n",
      "6/15, train_loss: 0.5253\n",
      "7/15, train_loss: 0.3753\n",
      "8/15, train_loss: 0.3682\n",
      "9/15, train_loss: 0.3956\n",
      "10/15, train_loss: 0.3420\n",
      "11/15, train_loss: 0.3949\n",
      "12/15, train_loss: 0.4754\n",
      "13/15, train_loss: 0.4522\n",
      "14/15, train_loss: 0.5086\n",
      "15/15, train_loss: 0.4269\n",
      "16/15, train_loss: 2.6705\n",
      "epoch 150 average loss: 0.5634\n",
      "----------\n",
      "epoch 151/500\n",
      "1/15, train_loss: 2.5640\n",
      "2/15, train_loss: 0.4910\n",
      "3/15, train_loss: 0.9046\n",
      "4/15, train_loss: 0.3888\n",
      "5/15, train_loss: 0.5516\n",
      "6/15, train_loss: 0.6447\n",
      "7/15, train_loss: 0.3905\n",
      "8/15, train_loss: 0.5760\n",
      "9/15, train_loss: 0.3880\n",
      "10/15, train_loss: 0.4604\n",
      "11/15, train_loss: 0.5287\n",
      "12/15, train_loss: 0.9751\n",
      "13/15, train_loss: 0.3611\n",
      "14/15, train_loss: 0.3655\n",
      "15/15, train_loss: 0.5180\n",
      "16/15, train_loss: 2.2795\n",
      "epoch 151 average loss: 0.7742\n",
      "----------\n",
      "epoch 152/500\n",
      "1/15, train_loss: 0.5112\n",
      "2/15, train_loss: 0.3713\n",
      "3/15, train_loss: 1.1307\n",
      "4/15, train_loss: 0.7342\n",
      "5/15, train_loss: 0.3466\n",
      "6/15, train_loss: 0.3760\n",
      "7/15, train_loss: 0.5540\n",
      "8/15, train_loss: 0.3667\n",
      "9/15, train_loss: 0.4834\n",
      "10/15, train_loss: 0.4213\n",
      "11/15, train_loss: 0.4158\n",
      "12/15, train_loss: 0.4004\n",
      "13/15, train_loss: 0.3312\n",
      "14/15, train_loss: 0.4423\n",
      "15/15, train_loss: 0.4031\n",
      "16/15, train_loss: 0.7335\n",
      "epoch 152 average loss: 0.5014\n",
      "----------\n",
      "epoch 153/500\n",
      "1/15, train_loss: 0.4271\n",
      "2/15, train_loss: 0.4739\n",
      "3/15, train_loss: 0.3821\n",
      "4/15, train_loss: 0.3821\n",
      "5/15, train_loss: 0.3598\n",
      "6/15, train_loss: 0.4489\n",
      "7/15, train_loss: 0.3499\n",
      "8/15, train_loss: 0.4260\n",
      "9/15, train_loss: 1.2560\n",
      "10/15, train_loss: 0.4146\n",
      "11/15, train_loss: 0.5120\n",
      "12/15, train_loss: 0.6690\n",
      "13/15, train_loss: 0.7500\n",
      "14/15, train_loss: 0.3896\n",
      "15/15, train_loss: 0.4292\n",
      "16/15, train_loss: 2.3946\n",
      "epoch 153 average loss: 0.6291\n",
      "----------\n",
      "epoch 154/500\n",
      "1/15, train_loss: 0.6261\n",
      "2/15, train_loss: 0.3989\n",
      "3/15, train_loss: 0.4207\n",
      "4/15, train_loss: 0.3935\n",
      "5/15, train_loss: 0.5392\n",
      "6/15, train_loss: 0.5639\n",
      "7/15, train_loss: 0.4443\n",
      "8/15, train_loss: 0.3654\n",
      "9/15, train_loss: 0.3924\n",
      "10/15, train_loss: 0.5880\n",
      "11/15, train_loss: 0.3855\n",
      "12/15, train_loss: 0.3889\n",
      "13/15, train_loss: 0.5676\n",
      "14/15, train_loss: 0.5585\n",
      "15/15, train_loss: 0.4916\n",
      "16/15, train_loss: 0.3968\n",
      "epoch 154 average loss: 0.4701\n",
      "----------\n",
      "epoch 155/500\n",
      "1/15, train_loss: 0.4244\n",
      "2/15, train_loss: 0.4598\n",
      "3/15, train_loss: 0.5114\n",
      "4/15, train_loss: 0.5288\n",
      "5/15, train_loss: 0.3684\n",
      "6/15, train_loss: 0.4911\n",
      "7/15, train_loss: 0.5179\n",
      "8/15, train_loss: 0.3989\n",
      "9/15, train_loss: 0.3968\n",
      "10/15, train_loss: 0.3629\n",
      "11/15, train_loss: 0.4136\n",
      "12/15, train_loss: 0.4022\n",
      "13/15, train_loss: 0.3790\n",
      "14/15, train_loss: 0.4228\n",
      "15/15, train_loss: 0.4055\n",
      "16/15, train_loss: 0.6454\n",
      "epoch 155 average loss: 0.4455\n",
      "----------\n",
      "epoch 156/500\n",
      "1/15, train_loss: 0.3378\n",
      "2/15, train_loss: 0.3932\n",
      "3/15, train_loss: 0.3963\n",
      "4/15, train_loss: 0.3749\n",
      "5/15, train_loss: 0.4388\n",
      "6/15, train_loss: 0.3235\n",
      "7/15, train_loss: 0.3946\n",
      "8/15, train_loss: 0.4959\n",
      "9/15, train_loss: 0.4384\n",
      "10/15, train_loss: 0.4639\n",
      "11/15, train_loss: 0.3860\n",
      "12/15, train_loss: 0.4289\n",
      "13/15, train_loss: 0.3661\n",
      "14/15, train_loss: 0.3358\n",
      "15/15, train_loss: 0.5404\n",
      "16/15, train_loss: 2.3472\n",
      "epoch 156 average loss: 0.5289\n",
      "----------\n",
      "epoch 157/500\n",
      "1/15, train_loss: 0.3807\n",
      "2/15, train_loss: 0.4481\n",
      "3/15, train_loss: 0.3964\n",
      "4/15, train_loss: 0.4716\n",
      "5/15, train_loss: 0.3907\n",
      "6/15, train_loss: 0.5469\n",
      "7/15, train_loss: 0.4388\n",
      "8/15, train_loss: 0.3229\n",
      "9/15, train_loss: 0.4045\n",
      "10/15, train_loss: 0.3660\n",
      "11/15, train_loss: 0.4542\n",
      "12/15, train_loss: 0.5393\n",
      "13/15, train_loss: 0.3495\n",
      "14/15, train_loss: 0.4235\n",
      "15/15, train_loss: 0.3688\n",
      "16/15, train_loss: 0.5859\n",
      "epoch 157 average loss: 0.4305\n",
      "----------\n",
      "epoch 158/500\n",
      "1/15, train_loss: 0.3691\n",
      "2/15, train_loss: 0.3661\n",
      "3/15, train_loss: 0.9427\n",
      "4/15, train_loss: 0.3660\n",
      "5/15, train_loss: 0.3501\n",
      "6/15, train_loss: 0.4089\n",
      "7/15, train_loss: 0.3223\n",
      "8/15, train_loss: 0.9479\n",
      "9/15, train_loss: 0.4578\n",
      "10/15, train_loss: 0.4094\n",
      "11/15, train_loss: 1.0311\n",
      "12/15, train_loss: 0.3358\n",
      "13/15, train_loss: 1.7747\n",
      "14/15, train_loss: 0.3209\n",
      "15/15, train_loss: 0.8308\n",
      "16/15, train_loss: 0.5626\n",
      "epoch 158 average loss: 0.6122\n",
      "----------\n",
      "epoch 159/500\n",
      "1/15, train_loss: 0.3675\n",
      "2/15, train_loss: 0.5003\n",
      "3/15, train_loss: 0.4457\n",
      "4/15, train_loss: 0.3538\n",
      "5/15, train_loss: 0.4180\n",
      "6/15, train_loss: 0.3404\n",
      "7/15, train_loss: 0.3791\n",
      "8/15, train_loss: 0.4570\n",
      "9/15, train_loss: 0.5783\n",
      "10/15, train_loss: 0.4502\n",
      "11/15, train_loss: 0.4127\n",
      "12/15, train_loss: 0.3666\n",
      "13/15, train_loss: 0.3478\n",
      "14/15, train_loss: 0.8134\n",
      "15/15, train_loss: 0.4428\n",
      "16/15, train_loss: 0.5424\n",
      "epoch 159 average loss: 0.4510\n",
      "----------\n",
      "epoch 160/500\n",
      "1/15, train_loss: 0.3788\n",
      "2/15, train_loss: 0.5630\n",
      "3/15, train_loss: 0.5182\n",
      "4/15, train_loss: 0.3832\n",
      "5/15, train_loss: 0.3465\n",
      "6/15, train_loss: 0.4723\n",
      "7/15, train_loss: 0.3716\n",
      "8/15, train_loss: 0.3667\n",
      "9/15, train_loss: 0.4323\n",
      "10/15, train_loss: 0.3975\n",
      "11/15, train_loss: 0.3751\n",
      "12/15, train_loss: 0.6913\n",
      "13/15, train_loss: 0.3985\n",
      "14/15, train_loss: 0.6315\n",
      "15/15, train_loss: 0.5312\n",
      "16/15, train_loss: 0.6263\n",
      "epoch 160 average loss: 0.4678\n",
      "----------\n",
      "epoch 161/500\n",
      "1/15, train_loss: 0.6430\n",
      "2/15, train_loss: 0.4239\n",
      "3/15, train_loss: 0.4689\n",
      "4/15, train_loss: 0.3964\n",
      "5/15, train_loss: 0.4433\n",
      "6/15, train_loss: 0.3969\n",
      "7/15, train_loss: 0.3452\n",
      "8/15, train_loss: 0.3968\n",
      "9/15, train_loss: 0.5200\n",
      "10/15, train_loss: 0.3319\n",
      "11/15, train_loss: 0.3405\n",
      "12/15, train_loss: 0.4290\n",
      "13/15, train_loss: 0.3869\n",
      "14/15, train_loss: 0.4825\n",
      "15/15, train_loss: 0.4023\n",
      "16/15, train_loss: 1.8262\n",
      "epoch 161 average loss: 0.5146\n",
      "----------\n",
      "epoch 162/500\n",
      "1/15, train_loss: 0.3168\n",
      "2/15, train_loss: 0.4621\n",
      "3/15, train_loss: 0.3077\n",
      "4/15, train_loss: 0.5824\n",
      "5/15, train_loss: 0.3730\n",
      "6/15, train_loss: 0.4644\n",
      "7/15, train_loss: 1.3485\n",
      "8/15, train_loss: 0.3046\n",
      "9/15, train_loss: 0.4875\n",
      "10/15, train_loss: 0.3686\n",
      "11/15, train_loss: 0.3825\n",
      "12/15, train_loss: 0.5951\n",
      "13/15, train_loss: 0.3505\n",
      "14/15, train_loss: 0.3824\n",
      "15/15, train_loss: 0.4267\n",
      "16/15, train_loss: 2.2488\n",
      "epoch 162 average loss: 0.5876\n",
      "----------\n",
      "epoch 163/500\n",
      "1/15, train_loss: 0.5059\n",
      "2/15, train_loss: 0.3070\n",
      "3/15, train_loss: 0.7261\n",
      "4/15, train_loss: 0.3465\n",
      "5/15, train_loss: 0.3969\n",
      "6/15, train_loss: 0.3791\n",
      "7/15, train_loss: 0.3125\n",
      "8/15, train_loss: 0.4554\n",
      "9/15, train_loss: 0.4254\n",
      "10/15, train_loss: 0.3102\n",
      "11/15, train_loss: 0.5378\n",
      "12/15, train_loss: 0.3988\n",
      "13/15, train_loss: 0.3477\n",
      "14/15, train_loss: 0.3570\n",
      "15/15, train_loss: 0.6355\n",
      "16/15, train_loss: 2.2927\n",
      "epoch 163 average loss: 0.5459\n",
      "----------\n",
      "epoch 164/500\n",
      "1/15, train_loss: 0.3570\n",
      "2/15, train_loss: 0.3495\n",
      "3/15, train_loss: 0.4253\n",
      "4/15, train_loss: 0.3243\n",
      "5/15, train_loss: 0.3986\n",
      "6/15, train_loss: 0.4157\n",
      "7/15, train_loss: 0.4223\n",
      "8/15, train_loss: 0.3668\n",
      "9/15, train_loss: 0.3391\n",
      "10/15, train_loss: 0.3984\n",
      "11/15, train_loss: 0.3062\n",
      "12/15, train_loss: 0.4651\n",
      "13/15, train_loss: 0.3140\n",
      "14/15, train_loss: 0.4872\n",
      "15/15, train_loss: 1.6005\n",
      "16/15, train_loss: 2.2314\n",
      "epoch 164 average loss: 0.5751\n",
      "----------\n",
      "epoch 165/500\n",
      "1/15, train_loss: 0.4390\n",
      "2/15, train_loss: 0.4443\n",
      "3/15, train_loss: 0.4131\n",
      "4/15, train_loss: 1.1444\n",
      "5/15, train_loss: 0.3287\n",
      "6/15, train_loss: 0.4751\n",
      "7/15, train_loss: 0.4256\n",
      "8/15, train_loss: 0.4371\n",
      "9/15, train_loss: 0.3112\n",
      "10/15, train_loss: 0.3117\n",
      "11/15, train_loss: 0.3311\n",
      "12/15, train_loss: 0.3620\n",
      "13/15, train_loss: 0.3297\n",
      "14/15, train_loss: 0.3816\n",
      "15/15, train_loss: 1.0507\n",
      "16/15, train_loss: 0.4231\n",
      "epoch 165 average loss: 0.4755\n",
      "----------\n",
      "epoch 166/500\n",
      "1/15, train_loss: 0.4663\n",
      "2/15, train_loss: 0.5363\n",
      "3/15, train_loss: 0.5710\n",
      "4/15, train_loss: 0.3838\n",
      "5/15, train_loss: 0.3335\n",
      "6/15, train_loss: 0.4118\n",
      "7/15, train_loss: 0.3568\n",
      "8/15, train_loss: 0.3505\n",
      "9/15, train_loss: 0.5938\n",
      "10/15, train_loss: 0.3706\n",
      "11/15, train_loss: 0.4172\n",
      "12/15, train_loss: 0.3094\n",
      "13/15, train_loss: 0.7942\n",
      "14/15, train_loss: 0.3295\n",
      "15/15, train_loss: 0.4803\n",
      "16/15, train_loss: 0.3087\n",
      "epoch 166 average loss: 0.4384\n",
      "----------\n",
      "epoch 167/500\n",
      "1/15, train_loss: 0.2965\n",
      "2/15, train_loss: 0.9058\n",
      "3/15, train_loss: 0.7158\n",
      "4/15, train_loss: 0.3615\n",
      "5/15, train_loss: 0.4269\n",
      "6/15, train_loss: 0.3233\n",
      "7/15, train_loss: 0.4422\n",
      "8/15, train_loss: 1.0033\n",
      "9/15, train_loss: 0.4052\n",
      "10/15, train_loss: 0.4402\n",
      "11/15, train_loss: 0.2949\n",
      "12/15, train_loss: 0.4700\n",
      "13/15, train_loss: 0.2988\n",
      "14/15, train_loss: 0.6492\n",
      "15/15, train_loss: 0.2952\n",
      "16/15, train_loss: 0.3576\n",
      "epoch 167 average loss: 0.4804\n",
      "----------\n",
      "epoch 168/500\n",
      "1/15, train_loss: 0.5426\n",
      "2/15, train_loss: 0.3117\n",
      "3/15, train_loss: 0.3340\n",
      "4/15, train_loss: 0.3603\n",
      "5/15, train_loss: 0.2854\n",
      "6/15, train_loss: 0.4174\n",
      "7/15, train_loss: 0.4428\n",
      "8/15, train_loss: 0.5161\n",
      "9/15, train_loss: 0.3372\n",
      "10/15, train_loss: 0.2986\n",
      "11/15, train_loss: 0.8883\n",
      "12/15, train_loss: 0.4647\n",
      "13/15, train_loss: 0.5208\n",
      "14/15, train_loss: 0.7031\n",
      "15/15, train_loss: 0.3783\n",
      "16/15, train_loss: 1.0276\n",
      "epoch 168 average loss: 0.4893\n",
      "----------\n",
      "epoch 169/500\n",
      "1/15, train_loss: 0.4099\n",
      "2/15, train_loss: 0.3448\n",
      "3/15, train_loss: 0.3467\n",
      "4/15, train_loss: 0.5804\n",
      "5/15, train_loss: 0.9405\n",
      "6/15, train_loss: 0.3683\n",
      "7/15, train_loss: 0.3551\n",
      "8/15, train_loss: 0.6420\n",
      "9/15, train_loss: 0.3935\n",
      "10/15, train_loss: 0.4386\n",
      "11/15, train_loss: 0.4102\n",
      "12/15, train_loss: 0.4465\n",
      "13/15, train_loss: 0.3180\n",
      "14/15, train_loss: 0.3327\n",
      "15/15, train_loss: 0.4327\n",
      "16/15, train_loss: 2.6862\n",
      "epoch 169 average loss: 0.5904\n",
      "----------\n",
      "epoch 170/500\n",
      "1/15, train_loss: 0.7775\n",
      "2/15, train_loss: 0.5896\n",
      "3/15, train_loss: 0.3159\n",
      "4/15, train_loss: 0.8412\n",
      "5/15, train_loss: 0.3295\n",
      "6/15, train_loss: 0.4239\n",
      "7/15, train_loss: 0.3568\n",
      "8/15, train_loss: 0.6597\n",
      "9/15, train_loss: 0.3490\n",
      "10/15, train_loss: 0.3434\n",
      "11/15, train_loss: 0.4009\n",
      "12/15, train_loss: 0.5150\n",
      "13/15, train_loss: 0.3545\n",
      "14/15, train_loss: 0.4577\n",
      "15/15, train_loss: 0.2957\n",
      "16/15, train_loss: 2.2280\n",
      "epoch 170 average loss: 0.5774\n",
      "----------\n",
      "epoch 171/500\n",
      "1/15, train_loss: 0.3431\n",
      "2/15, train_loss: 0.3763\n",
      "3/15, train_loss: 0.3604\n",
      "4/15, train_loss: 0.3536\n",
      "5/15, train_loss: 0.4327\n",
      "6/15, train_loss: 0.3530\n",
      "7/15, train_loss: 0.7486\n",
      "8/15, train_loss: 0.3544\n",
      "9/15, train_loss: 0.3804\n",
      "10/15, train_loss: 0.3859\n",
      "11/15, train_loss: 0.3965\n",
      "12/15, train_loss: 0.3014\n",
      "13/15, train_loss: 0.4366\n",
      "14/15, train_loss: 0.2855\n",
      "15/15, train_loss: 0.4067\n",
      "16/15, train_loss: 2.6393\n",
      "epoch 171 average loss: 0.5347\n",
      "----------\n",
      "epoch 172/500\n",
      "1/15, train_loss: 0.4014\n",
      "2/15, train_loss: 0.4514\n",
      "3/15, train_loss: 0.6398\n",
      "4/15, train_loss: 0.3349\n",
      "5/15, train_loss: 0.4028\n",
      "6/15, train_loss: 0.3712\n",
      "7/15, train_loss: 0.3456\n",
      "8/15, train_loss: 0.3618\n",
      "9/15, train_loss: 0.4176\n",
      "10/15, train_loss: 0.3822\n",
      "11/15, train_loss: 0.3237\n",
      "12/15, train_loss: 0.4187\n",
      "13/15, train_loss: 0.3861\n",
      "14/15, train_loss: 0.8023\n",
      "15/15, train_loss: 0.3308\n",
      "16/15, train_loss: 2.5648\n",
      "epoch 172 average loss: 0.5584\n",
      "----------\n",
      "epoch 173/500\n",
      "1/15, train_loss: 0.3612\n",
      "2/15, train_loss: 0.3249\n",
      "3/15, train_loss: 0.3672\n",
      "4/15, train_loss: 0.4819\n",
      "5/15, train_loss: 0.3093\n",
      "6/15, train_loss: 0.4158\n",
      "7/15, train_loss: 0.4334\n",
      "8/15, train_loss: 0.6135\n",
      "9/15, train_loss: 0.6032\n",
      "10/15, train_loss: 0.3622\n",
      "11/15, train_loss: 0.3921\n",
      "12/15, train_loss: 0.3286\n",
      "13/15, train_loss: 0.5530\n",
      "14/15, train_loss: 0.2909\n",
      "15/15, train_loss: 0.4093\n",
      "16/15, train_loss: 0.6070\n",
      "epoch 173 average loss: 0.4284\n",
      "----------\n",
      "epoch 174/500\n",
      "1/15, train_loss: 0.3129\n",
      "2/15, train_loss: 0.3806\n",
      "3/15, train_loss: 0.3822\n",
      "4/15, train_loss: 0.3228\n",
      "5/15, train_loss: 0.4957\n",
      "6/15, train_loss: 2.1889\n",
      "7/15, train_loss: 0.4449\n",
      "8/15, train_loss: 0.3513\n",
      "9/15, train_loss: 0.4905\n",
      "10/15, train_loss: 0.3660\n",
      "11/15, train_loss: 0.3134\n",
      "12/15, train_loss: 0.3150\n",
      "13/15, train_loss: 0.3994\n",
      "14/15, train_loss: 0.2723\n",
      "15/15, train_loss: 0.5081\n",
      "16/15, train_loss: 2.7788\n",
      "epoch 174 average loss: 0.6452\n",
      "----------\n",
      "epoch 175/500\n",
      "1/15, train_loss: 0.6290\n",
      "2/15, train_loss: 0.3642\n",
      "3/15, train_loss: 0.7909\n",
      "4/15, train_loss: 0.3464\n",
      "5/15, train_loss: 0.3104\n",
      "6/15, train_loss: 0.3645\n",
      "7/15, train_loss: 0.5747\n",
      "8/15, train_loss: 0.2967\n",
      "9/15, train_loss: 0.4346\n",
      "10/15, train_loss: 0.4358\n",
      "11/15, train_loss: 0.3418\n",
      "12/15, train_loss: 0.3679\n",
      "13/15, train_loss: 0.3303\n",
      "14/15, train_loss: 0.4039\n",
      "15/15, train_loss: 0.3691\n",
      "16/15, train_loss: 2.3741\n",
      "epoch 175 average loss: 0.5459\n",
      "----------\n",
      "epoch 176/500\n",
      "1/15, train_loss: 0.2941\n",
      "2/15, train_loss: 0.5888\n",
      "3/15, train_loss: 0.3581\n",
      "4/15, train_loss: 0.3919\n",
      "5/15, train_loss: 0.4026\n",
      "6/15, train_loss: 0.3759\n",
      "7/15, train_loss: 0.2735\n",
      "8/15, train_loss: 0.4418\n",
      "9/15, train_loss: 0.2905\n",
      "10/15, train_loss: 0.4856\n",
      "11/15, train_loss: 0.4666\n",
      "12/15, train_loss: 0.3279\n",
      "13/15, train_loss: 0.3514\n",
      "14/15, train_loss: 0.3691\n",
      "15/15, train_loss: 0.3479\n",
      "16/15, train_loss: 2.1518\n",
      "epoch 176 average loss: 0.4948\n",
      "----------\n",
      "epoch 177/500\n",
      "1/15, train_loss: 0.3304\n",
      "2/15, train_loss: 0.3574\n",
      "3/15, train_loss: 0.3291\n",
      "4/15, train_loss: 0.4898\n",
      "5/15, train_loss: 0.4854\n",
      "6/15, train_loss: 0.3001\n",
      "7/15, train_loss: 2.6032\n",
      "8/15, train_loss: 0.3545\n",
      "9/15, train_loss: 0.3723\n",
      "10/15, train_loss: 0.2849\n",
      "11/15, train_loss: 0.2741\n",
      "12/15, train_loss: 0.3121\n",
      "13/15, train_loss: 0.3059\n",
      "14/15, train_loss: 0.3130\n",
      "15/15, train_loss: 0.5034\n",
      "16/15, train_loss: 0.8363\n",
      "epoch 177 average loss: 0.5282\n",
      "----------\n",
      "epoch 178/500\n",
      "1/15, train_loss: 0.3026\n",
      "2/15, train_loss: 0.3145\n",
      "3/15, train_loss: 0.3351\n",
      "4/15, train_loss: 0.3200\n",
      "5/15, train_loss: 0.4012\n",
      "6/15, train_loss: 0.4953\n",
      "7/15, train_loss: 0.3011\n",
      "8/15, train_loss: 0.3419\n",
      "9/15, train_loss: 0.6131\n",
      "10/15, train_loss: 2.3409\n",
      "11/15, train_loss: 0.4099\n",
      "12/15, train_loss: 0.2917\n",
      "13/15, train_loss: 1.2145\n",
      "14/15, train_loss: 0.3624\n",
      "15/15, train_loss: 0.6072\n",
      "16/15, train_loss: 2.6957\n",
      "epoch 178 average loss: 0.7092\n",
      "----------\n",
      "epoch 179/500\n",
      "1/15, train_loss: 0.2713\n",
      "2/15, train_loss: 0.3919\n",
      "3/15, train_loss: 0.3297\n",
      "4/15, train_loss: 0.3070\n",
      "5/15, train_loss: 0.3298\n",
      "6/15, train_loss: 0.3510\n",
      "7/15, train_loss: 0.2989\n",
      "8/15, train_loss: 0.2896\n",
      "9/15, train_loss: 0.3161\n",
      "10/15, train_loss: 0.3460\n",
      "11/15, train_loss: 0.5107\n",
      "12/15, train_loss: 0.8230\n",
      "13/15, train_loss: 0.3727\n",
      "14/15, train_loss: 0.4873\n",
      "15/15, train_loss: 0.9403\n",
      "16/15, train_loss: 2.3833\n",
      "epoch 179 average loss: 0.5468\n",
      "----------\n",
      "epoch 180/500\n",
      "1/15, train_loss: 0.3395\n",
      "2/15, train_loss: 0.3626\n",
      "3/15, train_loss: 0.3401\n",
      "4/15, train_loss: 0.3974\n",
      "5/15, train_loss: 0.2956\n",
      "6/15, train_loss: 0.4501\n",
      "7/15, train_loss: 0.2922\n",
      "8/15, train_loss: 0.3450\n",
      "9/15, train_loss: 0.3493\n",
      "10/15, train_loss: 0.5576\n",
      "11/15, train_loss: 0.3806\n",
      "12/15, train_loss: 0.2711\n",
      "13/15, train_loss: 0.3865\n",
      "14/15, train_loss: 0.3052\n",
      "15/15, train_loss: 0.3040\n",
      "16/15, train_loss: 2.4751\n",
      "epoch 180 average loss: 0.4907\n",
      "----------\n",
      "epoch 181/500\n",
      "1/15, train_loss: 0.4321\n",
      "2/15, train_loss: 0.5067\n",
      "3/15, train_loss: 0.3034\n",
      "4/15, train_loss: 0.2802\n",
      "5/15, train_loss: 0.4180\n",
      "6/15, train_loss: 0.4574\n",
      "7/15, train_loss: 0.8324\n",
      "8/15, train_loss: 0.3314\n",
      "9/15, train_loss: 0.6372\n",
      "10/15, train_loss: 0.3634\n",
      "11/15, train_loss: 0.6411\n",
      "12/15, train_loss: 0.3668\n",
      "13/15, train_loss: 0.4440\n",
      "14/15, train_loss: 0.3091\n",
      "15/15, train_loss: 0.5884\n",
      "16/15, train_loss: 2.4887\n",
      "epoch 181 average loss: 0.5875\n",
      "----------\n",
      "epoch 182/500\n",
      "1/15, train_loss: 0.3710\n",
      "2/15, train_loss: 0.3363\n",
      "3/15, train_loss: 0.4009\n",
      "4/15, train_loss: 0.3238\n",
      "5/15, train_loss: 0.2937\n",
      "6/15, train_loss: 0.3413\n",
      "7/15, train_loss: 0.3772\n",
      "8/15, train_loss: 0.2818\n",
      "9/15, train_loss: 0.8952\n",
      "10/15, train_loss: 0.2911\n",
      "11/15, train_loss: 0.2952\n",
      "12/15, train_loss: 0.3923\n",
      "13/15, train_loss: 0.4079\n",
      "14/15, train_loss: 0.3958\n",
      "15/15, train_loss: 0.3461\n",
      "16/15, train_loss: 2.1590\n",
      "epoch 182 average loss: 0.4943\n",
      "----------\n",
      "epoch 183/500\n",
      "1/15, train_loss: 0.3729\n",
      "2/15, train_loss: 0.3434\n",
      "3/15, train_loss: 0.3329\n",
      "4/15, train_loss: 0.4662\n",
      "5/15, train_loss: 0.3363\n",
      "6/15, train_loss: 0.4002\n",
      "7/15, train_loss: 0.2915\n",
      "8/15, train_loss: 0.6573\n",
      "9/15, train_loss: 0.2903\n",
      "10/15, train_loss: 0.3134\n",
      "11/15, train_loss: 0.3852\n",
      "12/15, train_loss: 0.3489\n",
      "13/15, train_loss: 0.3027\n",
      "14/15, train_loss: 0.3929\n",
      "15/15, train_loss: 0.3926\n",
      "16/15, train_loss: 0.3574\n",
      "epoch 183 average loss: 0.3740\n",
      "----------\n",
      "epoch 184/500\n",
      "1/15, train_loss: 0.3055\n",
      "2/15, train_loss: 0.3494\n",
      "3/15, train_loss: 0.4272\n",
      "4/15, train_loss: 0.5470\n",
      "5/15, train_loss: 0.3249\n",
      "6/15, train_loss: 0.3083\n",
      "7/15, train_loss: 0.3467\n",
      "8/15, train_loss: 0.3124\n",
      "9/15, train_loss: 0.3015\n",
      "10/15, train_loss: 0.4051\n",
      "11/15, train_loss: 0.6040\n",
      "12/15, train_loss: 0.4162\n",
      "13/15, train_loss: 0.4765\n",
      "14/15, train_loss: 0.3613\n",
      "15/15, train_loss: 0.4263\n",
      "16/15, train_loss: 0.4013\n",
      "epoch 184 average loss: 0.3946\n",
      "----------\n",
      "epoch 185/500\n",
      "1/15, train_loss: 0.4187\n",
      "2/15, train_loss: 0.2806\n",
      "3/15, train_loss: 0.3846\n",
      "4/15, train_loss: 0.3217\n",
      "5/15, train_loss: 0.3503\n",
      "6/15, train_loss: 0.5974\n",
      "7/15, train_loss: 0.3003\n",
      "8/15, train_loss: 0.3488\n",
      "9/15, train_loss: 0.3305\n",
      "10/15, train_loss: 0.3901\n",
      "11/15, train_loss: 0.3884\n",
      "12/15, train_loss: 0.8253\n",
      "13/15, train_loss: 2.2080\n",
      "14/15, train_loss: 0.3574\n",
      "15/15, train_loss: 0.2978\n",
      "16/15, train_loss: 0.4598\n",
      "epoch 185 average loss: 0.5162\n",
      "----------\n",
      "epoch 186/500\n",
      "1/15, train_loss: 0.3177\n",
      "2/15, train_loss: 0.2861\n",
      "3/15, train_loss: 0.3640\n",
      "4/15, train_loss: 0.3207\n",
      "5/15, train_loss: 0.3690\n",
      "6/15, train_loss: 0.4484\n",
      "7/15, train_loss: 0.3641\n",
      "8/15, train_loss: 0.3434\n",
      "9/15, train_loss: 0.4171\n",
      "10/15, train_loss: 0.3222\n",
      "11/15, train_loss: 0.3280\n",
      "12/15, train_loss: 0.2919\n",
      "13/15, train_loss: 0.4424\n",
      "14/15, train_loss: 0.6922\n",
      "15/15, train_loss: 0.5137\n",
      "16/15, train_loss: 0.2613\n",
      "epoch 186 average loss: 0.3801\n",
      "----------\n",
      "epoch 187/500\n",
      "1/15, train_loss: 0.3621\n",
      "2/15, train_loss: 0.3556\n",
      "3/15, train_loss: 0.3851\n",
      "4/15, train_loss: 0.4429\n",
      "5/15, train_loss: 0.3273\n",
      "6/15, train_loss: 0.2963\n",
      "7/15, train_loss: 0.6122\n",
      "8/15, train_loss: 0.3051\n",
      "9/15, train_loss: 0.3957\n",
      "10/15, train_loss: 0.2676\n",
      "11/15, train_loss: 0.4221\n",
      "12/15, train_loss: 0.3540\n",
      "13/15, train_loss: 0.3161\n",
      "14/15, train_loss: 0.7648\n",
      "15/15, train_loss: 0.5020\n",
      "16/15, train_loss: 2.2292\n",
      "epoch 187 average loss: 0.5211\n",
      "----------\n",
      "epoch 188/500\n",
      "1/15, train_loss: 0.6635\n",
      "2/15, train_loss: 0.3246\n",
      "3/15, train_loss: 0.3263\n",
      "4/15, train_loss: 0.3942\n",
      "5/15, train_loss: 0.3741\n",
      "6/15, train_loss: 0.4021\n",
      "7/15, train_loss: 0.4639\n",
      "8/15, train_loss: 0.5233\n",
      "9/15, train_loss: 0.4188\n",
      "10/15, train_loss: 0.2816\n",
      "11/15, train_loss: 0.3405\n",
      "12/15, train_loss: 0.2904\n",
      "13/15, train_loss: 0.4438\n",
      "14/15, train_loss: 0.3065\n",
      "15/15, train_loss: 0.4072\n",
      "16/15, train_loss: 2.3507\n",
      "epoch 188 average loss: 0.5195\n",
      "----------\n",
      "epoch 189/500\n",
      "1/15, train_loss: 0.2790\n",
      "2/15, train_loss: 0.3646\n",
      "3/15, train_loss: 0.3336\n",
      "4/15, train_loss: 0.3514\n",
      "5/15, train_loss: 0.4651\n",
      "6/15, train_loss: 0.3332\n",
      "7/15, train_loss: 0.4047\n",
      "8/15, train_loss: 0.3011\n",
      "9/15, train_loss: 0.2602\n",
      "10/15, train_loss: 0.3590\n",
      "11/15, train_loss: 0.3270\n",
      "12/15, train_loss: 0.3602\n",
      "13/15, train_loss: 0.6852\n",
      "14/15, train_loss: 0.2766\n",
      "15/15, train_loss: 0.3929\n",
      "16/15, train_loss: 2.0176\n",
      "epoch 189 average loss: 0.4695\n",
      "----------\n",
      "epoch 190/500\n",
      "1/15, train_loss: 0.3204\n",
      "2/15, train_loss: 0.3507\n",
      "3/15, train_loss: 0.3265\n",
      "4/15, train_loss: 0.3445\n",
      "5/15, train_loss: 0.2648\n",
      "6/15, train_loss: 0.3300\n",
      "7/15, train_loss: 0.2880\n",
      "8/15, train_loss: 0.2881\n",
      "9/15, train_loss: 0.3234\n",
      "10/15, train_loss: 0.3865\n",
      "11/15, train_loss: 0.3649\n",
      "12/15, train_loss: 0.2766\n",
      "13/15, train_loss: 0.3992\n",
      "14/15, train_loss: 0.3805\n",
      "15/15, train_loss: 0.3624\n",
      "16/15, train_loss: 0.5328\n",
      "epoch 190 average loss: 0.3462\n",
      "----------\n",
      "epoch 191/500\n",
      "1/15, train_loss: 0.4036\n",
      "2/15, train_loss: 0.3843\n",
      "3/15, train_loss: 0.3123\n",
      "4/15, train_loss: 0.3192\n",
      "5/15, train_loss: 0.4028\n",
      "6/15, train_loss: 0.3242\n",
      "7/15, train_loss: 0.2756\n",
      "8/15, train_loss: 0.6978\n",
      "9/15, train_loss: 0.3451\n",
      "10/15, train_loss: 0.5887\n",
      "11/15, train_loss: 0.3469\n",
      "12/15, train_loss: 0.2968\n",
      "13/15, train_loss: 0.3003\n",
      "14/15, train_loss: 0.3254\n",
      "15/15, train_loss: 0.3226\n",
      "16/15, train_loss: 2.4687\n",
      "epoch 191 average loss: 0.5072\n",
      "----------\n",
      "epoch 192/500\n",
      "1/15, train_loss: 0.6799\n",
      "2/15, train_loss: 0.3450\n",
      "3/15, train_loss: 0.3763\n",
      "4/15, train_loss: 0.3920\n",
      "5/15, train_loss: 0.3367\n",
      "6/15, train_loss: 0.4764\n",
      "7/15, train_loss: 0.4270\n",
      "8/15, train_loss: 0.3837\n",
      "9/15, train_loss: 0.3101\n",
      "10/15, train_loss: 0.3010\n",
      "11/15, train_loss: 0.4172\n",
      "12/15, train_loss: 0.9731\n",
      "13/15, train_loss: 0.3010\n",
      "14/15, train_loss: 0.3543\n",
      "15/15, train_loss: 0.3418\n",
      "16/15, train_loss: 0.3114\n",
      "epoch 192 average loss: 0.4204\n",
      "----------\n",
      "epoch 193/500\n",
      "1/15, train_loss: 0.6241\n",
      "2/15, train_loss: 0.2938\n",
      "3/15, train_loss: 0.4029\n",
      "4/15, train_loss: 0.2860\n",
      "5/15, train_loss: 0.4008\n",
      "6/15, train_loss: 0.3501\n",
      "7/15, train_loss: 0.5044\n",
      "8/15, train_loss: 0.3570\n",
      "9/15, train_loss: 0.2665\n",
      "10/15, train_loss: 0.6432\n",
      "11/15, train_loss: 0.7876\n",
      "12/15, train_loss: 0.6690\n",
      "13/15, train_loss: 0.3482\n",
      "14/15, train_loss: 0.3129\n",
      "15/15, train_loss: 0.3145\n",
      "16/15, train_loss: 0.2551\n",
      "epoch 193 average loss: 0.4260\n",
      "----------\n",
      "epoch 194/500\n",
      "1/15, train_loss: 0.3442\n",
      "2/15, train_loss: 0.3108\n",
      "3/15, train_loss: 0.3470\n",
      "4/15, train_loss: 0.4229\n",
      "5/15, train_loss: 0.4711\n",
      "6/15, train_loss: 0.2795\n",
      "7/15, train_loss: 0.4921\n",
      "8/15, train_loss: 0.2962\n",
      "9/15, train_loss: 0.3944\n",
      "10/15, train_loss: 0.4719\n",
      "11/15, train_loss: 0.2985\n",
      "12/15, train_loss: 0.3697\n",
      "13/15, train_loss: 0.3107\n",
      "14/15, train_loss: 0.2779\n",
      "15/15, train_loss: 0.3212\n",
      "16/15, train_loss: 0.3393\n",
      "epoch 194 average loss: 0.3592\n",
      "----------\n",
      "epoch 195/500\n",
      "1/15, train_loss: 0.3351\n",
      "2/15, train_loss: 0.8245\n",
      "3/15, train_loss: 0.2730\n",
      "4/15, train_loss: 0.3388\n",
      "5/15, train_loss: 0.3785\n",
      "6/15, train_loss: 0.3198\n",
      "7/15, train_loss: 0.2547\n",
      "8/15, train_loss: 0.2964\n",
      "9/15, train_loss: 0.3312\n",
      "10/15, train_loss: 0.3667\n",
      "11/15, train_loss: 0.5791\n",
      "12/15, train_loss: 0.3302\n",
      "13/15, train_loss: 0.2796\n",
      "14/15, train_loss: 0.3668\n",
      "15/15, train_loss: 0.6605\n",
      "16/15, train_loss: 2.2483\n",
      "epoch 195 average loss: 0.5115\n",
      "----------\n",
      "epoch 196/500\n",
      "1/15, train_loss: 0.3062\n",
      "2/15, train_loss: 0.2725\n",
      "3/15, train_loss: 0.3334\n",
      "4/15, train_loss: 0.3375\n",
      "5/15, train_loss: 0.2788\n",
      "6/15, train_loss: 0.3494\n",
      "7/15, train_loss: 0.2581\n",
      "8/15, train_loss: 0.3992\n",
      "9/15, train_loss: 0.3009\n",
      "10/15, train_loss: 0.3529\n",
      "11/15, train_loss: 0.3803\n",
      "12/15, train_loss: 0.2737\n",
      "13/15, train_loss: 0.4516\n",
      "14/15, train_loss: 0.2553\n",
      "15/15, train_loss: 0.4148\n",
      "16/15, train_loss: 0.3164\n",
      "epoch 196 average loss: 0.3301\n",
      "----------\n",
      "epoch 197/500\n",
      "1/15, train_loss: 0.2617\n",
      "2/15, train_loss: 0.3739\n",
      "3/15, train_loss: 0.2869\n",
      "4/15, train_loss: 0.3352\n",
      "5/15, train_loss: 0.2801\n",
      "6/15, train_loss: 0.3131\n",
      "7/15, train_loss: 0.3695\n",
      "8/15, train_loss: 0.3129\n",
      "9/15, train_loss: 0.3296\n",
      "10/15, train_loss: 0.3712\n",
      "11/15, train_loss: 0.3466\n",
      "12/15, train_loss: 0.4557\n",
      "13/15, train_loss: 0.3082\n",
      "14/15, train_loss: 0.3181\n",
      "15/15, train_loss: 2.4282\n",
      "16/15, train_loss: 2.3697\n",
      "epoch 197 average loss: 0.5913\n",
      "----------\n",
      "epoch 198/500\n",
      "1/15, train_loss: 0.7959\n",
      "2/15, train_loss: 0.2507\n",
      "3/15, train_loss: 0.3828\n",
      "4/15, train_loss: 0.2638\n",
      "5/15, train_loss: 0.2855\n",
      "6/15, train_loss: 0.3487\n",
      "7/15, train_loss: 0.3469\n",
      "8/15, train_loss: 0.3774\n",
      "9/15, train_loss: 0.2729\n",
      "10/15, train_loss: 0.3094\n",
      "11/15, train_loss: 0.3931\n",
      "12/15, train_loss: 0.2750\n",
      "13/15, train_loss: 0.3592\n",
      "14/15, train_loss: 0.2550\n",
      "15/15, train_loss: 0.2410\n",
      "16/15, train_loss: 2.7069\n",
      "epoch 198 average loss: 0.4915\n",
      "----------\n",
      "epoch 199/500\n",
      "1/15, train_loss: 0.3002\n",
      "2/15, train_loss: 0.3327\n",
      "3/15, train_loss: 0.2750\n",
      "4/15, train_loss: 0.3566\n",
      "5/15, train_loss: 0.2878\n",
      "6/15, train_loss: 0.2953\n",
      "7/15, train_loss: 0.3568\n",
      "8/15, train_loss: 0.3807\n",
      "9/15, train_loss: 0.4949\n",
      "10/15, train_loss: 0.3929\n",
      "11/15, train_loss: 0.2920\n",
      "12/15, train_loss: 0.3789\n",
      "13/15, train_loss: 0.2523\n",
      "14/15, train_loss: 0.3274\n",
      "15/15, train_loss: 0.3141\n",
      "16/15, train_loss: 0.4416\n",
      "epoch 199 average loss: 0.3425\n",
      "----------\n",
      "epoch 200/500\n",
      "1/15, train_loss: 0.3153\n",
      "2/15, train_loss: 0.3492\n",
      "3/15, train_loss: 0.2868\n",
      "4/15, train_loss: 0.3410\n",
      "5/15, train_loss: 0.4500\n",
      "6/15, train_loss: 0.3134\n",
      "7/15, train_loss: 2.1267\n",
      "8/15, train_loss: 0.2742\n",
      "9/15, train_loss: 0.2988\n",
      "10/15, train_loss: 0.3381\n",
      "11/15, train_loss: 0.3487\n",
      "12/15, train_loss: 0.3184\n",
      "13/15, train_loss: 0.3896\n",
      "14/15, train_loss: 0.3911\n",
      "15/15, train_loss: 0.2340\n",
      "16/15, train_loss: 2.8530\n",
      "epoch 200 average loss: 0.6018\n",
      "----------\n",
      "epoch 201/500\n",
      "1/15, train_loss: 0.3183\n",
      "2/15, train_loss: 0.2975\n",
      "3/15, train_loss: 0.3806\n",
      "4/15, train_loss: 0.3395\n",
      "5/15, train_loss: 0.6300\n",
      "6/15, train_loss: 0.2597\n",
      "7/15, train_loss: 0.2583\n",
      "8/15, train_loss: 0.4566\n",
      "9/15, train_loss: 0.2638\n",
      "10/15, train_loss: 0.3584\n",
      "11/15, train_loss: 0.3348\n",
      "12/15, train_loss: 0.4351\n",
      "13/15, train_loss: 0.3853\n",
      "14/15, train_loss: 0.3208\n",
      "15/15, train_loss: 0.2887\n",
      "16/15, train_loss: 2.6813\n",
      "epoch 201 average loss: 0.5005\n",
      "----------\n",
      "epoch 202/500\n",
      "1/15, train_loss: 0.2474\n",
      "2/15, train_loss: 0.5609\n",
      "3/15, train_loss: 0.2789\n",
      "4/15, train_loss: 0.5153\n",
      "5/15, train_loss: 0.2912\n",
      "6/15, train_loss: 0.2474\n",
      "7/15, train_loss: 0.3441\n",
      "8/15, train_loss: 0.2934\n",
      "9/15, train_loss: 0.3740\n",
      "10/15, train_loss: 0.2841\n",
      "11/15, train_loss: 0.3086\n",
      "12/15, train_loss: 0.5476\n",
      "13/15, train_loss: 0.4260\n",
      "14/15, train_loss: 0.2913\n",
      "15/15, train_loss: 0.3226\n",
      "16/15, train_loss: 2.4583\n",
      "epoch 202 average loss: 0.4869\n",
      "----------\n",
      "epoch 203/500\n",
      "1/15, train_loss: 0.3347\n",
      "2/15, train_loss: 0.2610\n",
      "3/15, train_loss: 0.3281\n",
      "4/15, train_loss: 0.2841\n",
      "5/15, train_loss: 0.2994\n",
      "6/15, train_loss: 0.2906\n",
      "7/15, train_loss: 0.2971\n",
      "8/15, train_loss: 0.3046\n",
      "9/15, train_loss: 0.2707\n",
      "10/15, train_loss: 0.2727\n",
      "11/15, train_loss: 0.3701\n",
      "12/15, train_loss: 0.3227\n",
      "13/15, train_loss: 0.4729\n",
      "14/15, train_loss: 2.2605\n",
      "15/15, train_loss: 0.3441\n",
      "16/15, train_loss: 2.5698\n",
      "epoch 203 average loss: 0.5802\n",
      "----------\n",
      "epoch 204/500\n",
      "1/15, train_loss: 0.3199\n",
      "2/15, train_loss: 0.2614\n",
      "3/15, train_loss: 0.2706\n",
      "4/15, train_loss: 0.2523\n",
      "5/15, train_loss: 0.2579\n",
      "6/15, train_loss: 0.2533\n",
      "7/15, train_loss: 0.3272\n",
      "8/15, train_loss: 0.3173\n",
      "9/15, train_loss: 0.3000\n",
      "10/15, train_loss: 0.3014\n",
      "11/15, train_loss: 0.2727\n",
      "12/15, train_loss: 0.3769\n",
      "13/15, train_loss: 0.3166\n",
      "14/15, train_loss: 0.3089\n",
      "15/15, train_loss: 0.4612\n",
      "16/15, train_loss: 2.2903\n",
      "epoch 204 average loss: 0.4305\n",
      "----------\n",
      "epoch 205/500\n",
      "1/15, train_loss: 0.5030\n",
      "2/15, train_loss: 0.2791\n",
      "3/15, train_loss: 0.3008\n",
      "4/15, train_loss: 0.3144\n",
      "5/15, train_loss: 0.2707\n",
      "6/15, train_loss: 0.2898\n",
      "7/15, train_loss: 0.2835\n",
      "8/15, train_loss: 0.2831\n",
      "9/15, train_loss: 0.3409\n",
      "10/15, train_loss: 0.2644\n",
      "11/15, train_loss: 0.3414\n",
      "12/15, train_loss: 0.2491\n",
      "13/15, train_loss: 0.3203\n",
      "14/15, train_loss: 0.3118\n",
      "15/15, train_loss: 0.2413\n",
      "16/15, train_loss: 0.2987\n",
      "epoch 205 average loss: 0.3058\n",
      "----------\n",
      "epoch 206/500\n",
      "1/15, train_loss: 0.5673\n",
      "2/15, train_loss: 0.2570\n",
      "3/15, train_loss: 0.3390\n",
      "4/15, train_loss: 0.3374\n",
      "5/15, train_loss: 0.3927\n",
      "6/15, train_loss: 0.2765\n",
      "7/15, train_loss: 0.2845\n",
      "8/15, train_loss: 0.2915\n",
      "9/15, train_loss: 0.2826\n",
      "10/15, train_loss: 0.3224\n",
      "11/15, train_loss: 0.2912\n",
      "12/15, train_loss: 0.2651\n",
      "13/15, train_loss: 0.2726\n",
      "14/15, train_loss: 0.2995\n",
      "15/15, train_loss: 0.3430\n",
      "16/15, train_loss: 0.3990\n",
      "epoch 206 average loss: 0.3263\n",
      "----------\n",
      "epoch 207/500\n",
      "1/15, train_loss: 0.2778\n",
      "2/15, train_loss: 0.3287\n",
      "3/15, train_loss: 0.3140\n",
      "4/15, train_loss: 0.3503\n",
      "5/15, train_loss: 0.2664\n",
      "6/15, train_loss: 0.2978\n",
      "7/15, train_loss: 2.1028\n",
      "8/15, train_loss: 0.3009\n",
      "9/15, train_loss: 0.3040\n",
      "10/15, train_loss: 0.3313\n",
      "11/15, train_loss: 0.6092\n",
      "12/15, train_loss: 0.3001\n",
      "13/15, train_loss: 0.3437\n",
      "14/15, train_loss: 0.2659\n",
      "15/15, train_loss: 0.3596\n",
      "16/15, train_loss: 0.2657\n",
      "epoch 207 average loss: 0.4386\n",
      "----------\n",
      "epoch 208/500\n",
      "1/15, train_loss: 0.8376\n",
      "2/15, train_loss: 0.3485\n",
      "3/15, train_loss: 0.5074\n",
      "4/15, train_loss: 0.3763\n",
      "5/15, train_loss: 0.3073\n",
      "6/15, train_loss: 0.2683\n",
      "7/15, train_loss: 0.3449\n",
      "8/15, train_loss: 0.3281\n",
      "9/15, train_loss: 0.2421\n",
      "10/15, train_loss: 0.3638\n",
      "11/15, train_loss: 0.3371\n",
      "12/15, train_loss: 0.3311\n",
      "13/15, train_loss: 0.5515\n",
      "14/15, train_loss: 0.2666\n",
      "15/15, train_loss: 0.2748\n",
      "16/15, train_loss: 2.2864\n",
      "epoch 208 average loss: 0.4982\n",
      "----------\n",
      "epoch 209/500\n",
      "1/15, train_loss: 0.2816\n",
      "2/15, train_loss: 0.3285\n",
      "3/15, train_loss: 0.2824\n",
      "4/15, train_loss: 0.4580\n",
      "5/15, train_loss: 0.2534\n",
      "6/15, train_loss: 0.3062\n",
      "7/15, train_loss: 0.8075\n",
      "8/15, train_loss: 0.2865\n",
      "9/15, train_loss: 0.2727\n",
      "10/15, train_loss: 0.3334\n",
      "11/15, train_loss: 0.2905\n",
      "12/15, train_loss: 0.3594\n",
      "13/15, train_loss: 0.3497\n",
      "14/15, train_loss: 0.2831\n",
      "15/15, train_loss: 0.3606\n",
      "16/15, train_loss: 0.5173\n",
      "epoch 209 average loss: 0.3607\n",
      "----------\n",
      "epoch 210/500\n",
      "1/15, train_loss: 0.2942\n",
      "2/15, train_loss: 0.2873\n",
      "3/15, train_loss: 0.2798\n",
      "4/15, train_loss: 0.3120\n",
      "5/15, train_loss: 0.3617\n",
      "6/15, train_loss: 0.5418\n",
      "7/15, train_loss: 0.3113\n",
      "8/15, train_loss: 0.2575\n",
      "9/15, train_loss: 0.4068\n",
      "10/15, train_loss: 0.2741\n",
      "11/15, train_loss: 0.3600\n",
      "12/15, train_loss: 0.4695\n",
      "13/15, train_loss: 0.3022\n",
      "14/15, train_loss: 0.2862\n",
      "15/15, train_loss: 0.3344\n",
      "16/15, train_loss: 2.5764\n",
      "epoch 210 average loss: 0.4784\n",
      "----------\n",
      "epoch 211/500\n",
      "1/15, train_loss: 0.3197\n",
      "2/15, train_loss: 0.2582\n",
      "3/15, train_loss: 0.3643\n",
      "4/15, train_loss: 0.3679\n",
      "5/15, train_loss: 0.3754\n",
      "6/15, train_loss: 0.3621\n",
      "7/15, train_loss: 0.2697\n",
      "8/15, train_loss: 0.6826\n",
      "9/15, train_loss: 0.3300\n",
      "10/15, train_loss: 0.3696\n",
      "11/15, train_loss: 0.3208\n",
      "12/15, train_loss: 0.4627\n",
      "13/15, train_loss: 0.3141\n",
      "14/15, train_loss: 0.3150\n",
      "15/15, train_loss: 0.4140\n",
      "16/15, train_loss: 0.5811\n",
      "epoch 211 average loss: 0.3817\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 211 validation loss: 0.9785 dice_score: 0.8596 acc_metric: 0.6994 accuracy: 0.6994, f1score: 0.7177\n",
      " saved Best PMetric: 0.7795 at epoch: 211\n",
      "----------\n",
      "epoch 212/500\n",
      "1/15, train_loss: 0.2688\n",
      "2/15, train_loss: 0.4355\n",
      "3/15, train_loss: 0.2623\n",
      "4/15, train_loss: 0.3414\n",
      "5/15, train_loss: 0.9368\n",
      "6/15, train_loss: 0.4513\n",
      "7/15, train_loss: 0.4135\n",
      "8/15, train_loss: 0.3185\n",
      "9/15, train_loss: 0.3217\n",
      "10/15, train_loss: 0.3142\n",
      "11/15, train_loss: 0.2598\n",
      "12/15, train_loss: 0.3285\n",
      "13/15, train_loss: 0.2697\n",
      "14/15, train_loss: 0.3737\n",
      "15/15, train_loss: 0.2678\n",
      "16/15, train_loss: 0.3008\n",
      "epoch 212 average loss: 0.3665\n",
      "----------\n",
      "epoch 213/500\n",
      "1/15, train_loss: 0.2987\n",
      "2/15, train_loss: 0.2280\n",
      "3/15, train_loss: 0.3201\n",
      "4/15, train_loss: 0.2851\n",
      "5/15, train_loss: 0.2937\n",
      "6/15, train_loss: 0.3451\n",
      "7/15, train_loss: 0.3068\n",
      "8/15, train_loss: 0.3735\n",
      "9/15, train_loss: 0.4640\n",
      "10/15, train_loss: 0.2999\n",
      "11/15, train_loss: 0.3451\n",
      "12/15, train_loss: 0.3962\n",
      "13/15, train_loss: 0.3121\n",
      "14/15, train_loss: 0.3172\n",
      "15/15, train_loss: 0.2837\n",
      "16/15, train_loss: 2.3707\n",
      "epoch 213 average loss: 0.4525\n",
      "----------\n",
      "epoch 214/500\n",
      "1/15, train_loss: 0.2568\n",
      "2/15, train_loss: 0.3964\n",
      "3/15, train_loss: 2.3039\n",
      "4/15, train_loss: 0.2784\n",
      "5/15, train_loss: 0.2830\n",
      "6/15, train_loss: 0.2915\n",
      "7/15, train_loss: 0.2817\n",
      "8/15, train_loss: 0.3437\n",
      "9/15, train_loss: 0.3283\n",
      "10/15, train_loss: 0.2993\n",
      "11/15, train_loss: 0.2441\n",
      "12/15, train_loss: 0.3959\n",
      "13/15, train_loss: 0.3618\n",
      "14/15, train_loss: 0.2715\n",
      "15/15, train_loss: 0.5747\n",
      "16/15, train_loss: 2.0470\n",
      "epoch 214 average loss: 0.5599\n",
      "----------\n",
      "epoch 215/500\n",
      "1/15, train_loss: 0.3566\n",
      "2/15, train_loss: 0.2923\n",
      "3/15, train_loss: 1.9894\n",
      "4/15, train_loss: 0.3575\n",
      "5/15, train_loss: 0.3330\n",
      "6/15, train_loss: 0.2754\n",
      "7/15, train_loss: 0.2761\n",
      "8/15, train_loss: 0.3358\n",
      "9/15, train_loss: 0.3559\n",
      "10/15, train_loss: 0.2972\n",
      "11/15, train_loss: 0.2979\n",
      "12/15, train_loss: 0.2769\n",
      "13/15, train_loss: 0.2615\n",
      "14/15, train_loss: 0.2869\n",
      "15/15, train_loss: 0.3147\n",
      "16/15, train_loss: 0.3186\n",
      "epoch 215 average loss: 0.4141\n",
      "----------\n",
      "epoch 216/500\n",
      "1/15, train_loss: 0.3743\n",
      "2/15, train_loss: 0.3135\n",
      "3/15, train_loss: 0.3451\n",
      "4/15, train_loss: 0.2646\n",
      "5/15, train_loss: 0.3560\n",
      "6/15, train_loss: 0.2955\n",
      "7/15, train_loss: 0.7202\n",
      "8/15, train_loss: 0.3426\n",
      "9/15, train_loss: 0.3307\n",
      "10/15, train_loss: 0.5131\n",
      "11/15, train_loss: 0.2617\n",
      "12/15, train_loss: 0.6259\n",
      "13/15, train_loss: 0.2801\n",
      "14/15, train_loss: 0.2525\n",
      "15/15, train_loss: 0.3447\n",
      "16/15, train_loss: 0.2585\n",
      "epoch 216 average loss: 0.3674\n",
      "----------\n",
      "epoch 217/500\n",
      "1/15, train_loss: 0.2814\n",
      "2/15, train_loss: 0.2929\n",
      "3/15, train_loss: 0.3264\n",
      "4/15, train_loss: 0.3115\n",
      "5/15, train_loss: 0.3002\n",
      "6/15, train_loss: 0.2577\n",
      "7/15, train_loss: 0.6248\n",
      "8/15, train_loss: 0.2404\n",
      "9/15, train_loss: 0.2540\n",
      "10/15, train_loss: 0.2345\n",
      "11/15, train_loss: 0.3096\n",
      "12/15, train_loss: 0.3265\n",
      "13/15, train_loss: 0.2832\n",
      "14/15, train_loss: 0.3217\n",
      "15/15, train_loss: 0.3094\n",
      "16/15, train_loss: 2.4458\n",
      "epoch 217 average loss: 0.4450\n",
      "----------\n",
      "epoch 218/500\n",
      "1/15, train_loss: 0.6342\n",
      "2/15, train_loss: 0.2749\n",
      "3/15, train_loss: 0.3373\n",
      "4/15, train_loss: 0.2595\n",
      "5/15, train_loss: 0.2780\n",
      "6/15, train_loss: 1.0366\n",
      "7/15, train_loss: 0.2368\n",
      "8/15, train_loss: 0.3102\n",
      "9/15, train_loss: 0.2966\n",
      "10/15, train_loss: 0.2822\n",
      "11/15, train_loss: 0.2601\n",
      "12/15, train_loss: 0.3193\n",
      "13/15, train_loss: 0.3023\n",
      "14/15, train_loss: 0.2532\n",
      "15/15, train_loss: 0.2490\n",
      "16/15, train_loss: 2.7142\n",
      "epoch 218 average loss: 0.5028\n",
      "----------\n",
      "epoch 219/500\n",
      "1/15, train_loss: 0.5533\n",
      "2/15, train_loss: 0.3647\n",
      "3/15, train_loss: 0.3557\n",
      "4/15, train_loss: 0.3938\n",
      "5/15, train_loss: 0.2547\n",
      "6/15, train_loss: 0.2712\n",
      "7/15, train_loss: 0.3890\n",
      "8/15, train_loss: 0.2994\n",
      "9/15, train_loss: 0.2387\n",
      "10/15, train_loss: 0.2974\n",
      "11/15, train_loss: 0.6048\n",
      "12/15, train_loss: 0.3778\n",
      "13/15, train_loss: 0.4711\n",
      "14/15, train_loss: 0.3588\n",
      "15/15, train_loss: 0.2744\n",
      "16/15, train_loss: 2.8268\n",
      "epoch 219 average loss: 0.5207\n",
      "----------\n",
      "epoch 220/500\n",
      "1/15, train_loss: 0.2857\n",
      "2/15, train_loss: 0.2813\n",
      "3/15, train_loss: 0.3223\n",
      "4/15, train_loss: 0.6319\n",
      "5/15, train_loss: 0.5296\n",
      "6/15, train_loss: 0.2581\n",
      "7/15, train_loss: 0.3240\n",
      "8/15, train_loss: 0.2509\n",
      "9/15, train_loss: 0.3023\n",
      "10/15, train_loss: 0.2635\n",
      "11/15, train_loss: 0.3582\n",
      "12/15, train_loss: 0.2311\n",
      "13/15, train_loss: 0.6487\n",
      "14/15, train_loss: 0.2534\n",
      "15/15, train_loss: 0.2549\n",
      "16/15, train_loss: 0.2608\n",
      "epoch 220 average loss: 0.3410\n",
      "----------\n",
      "epoch 221/500\n",
      "1/15, train_loss: 0.3400\n",
      "2/15, train_loss: 0.3210\n",
      "3/15, train_loss: 0.4107\n",
      "4/15, train_loss: 0.2366\n",
      "5/15, train_loss: 0.4471\n",
      "6/15, train_loss: 0.2636\n",
      "7/15, train_loss: 0.2783\n",
      "8/15, train_loss: 0.2771\n",
      "9/15, train_loss: 0.2661\n",
      "10/15, train_loss: 0.3414\n",
      "11/15, train_loss: 0.3029\n",
      "12/15, train_loss: 0.3168\n",
      "13/15, train_loss: 0.3195\n",
      "14/15, train_loss: 0.2806\n",
      "15/15, train_loss: 0.3753\n",
      "16/15, train_loss: 2.5875\n",
      "epoch 221 average loss: 0.4603\n",
      "----------\n",
      "epoch 222/500\n",
      "1/15, train_loss: 0.4108\n",
      "2/15, train_loss: 0.3326\n",
      "3/15, train_loss: 0.3031\n",
      "4/15, train_loss: 0.2678\n",
      "5/15, train_loss: 0.2672\n",
      "6/15, train_loss: 0.3005\n",
      "7/15, train_loss: 0.2739\n",
      "8/15, train_loss: 0.2449\n",
      "9/15, train_loss: 0.2851\n",
      "10/15, train_loss: 0.3178\n",
      "11/15, train_loss: 0.2670\n",
      "12/15, train_loss: 0.3515\n",
      "13/15, train_loss: 0.3502\n",
      "14/15, train_loss: 0.3252\n",
      "15/15, train_loss: 0.2938\n",
      "16/15, train_loss: 2.7520\n",
      "epoch 222 average loss: 0.4590\n",
      "----------\n",
      "epoch 223/500\n",
      "1/15, train_loss: 0.2726\n",
      "2/15, train_loss: 0.2796\n",
      "3/15, train_loss: 0.2528\n",
      "4/15, train_loss: 0.3078\n",
      "5/15, train_loss: 0.2446\n",
      "6/15, train_loss: 0.2650\n",
      "7/15, train_loss: 0.2832\n",
      "8/15, train_loss: 0.2871\n",
      "9/15, train_loss: 0.2581\n",
      "10/15, train_loss: 0.2637\n",
      "11/15, train_loss: 0.2612\n",
      "12/15, train_loss: 0.5154\n",
      "13/15, train_loss: 0.2743\n",
      "14/15, train_loss: 0.2893\n",
      "15/15, train_loss: 0.2396\n",
      "16/15, train_loss: 2.7484\n",
      "epoch 223 average loss: 0.4402\n",
      "----------\n",
      "epoch 224/500\n",
      "1/15, train_loss: 0.2347\n",
      "2/15, train_loss: 0.2871\n",
      "3/15, train_loss: 0.2879\n",
      "4/15, train_loss: 0.2896\n",
      "5/15, train_loss: 0.2974\n",
      "6/15, train_loss: 0.3142\n",
      "7/15, train_loss: 0.3588\n",
      "8/15, train_loss: 0.4980\n",
      "9/15, train_loss: 0.2805\n",
      "10/15, train_loss: 0.5746\n",
      "11/15, train_loss: 0.2561\n",
      "12/15, train_loss: 0.4385\n",
      "13/15, train_loss: 0.2712\n",
      "14/15, train_loss: 0.2781\n",
      "15/15, train_loss: 0.2966\n",
      "16/15, train_loss: 2.3353\n",
      "epoch 224 average loss: 0.4562\n",
      "----------\n",
      "epoch 225/500\n",
      "1/15, train_loss: 0.2977\n",
      "2/15, train_loss: 0.2882\n",
      "3/15, train_loss: 0.2438\n",
      "4/15, train_loss: 0.3680\n",
      "5/15, train_loss: 0.2783\n",
      "6/15, train_loss: 0.2602\n",
      "7/15, train_loss: 0.2942\n",
      "8/15, train_loss: 0.2689\n",
      "9/15, train_loss: 0.2963\n",
      "10/15, train_loss: 0.2589\n",
      "11/15, train_loss: 0.8355\n",
      "12/15, train_loss: 0.2992\n",
      "13/15, train_loss: 0.5948\n",
      "14/15, train_loss: 0.3219\n",
      "15/15, train_loss: 0.2604\n",
      "16/15, train_loss: 2.3349\n",
      "epoch 225 average loss: 0.4688\n",
      "----------\n",
      "epoch 226/500\n",
      "1/15, train_loss: 0.2348\n",
      "2/15, train_loss: 0.2751\n",
      "3/15, train_loss: 0.2780\n",
      "4/15, train_loss: 0.3074\n",
      "5/15, train_loss: 0.2975\n",
      "6/15, train_loss: 0.7367\n",
      "7/15, train_loss: 0.3062\n",
      "8/15, train_loss: 0.2627\n",
      "9/15, train_loss: 0.2957\n",
      "10/15, train_loss: 0.2999\n",
      "11/15, train_loss: 0.3103\n",
      "12/15, train_loss: 0.2910\n",
      "13/15, train_loss: 0.3252\n",
      "14/15, train_loss: 0.2962\n",
      "15/15, train_loss: 0.4505\n",
      "16/15, train_loss: 0.2577\n",
      "epoch 226 average loss: 0.3265\n",
      "----------\n",
      "epoch 227/500\n",
      "1/15, train_loss: 0.2657\n",
      "2/15, train_loss: 0.2809\n",
      "3/15, train_loss: 0.2505\n",
      "4/15, train_loss: 0.2788\n",
      "5/15, train_loss: 0.2675\n",
      "6/15, train_loss: 0.4539\n",
      "7/15, train_loss: 0.2917\n",
      "8/15, train_loss: 0.3304\n",
      "9/15, train_loss: 0.2968\n",
      "10/15, train_loss: 0.2787\n",
      "11/15, train_loss: 0.3622\n",
      "12/15, train_loss: 0.2640\n",
      "13/15, train_loss: 0.2649\n",
      "14/15, train_loss: 0.2837\n",
      "15/15, train_loss: 0.2526\n",
      "16/15, train_loss: 0.2240\n",
      "epoch 227 average loss: 0.2904\n",
      "----------\n",
      "epoch 228/500\n",
      "1/15, train_loss: 0.3625\n",
      "2/15, train_loss: 0.2431\n",
      "3/15, train_loss: 0.4143\n",
      "4/15, train_loss: 0.2712\n",
      "5/15, train_loss: 0.2479\n",
      "6/15, train_loss: 0.2656\n",
      "7/15, train_loss: 0.3081\n",
      "8/15, train_loss: 0.4757\n",
      "9/15, train_loss: 0.4449\n",
      "10/15, train_loss: 0.3116\n",
      "11/15, train_loss: 0.3147\n",
      "12/15, train_loss: 0.3344\n",
      "13/15, train_loss: 0.3782\n",
      "14/15, train_loss: 0.3358\n",
      "15/15, train_loss: 0.2779\n",
      "16/15, train_loss: 0.3578\n",
      "epoch 228 average loss: 0.3340\n",
      "----------\n",
      "epoch 229/500\n",
      "1/15, train_loss: 0.4003\n",
      "2/15, train_loss: 0.2696\n",
      "3/15, train_loss: 0.3377\n",
      "4/15, train_loss: 0.2912\n",
      "5/15, train_loss: 0.2959\n",
      "6/15, train_loss: 0.2264\n",
      "7/15, train_loss: 0.2751\n",
      "8/15, train_loss: 0.2935\n",
      "9/15, train_loss: 0.3160\n",
      "10/15, train_loss: 0.4598\n",
      "11/15, train_loss: 0.3310\n",
      "12/15, train_loss: 0.2698\n",
      "13/15, train_loss: 0.2458\n",
      "14/15, train_loss: 0.2566\n",
      "15/15, train_loss: 0.2815\n",
      "16/15, train_loss: 2.4089\n",
      "epoch 229 average loss: 0.4349\n",
      "----------\n",
      "epoch 230/500\n",
      "1/15, train_loss: 0.2885\n",
      "2/15, train_loss: 0.2672\n",
      "3/15, train_loss: 0.2972\n",
      "4/15, train_loss: 0.3167\n",
      "5/15, train_loss: 0.2261\n",
      "6/15, train_loss: 0.2993\n",
      "7/15, train_loss: 0.2809\n",
      "8/15, train_loss: 0.2528\n",
      "9/15, train_loss: 0.3234\n",
      "10/15, train_loss: 0.3162\n",
      "11/15, train_loss: 0.3155\n",
      "12/15, train_loss: 0.2400\n",
      "13/15, train_loss: 0.3014\n",
      "14/15, train_loss: 0.2495\n",
      "15/15, train_loss: 0.3077\n",
      "16/15, train_loss: 2.2161\n",
      "epoch 230 average loss: 0.4061\n",
      "----------\n",
      "epoch 231/500\n",
      "1/15, train_loss: 0.5278\n",
      "2/15, train_loss: 0.3106\n",
      "3/15, train_loss: 0.2616\n",
      "4/15, train_loss: 0.6248\n",
      "5/15, train_loss: 0.3848\n",
      "6/15, train_loss: 0.3530\n",
      "7/15, train_loss: 0.2949\n",
      "8/15, train_loss: 0.2723\n",
      "9/15, train_loss: 0.2582\n",
      "10/15, train_loss: 0.2949\n",
      "11/15, train_loss: 0.3566\n",
      "12/15, train_loss: 0.2826\n",
      "13/15, train_loss: 0.3573\n",
      "14/15, train_loss: 0.2384\n",
      "15/15, train_loss: 0.2573\n",
      "16/15, train_loss: 0.2662\n",
      "epoch 231 average loss: 0.3338\n",
      "----------\n",
      "epoch 232/500\n",
      "1/15, train_loss: 0.4195\n",
      "2/15, train_loss: 0.3112\n",
      "3/15, train_loss: 0.4978\n",
      "4/15, train_loss: 0.2427\n",
      "5/15, train_loss: 0.2862\n",
      "6/15, train_loss: 0.2848\n",
      "7/15, train_loss: 0.3502\n",
      "8/15, train_loss: 0.2498\n",
      "9/15, train_loss: 0.3690\n",
      "10/15, train_loss: 0.2903\n",
      "11/15, train_loss: 0.8296\n",
      "12/15, train_loss: 0.3295\n",
      "13/15, train_loss: 0.2291\n",
      "14/15, train_loss: 0.2523\n",
      "15/15, train_loss: 0.2265\n",
      "16/15, train_loss: 1.9463\n",
      "epoch 232 average loss: 0.4447\n",
      "----------\n",
      "epoch 233/500\n",
      "1/15, train_loss: 0.3588\n",
      "2/15, train_loss: 0.2538\n",
      "3/15, train_loss: 0.6733\n",
      "4/15, train_loss: 0.2964\n",
      "5/15, train_loss: 0.3159\n",
      "6/15, train_loss: 0.3145\n",
      "7/15, train_loss: 0.2657\n",
      "8/15, train_loss: 0.3287\n",
      "9/15, train_loss: 0.2723\n",
      "10/15, train_loss: 0.2393\n",
      "11/15, train_loss: 0.2810\n",
      "12/15, train_loss: 0.3165\n",
      "13/15, train_loss: 0.2654\n",
      "14/15, train_loss: 0.2417\n",
      "15/15, train_loss: 0.2510\n",
      "16/15, train_loss: 2.0448\n",
      "epoch 233 average loss: 0.4200\n",
      "----------\n",
      "epoch 234/500\n",
      "1/15, train_loss: 0.5069\n",
      "2/15, train_loss: 0.2611\n",
      "3/15, train_loss: 0.2196\n",
      "4/15, train_loss: 0.2171\n",
      "5/15, train_loss: 0.2559\n",
      "6/15, train_loss: 0.2825\n",
      "7/15, train_loss: 0.3348\n",
      "8/15, train_loss: 0.2483\n",
      "9/15, train_loss: 0.2396\n",
      "10/15, train_loss: 0.3187\n",
      "11/15, train_loss: 0.3134\n",
      "12/15, train_loss: 0.3217\n",
      "13/15, train_loss: 0.2614\n",
      "14/15, train_loss: 0.2534\n",
      "15/15, train_loss: 0.2399\n",
      "16/15, train_loss: 0.3124\n",
      "epoch 234 average loss: 0.2867\n",
      "----------\n",
      "epoch 235/500\n",
      "1/15, train_loss: 0.3015\n",
      "2/15, train_loss: 0.2586\n",
      "3/15, train_loss: 0.3374\n",
      "4/15, train_loss: 0.2578\n",
      "5/15, train_loss: 0.2355\n",
      "6/15, train_loss: 0.3024\n",
      "7/15, train_loss: 0.2628\n",
      "8/15, train_loss: 0.2324\n",
      "9/15, train_loss: 0.4090\n",
      "10/15, train_loss: 0.2473\n",
      "11/15, train_loss: 0.3204\n",
      "12/15, train_loss: 0.2644\n",
      "13/15, train_loss: 0.2362\n",
      "14/15, train_loss: 0.2545\n",
      "15/15, train_loss: 0.2293\n",
      "16/15, train_loss: 0.4564\n",
      "epoch 235 average loss: 0.2879\n",
      "----------\n",
      "epoch 236/500\n",
      "1/15, train_loss: 0.2523\n",
      "2/15, train_loss: 0.3079\n",
      "3/15, train_loss: 0.2239\n",
      "4/15, train_loss: 0.2727\n",
      "5/15, train_loss: 0.2428\n",
      "6/15, train_loss: 0.3181\n",
      "7/15, train_loss: 0.2797\n",
      "8/15, train_loss: 0.3778\n",
      "9/15, train_loss: 0.2665\n",
      "10/15, train_loss: 0.2818\n",
      "11/15, train_loss: 0.2570\n",
      "12/15, train_loss: 0.2416\n",
      "13/15, train_loss: 0.2749\n",
      "14/15, train_loss: 0.2244\n",
      "15/15, train_loss: 0.2452\n",
      "16/15, train_loss: 0.2335\n",
      "epoch 236 average loss: 0.2688\n",
      "----------\n",
      "epoch 237/500\n",
      "1/15, train_loss: 0.2155\n",
      "2/15, train_loss: 0.2904\n",
      "3/15, train_loss: 0.2694\n",
      "4/15, train_loss: 0.3089\n",
      "5/15, train_loss: 0.2707\n",
      "6/15, train_loss: 0.2327\n",
      "7/15, train_loss: 0.2608\n",
      "8/15, train_loss: 0.2542\n",
      "9/15, train_loss: 0.2570\n",
      "10/15, train_loss: 0.2741\n",
      "11/15, train_loss: 0.2228\n",
      "12/15, train_loss: 0.7053\n",
      "13/15, train_loss: 0.1972\n",
      "14/15, train_loss: 0.2937\n",
      "15/15, train_loss: 0.2342\n",
      "16/15, train_loss: 0.5335\n",
      "epoch 237 average loss: 0.3013\n",
      "----------\n",
      "epoch 238/500\n",
      "1/15, train_loss: 0.2710\n",
      "2/15, train_loss: 0.8436\n",
      "3/15, train_loss: 0.2771\n",
      "4/15, train_loss: 0.2463\n",
      "5/15, train_loss: 0.2904\n",
      "6/15, train_loss: 2.4299\n",
      "7/15, train_loss: 0.2202\n",
      "8/15, train_loss: 0.2818\n",
      "9/15, train_loss: 0.2534\n",
      "10/15, train_loss: 0.2300\n",
      "11/15, train_loss: 0.2523\n",
      "12/15, train_loss: 0.2380\n",
      "13/15, train_loss: 0.2411\n",
      "14/15, train_loss: 0.2226\n",
      "15/15, train_loss: 0.2648\n",
      "16/15, train_loss: 2.1003\n",
      "epoch 238 average loss: 0.5414\n",
      "----------\n",
      "epoch 239/500\n",
      "1/15, train_loss: 0.2635\n",
      "2/15, train_loss: 0.2563\n",
      "3/15, train_loss: 0.2797\n",
      "4/15, train_loss: 0.2482\n",
      "5/15, train_loss: 0.2947\n",
      "6/15, train_loss: 0.2529\n",
      "7/15, train_loss: 0.2801\n",
      "8/15, train_loss: 0.2927\n",
      "9/15, train_loss: 0.2738\n",
      "10/15, train_loss: 0.2891\n",
      "11/15, train_loss: 0.2245\n",
      "12/15, train_loss: 0.3864\n",
      "13/15, train_loss: 0.2607\n",
      "14/15, train_loss: 0.2449\n",
      "15/15, train_loss: 0.2659\n",
      "16/15, train_loss: 1.8816\n",
      "epoch 239 average loss: 0.3747\n",
      "----------\n",
      "epoch 240/500\n",
      "1/15, train_loss: 0.2494\n",
      "2/15, train_loss: 0.3529\n",
      "3/15, train_loss: 0.3205\n",
      "4/15, train_loss: 0.3233\n",
      "5/15, train_loss: 0.2774\n",
      "6/15, train_loss: 0.3244\n",
      "7/15, train_loss: 0.2522\n",
      "8/15, train_loss: 0.2215\n",
      "9/15, train_loss: 0.2698\n",
      "10/15, train_loss: 0.3147\n",
      "11/15, train_loss: 0.3742\n",
      "12/15, train_loss: 0.2386\n",
      "13/15, train_loss: 0.2376\n",
      "14/15, train_loss: 0.2902\n",
      "15/15, train_loss: 0.2806\n",
      "16/15, train_loss: 2.0263\n",
      "epoch 240 average loss: 0.3971\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 240 validation loss: 0.9825 dice_score: 0.8515 acc_metric: 0.7114 accuracy: 0.7114, f1score: 0.6935\n",
      " saved Best PMetric: 0.7814 at epoch: 240\n",
      "----------\n",
      "epoch 241/500\n",
      "1/15, train_loss: 0.3491\n",
      "2/15, train_loss: 0.2466\n",
      "3/15, train_loss: 0.2540\n",
      "4/15, train_loss: 0.2240\n",
      "5/15, train_loss: 0.3707\n",
      "6/15, train_loss: 0.2264\n",
      "7/15, train_loss: 0.4236\n",
      "8/15, train_loss: 0.2687\n",
      "9/15, train_loss: 0.2458\n",
      "10/15, train_loss: 0.3596\n",
      "11/15, train_loss: 0.2488\n",
      "12/15, train_loss: 0.5097\n",
      "13/15, train_loss: 0.2623\n",
      "14/15, train_loss: 0.2501\n",
      "15/15, train_loss: 0.3102\n",
      "16/15, train_loss: 1.9488\n",
      "epoch 241 average loss: 0.4061\n",
      "----------\n",
      "epoch 242/500\n",
      "1/15, train_loss: 0.2427\n",
      "2/15, train_loss: 0.2753\n",
      "3/15, train_loss: 0.3883\n",
      "4/15, train_loss: 0.3156\n",
      "5/15, train_loss: 0.2359\n",
      "6/15, train_loss: 0.2191\n",
      "7/15, train_loss: 0.2448\n",
      "8/15, train_loss: 0.2186\n",
      "9/15, train_loss: 0.3763\n",
      "10/15, train_loss: 0.2751\n",
      "11/15, train_loss: 0.2398\n",
      "12/15, train_loss: 0.3132\n",
      "13/15, train_loss: 0.2467\n",
      "14/15, train_loss: 0.2812\n",
      "15/15, train_loss: 0.2557\n",
      "16/15, train_loss: 0.2529\n",
      "epoch 242 average loss: 0.2738\n",
      "----------\n",
      "epoch 243/500\n",
      "1/15, train_loss: 0.2474\n",
      "2/15, train_loss: 0.3056\n",
      "3/15, train_loss: 0.2812\n",
      "4/15, train_loss: 0.2440\n",
      "5/15, train_loss: 0.2682\n",
      "6/15, train_loss: 0.7027\n",
      "7/15, train_loss: 1.9432\n",
      "8/15, train_loss: 0.2485\n",
      "9/15, train_loss: 0.2407\n",
      "10/15, train_loss: 0.2634\n",
      "11/15, train_loss: 0.2683\n",
      "12/15, train_loss: 0.2195\n",
      "13/15, train_loss: 0.3061\n",
      "14/15, train_loss: 0.2817\n",
      "15/15, train_loss: 0.2348\n",
      "16/15, train_loss: 0.2291\n",
      "epoch 243 average loss: 0.3928\n",
      "----------\n",
      "epoch 244/500\n",
      "1/15, train_loss: 0.2192\n",
      "2/15, train_loss: 0.2484\n",
      "3/15, train_loss: 0.3053\n",
      "4/15, train_loss: 0.2750\n",
      "5/15, train_loss: 0.3417\n",
      "6/15, train_loss: 0.2158\n",
      "7/15, train_loss: 0.3681\n",
      "8/15, train_loss: 0.2635\n",
      "9/15, train_loss: 0.2357\n",
      "10/15, train_loss: 0.4099\n",
      "11/15, train_loss: 0.6906\n",
      "12/15, train_loss: 0.2593\n",
      "13/15, train_loss: 0.2355\n",
      "14/15, train_loss: 0.2411\n",
      "15/15, train_loss: 0.3153\n",
      "16/15, train_loss: 2.6477\n",
      "epoch 244 average loss: 0.4545\n",
      "----------\n",
      "epoch 245/500\n",
      "1/15, train_loss: 0.2706\n",
      "2/15, train_loss: 0.2890\n",
      "3/15, train_loss: 0.2295\n",
      "4/15, train_loss: 0.2528\n",
      "5/15, train_loss: 0.2130\n",
      "6/15, train_loss: 0.4081\n",
      "7/15, train_loss: 0.2192\n",
      "8/15, train_loss: 0.2371\n",
      "9/15, train_loss: 0.2545\n",
      "10/15, train_loss: 0.2608\n",
      "11/15, train_loss: 0.4053\n",
      "12/15, train_loss: 0.2548\n",
      "13/15, train_loss: 0.3051\n",
      "14/15, train_loss: 0.3126\n",
      "15/15, train_loss: 0.2790\n",
      "16/15, train_loss: 0.3523\n",
      "epoch 245 average loss: 0.2840\n",
      "----------\n",
      "epoch 246/500\n",
      "1/15, train_loss: 0.2719\n",
      "2/15, train_loss: 0.2207\n",
      "3/15, train_loss: 2.0726\n",
      "4/15, train_loss: 0.2590\n",
      "5/15, train_loss: 0.3342\n",
      "6/15, train_loss: 0.3620\n",
      "7/15, train_loss: 0.2232\n",
      "8/15, train_loss: 0.2630\n",
      "9/15, train_loss: 0.2168\n",
      "10/15, train_loss: 0.2989\n",
      "11/15, train_loss: 0.2753\n",
      "12/15, train_loss: 0.2466\n",
      "13/15, train_loss: 0.2782\n",
      "14/15, train_loss: 0.3061\n",
      "15/15, train_loss: 0.3154\n",
      "16/15, train_loss: 0.2360\n",
      "epoch 246 average loss: 0.3862\n",
      "----------\n",
      "epoch 247/500\n",
      "1/15, train_loss: 0.2327\n",
      "2/15, train_loss: 0.3250\n",
      "3/15, train_loss: 0.3275\n",
      "4/15, train_loss: 0.3088\n",
      "5/15, train_loss: 0.2425\n",
      "6/15, train_loss: 0.2215\n",
      "7/15, train_loss: 0.2493\n",
      "8/15, train_loss: 1.2837\n",
      "9/15, train_loss: 0.2875\n",
      "10/15, train_loss: 0.2195\n",
      "11/15, train_loss: 0.4090\n",
      "12/15, train_loss: 0.3214\n",
      "13/15, train_loss: 0.2242\n",
      "14/15, train_loss: 0.3689\n",
      "15/15, train_loss: 0.4140\n",
      "16/15, train_loss: 0.1979\n",
      "epoch 247 average loss: 0.3521\n",
      "----------\n",
      "epoch 248/500\n",
      "1/15, train_loss: 0.2019\n",
      "2/15, train_loss: 0.2309\n",
      "3/15, train_loss: 0.2340\n",
      "4/15, train_loss: 0.2431\n",
      "5/15, train_loss: 0.2653\n",
      "6/15, train_loss: 0.3132\n",
      "7/15, train_loss: 0.2221\n",
      "8/15, train_loss: 0.2531\n",
      "9/15, train_loss: 2.5041\n",
      "10/15, train_loss: 0.2937\n",
      "11/15, train_loss: 0.2527\n",
      "12/15, train_loss: 0.3125\n",
      "13/15, train_loss: 0.2476\n",
      "14/15, train_loss: 0.2643\n",
      "15/15, train_loss: 0.2057\n",
      "16/15, train_loss: 0.2581\n",
      "epoch 248 average loss: 0.3939\n",
      "----------\n",
      "epoch 249/500\n",
      "1/15, train_loss: 0.4869\n",
      "2/15, train_loss: 0.2478\n",
      "3/15, train_loss: 0.2506\n",
      "4/15, train_loss: 0.2297\n",
      "5/15, train_loss: 0.2470\n",
      "6/15, train_loss: 0.3997\n",
      "7/15, train_loss: 0.2833\n",
      "8/15, train_loss: 0.3642\n",
      "9/15, train_loss: 0.2223\n",
      "10/15, train_loss: 0.2681\n",
      "11/15, train_loss: 0.2032\n",
      "12/15, train_loss: 0.2607\n",
      "13/15, train_loss: 0.2200\n",
      "14/15, train_loss: 0.2371\n",
      "15/15, train_loss: 0.2656\n",
      "16/15, train_loss: 0.5343\n",
      "epoch 249 average loss: 0.2950\n",
      "----------\n",
      "epoch 250/500\n",
      "1/15, train_loss: 0.3444\n",
      "2/15, train_loss: 0.2343\n",
      "3/15, train_loss: 0.2187\n",
      "4/15, train_loss: 0.3931\n",
      "5/15, train_loss: 0.2442\n",
      "6/15, train_loss: 0.2893\n",
      "7/15, train_loss: 0.2523\n",
      "8/15, train_loss: 0.2167\n",
      "9/15, train_loss: 0.3140\n",
      "10/15, train_loss: 0.2520\n",
      "11/15, train_loss: 0.3697\n",
      "12/15, train_loss: 0.5381\n",
      "13/15, train_loss: 0.2429\n",
      "14/15, train_loss: 0.2490\n",
      "15/15, train_loss: 0.3148\n",
      "16/15, train_loss: 2.7996\n",
      "epoch 250 average loss: 0.4546\n",
      "----------\n",
      "epoch 251/500\n",
      "1/15, train_loss: 0.1994\n",
      "2/15, train_loss: 0.2264\n",
      "3/15, train_loss: 0.4149\n",
      "4/15, train_loss: 0.2619\n",
      "5/15, train_loss: 0.2851\n",
      "6/15, train_loss: 1.3932\n",
      "7/15, train_loss: 0.2258\n",
      "8/15, train_loss: 0.2397\n",
      "9/15, train_loss: 0.2501\n",
      "10/15, train_loss: 0.2377\n",
      "11/15, train_loss: 0.1927\n",
      "12/15, train_loss: 0.2869\n",
      "13/15, train_loss: 0.2415\n",
      "14/15, train_loss: 1.2430\n",
      "15/15, train_loss: 0.2707\n",
      "16/15, train_loss: 2.4477\n",
      "epoch 251 average loss: 0.5261\n",
      "----------\n",
      "epoch 252/500\n",
      "1/15, train_loss: 0.3204\n",
      "2/15, train_loss: 0.2358\n",
      "3/15, train_loss: 0.2111\n",
      "4/15, train_loss: 0.3154\n",
      "5/15, train_loss: 0.2177\n",
      "6/15, train_loss: 1.3919\n",
      "7/15, train_loss: 0.2759\n",
      "8/15, train_loss: 0.2224\n",
      "9/15, train_loss: 0.2552\n",
      "10/15, train_loss: 0.2433\n",
      "11/15, train_loss: 0.2092\n",
      "12/15, train_loss: 0.3329\n",
      "13/15, train_loss: 0.2085\n",
      "14/15, train_loss: 0.2771\n",
      "15/15, train_loss: 0.6371\n",
      "16/15, train_loss: 0.2074\n",
      "epoch 252 average loss: 0.3476\n",
      "----------\n",
      "epoch 253/500\n",
      "1/15, train_loss: 0.3021\n",
      "2/15, train_loss: 0.3329\n",
      "3/15, train_loss: 0.3943\n",
      "4/15, train_loss: 0.2495\n",
      "5/15, train_loss: 0.2030\n",
      "6/15, train_loss: 0.2286\n",
      "7/15, train_loss: 0.2730\n",
      "8/15, train_loss: 0.6251\n",
      "9/15, train_loss: 0.2717\n",
      "10/15, train_loss: 0.2910\n",
      "11/15, train_loss: 0.7119\n",
      "12/15, train_loss: 0.2203\n",
      "13/15, train_loss: 0.2685\n",
      "14/15, train_loss: 0.2250\n",
      "15/15, train_loss: 0.4311\n",
      "16/15, train_loss: 0.3118\n",
      "epoch 253 average loss: 0.3337\n",
      "----------\n",
      "epoch 254/500\n",
      "1/15, train_loss: 0.2748\n",
      "2/15, train_loss: 0.2395\n",
      "3/15, train_loss: 0.2184\n",
      "4/15, train_loss: 0.2519\n",
      "5/15, train_loss: 0.2239\n",
      "6/15, train_loss: 0.2601\n",
      "7/15, train_loss: 0.2596\n",
      "8/15, train_loss: 0.2462\n",
      "9/15, train_loss: 0.2084\n",
      "10/15, train_loss: 0.3325\n",
      "11/15, train_loss: 0.2234\n",
      "12/15, train_loss: 0.2564\n",
      "13/15, train_loss: 0.2227\n",
      "14/15, train_loss: 0.2603\n",
      "15/15, train_loss: 0.2637\n",
      "16/15, train_loss: 0.3870\n",
      "epoch 254 average loss: 0.2580\n",
      "----------\n",
      "epoch 255/500\n",
      "1/15, train_loss: 0.2722\n",
      "2/15, train_loss: 0.2228\n",
      "3/15, train_loss: 0.2501\n",
      "4/15, train_loss: 0.8429\n",
      "5/15, train_loss: 0.3095\n",
      "6/15, train_loss: 0.4817\n",
      "7/15, train_loss: 0.2859\n",
      "8/15, train_loss: 0.2720\n",
      "9/15, train_loss: 0.2937\n",
      "10/15, train_loss: 0.2909\n",
      "11/15, train_loss: 0.2216\n",
      "12/15, train_loss: 0.2653\n",
      "13/15, train_loss: 0.2646\n",
      "14/15, train_loss: 0.2551\n",
      "15/15, train_loss: 0.2159\n",
      "16/15, train_loss: 2.3592\n",
      "epoch 255 average loss: 0.4440\n",
      "----------\n",
      "epoch 256/500\n",
      "1/15, train_loss: 0.2130\n",
      "2/15, train_loss: 0.3085\n",
      "3/15, train_loss: 0.2720\n",
      "4/15, train_loss: 0.2940\n",
      "5/15, train_loss: 0.2508\n",
      "6/15, train_loss: 0.3605\n",
      "7/15, train_loss: 0.2369\n",
      "8/15, train_loss: 0.4549\n",
      "9/15, train_loss: 0.2299\n",
      "10/15, train_loss: 0.2604\n",
      "11/15, train_loss: 0.2421\n",
      "12/15, train_loss: 0.2309\n",
      "13/15, train_loss: 0.3115\n",
      "14/15, train_loss: 0.2892\n",
      "15/15, train_loss: 0.3142\n",
      "16/15, train_loss: 2.0620\n",
      "epoch 256 average loss: 0.3957\n",
      "----------\n",
      "epoch 257/500\n",
      "1/15, train_loss: 0.2230\n",
      "2/15, train_loss: 0.2433\n",
      "3/15, train_loss: 0.2012\n",
      "4/15, train_loss: 0.3284\n",
      "5/15, train_loss: 0.4789\n",
      "6/15, train_loss: 0.2328\n",
      "7/15, train_loss: 0.4318\n",
      "8/15, train_loss: 0.2159\n",
      "9/15, train_loss: 0.2258\n",
      "10/15, train_loss: 0.2489\n",
      "11/15, train_loss: 0.2303\n",
      "12/15, train_loss: 0.2892\n",
      "13/15, train_loss: 0.2218\n",
      "14/15, train_loss: 0.2514\n",
      "15/15, train_loss: 0.2501\n",
      "16/15, train_loss: 0.2611\n",
      "epoch 257 average loss: 0.2709\n",
      "----------\n",
      "epoch 258/500\n",
      "1/15, train_loss: 0.2773\n",
      "2/15, train_loss: 0.2512\n",
      "3/15, train_loss: 0.2409\n",
      "4/15, train_loss: 0.2102\n",
      "5/15, train_loss: 0.2486\n",
      "6/15, train_loss: 0.3103\n",
      "7/15, train_loss: 2.0253\n",
      "8/15, train_loss: 0.2145\n",
      "9/15, train_loss: 0.2669\n",
      "10/15, train_loss: 0.2458\n",
      "11/15, train_loss: 0.2329\n",
      "12/15, train_loss: 0.2431\n",
      "13/15, train_loss: 0.2833\n",
      "14/15, train_loss: 0.2288\n",
      "15/15, train_loss: 0.2829\n",
      "16/15, train_loss: 0.2463\n",
      "epoch 258 average loss: 0.3630\n",
      "----------\n",
      "epoch 259/500\n",
      "1/15, train_loss: 0.2707\n",
      "2/15, train_loss: 0.2915\n",
      "3/15, train_loss: 0.2496\n",
      "4/15, train_loss: 0.2230\n",
      "5/15, train_loss: 0.2657\n",
      "6/15, train_loss: 0.2888\n",
      "7/15, train_loss: 0.2480\n",
      "8/15, train_loss: 0.2643\n",
      "9/15, train_loss: 0.2653\n",
      "10/15, train_loss: 0.2732\n",
      "11/15, train_loss: 0.2597\n",
      "12/15, train_loss: 0.3053\n",
      "13/15, train_loss: 0.2053\n",
      "14/15, train_loss: 0.2914\n",
      "15/15, train_loss: 0.2828\n",
      "16/15, train_loss: 0.8366\n",
      "epoch 259 average loss: 0.3013\n",
      "----------\n",
      "epoch 260/500\n",
      "1/15, train_loss: 0.2481\n",
      "2/15, train_loss: 0.2777\n",
      "3/15, train_loss: 0.2502\n",
      "4/15, train_loss: 0.3165\n",
      "5/15, train_loss: 0.2193\n",
      "6/15, train_loss: 0.3405\n",
      "7/15, train_loss: 0.2374\n",
      "8/15, train_loss: 0.2101\n",
      "9/15, train_loss: 0.3154\n",
      "10/15, train_loss: 0.2997\n",
      "11/15, train_loss: 0.4219\n",
      "12/15, train_loss: 0.2238\n",
      "13/15, train_loss: 0.2523\n",
      "14/15, train_loss: 0.3153\n",
      "15/15, train_loss: 0.2580\n",
      "16/15, train_loss: 0.2678\n",
      "epoch 260 average loss: 0.2784\n",
      "----------\n",
      "epoch 261/500\n",
      "1/15, train_loss: 0.2359\n",
      "2/15, train_loss: 0.2268\n",
      "3/15, train_loss: 0.3297\n",
      "4/15, train_loss: 0.2443\n",
      "5/15, train_loss: 0.2814\n",
      "6/15, train_loss: 0.2601\n",
      "7/15, train_loss: 0.2247\n",
      "8/15, train_loss: 0.2624\n",
      "9/15, train_loss: 0.2066\n",
      "10/15, train_loss: 0.3040\n",
      "11/15, train_loss: 0.3222\n",
      "12/15, train_loss: 0.2790\n",
      "13/15, train_loss: 0.2740\n",
      "14/15, train_loss: 0.2296\n",
      "15/15, train_loss: 0.2297\n",
      "16/15, train_loss: 0.3604\n",
      "epoch 261 average loss: 0.2669\n",
      "----------\n",
      "epoch 262/500\n",
      "1/15, train_loss: 0.2511\n",
      "2/15, train_loss: 0.2395\n",
      "3/15, train_loss: 0.2695\n",
      "4/15, train_loss: 0.2831\n",
      "5/15, train_loss: 0.2836\n",
      "6/15, train_loss: 0.2524\n",
      "7/15, train_loss: 0.2362\n",
      "8/15, train_loss: 0.2913\n",
      "9/15, train_loss: 0.2246\n",
      "10/15, train_loss: 0.2739\n",
      "11/15, train_loss: 0.2111\n",
      "12/15, train_loss: 0.2765\n",
      "13/15, train_loss: 0.2303\n",
      "14/15, train_loss: 0.2588\n",
      "15/15, train_loss: 0.2373\n",
      "16/15, train_loss: 0.4942\n",
      "epoch 262 average loss: 0.2696\n",
      "----------\n",
      "epoch 263/500\n",
      "1/15, train_loss: 0.2632\n",
      "2/15, train_loss: 0.2679\n",
      "3/15, train_loss: 0.2239\n",
      "4/15, train_loss: 0.2105\n",
      "5/15, train_loss: 0.2477\n",
      "6/15, train_loss: 0.2168\n",
      "7/15, train_loss: 0.2850\n",
      "8/15, train_loss: 0.2110\n",
      "9/15, train_loss: 0.3631\n",
      "10/15, train_loss: 0.2338\n",
      "11/15, train_loss: 0.2364\n",
      "12/15, train_loss: 0.2240\n",
      "13/15, train_loss: 0.2608\n",
      "14/15, train_loss: 0.2566\n",
      "15/15, train_loss: 0.2199\n",
      "16/15, train_loss: 2.7136\n",
      "epoch 263 average loss: 0.4021\n",
      "----------\n",
      "epoch 264/500\n",
      "1/15, train_loss: 0.2294\n",
      "2/15, train_loss: 0.2681\n",
      "3/15, train_loss: 0.3229\n",
      "4/15, train_loss: 0.2434\n",
      "5/15, train_loss: 0.3156\n",
      "6/15, train_loss: 0.2805\n",
      "7/15, train_loss: 0.2197\n",
      "8/15, train_loss: 0.2332\n",
      "9/15, train_loss: 0.2643\n",
      "10/15, train_loss: 0.2939\n",
      "11/15, train_loss: 0.2607\n",
      "12/15, train_loss: 0.3813\n",
      "13/15, train_loss: 0.2187\n",
      "14/15, train_loss: 0.2227\n",
      "15/15, train_loss: 0.2329\n",
      "16/15, train_loss: 0.8978\n",
      "epoch 264 average loss: 0.3053\n",
      "----------\n",
      "epoch 265/500\n",
      "1/15, train_loss: 0.2976\n",
      "2/15, train_loss: 0.2248\n",
      "3/15, train_loss: 0.1954\n",
      "4/15, train_loss: 0.2901\n",
      "5/15, train_loss: 0.3203\n",
      "6/15, train_loss: 0.3292\n",
      "7/15, train_loss: 0.2803\n",
      "8/15, train_loss: 0.3440\n",
      "9/15, train_loss: 0.2624\n",
      "10/15, train_loss: 0.2040\n",
      "11/15, train_loss: 0.2309\n",
      "12/15, train_loss: 0.2166\n",
      "13/15, train_loss: 0.2184\n",
      "14/15, train_loss: 0.2423\n",
      "15/15, train_loss: 0.2387\n",
      "16/15, train_loss: 0.2777\n",
      "epoch 265 average loss: 0.2608\n",
      "----------\n",
      "epoch 266/500\n",
      "1/15, train_loss: 0.2645\n",
      "2/15, train_loss: 0.1892\n",
      "3/15, train_loss: 0.2616\n",
      "4/15, train_loss: 0.2013\n",
      "5/15, train_loss: 0.3640\n",
      "6/15, train_loss: 0.5441\n",
      "7/15, train_loss: 0.2998\n",
      "8/15, train_loss: 0.3044\n",
      "9/15, train_loss: 0.2566\n",
      "10/15, train_loss: 0.2773\n",
      "11/15, train_loss: 0.3936\n",
      "12/15, train_loss: 0.2129\n",
      "13/15, train_loss: 0.2689\n",
      "14/15, train_loss: 0.2820\n",
      "15/15, train_loss: 0.2370\n",
      "16/15, train_loss: 2.6025\n",
      "epoch 266 average loss: 0.4350\n",
      "----------\n",
      "epoch 267/500\n",
      "1/15, train_loss: 0.2242\n",
      "2/15, train_loss: 0.3025\n",
      "3/15, train_loss: 0.2232\n",
      "4/15, train_loss: 0.2706\n",
      "5/15, train_loss: 0.3464\n",
      "6/15, train_loss: 0.3141\n",
      "7/15, train_loss: 0.2612\n",
      "8/15, train_loss: 0.2036\n",
      "9/15, train_loss: 0.2248\n",
      "10/15, train_loss: 0.2482\n",
      "11/15, train_loss: 0.7689\n",
      "12/15, train_loss: 0.2483\n",
      "13/15, train_loss: 0.2209\n",
      "14/15, train_loss: 0.2303\n",
      "15/15, train_loss: 0.2361\n",
      "16/15, train_loss: 2.3947\n",
      "epoch 267 average loss: 0.4199\n",
      "----------\n",
      "epoch 268/500\n",
      "1/15, train_loss: 0.2994\n",
      "2/15, train_loss: 0.2307\n",
      "3/15, train_loss: 0.3006\n",
      "4/15, train_loss: 0.2981\n",
      "5/15, train_loss: 0.2610\n",
      "6/15, train_loss: 0.2660\n",
      "7/15, train_loss: 0.2644\n",
      "8/15, train_loss: 0.2562\n",
      "9/15, train_loss: 0.2729\n",
      "10/15, train_loss: 0.2853\n",
      "11/15, train_loss: 0.3603\n",
      "12/15, train_loss: 0.2406\n",
      "13/15, train_loss: 0.2416\n",
      "14/15, train_loss: 0.2372\n",
      "15/15, train_loss: 0.5540\n",
      "16/15, train_loss: 0.2464\n",
      "epoch 268 average loss: 0.2884\n",
      "----------\n",
      "epoch 269/500\n",
      "1/15, train_loss: 0.1968\n",
      "2/15, train_loss: 0.2329\n",
      "3/15, train_loss: 0.1934\n",
      "4/15, train_loss: 0.2301\n",
      "5/15, train_loss: 0.5637\n",
      "6/15, train_loss: 0.2054\n",
      "7/15, train_loss: 0.2262\n",
      "8/15, train_loss: 0.2046\n",
      "9/15, train_loss: 0.2341\n",
      "10/15, train_loss: 0.5003\n",
      "11/15, train_loss: 0.2444\n",
      "12/15, train_loss: 0.2386\n",
      "13/15, train_loss: 0.5071\n",
      "14/15, train_loss: 0.2371\n",
      "15/15, train_loss: 0.2268\n",
      "16/15, train_loss: 2.2987\n",
      "epoch 269 average loss: 0.4088\n",
      "----------\n",
      "epoch 270/500\n",
      "1/15, train_loss: 0.5820\n",
      "2/15, train_loss: 0.3017\n",
      "3/15, train_loss: 0.2045\n",
      "4/15, train_loss: 0.2614\n",
      "5/15, train_loss: 0.1986\n",
      "6/15, train_loss: 0.2879\n",
      "7/15, train_loss: 0.2334\n",
      "8/15, train_loss: 0.2582\n",
      "9/15, train_loss: 0.7949\n",
      "10/15, train_loss: 0.2141\n",
      "11/15, train_loss: 0.2594\n",
      "12/15, train_loss: 0.2115\n",
      "13/15, train_loss: 0.1948\n",
      "14/15, train_loss: 0.2078\n",
      "15/15, train_loss: 0.2083\n",
      "16/15, train_loss: 0.2310\n",
      "epoch 270 average loss: 0.2906\n",
      "----------\n",
      "epoch 271/500\n",
      "1/15, train_loss: 0.3025\n",
      "2/15, train_loss: 0.2725\n",
      "3/15, train_loss: 0.2396\n",
      "4/15, train_loss: 0.3135\n",
      "5/15, train_loss: 0.2331\n",
      "6/15, train_loss: 0.2246\n",
      "7/15, train_loss: 0.2440\n",
      "8/15, train_loss: 0.2357\n",
      "9/15, train_loss: 0.2020\n",
      "10/15, train_loss: 0.3658\n",
      "11/15, train_loss: 0.2158\n",
      "12/15, train_loss: 0.2266\n",
      "13/15, train_loss: 0.2115\n",
      "14/15, train_loss: 0.2122\n",
      "15/15, train_loss: 0.2326\n",
      "16/15, train_loss: 2.8955\n",
      "epoch 271 average loss: 0.4142\n",
      "----------\n",
      "epoch 272/500\n",
      "1/15, train_loss: 0.2252\n",
      "2/15, train_loss: 0.2879\n",
      "3/15, train_loss: 0.2428\n",
      "4/15, train_loss: 0.3089\n",
      "5/15, train_loss: 0.1977\n",
      "6/15, train_loss: 0.2338\n",
      "7/15, train_loss: 0.2611\n",
      "8/15, train_loss: 0.2288\n",
      "9/15, train_loss: 0.2511\n",
      "10/15, train_loss: 0.3779\n",
      "11/15, train_loss: 0.2710\n",
      "12/15, train_loss: 0.2525\n",
      "13/15, train_loss: 0.2119\n",
      "14/15, train_loss: 0.2753\n",
      "15/15, train_loss: 0.2283\n",
      "16/15, train_loss: 2.1687\n",
      "epoch 272 average loss: 0.3764\n",
      "----------\n",
      "epoch 273/500\n",
      "1/15, train_loss: 0.2433\n",
      "2/15, train_loss: 0.2295\n",
      "3/15, train_loss: 0.2125\n",
      "4/15, train_loss: 0.2174\n",
      "5/15, train_loss: 0.4019\n",
      "6/15, train_loss: 0.2137\n",
      "7/15, train_loss: 0.2308\n",
      "8/15, train_loss: 0.2917\n",
      "9/15, train_loss: 0.3287\n",
      "10/15, train_loss: 0.2052\n",
      "11/15, train_loss: 0.2496\n",
      "12/15, train_loss: 0.2321\n",
      "13/15, train_loss: 0.3086\n",
      "14/15, train_loss: 0.2850\n",
      "15/15, train_loss: 0.2135\n",
      "16/15, train_loss: 0.2289\n",
      "epoch 273 average loss: 0.2558\n",
      "----------\n",
      "epoch 274/500\n",
      "1/15, train_loss: 0.1982\n",
      "2/15, train_loss: 0.2561\n",
      "3/15, train_loss: 0.2797\n",
      "4/15, train_loss: 0.2256\n",
      "5/15, train_loss: 0.2357\n",
      "6/15, train_loss: 0.2339\n",
      "7/15, train_loss: 0.2651\n",
      "8/15, train_loss: 0.1849\n",
      "9/15, train_loss: 0.2916\n",
      "10/15, train_loss: 0.2352\n",
      "11/15, train_loss: 0.2298\n",
      "12/15, train_loss: 0.3280\n",
      "13/15, train_loss: 0.2535\n",
      "14/15, train_loss: 0.1947\n",
      "15/15, train_loss: 0.2348\n",
      "16/15, train_loss: 2.2923\n",
      "epoch 274 average loss: 0.3712\n",
      "----------\n",
      "epoch 275/500\n",
      "1/15, train_loss: 0.2443\n",
      "2/15, train_loss: 0.2590\n",
      "3/15, train_loss: 0.2708\n",
      "4/15, train_loss: 0.2289\n",
      "5/15, train_loss: 0.2082\n",
      "6/15, train_loss: 0.2542\n",
      "7/15, train_loss: 0.2219\n",
      "8/15, train_loss: 0.2616\n",
      "9/15, train_loss: 0.2319\n",
      "10/15, train_loss: 0.2346\n",
      "11/15, train_loss: 0.4136\n",
      "12/15, train_loss: 0.2108\n",
      "13/15, train_loss: 0.2248\n",
      "14/15, train_loss: 0.2144\n",
      "15/15, train_loss: 0.3429\n",
      "16/15, train_loss: 2.3810\n",
      "epoch 275 average loss: 0.3877\n",
      "----------\n",
      "epoch 276/500\n",
      "1/15, train_loss: 0.2114\n",
      "2/15, train_loss: 0.1996\n",
      "3/15, train_loss: 0.2128\n",
      "4/15, train_loss: 0.2106\n",
      "5/15, train_loss: 0.2325\n",
      "6/15, train_loss: 0.2307\n",
      "7/15, train_loss: 0.2132\n",
      "8/15, train_loss: 0.2447\n",
      "9/15, train_loss: 0.2087\n",
      "10/15, train_loss: 0.2772\n",
      "11/15, train_loss: 1.9123\n",
      "12/15, train_loss: 0.2080\n",
      "13/15, train_loss: 0.2488\n",
      "14/15, train_loss: 0.2007\n",
      "15/15, train_loss: 0.2856\n",
      "16/15, train_loss: 2.7438\n",
      "epoch 276 average loss: 0.4900\n",
      "----------\n",
      "epoch 277/500\n",
      "1/15, train_loss: 0.1975\n",
      "2/15, train_loss: 0.2280\n",
      "3/15, train_loss: 0.2428\n",
      "4/15, train_loss: 0.2470\n",
      "5/15, train_loss: 0.2020\n",
      "6/15, train_loss: 0.6287\n",
      "7/15, train_loss: 0.2624\n",
      "8/15, train_loss: 0.2135\n",
      "9/15, train_loss: 0.2597\n",
      "10/15, train_loss: 0.2851\n",
      "11/15, train_loss: 0.2641\n",
      "12/15, train_loss: 0.2556\n",
      "13/15, train_loss: 0.2132\n",
      "14/15, train_loss: 0.2405\n",
      "15/15, train_loss: 2.4866\n",
      "16/15, train_loss: 0.2074\n",
      "epoch 277 average loss: 0.4021\n",
      "----------\n",
      "epoch 278/500\n",
      "1/15, train_loss: 0.2294\n",
      "2/15, train_loss: 0.2459\n",
      "3/15, train_loss: 0.2093\n",
      "4/15, train_loss: 0.2632\n",
      "5/15, train_loss: 0.2077\n",
      "6/15, train_loss: 0.2281\n",
      "7/15, train_loss: 0.2254\n",
      "8/15, train_loss: 0.2000\n",
      "9/15, train_loss: 0.3443\n",
      "10/15, train_loss: 0.1986\n",
      "11/15, train_loss: 0.6961\n",
      "12/15, train_loss: 0.4030\n",
      "13/15, train_loss: 0.2833\n",
      "14/15, train_loss: 0.2171\n",
      "15/15, train_loss: 0.2406\n",
      "16/15, train_loss: 0.3328\n",
      "epoch 278 average loss: 0.2828\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 278 validation loss: 0.9576 dice_score: 0.8526 acc_metric: 0.7193 accuracy: 0.7193, f1score: 0.7500\n",
      " saved Best PMetric: 0.7860 at epoch: 278\n",
      "----------\n",
      "epoch 279/500\n",
      "1/15, train_loss: 0.1876\n",
      "2/15, train_loss: 0.2730\n",
      "3/15, train_loss: 0.3250\n",
      "4/15, train_loss: 0.1852\n",
      "5/15, train_loss: 0.2304\n",
      "6/15, train_loss: 2.4940\n",
      "7/15, train_loss: 0.2477\n",
      "8/15, train_loss: 0.2727\n",
      "9/15, train_loss: 0.2423\n",
      "10/15, train_loss: 0.2488\n",
      "11/15, train_loss: 0.1937\n",
      "12/15, train_loss: 0.2849\n",
      "13/15, train_loss: 0.2089\n",
      "14/15, train_loss: 0.2057\n",
      "15/15, train_loss: 0.2483\n",
      "16/15, train_loss: 2.2626\n",
      "epoch 279 average loss: 0.5069\n",
      "----------\n",
      "epoch 280/500\n",
      "1/15, train_loss: 0.6346\n",
      "2/15, train_loss: 0.1845\n",
      "3/15, train_loss: 0.2020\n",
      "4/15, train_loss: 0.2907\n",
      "5/15, train_loss: 0.2041\n",
      "6/15, train_loss: 0.2621\n",
      "7/15, train_loss: 0.2519\n",
      "8/15, train_loss: 0.2317\n",
      "9/15, train_loss: 0.2101\n",
      "10/15, train_loss: 0.2115\n",
      "11/15, train_loss: 0.2202\n",
      "12/15, train_loss: 0.1748\n",
      "13/15, train_loss: 0.2718\n",
      "14/15, train_loss: 0.2210\n",
      "15/15, train_loss: 0.2464\n",
      "16/15, train_loss: 0.2293\n",
      "epoch 280 average loss: 0.2529\n",
      "----------\n",
      "epoch 281/500\n",
      "1/15, train_loss: 0.1977\n",
      "2/15, train_loss: 0.2138\n",
      "3/15, train_loss: 0.4829\n",
      "4/15, train_loss: 0.2337\n",
      "5/15, train_loss: 0.2942\n",
      "6/15, train_loss: 0.2519\n",
      "7/15, train_loss: 0.2063\n",
      "8/15, train_loss: 0.2046\n",
      "9/15, train_loss: 0.2380\n",
      "10/15, train_loss: 0.2219\n",
      "11/15, train_loss: 0.2175\n",
      "12/15, train_loss: 0.2402\n",
      "13/15, train_loss: 0.2689\n",
      "14/15, train_loss: 0.1913\n",
      "15/15, train_loss: 0.2604\n",
      "16/15, train_loss: 0.7944\n",
      "epoch 281 average loss: 0.2823\n",
      "----------\n",
      "epoch 282/500\n",
      "1/15, train_loss: 0.2397\n",
      "2/15, train_loss: 0.3658\n",
      "3/15, train_loss: 0.5349\n",
      "4/15, train_loss: 0.2370\n",
      "5/15, train_loss: 0.2488\n",
      "6/15, train_loss: 0.2113\n",
      "7/15, train_loss: 0.2152\n",
      "8/15, train_loss: 0.2003\n",
      "9/15, train_loss: 0.2269\n",
      "10/15, train_loss: 0.5893\n",
      "11/15, train_loss: 0.2008\n",
      "12/15, train_loss: 0.2284\n",
      "13/15, train_loss: 0.2078\n",
      "14/15, train_loss: 0.3283\n",
      "15/15, train_loss: 0.1874\n",
      "16/15, train_loss: 2.7076\n",
      "epoch 282 average loss: 0.4331\n",
      "----------\n",
      "epoch 283/500\n",
      "1/15, train_loss: 0.2330\n",
      "2/15, train_loss: 0.2404\n",
      "3/15, train_loss: 0.2086\n",
      "4/15, train_loss: 0.2331\n",
      "5/15, train_loss: 0.2392\n",
      "6/15, train_loss: 0.2657\n",
      "7/15, train_loss: 0.1875\n",
      "8/15, train_loss: 0.4098\n",
      "9/15, train_loss: 0.2229\n",
      "10/15, train_loss: 0.1973\n",
      "11/15, train_loss: 0.2145\n",
      "12/15, train_loss: 0.2770\n",
      "13/15, train_loss: 0.3830\n",
      "14/15, train_loss: 0.1864\n",
      "15/15, train_loss: 0.2299\n",
      "16/15, train_loss: 2.2502\n",
      "epoch 283 average loss: 0.3737\n",
      "----------\n",
      "epoch 284/500\n",
      "1/15, train_loss: 0.2014\n",
      "2/15, train_loss: 0.4725\n",
      "3/15, train_loss: 0.6448\n",
      "4/15, train_loss: 0.1998\n",
      "5/15, train_loss: 0.2323\n",
      "6/15, train_loss: 0.2531\n",
      "7/15, train_loss: 0.3109\n",
      "8/15, train_loss: 0.1861\n",
      "9/15, train_loss: 0.1791\n",
      "10/15, train_loss: 0.2635\n",
      "11/15, train_loss: 0.2174\n",
      "12/15, train_loss: 0.1970\n",
      "13/15, train_loss: 0.1969\n",
      "14/15, train_loss: 0.2287\n",
      "15/15, train_loss: 0.3580\n",
      "16/15, train_loss: 0.1851\n",
      "epoch 284 average loss: 0.2704\n",
      "----------\n",
      "epoch 285/500\n",
      "1/15, train_loss: 0.2224\n",
      "2/15, train_loss: 0.2380\n",
      "3/15, train_loss: 0.2035\n",
      "4/15, train_loss: 0.3222\n",
      "5/15, train_loss: 0.2331\n",
      "6/15, train_loss: 0.2523\n",
      "7/15, train_loss: 0.2105\n",
      "8/15, train_loss: 0.1883\n",
      "9/15, train_loss: 0.2092\n",
      "10/15, train_loss: 0.2788\n",
      "11/15, train_loss: 0.2611\n",
      "12/15, train_loss: 0.2306\n",
      "13/15, train_loss: 0.2270\n",
      "14/15, train_loss: 0.3574\n",
      "15/15, train_loss: 0.2004\n",
      "16/15, train_loss: 2.4213\n",
      "epoch 285 average loss: 0.3785\n",
      "----------\n",
      "epoch 286/500\n",
      "1/15, train_loss: 0.1807\n",
      "2/15, train_loss: 0.2380\n",
      "3/15, train_loss: 0.2390\n",
      "4/15, train_loss: 0.1949\n",
      "5/15, train_loss: 0.2045\n",
      "6/15, train_loss: 0.2369\n",
      "7/15, train_loss: 0.1901\n",
      "8/15, train_loss: 0.2725\n",
      "9/15, train_loss: 0.2645\n",
      "10/15, train_loss: 0.3296\n",
      "11/15, train_loss: 0.2158\n",
      "12/15, train_loss: 0.2073\n",
      "13/15, train_loss: 0.2118\n",
      "14/15, train_loss: 0.2415\n",
      "15/15, train_loss: 0.2433\n",
      "16/15, train_loss: 0.2243\n",
      "epoch 286 average loss: 0.2309\n",
      "----------\n",
      "epoch 287/500\n",
      "1/15, train_loss: 0.2210\n",
      "2/15, train_loss: 0.2235\n",
      "3/15, train_loss: 0.2358\n",
      "4/15, train_loss: 0.2560\n",
      "5/15, train_loss: 0.2520\n",
      "6/15, train_loss: 0.2137\n",
      "7/15, train_loss: 0.2012\n",
      "8/15, train_loss: 0.3693\n",
      "9/15, train_loss: 0.1954\n",
      "10/15, train_loss: 0.1960\n",
      "11/15, train_loss: 0.4344\n",
      "12/15, train_loss: 0.2516\n",
      "13/15, train_loss: 0.2029\n",
      "14/15, train_loss: 0.2751\n",
      "15/15, train_loss: 0.2088\n",
      "16/15, train_loss: 2.3319\n",
      "epoch 287 average loss: 0.3793\n",
      "----------\n",
      "epoch 288/500\n",
      "1/15, train_loss: 0.3101\n",
      "2/15, train_loss: 0.2157\n",
      "3/15, train_loss: 0.2693\n",
      "4/15, train_loss: 0.2614\n",
      "5/15, train_loss: 0.1832\n",
      "6/15, train_loss: 0.3874\n",
      "7/15, train_loss: 0.1860\n",
      "8/15, train_loss: 0.2144\n",
      "9/15, train_loss: 0.1826\n",
      "10/15, train_loss: 0.1964\n",
      "11/15, train_loss: 0.2229\n",
      "12/15, train_loss: 0.2131\n",
      "13/15, train_loss: 0.2322\n",
      "14/15, train_loss: 0.2390\n",
      "15/15, train_loss: 0.1932\n",
      "16/15, train_loss: 2.0252\n",
      "epoch 288 average loss: 0.3458\n",
      "----------\n",
      "epoch 289/500\n",
      "1/15, train_loss: 0.5838\n",
      "2/15, train_loss: 0.2274\n",
      "3/15, train_loss: 0.2163\n",
      "4/15, train_loss: 0.4492\n",
      "5/15, train_loss: 0.2069\n",
      "6/15, train_loss: 0.2045\n",
      "7/15, train_loss: 0.2336\n",
      "8/15, train_loss: 0.2081\n",
      "9/15, train_loss: 0.2880\n",
      "10/15, train_loss: 0.2581\n",
      "11/15, train_loss: 0.2076\n",
      "12/15, train_loss: 0.4414\n",
      "13/15, train_loss: 0.2023\n",
      "14/15, train_loss: 0.2221\n",
      "15/15, train_loss: 0.2694\n",
      "16/15, train_loss: 0.4401\n",
      "epoch 289 average loss: 0.2912\n",
      "----------\n",
      "epoch 290/500\n",
      "1/15, train_loss: 0.2333\n",
      "2/15, train_loss: 0.2501\n",
      "3/15, train_loss: 0.2164\n",
      "4/15, train_loss: 0.2451\n",
      "5/15, train_loss: 0.2123\n",
      "6/15, train_loss: 0.2410\n",
      "7/15, train_loss: 0.2369\n",
      "8/15, train_loss: 0.2430\n",
      "9/15, train_loss: 0.3209\n",
      "10/15, train_loss: 0.2317\n",
      "11/15, train_loss: 0.2380\n",
      "12/15, train_loss: 0.2520\n",
      "13/15, train_loss: 0.2110\n",
      "14/15, train_loss: 0.2372\n",
      "15/15, train_loss: 0.2238\n",
      "16/15, train_loss: 0.2629\n",
      "epoch 290 average loss: 0.2410\n",
      "----------\n",
      "epoch 291/500\n",
      "1/15, train_loss: 0.2079\n",
      "2/15, train_loss: 0.1959\n",
      "3/15, train_loss: 0.2341\n",
      "4/15, train_loss: 0.1836\n",
      "5/15, train_loss: 0.2484\n",
      "6/15, train_loss: 0.2037\n",
      "7/15, train_loss: 0.2057\n",
      "8/15, train_loss: 0.3209\n",
      "9/15, train_loss: 0.2366\n",
      "10/15, train_loss: 0.2403\n",
      "11/15, train_loss: 0.1863\n",
      "12/15, train_loss: 0.1990\n",
      "13/15, train_loss: 0.2305\n",
      "14/15, train_loss: 0.2081\n",
      "15/15, train_loss: 0.2208\n",
      "16/15, train_loss: 0.4535\n",
      "epoch 291 average loss: 0.2360\n",
      "----------\n",
      "epoch 292/500\n",
      "1/15, train_loss: 0.2116\n",
      "2/15, train_loss: 0.2960\n",
      "3/15, train_loss: 0.2965\n",
      "4/15, train_loss: 0.2210\n",
      "5/15, train_loss: 0.1948\n",
      "6/15, train_loss: 0.1715\n",
      "7/15, train_loss: 0.2507\n",
      "8/15, train_loss: 0.5391\n",
      "9/15, train_loss: 0.2585\n",
      "10/15, train_loss: 0.2178\n",
      "11/15, train_loss: 0.2112\n",
      "12/15, train_loss: 0.2191\n",
      "13/15, train_loss: 0.2035\n",
      "14/15, train_loss: 0.2441\n",
      "15/15, train_loss: 0.2084\n",
      "16/15, train_loss: 2.1267\n",
      "epoch 292 average loss: 0.3669\n",
      "----------\n",
      "epoch 293/500\n",
      "1/15, train_loss: 0.2154\n",
      "2/15, train_loss: 0.1898\n",
      "3/15, train_loss: 0.2081\n",
      "4/15, train_loss: 0.2527\n",
      "5/15, train_loss: 0.1904\n",
      "6/15, train_loss: 0.2824\n",
      "7/15, train_loss: 0.2942\n",
      "8/15, train_loss: 0.2522\n",
      "9/15, train_loss: 0.1793\n",
      "10/15, train_loss: 0.2046\n",
      "11/15, train_loss: 0.2277\n",
      "12/15, train_loss: 0.1723\n",
      "13/15, train_loss: 0.2600\n",
      "14/15, train_loss: 0.2737\n",
      "15/15, train_loss: 0.1841\n",
      "16/15, train_loss: 0.2740\n",
      "epoch 293 average loss: 0.2288\n",
      "----------\n",
      "epoch 294/500\n",
      "1/15, train_loss: 0.2350\n",
      "2/15, train_loss: 0.1968\n",
      "3/15, train_loss: 0.1751\n",
      "4/15, train_loss: 0.2269\n",
      "5/15, train_loss: 0.2553\n",
      "6/15, train_loss: 0.1864\n",
      "7/15, train_loss: 0.2081\n",
      "8/15, train_loss: 0.2324\n",
      "9/15, train_loss: 0.5139\n",
      "10/15, train_loss: 0.2115\n",
      "11/15, train_loss: 0.2199\n",
      "12/15, train_loss: 0.2169\n",
      "13/15, train_loss: 0.1852\n",
      "14/15, train_loss: 0.1825\n",
      "15/15, train_loss: 0.2471\n",
      "16/15, train_loss: 2.0110\n",
      "epoch 294 average loss: 0.3440\n",
      "----------\n",
      "epoch 295/500\n",
      "1/15, train_loss: 0.2496\n",
      "2/15, train_loss: 0.2057\n",
      "3/15, train_loss: 0.2000\n",
      "4/15, train_loss: 0.2232\n",
      "5/15, train_loss: 0.2168\n",
      "6/15, train_loss: 0.2538\n",
      "7/15, train_loss: 0.1733\n",
      "8/15, train_loss: 0.1937\n",
      "9/15, train_loss: 0.2744\n",
      "10/15, train_loss: 0.2230\n",
      "11/15, train_loss: 0.2181\n",
      "12/15, train_loss: 0.3161\n",
      "13/15, train_loss: 0.2005\n",
      "14/15, train_loss: 0.2659\n",
      "15/15, train_loss: 0.1924\n",
      "16/15, train_loss: 2.3049\n",
      "epoch 295 average loss: 0.3570\n",
      "----------\n",
      "epoch 296/500\n",
      "1/15, train_loss: 0.1961\n",
      "2/15, train_loss: 0.1895\n",
      "3/15, train_loss: 0.2854\n",
      "4/15, train_loss: 0.2215\n",
      "5/15, train_loss: 0.2097\n",
      "6/15, train_loss: 0.2703\n",
      "7/15, train_loss: 0.1743\n",
      "8/15, train_loss: 0.2364\n",
      "9/15, train_loss: 0.1980\n",
      "10/15, train_loss: 0.2746\n",
      "11/15, train_loss: 0.1809\n",
      "12/15, train_loss: 0.1917\n",
      "13/15, train_loss: 0.2144\n",
      "14/15, train_loss: 0.1873\n",
      "15/15, train_loss: 0.2601\n",
      "16/15, train_loss: 0.2555\n",
      "epoch 296 average loss: 0.2216\n",
      "----------\n",
      "epoch 297/500\n",
      "1/15, train_loss: 0.2976\n",
      "2/15, train_loss: 0.2327\n",
      "3/15, train_loss: 0.2130\n",
      "4/15, train_loss: 0.2548\n",
      "5/15, train_loss: 0.1936\n",
      "6/15, train_loss: 0.4354\n",
      "7/15, train_loss: 2.7336\n",
      "8/15, train_loss: 0.2095\n",
      "9/15, train_loss: 0.6437\n",
      "10/15, train_loss: 0.1919\n",
      "11/15, train_loss: 0.1903\n",
      "12/15, train_loss: 0.2209\n",
      "13/15, train_loss: 0.2453\n",
      "14/15, train_loss: 0.2045\n",
      "15/15, train_loss: 0.1727\n",
      "16/15, train_loss: 0.1669\n",
      "epoch 297 average loss: 0.4129\n",
      "----------\n",
      "epoch 298/500\n",
      "1/15, train_loss: 0.2583\n",
      "2/15, train_loss: 0.1770\n",
      "3/15, train_loss: 0.2968\n",
      "4/15, train_loss: 0.1929\n",
      "5/15, train_loss: 0.3157\n",
      "6/15, train_loss: 0.2478\n",
      "7/15, train_loss: 0.1854\n",
      "8/15, train_loss: 0.2098\n",
      "9/15, train_loss: 0.1947\n",
      "10/15, train_loss: 0.1831\n",
      "11/15, train_loss: 0.2154\n",
      "12/15, train_loss: 0.1971\n",
      "13/15, train_loss: 0.1776\n",
      "14/15, train_loss: 0.2729\n",
      "15/15, train_loss: 0.3691\n",
      "16/15, train_loss: 2.3307\n",
      "epoch 298 average loss: 0.3640\n",
      "----------\n",
      "epoch 299/500\n",
      "1/15, train_loss: 0.1639\n",
      "2/15, train_loss: 0.2060\n",
      "3/15, train_loss: 0.2066\n",
      "4/15, train_loss: 0.2866\n",
      "5/15, train_loss: 0.2626\n",
      "6/15, train_loss: 0.1889\n",
      "7/15, train_loss: 0.2262\n",
      "8/15, train_loss: 0.1945\n",
      "9/15, train_loss: 0.3348\n",
      "10/15, train_loss: 0.2132\n",
      "11/15, train_loss: 0.1924\n",
      "12/15, train_loss: 0.2217\n",
      "13/15, train_loss: 0.2208\n",
      "14/15, train_loss: 0.2557\n",
      "15/15, train_loss: 0.2480\n",
      "16/15, train_loss: 2.1304\n",
      "epoch 299 average loss: 0.3470\n",
      "----------\n",
      "epoch 300/500\n",
      "1/15, train_loss: 0.1957\n",
      "2/15, train_loss: 0.1946\n",
      "3/15, train_loss: 0.1827\n",
      "4/15, train_loss: 0.2761\n",
      "5/15, train_loss: 0.1661\n",
      "6/15, train_loss: 0.4357\n",
      "7/15, train_loss: 0.1670\n",
      "8/15, train_loss: 0.1830\n",
      "9/15, train_loss: 0.2018\n",
      "10/15, train_loss: 0.2244\n",
      "11/15, train_loss: 0.2289\n",
      "12/15, train_loss: 0.2657\n",
      "13/15, train_loss: 0.2710\n",
      "14/15, train_loss: 2.1268\n",
      "15/15, train_loss: 0.1786\n",
      "16/15, train_loss: 1.0982\n",
      "epoch 300 average loss: 0.3998\n",
      "----------\n",
      "epoch 301/500\n",
      "1/15, train_loss: 0.2159\n",
      "2/15, train_loss: 0.2263\n",
      "3/15, train_loss: 0.1756\n",
      "4/15, train_loss: 0.2319\n",
      "5/15, train_loss: 0.2672\n",
      "6/15, train_loss: 0.2355\n",
      "7/15, train_loss: 0.1730\n",
      "8/15, train_loss: 0.1829\n",
      "9/15, train_loss: 0.2278\n",
      "10/15, train_loss: 0.3024\n",
      "11/15, train_loss: 0.2264\n",
      "12/15, train_loss: 0.1834\n",
      "13/15, train_loss: 0.1852\n",
      "14/15, train_loss: 0.2073\n",
      "15/15, train_loss: 0.2107\n",
      "16/15, train_loss: 0.2333\n",
      "epoch 301 average loss: 0.2178\n",
      "----------\n",
      "epoch 302/500\n",
      "1/15, train_loss: 0.1964\n",
      "2/15, train_loss: 0.1872\n",
      "3/15, train_loss: 0.1986\n",
      "4/15, train_loss: 0.1923\n",
      "5/15, train_loss: 0.2228\n",
      "6/15, train_loss: 0.2541\n",
      "7/15, train_loss: 0.2033\n",
      "8/15, train_loss: 0.3208\n",
      "9/15, train_loss: 0.1749\n",
      "10/15, train_loss: 0.2241\n",
      "11/15, train_loss: 0.1942\n",
      "12/15, train_loss: 0.2306\n",
      "13/15, train_loss: 0.2339\n",
      "14/15, train_loss: 0.1739\n",
      "15/15, train_loss: 0.2466\n",
      "16/15, train_loss: 0.1873\n",
      "epoch 302 average loss: 0.2151\n",
      "----------\n",
      "epoch 303/500\n",
      "1/15, train_loss: 0.1854\n",
      "2/15, train_loss: 0.2004\n",
      "3/15, train_loss: 0.2267\n",
      "4/15, train_loss: 0.2151\n",
      "5/15, train_loss: 0.1708\n",
      "6/15, train_loss: 0.1826\n",
      "7/15, train_loss: 0.6942\n",
      "8/15, train_loss: 0.3102\n",
      "9/15, train_loss: 0.2069\n",
      "10/15, train_loss: 0.2353\n",
      "11/15, train_loss: 0.2069\n",
      "12/15, train_loss: 0.2032\n",
      "13/15, train_loss: 0.2601\n",
      "14/15, train_loss: 0.2124\n",
      "15/15, train_loss: 0.1695\n",
      "16/15, train_loss: 2.0368\n",
      "epoch 303 average loss: 0.3573\n",
      "----------\n",
      "epoch 304/500\n",
      "1/15, train_loss: 0.2518\n",
      "2/15, train_loss: 0.2428\n",
      "3/15, train_loss: 0.2200\n",
      "4/15, train_loss: 0.1918\n",
      "5/15, train_loss: 0.1755\n",
      "6/15, train_loss: 0.1966\n",
      "7/15, train_loss: 0.2042\n",
      "8/15, train_loss: 0.2928\n",
      "9/15, train_loss: 0.1706\n",
      "10/15, train_loss: 0.1828\n",
      "11/15, train_loss: 0.2076\n",
      "12/15, train_loss: 0.1722\n",
      "13/15, train_loss: 0.2151\n",
      "14/15, train_loss: 0.1811\n",
      "15/15, train_loss: 0.1756\n",
      "16/15, train_loss: 2.9332\n",
      "epoch 304 average loss: 0.3759\n",
      "----------\n",
      "epoch 305/500\n",
      "1/15, train_loss: 0.1979\n",
      "2/15, train_loss: 0.2735\n",
      "3/15, train_loss: 0.1816\n",
      "4/15, train_loss: 0.2072\n",
      "5/15, train_loss: 0.2633\n",
      "6/15, train_loss: 0.1658\n",
      "7/15, train_loss: 0.2133\n",
      "8/15, train_loss: 0.2576\n",
      "9/15, train_loss: 0.1925\n",
      "10/15, train_loss: 0.1844\n",
      "11/15, train_loss: 0.2559\n",
      "12/15, train_loss: 0.3788\n",
      "13/15, train_loss: 0.3074\n",
      "14/15, train_loss: 0.1839\n",
      "15/15, train_loss: 0.1773\n",
      "16/15, train_loss: 0.1991\n",
      "epoch 305 average loss: 0.2275\n",
      "----------\n",
      "epoch 306/500\n",
      "1/15, train_loss: 0.1854\n",
      "2/15, train_loss: 0.1940\n",
      "3/15, train_loss: 0.2068\n",
      "4/15, train_loss: 0.2886\n",
      "5/15, train_loss: 0.1923\n",
      "6/15, train_loss: 0.3091\n",
      "7/15, train_loss: 0.1718\n",
      "8/15, train_loss: 0.2268\n",
      "9/15, train_loss: 0.2038\n",
      "10/15, train_loss: 0.2301\n",
      "11/15, train_loss: 0.1763\n",
      "12/15, train_loss: 0.1917\n",
      "13/15, train_loss: 0.1951\n",
      "14/15, train_loss: 0.2337\n",
      "15/15, train_loss: 0.1952\n",
      "16/15, train_loss: 0.1944\n",
      "epoch 306 average loss: 0.2122\n",
      "----------\n",
      "epoch 307/500\n",
      "1/15, train_loss: 0.1779\n",
      "2/15, train_loss: 0.2828\n",
      "3/15, train_loss: 0.2000\n",
      "4/15, train_loss: 0.2241\n",
      "5/15, train_loss: 0.3165\n",
      "6/15, train_loss: 0.2172\n",
      "7/15, train_loss: 0.1867\n",
      "8/15, train_loss: 0.2166\n",
      "9/15, train_loss: 0.2021\n",
      "10/15, train_loss: 0.1950\n",
      "11/15, train_loss: 0.2048\n",
      "12/15, train_loss: 0.2288\n",
      "13/15, train_loss: 0.2119\n",
      "14/15, train_loss: 0.1813\n",
      "15/15, train_loss: 0.2925\n",
      "16/15, train_loss: 2.5844\n",
      "epoch 307 average loss: 0.3702\n",
      "----------\n",
      "epoch 308/500\n",
      "1/15, train_loss: 0.2060\n",
      "2/15, train_loss: 0.2089\n",
      "3/15, train_loss: 0.1938\n",
      "4/15, train_loss: 0.1860\n",
      "5/15, train_loss: 0.2056\n",
      "6/15, train_loss: 0.1936\n",
      "7/15, train_loss: 0.1864\n",
      "8/15, train_loss: 0.1910\n",
      "9/15, train_loss: 0.2099\n",
      "10/15, train_loss: 0.5458\n",
      "11/15, train_loss: 0.2612\n",
      "12/15, train_loss: 0.2369\n",
      "13/15, train_loss: 0.3291\n",
      "14/15, train_loss: 0.2736\n",
      "15/15, train_loss: 0.1826\n",
      "16/15, train_loss: 2.7650\n",
      "epoch 308 average loss: 0.3985\n",
      "----------\n",
      "epoch 309/500\n",
      "1/15, train_loss: 0.2066\n",
      "2/15, train_loss: 0.2012\n",
      "3/15, train_loss: 0.2247\n",
      "4/15, train_loss: 0.2005\n",
      "5/15, train_loss: 0.2019\n",
      "6/15, train_loss: 0.3355\n",
      "7/15, train_loss: 0.1942\n",
      "8/15, train_loss: 0.2240\n",
      "9/15, train_loss: 0.2778\n",
      "10/15, train_loss: 0.1775\n",
      "11/15, train_loss: 0.3476\n",
      "12/15, train_loss: 0.2226\n",
      "13/15, train_loss: 0.2204\n",
      "14/15, train_loss: 0.1676\n",
      "15/15, train_loss: 0.1995\n",
      "16/15, train_loss: 0.2800\n",
      "epoch 309 average loss: 0.2301\n",
      "----------\n",
      "epoch 310/500\n",
      "1/15, train_loss: 0.1751\n",
      "2/15, train_loss: 0.2331\n",
      "3/15, train_loss: 0.1713\n",
      "4/15, train_loss: 0.2029\n",
      "5/15, train_loss: 0.2118\n",
      "6/15, train_loss: 0.2060\n",
      "7/15, train_loss: 0.2092\n",
      "8/15, train_loss: 0.4912\n",
      "9/15, train_loss: 0.1728\n",
      "10/15, train_loss: 0.2592\n",
      "11/15, train_loss: 0.2267\n",
      "12/15, train_loss: 0.2228\n",
      "13/15, train_loss: 0.1689\n",
      "14/15, train_loss: 0.2688\n",
      "15/15, train_loss: 0.2138\n",
      "16/15, train_loss: 0.4152\n",
      "epoch 310 average loss: 0.2405\n",
      "----------\n",
      "epoch 311/500\n",
      "1/15, train_loss: 0.2063\n",
      "2/15, train_loss: 0.1947\n",
      "3/15, train_loss: 0.1735\n",
      "4/15, train_loss: 0.2672\n",
      "5/15, train_loss: 0.1638\n",
      "6/15, train_loss: 0.2045\n",
      "7/15, train_loss: 0.1761\n",
      "8/15, train_loss: 0.1873\n",
      "9/15, train_loss: 0.1886\n",
      "10/15, train_loss: 0.1655\n",
      "11/15, train_loss: 0.2798\n",
      "12/15, train_loss: 0.2349\n",
      "13/15, train_loss: 0.2431\n",
      "14/15, train_loss: 0.2818\n",
      "15/15, train_loss: 0.1630\n",
      "16/15, train_loss: 2.6560\n",
      "epoch 311 average loss: 0.3616\n",
      "----------\n",
      "epoch 312/500\n",
      "1/15, train_loss: 0.4283\n",
      "2/15, train_loss: 0.1779\n",
      "3/15, train_loss: 0.2631\n",
      "4/15, train_loss: 0.1901\n",
      "5/15, train_loss: 0.2356\n",
      "6/15, train_loss: 0.2671\n",
      "7/15, train_loss: 0.2053\n",
      "8/15, train_loss: 0.2319\n",
      "9/15, train_loss: 0.2794\n",
      "10/15, train_loss: 0.2274\n",
      "11/15, train_loss: 0.2258\n",
      "12/15, train_loss: 0.2130\n",
      "13/15, train_loss: 0.1751\n",
      "14/15, train_loss: 0.3099\n",
      "15/15, train_loss: 0.2374\n",
      "16/15, train_loss: 2.0573\n",
      "epoch 312 average loss: 0.3578\n",
      "----------\n",
      "epoch 313/500\n",
      "1/15, train_loss: 0.2107\n",
      "2/15, train_loss: 0.2024\n",
      "3/15, train_loss: 0.2261\n",
      "4/15, train_loss: 0.1803\n",
      "5/15, train_loss: 0.1831\n",
      "6/15, train_loss: 1.3715\n",
      "7/15, train_loss: 0.2300\n",
      "8/15, train_loss: 0.1761\n",
      "9/15, train_loss: 0.1841\n",
      "10/15, train_loss: 0.1682\n",
      "11/15, train_loss: 0.1774\n",
      "12/15, train_loss: 0.2083\n",
      "13/15, train_loss: 0.2101\n",
      "14/15, train_loss: 0.2156\n",
      "15/15, train_loss: 0.2128\n",
      "16/15, train_loss: 0.3950\n",
      "epoch 313 average loss: 0.2845\n",
      "----------\n",
      "epoch 314/500\n",
      "1/15, train_loss: 0.2160\n",
      "2/15, train_loss: 0.3227\n",
      "3/15, train_loss: 0.1845\n",
      "4/15, train_loss: 0.1903\n",
      "5/15, train_loss: 0.1824\n",
      "6/15, train_loss: 0.2824\n",
      "7/15, train_loss: 0.1822\n",
      "8/15, train_loss: 0.2123\n",
      "9/15, train_loss: 0.1739\n",
      "10/15, train_loss: 0.1665\n",
      "11/15, train_loss: 0.3342\n",
      "12/15, train_loss: 0.2041\n",
      "13/15, train_loss: 0.1934\n",
      "14/15, train_loss: 0.1979\n",
      "15/15, train_loss: 0.1681\n",
      "16/15, train_loss: 0.2859\n",
      "epoch 314 average loss: 0.2185\n",
      "----------\n",
      "epoch 315/500\n",
      "1/15, train_loss: 0.1996\n",
      "2/15, train_loss: 0.2275\n",
      "3/15, train_loss: 0.1966\n",
      "4/15, train_loss: 0.1854\n",
      "5/15, train_loss: 0.2072\n",
      "6/15, train_loss: 0.1571\n",
      "7/15, train_loss: 0.2169\n",
      "8/15, train_loss: 0.2090\n",
      "9/15, train_loss: 0.2168\n",
      "10/15, train_loss: 0.2305\n",
      "11/15, train_loss: 0.2407\n",
      "12/15, train_loss: 0.2646\n",
      "13/15, train_loss: 0.3902\n",
      "14/15, train_loss: 0.1673\n",
      "15/15, train_loss: 0.2492\n",
      "16/15, train_loss: 2.0412\n",
      "epoch 315 average loss: 0.3375\n",
      "----------\n",
      "epoch 316/500\n",
      "1/15, train_loss: 0.1935\n",
      "2/15, train_loss: 0.1920\n",
      "3/15, train_loss: 0.3100\n",
      "4/15, train_loss: 0.2277\n",
      "5/15, train_loss: 0.1982\n",
      "6/15, train_loss: 0.1836\n",
      "7/15, train_loss: 0.1932\n",
      "8/15, train_loss: 0.1951\n",
      "9/15, train_loss: 0.2505\n",
      "10/15, train_loss: 0.1967\n",
      "11/15, train_loss: 0.1712\n",
      "12/15, train_loss: 0.1811\n",
      "13/15, train_loss: 0.1831\n",
      "14/15, train_loss: 0.2203\n",
      "15/15, train_loss: 0.1953\n",
      "16/15, train_loss: 0.2803\n",
      "epoch 316 average loss: 0.2107\n",
      "----------\n",
      "epoch 317/500\n",
      "1/15, train_loss: 0.1877\n",
      "2/15, train_loss: 0.1906\n",
      "3/15, train_loss: 0.2208\n",
      "4/15, train_loss: 0.1782\n",
      "5/15, train_loss: 0.2235\n",
      "6/15, train_loss: 0.1790\n",
      "7/15, train_loss: 0.3104\n",
      "8/15, train_loss: 0.2190\n",
      "9/15, train_loss: 0.2097\n",
      "10/15, train_loss: 0.2045\n",
      "11/15, train_loss: 0.1706\n",
      "12/15, train_loss: 0.2061\n",
      "13/15, train_loss: 0.2177\n",
      "14/15, train_loss: 0.1702\n",
      "15/15, train_loss: 0.2504\n",
      "16/15, train_loss: 0.1578\n",
      "epoch 317 average loss: 0.2060\n",
      "----------\n",
      "epoch 318/500\n",
      "1/15, train_loss: 0.1975\n",
      "2/15, train_loss: 0.2336\n",
      "3/15, train_loss: 0.1915\n",
      "4/15, train_loss: 0.2378\n",
      "5/15, train_loss: 0.2209\n",
      "6/15, train_loss: 0.1739\n",
      "7/15, train_loss: 0.2300\n",
      "8/15, train_loss: 0.1993\n",
      "9/15, train_loss: 0.2119\n",
      "10/15, train_loss: 0.2387\n",
      "11/15, train_loss: 0.3517\n",
      "12/15, train_loss: 0.2150\n",
      "13/15, train_loss: 0.1518\n",
      "14/15, train_loss: 0.2292\n",
      "15/15, train_loss: 0.2323\n",
      "16/15, train_loss: 0.1688\n",
      "epoch 318 average loss: 0.2178\n",
      "----------\n",
      "epoch 319/500\n",
      "1/15, train_loss: 0.2917\n",
      "2/15, train_loss: 0.2017\n",
      "3/15, train_loss: 0.2473\n",
      "4/15, train_loss: 0.2145\n",
      "5/15, train_loss: 0.1807\n",
      "6/15, train_loss: 0.2240\n",
      "7/15, train_loss: 0.2104\n",
      "8/15, train_loss: 0.2242\n",
      "9/15, train_loss: 0.2165\n",
      "10/15, train_loss: 0.2041\n",
      "11/15, train_loss: 0.4979\n",
      "12/15, train_loss: 0.6453\n",
      "13/15, train_loss: 0.1931\n",
      "14/15, train_loss: 0.1780\n",
      "15/15, train_loss: 0.1774\n",
      "16/15, train_loss: 0.1755\n",
      "epoch 319 average loss: 0.2551\n",
      "----------\n",
      "epoch 320/500\n",
      "1/15, train_loss: 0.2921\n",
      "2/15, train_loss: 0.2134\n",
      "3/15, train_loss: 0.2143\n",
      "4/15, train_loss: 0.2119\n",
      "5/15, train_loss: 0.1837\n",
      "6/15, train_loss: 0.1671\n",
      "7/15, train_loss: 0.2249\n",
      "8/15, train_loss: 0.1715\n",
      "9/15, train_loss: 0.3209\n",
      "10/15, train_loss: 0.2531\n",
      "11/15, train_loss: 0.1772\n",
      "12/15, train_loss: 0.4045\n",
      "13/15, train_loss: 0.2305\n",
      "14/15, train_loss: 0.1551\n",
      "15/15, train_loss: 0.1724\n",
      "16/15, train_loss: 0.2899\n",
      "epoch 320 average loss: 0.2302\n",
      "----------\n",
      "epoch 321/500\n",
      "1/15, train_loss: 0.2235\n",
      "2/15, train_loss: 0.2714\n",
      "3/15, train_loss: 0.1893\n",
      "4/15, train_loss: 0.2498\n",
      "5/15, train_loss: 0.1820\n",
      "6/15, train_loss: 0.2402\n",
      "7/15, train_loss: 0.1879\n",
      "8/15, train_loss: 0.1770\n",
      "9/15, train_loss: 0.1923\n",
      "10/15, train_loss: 0.1993\n",
      "11/15, train_loss: 0.2507\n",
      "12/15, train_loss: 0.2126\n",
      "13/15, train_loss: 0.1882\n",
      "14/15, train_loss: 2.6221\n",
      "15/15, train_loss: 0.1778\n",
      "16/15, train_loss: 0.1803\n",
      "epoch 321 average loss: 0.3590\n",
      "----------\n",
      "epoch 322/500\n",
      "1/15, train_loss: 0.1628\n",
      "2/15, train_loss: 0.2022\n",
      "3/15, train_loss: 0.2874\n",
      "4/15, train_loss: 0.1923\n",
      "5/15, train_loss: 0.1954\n",
      "6/15, train_loss: 0.2892\n",
      "7/15, train_loss: 0.2162\n",
      "8/15, train_loss: 0.1955\n",
      "9/15, train_loss: 0.1795\n",
      "10/15, train_loss: 0.2115\n",
      "11/15, train_loss: 0.1826\n",
      "12/15, train_loss: 0.1819\n",
      "13/15, train_loss: 0.1953\n",
      "14/15, train_loss: 0.1712\n",
      "15/15, train_loss: 0.2143\n",
      "16/15, train_loss: 0.3395\n",
      "epoch 322 average loss: 0.2136\n",
      "----------\n",
      "epoch 323/500\n",
      "1/15, train_loss: 0.1779\n",
      "2/15, train_loss: 0.2197\n",
      "3/15, train_loss: 0.1949\n",
      "4/15, train_loss: 0.1895\n",
      "5/15, train_loss: 0.1856\n",
      "6/15, train_loss: 0.1769\n",
      "7/15, train_loss: 0.2547\n",
      "8/15, train_loss: 0.2270\n",
      "9/15, train_loss: 0.2625\n",
      "10/15, train_loss: 0.1576\n",
      "11/15, train_loss: 0.1582\n",
      "12/15, train_loss: 0.2995\n",
      "13/15, train_loss: 0.2346\n",
      "14/15, train_loss: 2.1985\n",
      "15/15, train_loss: 0.3661\n",
      "16/15, train_loss: 0.3274\n",
      "epoch 323 average loss: 0.3519\n",
      "----------\n",
      "epoch 324/500\n",
      "1/15, train_loss: 0.1965\n",
      "2/15, train_loss: 0.2088\n",
      "3/15, train_loss: 0.1924\n",
      "4/15, train_loss: 0.2624\n",
      "5/15, train_loss: 0.1815\n",
      "6/15, train_loss: 0.2614\n",
      "7/15, train_loss: 0.1720\n",
      "8/15, train_loss: 0.1966\n",
      "9/15, train_loss: 0.1791\n",
      "10/15, train_loss: 0.2132\n",
      "11/15, train_loss: 0.2748\n",
      "12/15, train_loss: 0.2267\n",
      "13/15, train_loss: 0.2350\n",
      "14/15, train_loss: 0.2086\n",
      "15/15, train_loss: 0.2708\n",
      "16/15, train_loss: 0.1785\n",
      "epoch 324 average loss: 0.2162\n",
      "----------\n",
      "epoch 325/500\n",
      "1/15, train_loss: 0.1675\n",
      "2/15, train_loss: 0.1930\n",
      "3/15, train_loss: 0.1935\n",
      "4/15, train_loss: 0.1835\n",
      "5/15, train_loss: 0.2899\n",
      "6/15, train_loss: 0.1872\n",
      "7/15, train_loss: 0.1966\n",
      "8/15, train_loss: 0.2071\n",
      "9/15, train_loss: 0.2177\n",
      "10/15, train_loss: 0.2033\n",
      "11/15, train_loss: 0.1889\n",
      "12/15, train_loss: 0.2603\n",
      "13/15, train_loss: 0.1946\n",
      "14/15, train_loss: 0.1793\n",
      "15/15, train_loss: 0.2283\n",
      "16/15, train_loss: 0.2085\n",
      "epoch 325 average loss: 0.2062\n",
      "----------\n",
      "epoch 326/500\n",
      "1/15, train_loss: 0.2126\n",
      "2/15, train_loss: 0.1888\n",
      "3/15, train_loss: 0.1799\n",
      "4/15, train_loss: 0.1975\n",
      "5/15, train_loss: 0.1992\n",
      "6/15, train_loss: 0.1823\n",
      "7/15, train_loss: 0.1858\n",
      "8/15, train_loss: 0.1626\n",
      "9/15, train_loss: 0.2143\n",
      "10/15, train_loss: 0.2471\n",
      "11/15, train_loss: 0.2359\n",
      "12/15, train_loss: 0.1811\n",
      "13/15, train_loss: 0.1868\n",
      "14/15, train_loss: 0.2172\n",
      "15/15, train_loss: 0.1773\n",
      "16/15, train_loss: 0.2860\n",
      "epoch 326 average loss: 0.2034\n",
      "----------\n",
      "epoch 327/500\n",
      "1/15, train_loss: 0.2266\n",
      "2/15, train_loss: 0.2398\n",
      "3/15, train_loss: 0.2290\n",
      "4/15, train_loss: 0.2922\n",
      "5/15, train_loss: 0.2029\n",
      "6/15, train_loss: 0.2262\n",
      "7/15, train_loss: 0.1692\n",
      "8/15, train_loss: 0.1969\n",
      "9/15, train_loss: 0.1970\n",
      "10/15, train_loss: 0.3095\n",
      "11/15, train_loss: 0.1868\n",
      "12/15, train_loss: 0.1851\n",
      "13/15, train_loss: 0.2192\n",
      "14/15, train_loss: 0.1916\n",
      "15/15, train_loss: 0.2161\n",
      "16/15, train_loss: 0.2394\n",
      "epoch 327 average loss: 0.2205\n",
      "----------\n",
      "epoch 328/500\n",
      "1/15, train_loss: 0.1842\n",
      "2/15, train_loss: 0.2028\n",
      "3/15, train_loss: 0.1799\n",
      "4/15, train_loss: 0.5293\n",
      "5/15, train_loss: 0.1594\n",
      "6/15, train_loss: 0.1865\n",
      "7/15, train_loss: 0.2142\n",
      "8/15, train_loss: 0.2009\n",
      "9/15, train_loss: 0.2346\n",
      "10/15, train_loss: 0.1781\n",
      "11/15, train_loss: 0.1528\n",
      "12/15, train_loss: 0.2114\n",
      "13/15, train_loss: 0.2111\n",
      "14/15, train_loss: 0.2801\n",
      "15/15, train_loss: 0.5173\n",
      "16/15, train_loss: 1.3374\n",
      "epoch 328 average loss: 0.3112\n",
      "----------\n",
      "epoch 329/500\n",
      "1/15, train_loss: 0.1710\n",
      "2/15, train_loss: 0.1808\n",
      "3/15, train_loss: 0.1856\n",
      "4/15, train_loss: 0.1693\n",
      "5/15, train_loss: 0.1809\n",
      "6/15, train_loss: 0.2268\n",
      "7/15, train_loss: 0.2062\n",
      "8/15, train_loss: 0.2101\n",
      "9/15, train_loss: 0.1827\n",
      "10/15, train_loss: 0.1755\n",
      "11/15, train_loss: 0.1821\n",
      "12/15, train_loss: 0.2053\n",
      "13/15, train_loss: 0.1988\n",
      "14/15, train_loss: 0.1626\n",
      "15/15, train_loss: 0.2672\n",
      "16/15, train_loss: 2.9725\n",
      "epoch 329 average loss: 0.3673\n",
      "----------\n",
      "epoch 330/500\n",
      "1/15, train_loss: 0.2088\n",
      "2/15, train_loss: 0.1877\n",
      "3/15, train_loss: 0.1848\n",
      "4/15, train_loss: 0.1658\n",
      "5/15, train_loss: 0.2101\n",
      "6/15, train_loss: 0.1797\n",
      "7/15, train_loss: 0.2328\n",
      "8/15, train_loss: 0.1763\n",
      "9/15, train_loss: 0.1727\n",
      "10/15, train_loss: 0.2342\n",
      "11/15, train_loss: 0.1813\n",
      "12/15, train_loss: 0.1995\n",
      "13/15, train_loss: 0.1533\n",
      "14/15, train_loss: 0.2203\n",
      "15/15, train_loss: 0.2573\n",
      "16/15, train_loss: 0.2589\n",
      "epoch 330 average loss: 0.2015\n",
      "----------\n",
      "epoch 331/500\n",
      "1/15, train_loss: 0.2587\n",
      "2/15, train_loss: 0.2391\n",
      "3/15, train_loss: 0.1974\n",
      "4/15, train_loss: 0.1968\n",
      "5/15, train_loss: 0.1882\n",
      "6/15, train_loss: 0.2321\n",
      "7/15, train_loss: 0.1888\n",
      "8/15, train_loss: 0.1996\n",
      "9/15, train_loss: 0.1850\n",
      "10/15, train_loss: 0.1551\n",
      "11/15, train_loss: 0.1870\n",
      "12/15, train_loss: 0.1789\n",
      "13/15, train_loss: 0.1982\n",
      "14/15, train_loss: 0.1993\n",
      "15/15, train_loss: 0.1869\n",
      "16/15, train_loss: 0.2219\n",
      "epoch 331 average loss: 0.2008\n",
      "----------\n",
      "epoch 332/500\n",
      "1/15, train_loss: 0.1801\n",
      "2/15, train_loss: 0.2005\n",
      "3/15, train_loss: 0.3813\n",
      "4/15, train_loss: 0.1768\n",
      "5/15, train_loss: 0.2070\n",
      "6/15, train_loss: 0.1779\n",
      "7/15, train_loss: 0.2235\n",
      "8/15, train_loss: 0.1683\n",
      "9/15, train_loss: 0.2347\n",
      "10/15, train_loss: 0.2382\n",
      "11/15, train_loss: 0.2108\n",
      "12/15, train_loss: 0.5158\n",
      "13/15, train_loss: 0.2789\n",
      "14/15, train_loss: 0.1669\n",
      "15/15, train_loss: 0.2033\n",
      "16/15, train_loss: 0.1857\n",
      "epoch 332 average loss: 0.2344\n",
      "----------\n",
      "epoch 333/500\n",
      "1/15, train_loss: 0.1718\n",
      "2/15, train_loss: 0.2494\n",
      "3/15, train_loss: 0.1573\n",
      "4/15, train_loss: 0.1984\n",
      "5/15, train_loss: 0.1739\n",
      "6/15, train_loss: 0.1690\n",
      "7/15, train_loss: 0.2013\n",
      "8/15, train_loss: 0.2480\n",
      "9/15, train_loss: 0.1758\n",
      "10/15, train_loss: 0.2043\n",
      "11/15, train_loss: 0.2495\n",
      "12/15, train_loss: 0.1859\n",
      "13/15, train_loss: 0.2150\n",
      "14/15, train_loss: 2.2460\n",
      "15/15, train_loss: 0.1581\n",
      "16/15, train_loss: 0.1597\n",
      "epoch 333 average loss: 0.3227\n",
      "----------\n",
      "epoch 334/500\n",
      "1/15, train_loss: 0.1712\n",
      "2/15, train_loss: 0.2037\n",
      "3/15, train_loss: 0.2865\n",
      "4/15, train_loss: 0.2959\n",
      "5/15, train_loss: 0.1827\n",
      "6/15, train_loss: 0.2372\n",
      "7/15, train_loss: 0.1607\n",
      "8/15, train_loss: 0.2014\n",
      "9/15, train_loss: 0.1594\n",
      "10/15, train_loss: 0.1600\n",
      "11/15, train_loss: 0.2031\n",
      "12/15, train_loss: 0.2087\n",
      "13/15, train_loss: 0.2044\n",
      "14/15, train_loss: 0.1590\n",
      "15/15, train_loss: 0.3425\n",
      "16/15, train_loss: 0.1896\n",
      "epoch 334 average loss: 0.2104\n",
      "----------\n",
      "epoch 335/500\n",
      "1/15, train_loss: 0.1639\n",
      "2/15, train_loss: 0.2038\n",
      "3/15, train_loss: 0.2065\n",
      "4/15, train_loss: 1.3158\n",
      "5/15, train_loss: 0.1719\n",
      "6/15, train_loss: 0.1942\n",
      "7/15, train_loss: 0.2043\n",
      "8/15, train_loss: 0.1680\n",
      "9/15, train_loss: 0.2736\n",
      "10/15, train_loss: 0.1944\n",
      "11/15, train_loss: 0.1820\n",
      "12/15, train_loss: 0.1654\n",
      "13/15, train_loss: 0.2335\n",
      "14/15, train_loss: 0.1612\n",
      "15/15, train_loss: 0.2476\n",
      "16/15, train_loss: 0.1602\n",
      "epoch 335 average loss: 0.2654\n",
      "----------\n",
      "epoch 336/500\n",
      "1/15, train_loss: 0.2217\n",
      "2/15, train_loss: 0.2986\n",
      "3/15, train_loss: 0.1930\n",
      "4/15, train_loss: 0.2136\n",
      "5/15, train_loss: 0.1579\n",
      "6/15, train_loss: 0.1881\n",
      "7/15, train_loss: 0.1764\n",
      "8/15, train_loss: 0.2031\n",
      "9/15, train_loss: 0.1890\n",
      "10/15, train_loss: 0.1576\n",
      "11/15, train_loss: 0.2300\n",
      "12/15, train_loss: 0.2138\n",
      "13/15, train_loss: 0.1744\n",
      "14/15, train_loss: 0.5842\n",
      "15/15, train_loss: 0.1867\n",
      "16/15, train_loss: 0.2396\n",
      "epoch 336 average loss: 0.2267\n",
      "----------\n",
      "epoch 337/500\n",
      "1/15, train_loss: 0.1660\n",
      "2/15, train_loss: 0.1884\n",
      "3/15, train_loss: 0.2806\n",
      "4/15, train_loss: 0.1850\n",
      "5/15, train_loss: 0.2290\n",
      "6/15, train_loss: 0.1603\n",
      "7/15, train_loss: 0.2249\n",
      "8/15, train_loss: 0.2026\n",
      "9/15, train_loss: 0.1839\n",
      "10/15, train_loss: 0.3345\n",
      "11/15, train_loss: 0.1742\n",
      "12/15, train_loss: 0.1751\n",
      "13/15, train_loss: 2.2265\n",
      "14/15, train_loss: 0.2769\n",
      "15/15, train_loss: 0.1631\n",
      "16/15, train_loss: 0.2008\n",
      "epoch 337 average loss: 0.3357\n",
      "----------\n",
      "epoch 338/500\n",
      "1/15, train_loss: 0.1858\n",
      "2/15, train_loss: 0.2157\n",
      "3/15, train_loss: 0.1626\n",
      "4/15, train_loss: 0.2008\n",
      "5/15, train_loss: 0.1458\n",
      "6/15, train_loss: 0.1650\n",
      "7/15, train_loss: 0.1856\n",
      "8/15, train_loss: 0.1620\n",
      "9/15, train_loss: 0.2010\n",
      "10/15, train_loss: 0.2471\n",
      "11/15, train_loss: 0.1908\n",
      "12/15, train_loss: 0.2243\n",
      "13/15, train_loss: 0.1652\n",
      "14/15, train_loss: 0.2244\n",
      "15/15, train_loss: 0.2166\n",
      "16/15, train_loss: 0.1766\n",
      "epoch 338 average loss: 0.1918\n",
      "----------\n",
      "epoch 339/500\n",
      "1/15, train_loss: 0.2068\n",
      "2/15, train_loss: 0.2062\n",
      "3/15, train_loss: 0.8468\n",
      "4/15, train_loss: 0.1805\n",
      "5/15, train_loss: 0.1736\n",
      "6/15, train_loss: 0.2166\n",
      "7/15, train_loss: 0.3346\n",
      "8/15, train_loss: 0.1823\n",
      "9/15, train_loss: 0.1734\n",
      "10/15, train_loss: 0.1854\n",
      "11/15, train_loss: 0.1985\n",
      "12/15, train_loss: 0.2312\n",
      "13/15, train_loss: 0.2857\n",
      "14/15, train_loss: 0.1706\n",
      "15/15, train_loss: 0.1917\n",
      "16/15, train_loss: 0.2324\n",
      "epoch 339 average loss: 0.2510\n",
      "----------\n",
      "epoch 340/500\n",
      "1/15, train_loss: 0.1704\n",
      "2/15, train_loss: 0.2771\n",
      "3/15, train_loss: 0.1739\n",
      "4/15, train_loss: 0.1584\n",
      "5/15, train_loss: 0.1952\n",
      "6/15, train_loss: 0.1847\n",
      "7/15, train_loss: 0.2275\n",
      "8/15, train_loss: 0.1736\n",
      "9/15, train_loss: 0.1533\n",
      "10/15, train_loss: 0.2375\n",
      "11/15, train_loss: 0.2083\n",
      "12/15, train_loss: 0.2119\n",
      "13/15, train_loss: 0.1463\n",
      "14/15, train_loss: 0.2145\n",
      "15/15, train_loss: 0.1973\n",
      "16/15, train_loss: 2.1633\n",
      "epoch 340 average loss: 0.3183\n",
      "----------\n",
      "epoch 341/500\n",
      "1/15, train_loss: 0.1849\n",
      "2/15, train_loss: 0.2467\n",
      "3/15, train_loss: 0.1927\n",
      "4/15, train_loss: 0.6025\n",
      "5/15, train_loss: 0.1912\n",
      "6/15, train_loss: 0.2268\n",
      "7/15, train_loss: 0.1641\n",
      "8/15, train_loss: 0.2106\n",
      "9/15, train_loss: 0.1700\n",
      "10/15, train_loss: 0.2627\n",
      "11/15, train_loss: 0.2074\n",
      "12/15, train_loss: 0.1918\n",
      "13/15, train_loss: 0.2059\n",
      "14/15, train_loss: 0.2624\n",
      "15/15, train_loss: 0.1876\n",
      "16/15, train_loss: 0.2863\n",
      "epoch 341 average loss: 0.2371\n",
      "----------\n",
      "epoch 342/500\n",
      "1/15, train_loss: 0.1754\n",
      "2/15, train_loss: 0.1685\n",
      "3/15, train_loss: 0.1819\n",
      "4/15, train_loss: 0.2025\n",
      "5/15, train_loss: 0.1865\n",
      "6/15, train_loss: 0.2594\n",
      "7/15, train_loss: 0.1901\n",
      "8/15, train_loss: 0.2146\n",
      "9/15, train_loss: 0.1712\n",
      "10/15, train_loss: 0.1932\n",
      "11/15, train_loss: 0.1759\n",
      "12/15, train_loss: 1.9950\n",
      "13/15, train_loss: 0.1911\n",
      "14/15, train_loss: 0.1628\n",
      "15/15, train_loss: 0.1736\n",
      "16/15, train_loss: 2.2958\n",
      "epoch 342 average loss: 0.4336\n",
      "----------\n",
      "epoch 343/500\n",
      "1/15, train_loss: 0.1663\n",
      "2/15, train_loss: 0.1772\n",
      "3/15, train_loss: 0.1726\n",
      "4/15, train_loss: 0.2312\n",
      "5/15, train_loss: 0.1543\n",
      "6/15, train_loss: 0.2159\n",
      "7/15, train_loss: 0.1710\n",
      "8/15, train_loss: 0.1889\n",
      "9/15, train_loss: 0.1655\n",
      "10/15, train_loss: 0.1734\n",
      "11/15, train_loss: 0.2341\n",
      "12/15, train_loss: 0.2705\n",
      "13/15, train_loss: 0.1791\n",
      "14/15, train_loss: 0.2048\n",
      "15/15, train_loss: 0.3819\n",
      "16/15, train_loss: 0.8896\n",
      "epoch 343 average loss: 0.2485\n",
      "----------\n",
      "epoch 344/500\n",
      "1/15, train_loss: 0.1600\n",
      "2/15, train_loss: 0.2074\n",
      "3/15, train_loss: 0.1895\n",
      "4/15, train_loss: 0.2221\n",
      "5/15, train_loss: 0.1671\n",
      "6/15, train_loss: 0.2911\n",
      "7/15, train_loss: 0.1731\n",
      "8/15, train_loss: 0.2140\n",
      "9/15, train_loss: 0.2044\n",
      "10/15, train_loss: 0.1805\n",
      "11/15, train_loss: 0.1525\n",
      "12/15, train_loss: 0.2094\n",
      "13/15, train_loss: 0.1806\n",
      "14/15, train_loss: 0.1731\n",
      "15/15, train_loss: 0.2118\n",
      "16/15, train_loss: 2.5701\n",
      "epoch 344 average loss: 0.3442\n",
      "----------\n",
      "epoch 345/500\n",
      "1/15, train_loss: 0.2279\n",
      "2/15, train_loss: 0.1468\n",
      "3/15, train_loss: 0.2061\n",
      "4/15, train_loss: 0.2263\n",
      "5/15, train_loss: 0.1689\n",
      "6/15, train_loss: 0.1953\n",
      "7/15, train_loss: 0.1799\n",
      "8/15, train_loss: 0.2088\n",
      "9/15, train_loss: 0.1694\n",
      "10/15, train_loss: 0.1486\n",
      "11/15, train_loss: 0.1828\n",
      "12/15, train_loss: 0.1954\n",
      "13/15, train_loss: 0.1924\n",
      "14/15, train_loss: 0.2004\n",
      "15/15, train_loss: 0.1607\n",
      "16/15, train_loss: 2.5146\n",
      "epoch 345 average loss: 0.3328\n",
      "----------\n",
      "epoch 346/500\n",
      "1/15, train_loss: 0.1554\n",
      "2/15, train_loss: 0.2029\n",
      "3/15, train_loss: 0.1609\n",
      "4/15, train_loss: 0.1683\n",
      "5/15, train_loss: 0.2159\n",
      "6/15, train_loss: 0.1498\n",
      "7/15, train_loss: 0.4359\n",
      "8/15, train_loss: 0.1757\n",
      "9/15, train_loss: 0.1974\n",
      "10/15, train_loss: 0.2760\n",
      "11/15, train_loss: 0.1681\n",
      "12/15, train_loss: 0.1798\n",
      "13/15, train_loss: 0.1740\n",
      "14/15, train_loss: 0.1779\n",
      "15/15, train_loss: 0.2181\n",
      "16/15, train_loss: 2.0213\n",
      "epoch 346 average loss: 0.3173\n",
      "----------\n",
      "epoch 347/500\n",
      "1/15, train_loss: 0.1884\n",
      "2/15, train_loss: 0.1632\n",
      "3/15, train_loss: 0.1417\n",
      "4/15, train_loss: 0.1859\n",
      "5/15, train_loss: 0.1944\n",
      "6/15, train_loss: 0.2059\n",
      "7/15, train_loss: 0.1611\n",
      "8/15, train_loss: 0.1507\n",
      "9/15, train_loss: 0.2463\n",
      "10/15, train_loss: 0.1586\n",
      "11/15, train_loss: 0.2561\n",
      "12/15, train_loss: 0.2087\n",
      "13/15, train_loss: 0.1902\n",
      "14/15, train_loss: 0.1935\n",
      "15/15, train_loss: 0.1680\n",
      "16/15, train_loss: 2.5774\n",
      "epoch 347 average loss: 0.3369\n",
      "----------\n",
      "epoch 348/500\n",
      "1/15, train_loss: 0.1547\n",
      "2/15, train_loss: 0.1750\n",
      "3/15, train_loss: 0.2154\n",
      "4/15, train_loss: 0.1887\n",
      "5/15, train_loss: 0.2286\n",
      "6/15, train_loss: 0.1496\n",
      "7/15, train_loss: 0.1600\n",
      "8/15, train_loss: 0.1874\n",
      "9/15, train_loss: 0.1892\n",
      "10/15, train_loss: 0.2406\n",
      "11/15, train_loss: 0.1381\n",
      "12/15, train_loss: 0.2296\n",
      "13/15, train_loss: 0.1770\n",
      "14/15, train_loss: 0.1947\n",
      "15/15, train_loss: 0.2084\n",
      "16/15, train_loss: 0.1877\n",
      "epoch 348 average loss: 0.1890\n",
      "----------\n",
      "epoch 349/500\n",
      "1/15, train_loss: 0.1479\n",
      "2/15, train_loss: 0.2486\n",
      "3/15, train_loss: 0.5209\n",
      "4/15, train_loss: 0.2386\n",
      "5/15, train_loss: 0.1684\n",
      "6/15, train_loss: 0.3173\n",
      "7/15, train_loss: 0.2580\n",
      "8/15, train_loss: 0.1710\n",
      "9/15, train_loss: 0.3765\n",
      "10/15, train_loss: 0.5340\n",
      "11/15, train_loss: 0.2027\n",
      "12/15, train_loss: 0.1902\n",
      "13/15, train_loss: 0.1470\n",
      "14/15, train_loss: 0.2371\n",
      "15/15, train_loss: 0.4526\n",
      "16/15, train_loss: 0.2660\n",
      "epoch 349 average loss: 0.2798\n",
      "----------\n",
      "epoch 350/500\n",
      "1/15, train_loss: 0.1841\n",
      "2/15, train_loss: 0.1519\n",
      "3/15, train_loss: 0.1950\n",
      "4/15, train_loss: 0.1870\n",
      "5/15, train_loss: 0.1745\n",
      "6/15, train_loss: 0.2500\n",
      "7/15, train_loss: 0.1763\n",
      "8/15, train_loss: 0.2459\n",
      "9/15, train_loss: 0.1676\n",
      "10/15, train_loss: 0.1808\n",
      "11/15, train_loss: 0.2036\n",
      "12/15, train_loss: 0.1716\n",
      "13/15, train_loss: 0.1859\n",
      "14/15, train_loss: 0.1608\n",
      "15/15, train_loss: 0.2254\n",
      "16/15, train_loss: 0.1769\n",
      "epoch 350 average loss: 0.1898\n",
      "----------\n",
      "epoch 351/500\n",
      "1/15, train_loss: 0.1727\n",
      "2/15, train_loss: 0.2330\n",
      "3/15, train_loss: 0.1780\n",
      "4/15, train_loss: 0.1580\n",
      "5/15, train_loss: 0.1752\n",
      "6/15, train_loss: 0.1610\n",
      "7/15, train_loss: 0.2151\n",
      "8/15, train_loss: 0.1624\n",
      "9/15, train_loss: 0.2027\n",
      "10/15, train_loss: 0.2099\n",
      "11/15, train_loss: 0.1705\n",
      "12/15, train_loss: 0.1938\n",
      "13/15, train_loss: 0.1638\n",
      "14/15, train_loss: 0.2041\n",
      "15/15, train_loss: 0.1765\n",
      "16/15, train_loss: 0.1725\n",
      "epoch 351 average loss: 0.1843\n",
      "----------\n",
      "epoch 352/500\n",
      "1/15, train_loss: 0.1757\n",
      "2/15, train_loss: 0.4113\n",
      "3/15, train_loss: 0.1657\n",
      "4/15, train_loss: 0.2217\n",
      "5/15, train_loss: 0.2122\n",
      "6/15, train_loss: 0.2583\n",
      "7/15, train_loss: 0.2020\n",
      "8/15, train_loss: 0.1845\n",
      "9/15, train_loss: 0.1898\n",
      "10/15, train_loss: 0.1750\n",
      "11/15, train_loss: 0.1694\n",
      "12/15, train_loss: 0.1822\n",
      "13/15, train_loss: 0.1728\n",
      "14/15, train_loss: 0.2356\n",
      "15/15, train_loss: 0.1511\n",
      "16/15, train_loss: 2.3468\n",
      "epoch 352 average loss: 0.3409\n",
      "----------\n",
      "epoch 353/500\n",
      "1/15, train_loss: 0.2794\n",
      "2/15, train_loss: 0.1623\n",
      "3/15, train_loss: 0.1696\n",
      "4/15, train_loss: 0.1643\n",
      "5/15, train_loss: 0.2059\n",
      "6/15, train_loss: 0.1618\n",
      "7/15, train_loss: 0.2207\n",
      "8/15, train_loss: 0.1848\n",
      "9/15, train_loss: 0.1475\n",
      "10/15, train_loss: 0.1912\n",
      "11/15, train_loss: 0.1575\n",
      "12/15, train_loss: 0.1447\n",
      "13/15, train_loss: 0.2006\n",
      "14/15, train_loss: 0.1579\n",
      "15/15, train_loss: 0.2051\n",
      "16/15, train_loss: 2.1092\n",
      "epoch 353 average loss: 0.3039\n",
      "----------\n",
      "epoch 354/500\n",
      "1/15, train_loss: 0.1800\n",
      "2/15, train_loss: 0.2265\n",
      "3/15, train_loss: 0.1865\n",
      "4/15, train_loss: 0.2256\n",
      "5/15, train_loss: 0.1433\n",
      "6/15, train_loss: 0.1736\n",
      "7/15, train_loss: 0.1773\n",
      "8/15, train_loss: 0.1630\n",
      "9/15, train_loss: 0.1812\n",
      "10/15, train_loss: 0.2013\n",
      "11/15, train_loss: 0.1785\n",
      "12/15, train_loss: 0.1871\n",
      "13/15, train_loss: 0.1915\n",
      "14/15, train_loss: 0.1905\n",
      "15/15, train_loss: 0.2026\n",
      "16/15, train_loss: 0.1847\n",
      "epoch 354 average loss: 0.1871\n",
      "----------\n",
      "epoch 355/500\n",
      "1/15, train_loss: 0.2070\n",
      "2/15, train_loss: 0.1694\n",
      "3/15, train_loss: 0.2043\n",
      "4/15, train_loss: 0.1680\n",
      "5/15, train_loss: 0.1435\n",
      "6/15, train_loss: 0.1904\n",
      "7/15, train_loss: 0.1502\n",
      "8/15, train_loss: 0.1962\n",
      "9/15, train_loss: 0.1732\n",
      "10/15, train_loss: 0.1749\n",
      "11/15, train_loss: 0.2022\n",
      "12/15, train_loss: 0.1396\n",
      "13/15, train_loss: 0.2397\n",
      "14/15, train_loss: 0.1744\n",
      "15/15, train_loss: 0.1569\n",
      "16/15, train_loss: 0.1883\n",
      "epoch 355 average loss: 0.1799\n",
      "----------\n",
      "epoch 356/500\n",
      "1/15, train_loss: 0.1491\n",
      "2/15, train_loss: 0.1905\n",
      "3/15, train_loss: 0.1700\n",
      "4/15, train_loss: 0.1717\n",
      "5/15, train_loss: 0.3233\n",
      "6/15, train_loss: 0.1542\n",
      "7/15, train_loss: 0.2482\n",
      "8/15, train_loss: 0.1823\n",
      "9/15, train_loss: 0.1347\n",
      "10/15, train_loss: 0.1793\n",
      "11/15, train_loss: 0.1746\n",
      "12/15, train_loss: 0.2315\n",
      "13/15, train_loss: 0.1540\n",
      "14/15, train_loss: 0.2044\n",
      "15/15, train_loss: 0.2867\n",
      "16/15, train_loss: 0.1524\n",
      "epoch 356 average loss: 0.1942\n",
      "----------\n",
      "epoch 357/500\n",
      "1/15, train_loss: 0.1864\n",
      "2/15, train_loss: 0.2589\n",
      "3/15, train_loss: 0.1598\n",
      "4/15, train_loss: 0.1770\n",
      "5/15, train_loss: 0.1739\n",
      "6/15, train_loss: 0.1764\n",
      "7/15, train_loss: 0.2423\n",
      "8/15, train_loss: 0.1755\n",
      "9/15, train_loss: 0.2303\n",
      "10/15, train_loss: 0.1747\n",
      "11/15, train_loss: 0.1854\n",
      "12/15, train_loss: 0.2726\n",
      "13/15, train_loss: 0.1576\n",
      "14/15, train_loss: 0.2194\n",
      "15/15, train_loss: 0.1629\n",
      "16/15, train_loss: 0.1814\n",
      "epoch 357 average loss: 0.1959\n",
      "----------\n",
      "epoch 358/500\n",
      "1/15, train_loss: 0.1875\n",
      "2/15, train_loss: 0.1509\n",
      "3/15, train_loss: 0.1704\n",
      "4/15, train_loss: 0.1807\n",
      "5/15, train_loss: 0.3292\n",
      "6/15, train_loss: 0.1601\n",
      "7/15, train_loss: 0.1603\n",
      "8/15, train_loss: 0.7094\n",
      "9/15, train_loss: 0.1650\n",
      "10/15, train_loss: 0.1956\n",
      "11/15, train_loss: 0.3032\n",
      "12/15, train_loss: 0.1642\n",
      "13/15, train_loss: 0.1908\n",
      "14/15, train_loss: 0.1990\n",
      "15/15, train_loss: 0.1636\n",
      "16/15, train_loss: 0.1501\n",
      "epoch 358 average loss: 0.2238\n",
      "----------\n",
      "epoch 359/500\n",
      "1/15, train_loss: 0.1606\n",
      "2/15, train_loss: 0.1953\n",
      "3/15, train_loss: 0.1798\n",
      "4/15, train_loss: 0.1838\n",
      "5/15, train_loss: 0.1491\n",
      "6/15, train_loss: 0.1936\n",
      "7/15, train_loss: 0.2120\n",
      "8/15, train_loss: 0.1549\n",
      "9/15, train_loss: 0.1609\n",
      "10/15, train_loss: 0.1861\n",
      "11/15, train_loss: 0.1590\n",
      "12/15, train_loss: 0.2322\n",
      "13/15, train_loss: 0.1914\n",
      "14/15, train_loss: 0.1882\n",
      "15/15, train_loss: 0.1929\n",
      "16/15, train_loss: 0.1719\n",
      "epoch 359 average loss: 0.1820\n",
      "----------\n",
      "epoch 360/500\n",
      "1/15, train_loss: 0.1752\n",
      "2/15, train_loss: 0.1771\n",
      "3/15, train_loss: 0.1705\n",
      "4/15, train_loss: 0.1629\n",
      "5/15, train_loss: 0.1760\n",
      "6/15, train_loss: 0.1841\n",
      "7/15, train_loss: 0.1778\n",
      "8/15, train_loss: 0.2160\n",
      "9/15, train_loss: 0.2486\n",
      "10/15, train_loss: 0.1534\n",
      "11/15, train_loss: 0.2052\n",
      "12/15, train_loss: 0.1691\n",
      "13/15, train_loss: 0.1799\n",
      "14/15, train_loss: 0.1621\n",
      "15/15, train_loss: 0.1648\n",
      "16/15, train_loss: 2.3799\n",
      "epoch 360 average loss: 0.3189\n",
      "----------\n",
      "epoch 361/500\n",
      "1/15, train_loss: 0.1794\n",
      "2/15, train_loss: 0.2099\n",
      "\n",
      "** Ranger21 update: Warmdown starting now.  Current iteration = 5760....\n",
      "\n",
      "3/15, train_loss: 0.1646\n",
      "4/15, train_loss: 0.1801\n",
      "5/15, train_loss: 0.1749\n",
      "6/15, train_loss: 0.1784\n",
      "7/15, train_loss: 0.1691\n",
      "8/15, train_loss: 0.2228\n",
      "9/15, train_loss: 0.1762\n",
      "10/15, train_loss: 0.2516\n",
      "11/15, train_loss: 0.1628\n",
      "12/15, train_loss: 0.1640\n",
      "13/15, train_loss: 0.1562\n",
      "14/15, train_loss: 0.1454\n",
      "15/15, train_loss: 0.2416\n",
      "16/15, train_loss: 0.1575\n",
      "epoch 361 average loss: 0.1834\n",
      "----------\n",
      "epoch 362/500\n",
      "1/15, train_loss: 0.1994\n",
      "2/15, train_loss: 0.2567\n",
      "3/15, train_loss: 0.1797\n",
      "4/15, train_loss: 0.1710\n",
      "5/15, train_loss: 0.1791\n",
      "6/15, train_loss: 0.1987\n",
      "7/15, train_loss: 0.2419\n",
      "8/15, train_loss: 0.1412\n",
      "9/15, train_loss: 0.1821\n",
      "10/15, train_loss: 0.2037\n",
      "11/15, train_loss: 0.2485\n",
      "12/15, train_loss: 0.1471\n",
      "13/15, train_loss: 0.1482\n",
      "14/15, train_loss: 0.2335\n",
      "15/15, train_loss: 0.2127\n",
      "16/15, train_loss: 2.0742\n",
      "epoch 362 average loss: 0.3136\n",
      "----------\n",
      "epoch 363/500\n",
      "1/15, train_loss: 0.1790\n",
      "2/15, train_loss: 0.1873\n",
      "3/15, train_loss: 0.1699\n",
      "4/15, train_loss: 0.2030\n",
      "5/15, train_loss: 0.1946\n",
      "6/15, train_loss: 0.1875\n",
      "7/15, train_loss: 0.2193\n",
      "8/15, train_loss: 0.1915\n",
      "9/15, train_loss: 0.1506\n",
      "10/15, train_loss: 0.2004\n",
      "11/15, train_loss: 0.1971\n",
      "12/15, train_loss: 0.1815\n",
      "13/15, train_loss: 0.1564\n",
      "14/15, train_loss: 0.2110\n",
      "15/15, train_loss: 0.2049\n",
      "16/15, train_loss: 2.1080\n",
      "epoch 363 average loss: 0.3089\n",
      "----------\n",
      "epoch 364/500\n",
      "1/15, train_loss: 0.1479\n",
      "2/15, train_loss: 0.1825\n",
      "3/15, train_loss: 0.1705\n",
      "4/15, train_loss: 0.2068\n",
      "5/15, train_loss: 0.1951\n",
      "6/15, train_loss: 0.1693\n",
      "7/15, train_loss: 0.1991\n",
      "8/15, train_loss: 0.1899\n",
      "9/15, train_loss: 0.2461\n",
      "10/15, train_loss: 0.1649\n",
      "11/15, train_loss: 0.1887\n",
      "12/15, train_loss: 0.1948\n",
      "13/15, train_loss: 0.1546\n",
      "14/15, train_loss: 0.1630\n",
      "15/15, train_loss: 0.1581\n",
      "16/15, train_loss: 0.2003\n",
      "epoch 364 average loss: 0.1832\n",
      "----------\n",
      "epoch 365/500\n",
      "1/15, train_loss: 0.2169\n",
      "2/15, train_loss: 0.1732\n",
      "3/15, train_loss: 0.2861\n",
      "4/15, train_loss: 0.1557\n",
      "5/15, train_loss: 0.1357\n",
      "6/15, train_loss: 0.1559\n",
      "7/15, train_loss: 0.1499\n",
      "8/15, train_loss: 0.1623\n",
      "9/15, train_loss: 0.1991\n",
      "10/15, train_loss: 0.2075\n",
      "11/15, train_loss: 0.3428\n",
      "12/15, train_loss: 0.1471\n",
      "13/15, train_loss: 0.3052\n",
      "14/15, train_loss: 0.1448\n",
      "15/15, train_loss: 0.1608\n",
      "16/15, train_loss: 2.1888\n",
      "epoch 365 average loss: 0.3207\n",
      "----------\n",
      "epoch 366/500\n",
      "1/15, train_loss: 0.1614\n",
      "2/15, train_loss: 0.1680\n",
      "3/15, train_loss: 0.1599\n",
      "4/15, train_loss: 0.1381\n",
      "5/15, train_loss: 0.2132\n",
      "6/15, train_loss: 0.1635\n",
      "7/15, train_loss: 0.1907\n",
      "8/15, train_loss: 0.1625\n",
      "9/15, train_loss: 0.2663\n",
      "10/15, train_loss: 0.1839\n",
      "11/15, train_loss: 0.1645\n",
      "12/15, train_loss: 0.1882\n",
      "13/15, train_loss: 0.2068\n",
      "14/15, train_loss: 0.1744\n",
      "15/15, train_loss: 0.1963\n",
      "16/15, train_loss: 0.1651\n",
      "epoch 366 average loss: 0.1814\n",
      "----------\n",
      "epoch 367/500\n",
      "1/15, train_loss: 0.3105\n",
      "2/15, train_loss: 0.1730\n",
      "3/15, train_loss: 0.1775\n",
      "4/15, train_loss: 0.1914\n",
      "5/15, train_loss: 0.1802\n",
      "6/15, train_loss: 0.1696\n",
      "7/15, train_loss: 0.1691\n",
      "8/15, train_loss: 0.1715\n",
      "9/15, train_loss: 0.1713\n",
      "10/15, train_loss: 0.2017\n",
      "11/15, train_loss: 0.1837\n",
      "12/15, train_loss: 0.1567\n",
      "13/15, train_loss: 0.1590\n",
      "14/15, train_loss: 0.2325\n",
      "15/15, train_loss: 0.1546\n",
      "16/15, train_loss: 0.2154\n",
      "epoch 367 average loss: 0.1886\n",
      "----------\n",
      "epoch 368/500\n",
      "1/15, train_loss: 0.1960\n",
      "2/15, train_loss: 0.1496\n",
      "3/15, train_loss: 0.2341\n",
      "4/15, train_loss: 0.1677\n",
      "5/15, train_loss: 0.1706\n",
      "6/15, train_loss: 0.1609\n",
      "7/15, train_loss: 0.1719\n",
      "8/15, train_loss: 0.2151\n",
      "9/15, train_loss: 0.1831\n",
      "10/15, train_loss: 0.1961\n",
      "11/15, train_loss: 0.1602\n",
      "12/15, train_loss: 0.1540\n",
      "13/15, train_loss: 0.2752\n",
      "14/15, train_loss: 0.1455\n",
      "15/15, train_loss: 0.1899\n",
      "16/15, train_loss: 0.1493\n",
      "epoch 368 average loss: 0.1824\n",
      "----------\n",
      "epoch 369/500\n",
      "1/15, train_loss: 0.1382\n",
      "2/15, train_loss: 0.2435\n",
      "3/15, train_loss: 0.1754\n",
      "4/15, train_loss: 0.1945\n",
      "5/15, train_loss: 0.1510\n",
      "6/15, train_loss: 0.2943\n",
      "7/15, train_loss: 0.1652\n",
      "8/15, train_loss: 0.1527\n",
      "9/15, train_loss: 0.2096\n",
      "10/15, train_loss: 0.1884\n",
      "11/15, train_loss: 0.1434\n",
      "12/15, train_loss: 0.1980\n",
      "13/15, train_loss: 0.1682\n",
      "14/15, train_loss: 0.2083\n",
      "15/15, train_loss: 0.1623\n",
      "16/15, train_loss: 0.1341\n",
      "epoch 369 average loss: 0.1830\n",
      "----------\n",
      "epoch 370/500\n",
      "1/15, train_loss: 0.2201\n",
      "2/15, train_loss: 0.1408\n",
      "3/15, train_loss: 0.1625\n",
      "4/15, train_loss: 0.1502\n",
      "5/15, train_loss: 0.1568\n",
      "6/15, train_loss: 0.2212\n",
      "7/15, train_loss: 0.1447\n",
      "8/15, train_loss: 0.1916\n",
      "9/15, train_loss: 0.1384\n",
      "10/15, train_loss: 0.1610\n",
      "11/15, train_loss: 0.1932\n",
      "12/15, train_loss: 0.1618\n",
      "13/15, train_loss: 0.2754\n",
      "14/15, train_loss: 0.1694\n",
      "15/15, train_loss: 0.1436\n",
      "16/15, train_loss: 0.1745\n",
      "epoch 370 average loss: 0.1753\n",
      "----------\n",
      "epoch 371/500\n",
      "1/15, train_loss: 0.1553\n",
      "2/15, train_loss: 0.2607\n",
      "3/15, train_loss: 0.1389\n",
      "4/15, train_loss: 0.1491\n",
      "5/15, train_loss: 0.1985\n",
      "6/15, train_loss: 0.1432\n",
      "7/15, train_loss: 0.1983\n",
      "8/15, train_loss: 0.1993\n",
      "9/15, train_loss: 0.1533\n",
      "10/15, train_loss: 0.2138\n",
      "11/15, train_loss: 0.3298\n",
      "12/15, train_loss: 0.2817\n",
      "13/15, train_loss: 0.1641\n",
      "14/15, train_loss: 0.2198\n",
      "15/15, train_loss: 0.2459\n",
      "16/15, train_loss: 1.8617\n",
      "epoch 371 average loss: 0.3071\n",
      "----------\n",
      "epoch 372/500\n",
      "1/15, train_loss: 0.1644\n",
      "2/15, train_loss: 0.1757\n",
      "3/15, train_loss: 0.2170\n",
      "4/15, train_loss: 0.2788\n",
      "5/15, train_loss: 0.1644\n",
      "6/15, train_loss: 0.1998\n",
      "7/15, train_loss: 0.1440\n",
      "8/15, train_loss: 0.1464\n",
      "9/15, train_loss: 0.2202\n",
      "10/15, train_loss: 0.1585\n",
      "11/15, train_loss: 0.2012\n",
      "12/15, train_loss: 0.1356\n",
      "13/15, train_loss: 0.2047\n",
      "14/15, train_loss: 0.1721\n",
      "15/15, train_loss: 0.2022\n",
      "16/15, train_loss: 0.2121\n",
      "epoch 372 average loss: 0.1873\n",
      "----------\n",
      "epoch 373/500\n",
      "1/15, train_loss: 0.1768\n",
      "2/15, train_loss: 0.1349\n",
      "3/15, train_loss: 0.1906\n",
      "4/15, train_loss: 0.1611\n",
      "5/15, train_loss: 0.2105\n",
      "6/15, train_loss: 0.1730\n",
      "7/15, train_loss: 0.1776\n",
      "8/15, train_loss: 0.1498\n",
      "9/15, train_loss: 0.1750\n",
      "10/15, train_loss: 0.2026\n",
      "11/15, train_loss: 0.1814\n",
      "12/15, train_loss: 0.1267\n",
      "13/15, train_loss: 0.1854\n",
      "14/15, train_loss: 0.1438\n",
      "15/15, train_loss: 0.1802\n",
      "16/15, train_loss: 2.1619\n",
      "epoch 373 average loss: 0.2957\n",
      "----------\n",
      "epoch 374/500\n",
      "1/15, train_loss: 0.1926\n",
      "2/15, train_loss: 0.1787\n",
      "3/15, train_loss: 0.1514\n",
      "4/15, train_loss: 0.1661\n",
      "5/15, train_loss: 0.1790\n",
      "6/15, train_loss: 0.2008\n",
      "7/15, train_loss: 0.1597\n",
      "8/15, train_loss: 0.1906\n",
      "9/15, train_loss: 0.1896\n",
      "10/15, train_loss: 0.1476\n",
      "11/15, train_loss: 0.1589\n",
      "12/15, train_loss: 0.1895\n",
      "13/15, train_loss: 0.1842\n",
      "14/15, train_loss: 0.1840\n",
      "15/15, train_loss: 0.2134\n",
      "16/15, train_loss: 2.6107\n",
      "epoch 374 average loss: 0.3310\n",
      "----------\n",
      "epoch 375/500\n",
      "1/15, train_loss: 0.1910\n",
      "2/15, train_loss: 0.1654\n",
      "3/15, train_loss: 0.1659\n",
      "4/15, train_loss: 0.1571\n",
      "5/15, train_loss: 0.1787\n",
      "6/15, train_loss: 0.1972\n",
      "7/15, train_loss: 0.1693\n",
      "8/15, train_loss: 0.3770\n",
      "9/15, train_loss: 0.1503\n",
      "10/15, train_loss: 0.1837\n",
      "11/15, train_loss: 0.1368\n",
      "12/15, train_loss: 0.2179\n",
      "13/15, train_loss: 0.1773\n",
      "14/15, train_loss: 0.1764\n",
      "15/15, train_loss: 0.1471\n",
      "16/15, train_loss: 0.1585\n",
      "epoch 375 average loss: 0.1843\n",
      "----------\n",
      "epoch 376/500\n",
      "1/15, train_loss: 0.1305\n",
      "2/15, train_loss: 0.1915\n",
      "3/15, train_loss: 0.1595\n",
      "4/15, train_loss: 0.2458\n",
      "5/15, train_loss: 0.1808\n",
      "6/15, train_loss: 0.1612\n",
      "7/15, train_loss: 0.1820\n",
      "8/15, train_loss: 0.1552\n",
      "9/15, train_loss: 0.1495\n",
      "10/15, train_loss: 0.1806\n",
      "11/15, train_loss: 0.1601\n",
      "12/15, train_loss: 0.1765\n",
      "13/15, train_loss: 0.1381\n",
      "14/15, train_loss: 0.1690\n",
      "15/15, train_loss: 0.3358\n",
      "16/15, train_loss: 2.5392\n",
      "epoch 376 average loss: 0.3285\n",
      "----------\n",
      "epoch 377/500\n",
      "1/15, train_loss: 0.1935\n",
      "2/15, train_loss: 0.1413\n",
      "3/15, train_loss: 0.2464\n",
      "4/15, train_loss: 0.2289\n",
      "5/15, train_loss: 0.1581\n",
      "6/15, train_loss: 0.1790\n",
      "7/15, train_loss: 0.1488\n",
      "8/15, train_loss: 0.1456\n",
      "9/15, train_loss: 0.1615\n",
      "10/15, train_loss: 0.1524\n",
      "11/15, train_loss: 0.1983\n",
      "12/15, train_loss: 0.1965\n",
      "13/15, train_loss: 0.2052\n",
      "14/15, train_loss: 0.1749\n",
      "15/15, train_loss: 0.1318\n",
      "16/15, train_loss: 2.3093\n",
      "epoch 377 average loss: 0.3107\n",
      "----------\n",
      "epoch 378/500\n",
      "1/15, train_loss: 0.2051\n",
      "2/15, train_loss: 0.1686\n",
      "3/15, train_loss: 0.1648\n",
      "4/15, train_loss: 0.1636\n",
      "5/15, train_loss: 0.3595\n",
      "6/15, train_loss: 0.1497\n",
      "7/15, train_loss: 0.1570\n",
      "8/15, train_loss: 0.1661\n",
      "9/15, train_loss: 0.1597\n",
      "10/15, train_loss: 0.1700\n",
      "11/15, train_loss: 0.2463\n",
      "12/15, train_loss: 0.1403\n",
      "13/15, train_loss: 0.2551\n",
      "14/15, train_loss: 0.1449\n",
      "15/15, train_loss: 0.2844\n",
      "16/15, train_loss: 0.1839\n",
      "epoch 378 average loss: 0.1949\n",
      "----------\n",
      "epoch 379/500\n",
      "1/15, train_loss: 0.1454\n",
      "2/15, train_loss: 0.1814\n",
      "3/15, train_loss: 0.1637\n",
      "4/15, train_loss: 0.3885\n",
      "5/15, train_loss: 0.3507\n",
      "6/15, train_loss: 0.1805\n",
      "7/15, train_loss: 0.1721\n",
      "8/15, train_loss: 0.1509\n",
      "9/15, train_loss: 0.1763\n",
      "10/15, train_loss: 0.1488\n",
      "11/15, train_loss: 0.1563\n",
      "12/15, train_loss: 0.1636\n",
      "13/15, train_loss: 0.1597\n",
      "14/15, train_loss: 0.4032\n",
      "15/15, train_loss: 0.1807\n",
      "16/15, train_loss: 0.2343\n",
      "epoch 379 average loss: 0.2097\n",
      "----------\n",
      "epoch 380/500\n",
      "1/15, train_loss: 0.1695\n",
      "2/15, train_loss: 0.2079\n",
      "3/15, train_loss: 0.1509\n",
      "4/15, train_loss: 0.1562\n",
      "5/15, train_loss: 0.1410\n",
      "6/15, train_loss: 0.2543\n",
      "7/15, train_loss: 0.1519\n",
      "8/15, train_loss: 0.1812\n",
      "9/15, train_loss: 0.1624\n",
      "10/15, train_loss: 0.1978\n",
      "11/15, train_loss: 0.1708\n",
      "12/15, train_loss: 0.1950\n",
      "13/15, train_loss: 0.2369\n",
      "14/15, train_loss: 0.2094\n",
      "15/15, train_loss: 0.1348\n",
      "16/15, train_loss: 0.2001\n",
      "epoch 380 average loss: 0.1825\n",
      "----------\n",
      "epoch 381/500\n",
      "1/15, train_loss: 0.1391\n",
      "2/15, train_loss: 0.1906\n",
      "3/15, train_loss: 0.2030\n",
      "4/15, train_loss: 0.1639\n",
      "5/15, train_loss: 0.1768\n",
      "6/15, train_loss: 0.1613\n",
      "7/15, train_loss: 0.1689\n",
      "8/15, train_loss: 0.1574\n",
      "9/15, train_loss: 0.1373\n",
      "10/15, train_loss: 0.1414\n",
      "11/15, train_loss: 0.1586\n",
      "12/15, train_loss: 0.1642\n",
      "13/15, train_loss: 0.2003\n",
      "14/15, train_loss: 0.1526\n",
      "15/15, train_loss: 0.2385\n",
      "16/15, train_loss: 0.1384\n",
      "epoch 381 average loss: 0.1683\n",
      "----------\n",
      "epoch 382/500\n",
      "1/15, train_loss: 0.1667\n",
      "2/15, train_loss: 0.1573\n",
      "3/15, train_loss: 0.1444\n",
      "4/15, train_loss: 0.1870\n",
      "5/15, train_loss: 0.1529\n",
      "6/15, train_loss: 0.1788\n",
      "7/15, train_loss: 0.3019\n",
      "8/15, train_loss: 0.1570\n",
      "9/15, train_loss: 0.1565\n",
      "10/15, train_loss: 0.1530\n",
      "11/15, train_loss: 0.1634\n",
      "12/15, train_loss: 0.1930\n",
      "13/15, train_loss: 0.1528\n",
      "14/15, train_loss: 0.1950\n",
      "15/15, train_loss: 0.1484\n",
      "16/15, train_loss: 0.1691\n",
      "epoch 382 average loss: 0.1736\n",
      "----------\n",
      "epoch 383/500\n",
      "1/15, train_loss: 0.1633\n",
      "2/15, train_loss: 0.1542\n",
      "3/15, train_loss: 0.2105\n",
      "4/15, train_loss: 0.1603\n",
      "5/15, train_loss: 0.1613\n",
      "6/15, train_loss: 0.1479\n",
      "7/15, train_loss: 0.1562\n",
      "8/15, train_loss: 0.2127\n",
      "9/15, train_loss: 0.1624\n",
      "10/15, train_loss: 0.1941\n",
      "11/15, train_loss: 0.1628\n",
      "12/15, train_loss: 0.1824\n",
      "13/15, train_loss: 0.1640\n",
      "14/15, train_loss: 0.1839\n",
      "15/15, train_loss: 0.1543\n",
      "16/15, train_loss: 2.0252\n",
      "epoch 383 average loss: 0.2872\n",
      "----------\n",
      "epoch 384/500\n",
      "1/15, train_loss: 0.1877\n",
      "2/15, train_loss: 0.2417\n",
      "3/15, train_loss: 0.1384\n",
      "4/15, train_loss: 0.1865\n",
      "5/15, train_loss: 0.1570\n",
      "6/15, train_loss: 0.1634\n",
      "7/15, train_loss: 0.1682\n",
      "8/15, train_loss: 0.1576\n",
      "9/15, train_loss: 0.1744\n",
      "10/15, train_loss: 0.1315\n",
      "11/15, train_loss: 0.1665\n",
      "12/15, train_loss: 0.1779\n",
      "13/15, train_loss: 0.1543\n",
      "14/15, train_loss: 0.1694\n",
      "15/15, train_loss: 0.1493\n",
      "16/15, train_loss: 2.0214\n",
      "epoch 384 average loss: 0.2841\n",
      "----------\n",
      "epoch 385/500\n",
      "1/15, train_loss: 0.1668\n",
      "2/15, train_loss: 0.1432\n",
      "3/15, train_loss: 0.1501\n",
      "4/15, train_loss: 0.1495\n",
      "5/15, train_loss: 0.1405\n",
      "6/15, train_loss: 0.1799\n",
      "7/15, train_loss: 0.1367\n",
      "8/15, train_loss: 0.1596\n",
      "9/15, train_loss: 0.1695\n",
      "10/15, train_loss: 0.4435\n",
      "11/15, train_loss: 0.3742\n",
      "12/15, train_loss: 0.1667\n",
      "13/15, train_loss: 0.1684\n",
      "14/15, train_loss: 0.1361\n",
      "15/15, train_loss: 0.1865\n",
      "16/15, train_loss: 1.9596\n",
      "epoch 385 average loss: 0.3019\n",
      "----------\n",
      "epoch 386/500\n",
      "1/15, train_loss: 0.1405\n",
      "2/15, train_loss: 0.2645\n",
      "3/15, train_loss: 0.1816\n",
      "4/15, train_loss: 0.1406\n",
      "5/15, train_loss: 0.1565\n",
      "6/15, train_loss: 0.2140\n",
      "7/15, train_loss: 0.1462\n",
      "8/15, train_loss: 1.8933\n",
      "9/15, train_loss: 0.1560\n",
      "10/15, train_loss: 0.1974\n",
      "11/15, train_loss: 0.1428\n",
      "12/15, train_loss: 0.1589\n",
      "13/15, train_loss: 0.2415\n",
      "14/15, train_loss: 0.2034\n",
      "15/15, train_loss: 0.1623\n",
      "16/15, train_loss: 0.1494\n",
      "epoch 386 average loss: 0.2843\n",
      "----------\n",
      "epoch 387/500\n",
      "1/15, train_loss: 0.1683\n",
      "2/15, train_loss: 0.1711\n",
      "3/15, train_loss: 0.1599\n",
      "4/15, train_loss: 0.1683\n",
      "5/15, train_loss: 0.1703\n",
      "6/15, train_loss: 0.1613\n",
      "7/15, train_loss: 0.1675\n",
      "8/15, train_loss: 0.1388\n",
      "9/15, train_loss: 0.1677\n",
      "10/15, train_loss: 0.1681\n",
      "11/15, train_loss: 0.1654\n",
      "12/15, train_loss: 0.2048\n",
      "13/15, train_loss: 0.1388\n",
      "14/15, train_loss: 0.1872\n",
      "15/15, train_loss: 0.1312\n",
      "16/15, train_loss: 0.1663\n",
      "epoch 387 average loss: 0.1647\n",
      "----------\n",
      "epoch 388/500\n",
      "1/15, train_loss: 0.1242\n",
      "2/15, train_loss: 0.1478\n",
      "3/15, train_loss: 0.1693\n",
      "4/15, train_loss: 0.1602\n",
      "5/15, train_loss: 0.1593\n",
      "6/15, train_loss: 0.1395\n",
      "7/15, train_loss: 0.1190\n",
      "8/15, train_loss: 0.2019\n",
      "9/15, train_loss: 0.1470\n",
      "10/15, train_loss: 0.1802\n",
      "11/15, train_loss: 0.1515\n",
      "12/15, train_loss: 0.1601\n",
      "13/15, train_loss: 0.1947\n",
      "14/15, train_loss: 0.1335\n",
      "15/15, train_loss: 0.2070\n",
      "16/15, train_loss: 1.0427\n",
      "epoch 388 average loss: 0.2149\n",
      "----------\n",
      "epoch 389/500\n",
      "1/15, train_loss: 0.1705\n",
      "2/15, train_loss: 0.1591\n",
      "3/15, train_loss: 0.1345\n",
      "4/15, train_loss: 0.1964\n",
      "5/15, train_loss: 0.1698\n",
      "6/15, train_loss: 0.1773\n",
      "7/15, train_loss: 0.1459\n",
      "8/15, train_loss: 0.1622\n",
      "9/15, train_loss: 0.1831\n",
      "10/15, train_loss: 0.1485\n",
      "11/15, train_loss: 0.1955\n",
      "12/15, train_loss: 0.1592\n",
      "13/15, train_loss: 0.1274\n",
      "14/15, train_loss: 0.1567\n",
      "15/15, train_loss: 0.1410\n",
      "16/15, train_loss: 0.1341\n",
      "epoch 389 average loss: 0.1601\n",
      "----------\n",
      "epoch 390/500\n",
      "1/15, train_loss: 0.1890\n",
      "2/15, train_loss: 0.1615\n",
      "3/15, train_loss: 0.1760\n",
      "4/15, train_loss: 0.2169\n",
      "5/15, train_loss: 0.2013\n",
      "6/15, train_loss: 0.2210\n",
      "7/15, train_loss: 0.1515\n",
      "8/15, train_loss: 0.1722\n",
      "9/15, train_loss: 0.1601\n",
      "10/15, train_loss: 0.1385\n",
      "11/15, train_loss: 0.1765\n",
      "12/15, train_loss: 0.1371\n",
      "13/15, train_loss: 0.1618\n",
      "14/15, train_loss: 0.1711\n",
      "15/15, train_loss: 0.1480\n",
      "16/15, train_loss: 0.4544\n",
      "epoch 390 average loss: 0.1898\n",
      "----------\n",
      "epoch 391/500\n",
      "1/15, train_loss: 0.1504\n",
      "2/15, train_loss: 0.1746\n",
      "3/15, train_loss: 0.1426\n",
      "4/15, train_loss: 0.1289\n",
      "5/15, train_loss: 0.1583\n",
      "6/15, train_loss: 0.1466\n",
      "7/15, train_loss: 0.1672\n",
      "8/15, train_loss: 0.1376\n",
      "9/15, train_loss: 0.1443\n",
      "10/15, train_loss: 0.1761\n",
      "11/15, train_loss: 0.1908\n",
      "12/15, train_loss: 0.3319\n",
      "13/15, train_loss: 0.1583\n",
      "14/15, train_loss: 0.2799\n",
      "15/15, train_loss: 0.1550\n",
      "16/15, train_loss: 0.1370\n",
      "epoch 391 average loss: 0.1737\n",
      "----------\n",
      "epoch 392/500\n",
      "1/15, train_loss: 0.1575\n",
      "2/15, train_loss: 0.1627\n",
      "3/15, train_loss: 0.1311\n",
      "4/15, train_loss: 0.2022\n",
      "5/15, train_loss: 0.1826\n",
      "6/15, train_loss: 0.1951\n",
      "7/15, train_loss: 0.1516\n",
      "8/15, train_loss: 0.1505\n",
      "9/15, train_loss: 0.1598\n",
      "10/15, train_loss: 0.1654\n",
      "11/15, train_loss: 0.1652\n",
      "12/15, train_loss: 0.1631\n",
      "13/15, train_loss: 0.1312\n",
      "14/15, train_loss: 0.1890\n",
      "15/15, train_loss: 0.1418\n",
      "16/15, train_loss: 0.1453\n",
      "epoch 392 average loss: 0.1621\n",
      "----------\n",
      "epoch 393/500\n",
      "1/15, train_loss: 0.1476\n",
      "2/15, train_loss: 0.1842\n",
      "3/15, train_loss: 0.1894\n",
      "4/15, train_loss: 0.2039\n",
      "5/15, train_loss: 0.2051\n",
      "6/15, train_loss: 0.1863\n",
      "7/15, train_loss: 0.1600\n",
      "8/15, train_loss: 0.1396\n",
      "9/15, train_loss: 0.1498\n",
      "10/15, train_loss: 0.1702\n",
      "11/15, train_loss: 0.1618\n",
      "12/15, train_loss: 0.1928\n",
      "13/15, train_loss: 0.1724\n",
      "14/15, train_loss: 0.1597\n",
      "15/15, train_loss: 0.2592\n",
      "16/15, train_loss: 2.6010\n",
      "epoch 393 average loss: 0.3302\n",
      "----------\n",
      "epoch 394/500\n",
      "1/15, train_loss: 0.1926\n",
      "2/15, train_loss: 0.1789\n",
      "3/15, train_loss: 0.1754\n",
      "4/15, train_loss: 0.1827\n",
      "5/15, train_loss: 0.1324\n",
      "6/15, train_loss: 0.1321\n",
      "7/15, train_loss: 0.2640\n",
      "8/15, train_loss: 0.1493\n",
      "9/15, train_loss: 0.1558\n",
      "10/15, train_loss: 0.1900\n",
      "11/15, train_loss: 0.1458\n",
      "12/15, train_loss: 0.1701\n",
      "13/15, train_loss: 0.2846\n",
      "14/15, train_loss: 0.1626\n",
      "15/15, train_loss: 0.3167\n",
      "16/15, train_loss: 1.9329\n",
      "epoch 394 average loss: 0.2979\n",
      "----------\n",
      "epoch 395/500\n",
      "1/15, train_loss: 0.1632\n",
      "2/15, train_loss: 0.1423\n",
      "3/15, train_loss: 0.1503\n",
      "4/15, train_loss: 0.1524\n",
      "5/15, train_loss: 0.2130\n",
      "6/15, train_loss: 0.1624\n",
      "7/15, train_loss: 0.1556\n",
      "8/15, train_loss: 0.1849\n",
      "9/15, train_loss: 0.1598\n",
      "10/15, train_loss: 0.1359\n",
      "11/15, train_loss: 0.1444\n",
      "12/15, train_loss: 0.1344\n",
      "13/15, train_loss: 0.1617\n",
      "14/15, train_loss: 0.1474\n",
      "15/15, train_loss: 0.1397\n",
      "16/15, train_loss: 1.9616\n",
      "epoch 395 average loss: 0.2693\n",
      "----------\n",
      "epoch 396/500\n",
      "1/15, train_loss: 0.1524\n",
      "2/15, train_loss: 0.1909\n",
      "3/15, train_loss: 0.1459\n",
      "4/15, train_loss: 0.2510\n",
      "5/15, train_loss: 0.1472\n",
      "6/15, train_loss: 0.1320\n",
      "7/15, train_loss: 0.1667\n",
      "8/15, train_loss: 0.1472\n",
      "9/15, train_loss: 0.1506\n",
      "10/15, train_loss: 0.1473\n",
      "11/15, train_loss: 0.1472\n",
      "12/15, train_loss: 0.1680\n",
      "13/15, train_loss: 0.2008\n",
      "14/15, train_loss: 0.2067\n",
      "15/15, train_loss: 0.1777\n",
      "16/15, train_loss: 2.7343\n",
      "epoch 396 average loss: 0.3291\n",
      "----------\n",
      "epoch 397/500\n",
      "1/15, train_loss: 0.1904\n",
      "2/15, train_loss: 0.1383\n",
      "3/15, train_loss: 0.1290\n",
      "4/15, train_loss: 0.1541\n",
      "5/15, train_loss: 0.1447\n",
      "6/15, train_loss: 0.2018\n",
      "7/15, train_loss: 0.1492\n",
      "8/15, train_loss: 0.1379\n",
      "9/15, train_loss: 0.2737\n",
      "10/15, train_loss: 0.1492\n",
      "11/15, train_loss: 0.1635\n",
      "12/15, train_loss: 0.1475\n",
      "13/15, train_loss: 0.1228\n",
      "14/15, train_loss: 0.1624\n",
      "15/15, train_loss: 0.1639\n",
      "16/15, train_loss: 0.1651\n",
      "epoch 397 average loss: 0.1621\n",
      "----------\n",
      "epoch 398/500\n",
      "1/15, train_loss: 0.1517\n",
      "2/15, train_loss: 0.1756\n",
      "3/15, train_loss: 0.1798\n",
      "4/15, train_loss: 0.1459\n",
      "5/15, train_loss: 0.2414\n",
      "6/15, train_loss: 0.1975\n",
      "7/15, train_loss: 0.1484\n",
      "8/15, train_loss: 0.1640\n",
      "9/15, train_loss: 0.1404\n",
      "10/15, train_loss: 0.1802\n",
      "11/15, train_loss: 0.1366\n",
      "12/15, train_loss: 0.1707\n",
      "13/15, train_loss: 0.1598\n",
      "14/15, train_loss: 0.1475\n",
      "15/15, train_loss: 0.1641\n",
      "16/15, train_loss: 0.6163\n",
      "epoch 398 average loss: 0.1950\n",
      "----------\n",
      "epoch 399/500\n",
      "1/15, train_loss: 0.1508\n",
      "2/15, train_loss: 0.2163\n",
      "3/15, train_loss: 0.1277\n",
      "4/15, train_loss: 0.1549\n",
      "5/15, train_loss: 0.1338\n",
      "6/15, train_loss: 0.1625\n",
      "7/15, train_loss: 0.1750\n",
      "8/15, train_loss: 0.1527\n",
      "9/15, train_loss: 0.1683\n",
      "10/15, train_loss: 0.1580\n",
      "11/15, train_loss: 0.1472\n",
      "12/15, train_loss: 0.1926\n",
      "13/15, train_loss: 0.1682\n",
      "14/15, train_loss: 0.1868\n",
      "15/15, train_loss: 0.1497\n",
      "16/15, train_loss: 1.7839\n",
      "epoch 399 average loss: 0.2643\n",
      "----------\n",
      "epoch 400/500\n",
      "1/15, train_loss: 0.1740\n",
      "2/15, train_loss: 0.1463\n",
      "3/15, train_loss: 0.1613\n",
      "4/15, train_loss: 0.2442\n",
      "5/15, train_loss: 0.1348\n",
      "6/15, train_loss: 0.1525\n",
      "7/15, train_loss: 0.1513\n",
      "8/15, train_loss: 0.1468\n",
      "9/15, train_loss: 0.3778\n",
      "10/15, train_loss: 0.1410\n",
      "11/15, train_loss: 0.1680\n",
      "12/15, train_loss: 0.1790\n",
      "13/15, train_loss: 0.1481\n",
      "14/15, train_loss: 0.1443\n",
      "15/15, train_loss: 0.1617\n",
      "16/15, train_loss: 1.8806\n",
      "epoch 400 average loss: 0.2820\n",
      "----------\n",
      "epoch 401/500\n",
      "1/15, train_loss: 0.1328\n",
      "2/15, train_loss: 0.1723\n",
      "3/15, train_loss: 0.8842\n",
      "4/15, train_loss: 0.1650\n",
      "5/15, train_loss: 0.2695\n",
      "6/15, train_loss: 0.1398\n",
      "7/15, train_loss: 0.2194\n",
      "8/15, train_loss: 0.1394\n",
      "9/15, train_loss: 0.1771\n",
      "10/15, train_loss: 0.1457\n",
      "11/15, train_loss: 0.1404\n",
      "12/15, train_loss: 0.2113\n",
      "13/15, train_loss: 0.1517\n",
      "14/15, train_loss: 0.1499\n",
      "15/15, train_loss: 0.1723\n",
      "16/15, train_loss: 0.1240\n",
      "epoch 401 average loss: 0.2122\n",
      "----------\n",
      "epoch 402/500\n",
      "1/15, train_loss: 0.1554\n",
      "2/15, train_loss: 0.1489\n",
      "3/15, train_loss: 0.1563\n",
      "4/15, train_loss: 0.1535\n",
      "5/15, train_loss: 0.1475\n",
      "6/15, train_loss: 0.1587\n",
      "7/15, train_loss: 0.1641\n",
      "8/15, train_loss: 0.1551\n",
      "9/15, train_loss: 0.1672\n",
      "10/15, train_loss: 0.1620\n",
      "11/15, train_loss: 0.2261\n",
      "12/15, train_loss: 0.2199\n",
      "13/15, train_loss: 0.1379\n",
      "14/15, train_loss: 0.1660\n",
      "15/15, train_loss: 1.8901\n",
      "16/15, train_loss: 0.1560\n",
      "epoch 402 average loss: 0.2728\n",
      "----------\n",
      "epoch 403/500\n",
      "1/15, train_loss: 0.1501\n",
      "2/15, train_loss: 0.2048\n",
      "3/15, train_loss: 0.1640\n",
      "4/15, train_loss: 0.1483\n",
      "5/15, train_loss: 0.1675\n",
      "6/15, train_loss: 0.1497\n",
      "7/15, train_loss: 0.1286\n",
      "8/15, train_loss: 0.1723\n",
      "9/15, train_loss: 0.1255\n",
      "10/15, train_loss: 0.1805\n",
      "11/15, train_loss: 0.1421\n",
      "12/15, train_loss: 0.1613\n",
      "13/15, train_loss: 0.2024\n",
      "14/15, train_loss: 0.1523\n",
      "15/15, train_loss: 0.1490\n",
      "16/15, train_loss: 3.0757\n",
      "epoch 403 average loss: 0.3421\n",
      "----------\n",
      "epoch 404/500\n",
      "1/15, train_loss: 0.1948\n",
      "2/15, train_loss: 0.1713\n",
      "3/15, train_loss: 0.1602\n",
      "4/15, train_loss: 0.1710\n",
      "5/15, train_loss: 0.1537\n",
      "6/15, train_loss: 0.1406\n",
      "7/15, train_loss: 0.1494\n",
      "8/15, train_loss: 0.1346\n",
      "9/15, train_loss: 0.1652\n",
      "10/15, train_loss: 0.2119\n",
      "11/15, train_loss: 0.1323\n",
      "12/15, train_loss: 0.1942\n",
      "13/15, train_loss: 0.1396\n",
      "14/15, train_loss: 0.1561\n",
      "15/15, train_loss: 0.1551\n",
      "16/15, train_loss: 2.8426\n",
      "epoch 404 average loss: 0.3295\n",
      "----------\n",
      "epoch 405/500\n",
      "1/15, train_loss: 0.1605\n",
      "2/15, train_loss: 0.1379\n",
      "3/15, train_loss: 0.1496\n",
      "4/15, train_loss: 0.1470\n",
      "5/15, train_loss: 0.1351\n",
      "6/15, train_loss: 0.1784\n",
      "7/15, train_loss: 0.1270\n",
      "8/15, train_loss: 0.1452\n",
      "9/15, train_loss: 0.1564\n",
      "10/15, train_loss: 0.1364\n",
      "11/15, train_loss: 0.1514\n",
      "12/15, train_loss: 0.1334\n",
      "13/15, train_loss: 0.7754\n",
      "14/15, train_loss: 0.4542\n",
      "15/15, train_loss: 0.1382\n",
      "16/15, train_loss: 0.1693\n",
      "epoch 405 average loss: 0.2060\n",
      "----------\n",
      "epoch 406/500\n",
      "1/15, train_loss: 0.1747\n",
      "2/15, train_loss: 0.2340\n",
      "3/15, train_loss: 0.1979\n",
      "4/15, train_loss: 0.1830\n",
      "5/15, train_loss: 0.3309\n",
      "6/15, train_loss: 0.1402\n",
      "7/15, train_loss: 0.1437\n",
      "8/15, train_loss: 0.1536\n",
      "9/15, train_loss: 0.1836\n",
      "10/15, train_loss: 0.1408\n",
      "11/15, train_loss: 0.1294\n",
      "12/15, train_loss: 0.1419\n",
      "13/15, train_loss: 0.1694\n",
      "14/15, train_loss: 0.7183\n",
      "15/15, train_loss: 0.1728\n",
      "16/15, train_loss: 2.2173\n",
      "epoch 406 average loss: 0.3395\n",
      "----------\n",
      "epoch 407/500\n",
      "1/15, train_loss: 0.1419\n",
      "2/15, train_loss: 0.2152\n",
      "3/15, train_loss: 0.1432\n",
      "4/15, train_loss: 0.1439\n",
      "5/15, train_loss: 0.1823\n",
      "6/15, train_loss: 0.1725\n",
      "7/15, train_loss: 0.1300\n",
      "8/15, train_loss: 0.1465\n",
      "9/15, train_loss: 0.1507\n",
      "10/15, train_loss: 0.1942\n",
      "11/15, train_loss: 0.1532\n",
      "12/15, train_loss: 0.2179\n",
      "13/15, train_loss: 0.1394\n",
      "14/15, train_loss: 0.1699\n",
      "15/15, train_loss: 0.1575\n",
      "16/15, train_loss: 0.1365\n",
      "epoch 407 average loss: 0.1622\n",
      "----------\n",
      "epoch 408/500\n",
      "1/15, train_loss: 0.1996\n",
      "2/15, train_loss: 0.1521\n",
      "3/15, train_loss: 0.1961\n",
      "4/15, train_loss: 0.1342\n",
      "5/15, train_loss: 0.2045\n",
      "6/15, train_loss: 0.1630\n",
      "7/15, train_loss: 0.1864\n",
      "8/15, train_loss: 0.2083\n",
      "9/15, train_loss: 0.1208\n",
      "10/15, train_loss: 0.1476\n",
      "11/15, train_loss: 0.2178\n",
      "12/15, train_loss: 0.1383\n",
      "13/15, train_loss: 0.1588\n",
      "14/15, train_loss: 0.1488\n",
      "15/15, train_loss: 0.1642\n",
      "16/15, train_loss: 0.1177\n",
      "epoch 408 average loss: 0.1661\n",
      "----------\n",
      "epoch 409/500\n",
      "1/15, train_loss: 0.1251\n",
      "2/15, train_loss: 0.1978\n",
      "3/15, train_loss: 0.1461\n",
      "4/15, train_loss: 0.1456\n",
      "5/15, train_loss: 0.1727\n",
      "6/15, train_loss: 0.1605\n",
      "7/15, train_loss: 0.1804\n",
      "8/15, train_loss: 0.1644\n",
      "9/15, train_loss: 0.1535\n",
      "10/15, train_loss: 0.1880\n",
      "11/15, train_loss: 0.1746\n",
      "12/15, train_loss: 0.1704\n",
      "13/15, train_loss: 0.1544\n",
      "14/15, train_loss: 0.1502\n",
      "15/15, train_loss: 0.1645\n",
      "16/15, train_loss: 2.1001\n",
      "epoch 409 average loss: 0.2843\n",
      "----------\n",
      "epoch 410/500\n",
      "1/15, train_loss: 0.2710\n",
      "2/15, train_loss: 0.1327\n",
      "3/15, train_loss: 0.1485\n",
      "4/15, train_loss: 0.1639\n",
      "5/15, train_loss: 0.1423\n",
      "6/15, train_loss: 0.1444\n",
      "7/15, train_loss: 0.2220\n",
      "8/15, train_loss: 0.1384\n",
      "9/15, train_loss: 0.1473\n",
      "10/15, train_loss: 0.1781\n",
      "11/15, train_loss: 0.2282\n",
      "12/15, train_loss: 0.1503\n",
      "13/15, train_loss: 0.1438\n",
      "14/15, train_loss: 0.1458\n",
      "15/15, train_loss: 0.1783\n",
      "16/15, train_loss: 0.2070\n",
      "epoch 410 average loss: 0.1714\n",
      "----------\n",
      "epoch 411/500\n",
      "1/15, train_loss: 0.1397\n",
      "2/15, train_loss: 0.1544\n",
      "3/15, train_loss: 0.1397\n",
      "4/15, train_loss: 0.1301\n",
      "5/15, train_loss: 0.2122\n",
      "6/15, train_loss: 0.1347\n",
      "7/15, train_loss: 0.1777\n",
      "8/15, train_loss: 0.1477\n",
      "9/15, train_loss: 0.1446\n",
      "10/15, train_loss: 0.1589\n",
      "11/15, train_loss: 0.1179\n",
      "12/15, train_loss: 0.1927\n",
      "13/15, train_loss: 0.1406\n",
      "14/15, train_loss: 0.1357\n",
      "15/15, train_loss: 0.1931\n",
      "16/15, train_loss: 2.3301\n",
      "epoch 411 average loss: 0.2906\n",
      "----------\n",
      "epoch 412/500\n",
      "1/15, train_loss: 0.1541\n",
      "2/15, train_loss: 0.1310\n",
      "3/15, train_loss: 0.1735\n",
      "4/15, train_loss: 0.2933\n",
      "5/15, train_loss: 0.2132\n",
      "6/15, train_loss: 0.2902\n",
      "7/15, train_loss: 0.1477\n",
      "8/15, train_loss: 0.1327\n",
      "9/15, train_loss: 0.1492\n",
      "10/15, train_loss: 0.1389\n",
      "11/15, train_loss: 0.1879\n",
      "12/15, train_loss: 0.1603\n",
      "13/15, train_loss: 0.1404\n",
      "14/15, train_loss: 0.1652\n",
      "15/15, train_loss: 0.2963\n",
      "16/15, train_loss: 0.2188\n",
      "epoch 412 average loss: 0.1870\n",
      "----------\n",
      "epoch 413/500\n",
      "1/15, train_loss: 0.1276\n",
      "2/15, train_loss: 0.1543\n",
      "3/15, train_loss: 0.1403\n",
      "4/15, train_loss: 0.1336\n",
      "5/15, train_loss: 0.1407\n",
      "6/15, train_loss: 0.1304\n",
      "7/15, train_loss: 0.1570\n",
      "8/15, train_loss: 0.1611\n",
      "9/15, train_loss: 0.1245\n",
      "10/15, train_loss: 0.1565\n",
      "11/15, train_loss: 0.1738\n",
      "12/15, train_loss: 0.1782\n",
      "13/15, train_loss: 0.1408\n",
      "14/15, train_loss: 0.1441\n",
      "15/15, train_loss: 0.1451\n",
      "16/15, train_loss: 0.1516\n",
      "epoch 413 average loss: 0.1475\n",
      "----------\n",
      "epoch 414/500\n",
      "1/15, train_loss: 0.1685\n",
      "2/15, train_loss: 0.2641\n",
      "3/15, train_loss: 0.1639\n",
      "4/15, train_loss: 0.1666\n",
      "5/15, train_loss: 0.1442\n",
      "6/15, train_loss: 0.1746\n",
      "7/15, train_loss: 0.1795\n",
      "8/15, train_loss: 0.1281\n",
      "9/15, train_loss: 0.1303\n",
      "10/15, train_loss: 0.1722\n",
      "11/15, train_loss: 0.1664\n",
      "12/15, train_loss: 0.1547\n",
      "13/15, train_loss: 0.1417\n",
      "14/15, train_loss: 0.1516\n",
      "15/15, train_loss: 0.1534\n",
      "16/15, train_loss: 0.1533\n",
      "epoch 414 average loss: 0.1633\n",
      "----------\n",
      "epoch 415/500\n",
      "1/15, train_loss: 0.1988\n",
      "2/15, train_loss: 0.1633\n",
      "3/15, train_loss: 0.1515\n",
      "4/15, train_loss: 0.1312\n",
      "5/15, train_loss: 0.1428\n",
      "6/15, train_loss: 0.1792\n",
      "7/15, train_loss: 0.1398\n",
      "8/15, train_loss: 0.1852\n",
      "9/15, train_loss: 0.1885\n",
      "10/15, train_loss: 0.1597\n",
      "11/15, train_loss: 0.1471\n",
      "12/15, train_loss: 0.1315\n",
      "13/15, train_loss: 0.1311\n",
      "14/15, train_loss: 0.1802\n",
      "15/15, train_loss: 0.1171\n",
      "16/15, train_loss: 0.1238\n",
      "epoch 415 average loss: 0.1544\n",
      "----------\n",
      "epoch 416/500\n",
      "1/15, train_loss: 0.1426\n",
      "2/15, train_loss: 0.1319\n",
      "3/15, train_loss: 0.1849\n",
      "4/15, train_loss: 0.1397\n",
      "5/15, train_loss: 0.2090\n",
      "6/15, train_loss: 0.1343\n",
      "7/15, train_loss: 0.1560\n",
      "8/15, train_loss: 0.1463\n",
      "9/15, train_loss: 0.1402\n",
      "10/15, train_loss: 0.1503\n",
      "11/15, train_loss: 0.1325\n",
      "12/15, train_loss: 0.2121\n",
      "13/15, train_loss: 0.1628\n",
      "14/15, train_loss: 0.1307\n",
      "15/15, train_loss: 0.1896\n",
      "16/15, train_loss: 0.1685\n",
      "epoch 416 average loss: 0.1582\n",
      "----------\n",
      "epoch 417/500\n",
      "1/15, train_loss: 0.1580\n",
      "2/15, train_loss: 0.1570\n",
      "3/15, train_loss: 0.1619\n",
      "4/15, train_loss: 0.1817\n",
      "5/15, train_loss: 0.1655\n",
      "6/15, train_loss: 0.1442\n",
      "7/15, train_loss: 0.1769\n",
      "8/15, train_loss: 0.1299\n",
      "9/15, train_loss: 0.1830\n",
      "10/15, train_loss: 0.1357\n",
      "11/15, train_loss: 0.1889\n",
      "12/15, train_loss: 0.1416\n",
      "13/15, train_loss: 0.1427\n",
      "14/15, train_loss: 0.1854\n",
      "15/15, train_loss: 0.1295\n",
      "16/15, train_loss: 2.0285\n",
      "epoch 417 average loss: 0.2756\n",
      "----------\n",
      "epoch 418/500\n",
      "1/15, train_loss: 0.1386\n",
      "2/15, train_loss: 0.1298\n",
      "3/15, train_loss: 0.1426\n",
      "4/15, train_loss: 0.1514\n",
      "5/15, train_loss: 0.1516\n",
      "6/15, train_loss: 0.1482\n",
      "7/15, train_loss: 0.1277\n",
      "8/15, train_loss: 0.1612\n",
      "9/15, train_loss: 0.1406\n",
      "10/15, train_loss: 0.1614\n",
      "11/15, train_loss: 0.1453\n",
      "12/15, train_loss: 0.1424\n",
      "13/15, train_loss: 0.1601\n",
      "14/15, train_loss: 0.1606\n",
      "15/15, train_loss: 0.1420\n",
      "16/15, train_loss: 2.5251\n",
      "epoch 418 average loss: 0.2955\n",
      "----------\n",
      "epoch 419/500\n",
      "1/15, train_loss: 0.1230\n",
      "2/15, train_loss: 0.1411\n",
      "3/15, train_loss: 0.1686\n",
      "4/15, train_loss: 0.1715\n",
      "5/15, train_loss: 0.1524\n",
      "6/15, train_loss: 0.1299\n",
      "7/15, train_loss: 0.1752\n",
      "8/15, train_loss: 0.1502\n",
      "9/15, train_loss: 0.1294\n",
      "10/15, train_loss: 0.2849\n",
      "11/15, train_loss: 0.1269\n",
      "12/15, train_loss: 0.1480\n",
      "13/15, train_loss: 0.1805\n",
      "14/15, train_loss: 0.1376\n",
      "15/15, train_loss: 0.1247\n",
      "16/15, train_loss: 0.1061\n",
      "epoch 419 average loss: 0.1531\n",
      "----------\n",
      "epoch 420/500\n",
      "1/15, train_loss: 0.3399\n",
      "2/15, train_loss: 0.1820\n",
      "3/15, train_loss: 0.1855\n",
      "4/15, train_loss: 0.1367\n",
      "5/15, train_loss: 0.1528\n",
      "6/15, train_loss: 0.1638\n",
      "7/15, train_loss: 0.1436\n",
      "8/15, train_loss: 0.1730\n",
      "9/15, train_loss: 0.1452\n",
      "10/15, train_loss: 0.1622\n",
      "11/15, train_loss: 0.1612\n",
      "12/15, train_loss: 0.2166\n",
      "13/15, train_loss: 0.1513\n",
      "14/15, train_loss: 0.1354\n",
      "15/15, train_loss: 0.1205\n",
      "16/15, train_loss: 0.5645\n",
      "epoch 420 average loss: 0.1959\n",
      "----------\n",
      "epoch 421/500\n",
      "1/15, train_loss: 0.1214\n",
      "2/15, train_loss: 0.1858\n",
      "3/15, train_loss: 0.1871\n",
      "4/15, train_loss: 0.1436\n",
      "5/15, train_loss: 0.1407\n",
      "6/15, train_loss: 0.1603\n",
      "7/15, train_loss: 0.1401\n",
      "8/15, train_loss: 0.1387\n",
      "9/15, train_loss: 0.1415\n",
      "10/15, train_loss: 0.1411\n",
      "11/15, train_loss: 0.1279\n",
      "12/15, train_loss: 0.1327\n",
      "13/15, train_loss: 0.1261\n",
      "14/15, train_loss: 0.1703\n",
      "15/15, train_loss: 0.1580\n",
      "16/15, train_loss: 0.2041\n",
      "epoch 421 average loss: 0.1512\n",
      "----------\n",
      "epoch 422/500\n",
      "1/15, train_loss: 0.1475\n",
      "2/15, train_loss: 0.1255\n",
      "3/15, train_loss: 0.1267\n",
      "4/15, train_loss: 0.1818\n",
      "5/15, train_loss: 0.1399\n",
      "6/15, train_loss: 0.1400\n",
      "7/15, train_loss: 0.1369\n",
      "8/15, train_loss: 0.1443\n",
      "9/15, train_loss: 0.1354\n",
      "10/15, train_loss: 0.1586\n",
      "11/15, train_loss: 0.1439\n",
      "12/15, train_loss: 0.1292\n",
      "13/15, train_loss: 0.1869\n",
      "14/15, train_loss: 0.1738\n",
      "15/15, train_loss: 0.1412\n",
      "16/15, train_loss: 0.3230\n",
      "epoch 422 average loss: 0.1584\n",
      "----------\n",
      "epoch 423/500\n",
      "1/15, train_loss: 0.2216\n",
      "2/15, train_loss: 0.1431\n",
      "3/15, train_loss: 0.1408\n",
      "4/15, train_loss: 0.1383\n",
      "5/15, train_loss: 0.1769\n",
      "6/15, train_loss: 0.1471\n",
      "7/15, train_loss: 0.1570\n",
      "8/15, train_loss: 0.1594\n",
      "9/15, train_loss: 0.1734\n",
      "10/15, train_loss: 0.1534\n",
      "11/15, train_loss: 0.1574\n",
      "12/15, train_loss: 0.1220\n",
      "13/15, train_loss: 0.1779\n",
      "14/15, train_loss: 0.1455\n",
      "15/15, train_loss: 0.2012\n",
      "16/15, train_loss: 0.2280\n",
      "epoch 423 average loss: 0.1652\n",
      "----------\n",
      "epoch 424/500\n",
      "1/15, train_loss: 0.1463\n",
      "2/15, train_loss: 0.1502\n",
      "3/15, train_loss: 0.1690\n",
      "4/15, train_loss: 0.2098\n",
      "5/15, train_loss: 0.1194\n",
      "6/15, train_loss: 0.1532\n",
      "7/15, train_loss: 0.1367\n",
      "8/15, train_loss: 0.1667\n",
      "9/15, train_loss: 0.1713\n",
      "10/15, train_loss: 0.1215\n",
      "11/15, train_loss: 0.1319\n",
      "12/15, train_loss: 0.2070\n",
      "13/15, train_loss: 0.1705\n",
      "14/15, train_loss: 0.1437\n",
      "15/15, train_loss: 0.1594\n",
      "16/15, train_loss: 0.7564\n",
      "epoch 424 average loss: 0.1946\n",
      "----------\n",
      "epoch 425/500\n",
      "1/15, train_loss: 0.2290\n",
      "2/15, train_loss: 0.1546\n",
      "3/15, train_loss: 0.1751\n",
      "4/15, train_loss: 0.1450\n",
      "5/15, train_loss: 0.1637\n",
      "6/15, train_loss: 0.1444\n",
      "7/15, train_loss: 0.1519\n",
      "8/15, train_loss: 0.1648\n",
      "9/15, train_loss: 0.1511\n",
      "10/15, train_loss: 0.1665\n",
      "11/15, train_loss: 0.1397\n",
      "12/15, train_loss: 0.1565\n",
      "13/15, train_loss: 0.1531\n",
      "14/15, train_loss: 0.1566\n",
      "15/15, train_loss: 0.1444\n",
      "16/15, train_loss: 0.2964\n",
      "epoch 425 average loss: 0.1683\n",
      "----------\n",
      "epoch 426/500\n",
      "1/15, train_loss: 0.1523\n",
      "2/15, train_loss: 0.1707\n",
      "3/15, train_loss: 0.1358\n",
      "4/15, train_loss: 0.2093\n",
      "5/15, train_loss: 0.1589\n",
      "6/15, train_loss: 0.1415\n",
      "7/15, train_loss: 0.1541\n",
      "8/15, train_loss: 0.1464\n",
      "9/15, train_loss: 0.1327\n",
      "10/15, train_loss: 0.1213\n",
      "11/15, train_loss: 0.1500\n",
      "12/15, train_loss: 0.1630\n",
      "13/15, train_loss: 0.1514\n",
      "14/15, train_loss: 0.1244\n",
      "15/15, train_loss: 0.1338\n",
      "16/15, train_loss: 2.3238\n",
      "epoch 426 average loss: 0.2856\n",
      "----------\n",
      "epoch 427/500\n",
      "1/15, train_loss: 0.1248\n",
      "2/15, train_loss: 0.1519\n",
      "3/15, train_loss: 0.1415\n",
      "4/15, train_loss: 0.1320\n",
      "5/15, train_loss: 0.1328\n",
      "6/15, train_loss: 0.3152\n",
      "7/15, train_loss: 0.1415\n",
      "8/15, train_loss: 0.3179\n",
      "9/15, train_loss: 0.1740\n",
      "10/15, train_loss: 0.1487\n",
      "11/15, train_loss: 0.1489\n",
      "12/15, train_loss: 0.1380\n",
      "13/15, train_loss: 0.1500\n",
      "14/15, train_loss: 0.1738\n",
      "15/15, train_loss: 0.1418\n",
      "16/15, train_loss: 2.1040\n",
      "epoch 427 average loss: 0.2898\n",
      "----------\n",
      "epoch 428/500\n",
      "1/15, train_loss: 0.1438\n",
      "2/15, train_loss: 0.1193\n",
      "3/15, train_loss: 0.1802\n",
      "4/15, train_loss: 0.1412\n",
      "5/15, train_loss: 0.1797\n",
      "6/15, train_loss: 0.1506\n",
      "7/15, train_loss: 0.1586\n",
      "8/15, train_loss: 0.1586\n",
      "9/15, train_loss: 0.1788\n",
      "10/15, train_loss: 0.1757\n",
      "11/15, train_loss: 0.1515\n",
      "12/15, train_loss: 0.1382\n",
      "13/15, train_loss: 0.1367\n",
      "14/15, train_loss: 0.1318\n",
      "15/15, train_loss: 0.1814\n",
      "16/15, train_loss: 0.2454\n",
      "epoch 428 average loss: 0.1607\n",
      "----------\n",
      "epoch 429/500\n",
      "1/15, train_loss: 0.1228\n",
      "2/15, train_loss: 0.1812\n",
      "3/15, train_loss: 0.1509\n",
      "4/15, train_loss: 0.1721\n",
      "5/15, train_loss: 0.1243\n",
      "6/15, train_loss: 0.1245\n",
      "7/15, train_loss: 0.1682\n",
      "8/15, train_loss: 0.1231\n",
      "9/15, train_loss: 0.1395\n",
      "10/15, train_loss: 0.1556\n",
      "11/15, train_loss: 0.1253\n",
      "12/15, train_loss: 0.1711\n",
      "13/15, train_loss: 0.1514\n",
      "14/15, train_loss: 0.2092\n",
      "15/15, train_loss: 0.1721\n",
      "16/15, train_loss: 0.6894\n",
      "epoch 429 average loss: 0.1863\n",
      "----------\n",
      "epoch 430/500\n",
      "1/15, train_loss: 0.2676\n",
      "2/15, train_loss: 0.1363\n",
      "3/15, train_loss: 0.1432\n",
      "4/15, train_loss: 0.1343\n",
      "5/15, train_loss: 0.1336\n",
      "6/15, train_loss: 0.1419\n",
      "7/15, train_loss: 0.1567\n",
      "8/15, train_loss: 0.1672\n",
      "9/15, train_loss: 0.1272\n",
      "10/15, train_loss: 0.1297\n",
      "11/15, train_loss: 0.1330\n",
      "12/15, train_loss: 0.1643\n",
      "13/15, train_loss: 0.1488\n",
      "14/15, train_loss: 0.1939\n",
      "15/15, train_loss: 0.1706\n",
      "16/15, train_loss: 0.1748\n",
      "epoch 430 average loss: 0.1577\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 430 validation loss: 0.9779 dice_score: 0.8602 acc_metric: 0.7159 accuracy: 0.7159, f1score: 0.7258\n",
      " saved Best PMetric: 0.7880 at epoch: 430\n",
      "----------\n",
      "epoch 431/500\n",
      "1/15, train_loss: 0.1337\n",
      "2/15, train_loss: 0.1438\n",
      "3/15, train_loss: 0.1452\n",
      "4/15, train_loss: 0.1583\n",
      "5/15, train_loss: 0.1230\n",
      "6/15, train_loss: 0.1339\n",
      "7/15, train_loss: 0.1384\n",
      "8/15, train_loss: 0.1549\n",
      "9/15, train_loss: 0.1782\n",
      "10/15, train_loss: 0.1429\n",
      "11/15, train_loss: 0.1420\n",
      "12/15, train_loss: 0.1321\n",
      "13/15, train_loss: 0.1345\n",
      "14/15, train_loss: 0.1733\n",
      "15/15, train_loss: 0.1538\n",
      "16/15, train_loss: 2.6618\n",
      "epoch 431 average loss: 0.3031\n",
      "----------\n",
      "epoch 432/500\n",
      "1/15, train_loss: 0.1490\n",
      "2/15, train_loss: 0.1635\n",
      "3/15, train_loss: 0.1268\n",
      "4/15, train_loss: 0.1830\n",
      "5/15, train_loss: 0.1759\n",
      "6/15, train_loss: 0.1377\n",
      "7/15, train_loss: 0.1410\n",
      "8/15, train_loss: 0.1260\n",
      "9/15, train_loss: 0.1737\n",
      "10/15, train_loss: 0.1236\n",
      "11/15, train_loss: 0.1342\n",
      "12/15, train_loss: 0.3141\n",
      "13/15, train_loss: 0.1248\n",
      "14/15, train_loss: 0.1339\n",
      "15/15, train_loss: 0.1577\n",
      "16/15, train_loss: 0.1343\n",
      "epoch 432 average loss: 0.1562\n",
      "----------\n",
      "epoch 433/500\n",
      "1/15, train_loss: 0.1343\n",
      "2/15, train_loss: 0.1308\n",
      "3/15, train_loss: 0.2822\n",
      "4/15, train_loss: 0.1280\n",
      "5/15, train_loss: 0.2454\n",
      "6/15, train_loss: 0.1190\n",
      "7/15, train_loss: 0.2055\n",
      "8/15, train_loss: 0.1251\n",
      "9/15, train_loss: 0.1405\n",
      "10/15, train_loss: 0.1339\n",
      "11/15, train_loss: 0.2519\n",
      "12/15, train_loss: 0.1558\n",
      "13/15, train_loss: 0.1396\n",
      "14/15, train_loss: 0.2162\n",
      "15/15, train_loss: 0.1342\n",
      "16/15, train_loss: 2.8677\n",
      "epoch 433 average loss: 0.3381\n",
      "----------\n",
      "epoch 434/500\n",
      "1/15, train_loss: 0.1830\n",
      "2/15, train_loss: 0.1964\n",
      "3/15, train_loss: 0.1454\n",
      "4/15, train_loss: 0.1854\n",
      "5/15, train_loss: 0.1280\n",
      "6/15, train_loss: 0.3320\n",
      "7/15, train_loss: 0.1819\n",
      "8/15, train_loss: 0.1447\n",
      "9/15, train_loss: 0.1420\n",
      "10/15, train_loss: 0.1246\n",
      "11/15, train_loss: 0.1400\n",
      "12/15, train_loss: 0.1460\n",
      "13/15, train_loss: 0.1295\n",
      "14/15, train_loss: 0.1470\n",
      "15/15, train_loss: 0.1453\n",
      "16/15, train_loss: 2.3151\n",
      "epoch 434 average loss: 0.2992\n",
      "----------\n",
      "epoch 435/500\n",
      "1/15, train_loss: 0.1551\n",
      "2/15, train_loss: 0.1763\n",
      "3/15, train_loss: 0.1589\n",
      "4/15, train_loss: 0.1751\n",
      "5/15, train_loss: 0.2071\n",
      "6/15, train_loss: 0.1705\n",
      "7/15, train_loss: 0.3353\n",
      "8/15, train_loss: 0.2251\n",
      "9/15, train_loss: 2.1603\n",
      "10/15, train_loss: 0.1850\n",
      "11/15, train_loss: 0.1246\n",
      "12/15, train_loss: 0.1326\n",
      "13/15, train_loss: 0.1575\n",
      "14/15, train_loss: 0.1508\n",
      "15/15, train_loss: 0.1273\n",
      "16/15, train_loss: 2.2539\n",
      "epoch 435 average loss: 0.4310\n",
      "----------\n",
      "epoch 436/500\n",
      "1/15, train_loss: 0.1476\n",
      "2/15, train_loss: 0.1588\n",
      "3/15, train_loss: 0.1203\n",
      "4/15, train_loss: 0.1760\n",
      "5/15, train_loss: 0.1396\n",
      "6/15, train_loss: 0.1585\n",
      "7/15, train_loss: 0.1588\n",
      "8/15, train_loss: 0.1332\n",
      "9/15, train_loss: 0.1623\n",
      "10/15, train_loss: 0.1493\n",
      "11/15, train_loss: 0.3929\n",
      "12/15, train_loss: 0.1395\n",
      "13/15, train_loss: 0.1338\n",
      "14/15, train_loss: 0.1498\n",
      "15/15, train_loss: 0.1933\n",
      "16/15, train_loss: 0.8790\n",
      "epoch 436 average loss: 0.2120\n",
      "----------\n",
      "epoch 437/500\n",
      "1/15, train_loss: 0.1340\n",
      "2/15, train_loss: 1.9590\n",
      "3/15, train_loss: 0.1517\n",
      "4/15, train_loss: 0.1732\n",
      "5/15, train_loss: 0.1278\n",
      "6/15, train_loss: 0.1712\n",
      "7/15, train_loss: 0.1530\n",
      "8/15, train_loss: 0.1262\n",
      "9/15, train_loss: 0.1706\n",
      "10/15, train_loss: 0.1401\n",
      "11/15, train_loss: 0.1383\n",
      "12/15, train_loss: 0.1394\n",
      "13/15, train_loss: 0.1182\n",
      "14/15, train_loss: 0.1453\n",
      "15/15, train_loss: 0.1370\n",
      "16/15, train_loss: 2.0210\n",
      "epoch 437 average loss: 0.3754\n",
      "----------\n",
      "epoch 438/500\n",
      "1/15, train_loss: 0.1427\n",
      "2/15, train_loss: 2.2590\n",
      "3/15, train_loss: 0.1418\n",
      "4/15, train_loss: 0.1527\n",
      "5/15, train_loss: 0.1432\n",
      "6/15, train_loss: 0.1355\n",
      "7/15, train_loss: 0.1259\n",
      "8/15, train_loss: 0.1511\n",
      "9/15, train_loss: 0.2542\n",
      "10/15, train_loss: 0.1260\n",
      "11/15, train_loss: 0.1499\n",
      "12/15, train_loss: 0.1262\n",
      "13/15, train_loss: 0.2077\n",
      "14/15, train_loss: 0.1829\n",
      "15/15, train_loss: 0.1577\n",
      "16/15, train_loss: 0.4618\n",
      "epoch 438 average loss: 0.3074\n",
      "----------\n",
      "epoch 439/500\n",
      "1/15, train_loss: 0.1299\n",
      "2/15, train_loss: 0.1164\n",
      "3/15, train_loss: 0.1246\n",
      "4/15, train_loss: 0.1367\n",
      "5/15, train_loss: 0.1438\n",
      "6/15, train_loss: 0.1332\n",
      "7/15, train_loss: 0.1496\n",
      "8/15, train_loss: 0.1093\n",
      "9/15, train_loss: 0.2307\n",
      "10/15, train_loss: 0.1229\n",
      "11/15, train_loss: 0.1481\n",
      "12/15, train_loss: 0.1312\n",
      "13/15, train_loss: 0.1228\n",
      "14/15, train_loss: 0.1618\n",
      "15/15, train_loss: 0.1232\n",
      "16/15, train_loss: 2.1578\n",
      "epoch 439 average loss: 0.2651\n",
      "----------\n",
      "epoch 440/500\n",
      "1/15, train_loss: 0.1676\n",
      "2/15, train_loss: 0.1303\n",
      "3/15, train_loss: 0.1296\n",
      "4/15, train_loss: 0.1484\n",
      "5/15, train_loss: 0.1426\n",
      "6/15, train_loss: 0.1754\n",
      "7/15, train_loss: 0.1661\n",
      "8/15, train_loss: 0.1742\n",
      "9/15, train_loss: 0.1275\n",
      "10/15, train_loss: 0.1545\n",
      "11/15, train_loss: 0.1596\n",
      "12/15, train_loss: 0.1281\n",
      "13/15, train_loss: 0.1213\n",
      "14/15, train_loss: 0.1525\n",
      "15/15, train_loss: 0.2081\n",
      "16/15, train_loss: 0.1312\n",
      "epoch 440 average loss: 0.1510\n",
      "----------\n",
      "epoch 441/500\n",
      "1/15, train_loss: 0.1205\n",
      "2/15, train_loss: 0.1567\n",
      "3/15, train_loss: 0.2340\n",
      "4/15, train_loss: 0.1230\n",
      "5/15, train_loss: 0.1960\n",
      "6/15, train_loss: 0.1553\n",
      "7/15, train_loss: 0.2774\n",
      "8/15, train_loss: 0.1789\n",
      "9/15, train_loss: 0.1203\n",
      "10/15, train_loss: 0.1591\n",
      "11/15, train_loss: 0.1240\n",
      "12/15, train_loss: 0.1407\n",
      "13/15, train_loss: 0.1586\n",
      "14/15, train_loss: 2.0058\n",
      "15/15, train_loss: 0.1311\n",
      "16/15, train_loss: 1.9246\n",
      "epoch 441 average loss: 0.3879\n",
      "----------\n",
      "epoch 442/500\n",
      "1/15, train_loss: 0.1456\n",
      "2/15, train_loss: 0.1224\n",
      "3/15, train_loss: 0.1465\n",
      "4/15, train_loss: 0.1455\n",
      "5/15, train_loss: 0.1340\n",
      "6/15, train_loss: 0.1426\n",
      "7/15, train_loss: 0.1637\n",
      "8/15, train_loss: 0.1429\n",
      "9/15, train_loss: 0.1208\n",
      "10/15, train_loss: 0.1371\n",
      "11/15, train_loss: 0.1508\n",
      "12/15, train_loss: 0.1377\n",
      "13/15, train_loss: 0.1258\n",
      "14/15, train_loss: 0.1343\n",
      "15/15, train_loss: 0.1189\n",
      "16/15, train_loss: 2.1014\n",
      "epoch 442 average loss: 0.2606\n",
      "----------\n",
      "epoch 443/500\n",
      "1/15, train_loss: 0.1415\n",
      "2/15, train_loss: 0.1544\n",
      "3/15, train_loss: 0.1562\n",
      "4/15, train_loss: 0.1332\n",
      "5/15, train_loss: 0.1514\n",
      "6/15, train_loss: 0.1693\n",
      "7/15, train_loss: 0.1742\n",
      "8/15, train_loss: 0.1574\n",
      "9/15, train_loss: 0.1380\n",
      "10/15, train_loss: 0.1651\n",
      "11/15, train_loss: 0.1393\n",
      "12/15, train_loss: 0.1328\n",
      "13/15, train_loss: 0.1420\n",
      "14/15, train_loss: 0.1255\n",
      "15/15, train_loss: 0.1297\n",
      "16/15, train_loss: 1.7386\n",
      "epoch 443 average loss: 0.2468\n",
      "----------\n",
      "epoch 444/500\n",
      "1/15, train_loss: 0.1905\n",
      "2/15, train_loss: 0.1471\n",
      "3/15, train_loss: 0.1306\n",
      "4/15, train_loss: 0.1231\n",
      "5/15, train_loss: 0.1346\n",
      "6/15, train_loss: 0.1232\n",
      "7/15, train_loss: 0.1364\n",
      "8/15, train_loss: 0.1153\n",
      "9/15, train_loss: 0.2013\n",
      "10/15, train_loss: 0.1250\n",
      "11/15, train_loss: 0.1499\n",
      "12/15, train_loss: 0.1523\n",
      "13/15, train_loss: 0.1206\n",
      "14/15, train_loss: 0.1340\n",
      "15/15, train_loss: 0.1444\n",
      "16/15, train_loss: 1.6743\n",
      "epoch 444 average loss: 0.2377\n",
      "----------\n",
      "epoch 445/500\n",
      "1/15, train_loss: 0.1352\n",
      "2/15, train_loss: 0.1343\n",
      "3/15, train_loss: 0.1523\n",
      "4/15, train_loss: 0.1275\n",
      "5/15, train_loss: 0.1509\n",
      "6/15, train_loss: 0.1294\n",
      "7/15, train_loss: 0.1144\n",
      "8/15, train_loss: 0.1492\n",
      "9/15, train_loss: 0.1426\n",
      "10/15, train_loss: 0.1466\n",
      "11/15, train_loss: 0.1544\n",
      "12/15, train_loss: 0.1604\n",
      "13/15, train_loss: 0.1251\n",
      "14/15, train_loss: 0.1441\n",
      "15/15, train_loss: 0.1398\n",
      "16/15, train_loss: 2.6558\n",
      "epoch 445 average loss: 0.2976\n",
      "----------\n",
      "epoch 446/500\n",
      "1/15, train_loss: 0.1341\n",
      "2/15, train_loss: 0.1659\n",
      "3/15, train_loss: 0.1322\n",
      "4/15, train_loss: 0.1228\n",
      "5/15, train_loss: 0.1372\n",
      "6/15, train_loss: 0.1419\n",
      "7/15, train_loss: 0.1306\n",
      "8/15, train_loss: 0.1627\n",
      "9/15, train_loss: 0.1299\n",
      "10/15, train_loss: 0.1808\n",
      "11/15, train_loss: 0.1332\n",
      "12/15, train_loss: 0.1346\n",
      "13/15, train_loss: 0.5931\n",
      "14/15, train_loss: 0.1313\n",
      "15/15, train_loss: 0.1210\n",
      "16/15, train_loss: 2.5034\n",
      "epoch 446 average loss: 0.3159\n",
      "----------\n",
      "epoch 447/500\n",
      "1/15, train_loss: 0.1329\n",
      "2/15, train_loss: 0.1377\n",
      "3/15, train_loss: 1.7905\n",
      "4/15, train_loss: 0.1375\n",
      "5/15, train_loss: 0.1419\n",
      "6/15, train_loss: 0.1284\n",
      "7/15, train_loss: 0.1265\n",
      "8/15, train_loss: 0.1205\n",
      "9/15, train_loss: 0.1415\n",
      "10/15, train_loss: 0.1199\n",
      "11/15, train_loss: 0.2077\n",
      "12/15, train_loss: 0.1527\n",
      "13/15, train_loss: 0.1297\n",
      "14/15, train_loss: 0.1296\n",
      "15/15, train_loss: 0.2162\n",
      "16/15, train_loss: 2.6601\n",
      "epoch 447 average loss: 0.4046\n",
      "----------\n",
      "epoch 448/500\n",
      "1/15, train_loss: 0.1716\n",
      "2/15, train_loss: 0.1484\n",
      "3/15, train_loss: 0.2607\n",
      "4/15, train_loss: 0.1345\n",
      "5/15, train_loss: 0.1246\n",
      "6/15, train_loss: 0.1346\n",
      "7/15, train_loss: 0.1367\n",
      "8/15, train_loss: 0.1420\n",
      "9/15, train_loss: 0.1324\n",
      "10/15, train_loss: 0.1314\n",
      "11/15, train_loss: 0.1284\n",
      "12/15, train_loss: 0.1223\n",
      "13/15, train_loss: 0.1450\n",
      "14/15, train_loss: 0.2132\n",
      "15/15, train_loss: 0.1243\n",
      "16/15, train_loss: 0.1490\n",
      "epoch 448 average loss: 0.1499\n",
      "----------\n",
      "epoch 449/500\n",
      "1/15, train_loss: 0.1324\n",
      "2/15, train_loss: 0.1450\n",
      "3/15, train_loss: 0.1346\n",
      "4/15, train_loss: 0.1731\n",
      "5/15, train_loss: 0.2062\n",
      "6/15, train_loss: 0.1573\n",
      "7/15, train_loss: 0.1522\n",
      "8/15, train_loss: 0.1391\n",
      "9/15, train_loss: 0.2003\n",
      "10/15, train_loss: 0.1236\n",
      "11/15, train_loss: 0.1618\n",
      "12/15, train_loss: 0.1369\n",
      "13/15, train_loss: 0.1232\n",
      "14/15, train_loss: 0.1564\n",
      "15/15, train_loss: 0.1391\n",
      "16/15, train_loss: 1.7930\n",
      "epoch 449 average loss: 0.2546\n",
      "----------\n",
      "epoch 450/500\n",
      "1/15, train_loss: 0.1270\n",
      "2/15, train_loss: 0.1244\n",
      "3/15, train_loss: 0.1316\n",
      "4/15, train_loss: 0.1346\n",
      "5/15, train_loss: 0.1493\n",
      "6/15, train_loss: 0.1355\n",
      "7/15, train_loss: 0.1530\n",
      "8/15, train_loss: 0.1814\n",
      "9/15, train_loss: 0.1319\n",
      "10/15, train_loss: 0.1435\n",
      "11/15, train_loss: 0.1594\n",
      "12/15, train_loss: 0.6372\n",
      "13/15, train_loss: 0.1238\n",
      "14/15, train_loss: 0.1585\n",
      "15/15, train_loss: 0.1639\n",
      "16/15, train_loss: 1.8249\n",
      "epoch 450 average loss: 0.2800\n",
      "----------\n",
      "epoch 451/500\n",
      "1/15, train_loss: 0.1644\n",
      "2/15, train_loss: 0.1709\n",
      "3/15, train_loss: 0.1213\n",
      "4/15, train_loss: 0.1670\n",
      "5/15, train_loss: 0.1576\n",
      "6/15, train_loss: 0.1640\n",
      "7/15, train_loss: 0.1542\n",
      "8/15, train_loss: 0.1327\n",
      "9/15, train_loss: 0.1416\n",
      "10/15, train_loss: 0.1228\n",
      "11/15, train_loss: 0.1420\n",
      "12/15, train_loss: 0.1637\n",
      "13/15, train_loss: 0.1330\n",
      "14/15, train_loss: 0.1680\n",
      "15/15, train_loss: 0.1485\n",
      "16/15, train_loss: 0.1286\n",
      "epoch 451 average loss: 0.1488\n",
      "----------\n",
      "epoch 452/500\n",
      "1/15, train_loss: 0.1418\n",
      "2/15, train_loss: 0.1656\n",
      "3/15, train_loss: 0.1303\n",
      "4/15, train_loss: 0.1359\n",
      "5/15, train_loss: 0.1464\n",
      "6/15, train_loss: 0.1427\n",
      "7/15, train_loss: 0.1307\n",
      "8/15, train_loss: 0.1398\n",
      "9/15, train_loss: 0.1785\n",
      "10/15, train_loss: 0.1432\n",
      "11/15, train_loss: 0.1535\n",
      "12/15, train_loss: 0.1333\n",
      "13/15, train_loss: 0.1506\n",
      "14/15, train_loss: 0.1538\n",
      "15/15, train_loss: 0.4991\n",
      "16/15, train_loss: 2.4917\n",
      "epoch 452 average loss: 0.3148\n",
      "----------\n",
      "epoch 453/500\n",
      "1/15, train_loss: 0.1238\n",
      "2/15, train_loss: 0.1564\n",
      "3/15, train_loss: 0.1312\n",
      "4/15, train_loss: 0.1368\n",
      "5/15, train_loss: 0.1358\n",
      "6/15, train_loss: 0.1126\n",
      "7/15, train_loss: 0.1116\n",
      "8/15, train_loss: 0.1381\n",
      "9/15, train_loss: 0.1348\n",
      "10/15, train_loss: 0.1367\n",
      "11/15, train_loss: 0.1339\n",
      "12/15, train_loss: 0.1621\n",
      "13/15, train_loss: 0.1518\n",
      "14/15, train_loss: 0.1170\n",
      "15/15, train_loss: 0.1709\n",
      "16/15, train_loss: 1.8265\n",
      "epoch 453 average loss: 0.2425\n",
      "----------\n",
      "epoch 454/500\n",
      "1/15, train_loss: 0.1929\n",
      "2/15, train_loss: 0.1547\n",
      "3/15, train_loss: 0.1520\n",
      "4/15, train_loss: 0.1264\n",
      "5/15, train_loss: 0.1216\n",
      "6/15, train_loss: 0.1285\n",
      "7/15, train_loss: 0.1981\n",
      "8/15, train_loss: 0.1794\n",
      "9/15, train_loss: 0.2075\n",
      "10/15, train_loss: 0.1571\n",
      "11/15, train_loss: 0.2615\n",
      "12/15, train_loss: 0.1409\n",
      "13/15, train_loss: 0.1603\n",
      "14/15, train_loss: 0.1196\n",
      "15/15, train_loss: 0.1611\n",
      "16/15, train_loss: 0.1240\n",
      "epoch 454 average loss: 0.1616\n",
      "----------\n",
      "epoch 455/500\n",
      "1/15, train_loss: 0.1418\n",
      "2/15, train_loss: 0.1451\n",
      "3/15, train_loss: 0.1257\n",
      "4/15, train_loss: 0.1208\n",
      "5/15, train_loss: 0.1260\n",
      "6/15, train_loss: 0.1313\n",
      "7/15, train_loss: 0.1154\n",
      "8/15, train_loss: 0.1289\n",
      "9/15, train_loss: 0.1472\n",
      "10/15, train_loss: 0.1417\n",
      "11/15, train_loss: 0.1201\n",
      "12/15, train_loss: 0.1645\n",
      "13/15, train_loss: 0.1329\n",
      "14/15, train_loss: 0.1290\n",
      "15/15, train_loss: 0.1594\n",
      "16/15, train_loss: 0.1305\n",
      "epoch 455 average loss: 0.1350\n",
      "----------\n",
      "epoch 456/500\n",
      "1/15, train_loss: 0.1140\n",
      "2/15, train_loss: 0.1970\n",
      "3/15, train_loss: 0.1321\n",
      "4/15, train_loss: 0.1333\n",
      "5/15, train_loss: 0.1459\n",
      "6/15, train_loss: 0.1147\n",
      "7/15, train_loss: 0.1316\n",
      "8/15, train_loss: 0.1209\n",
      "9/15, train_loss: 0.1342\n",
      "10/15, train_loss: 0.1443\n",
      "11/15, train_loss: 0.1379\n",
      "12/15, train_loss: 0.1363\n",
      "13/15, train_loss: 0.1335\n",
      "14/15, train_loss: 0.1382\n",
      "15/15, train_loss: 0.1541\n",
      "16/15, train_loss: 2.1769\n",
      "epoch 456 average loss: 0.2653\n",
      "----------\n",
      "epoch 457/500\n",
      "1/15, train_loss: 0.1251\n",
      "2/15, train_loss: 0.1358\n",
      "3/15, train_loss: 0.1120\n",
      "4/15, train_loss: 0.1527\n",
      "5/15, train_loss: 0.1479\n",
      "6/15, train_loss: 0.1728\n",
      "7/15, train_loss: 0.1243\n",
      "8/15, train_loss: 0.1159\n",
      "9/15, train_loss: 0.1625\n",
      "10/15, train_loss: 0.1080\n",
      "11/15, train_loss: 0.1458\n",
      "12/15, train_loss: 0.1247\n",
      "13/15, train_loss: 0.1877\n",
      "14/15, train_loss: 0.1883\n",
      "15/15, train_loss: 0.1426\n",
      "16/15, train_loss: 0.1607\n",
      "epoch 457 average loss: 0.1442\n",
      "----------\n",
      "epoch 458/500\n",
      "1/15, train_loss: 0.1278\n",
      "2/15, train_loss: 0.1461\n",
      "3/15, train_loss: 0.1313\n",
      "4/15, train_loss: 0.1497\n",
      "5/15, train_loss: 0.1266\n",
      "6/15, train_loss: 0.1198\n",
      "7/15, train_loss: 0.1130\n",
      "8/15, train_loss: 0.1369\n",
      "9/15, train_loss: 0.1308\n",
      "10/15, train_loss: 0.1685\n",
      "11/15, train_loss: 0.1474\n",
      "12/15, train_loss: 0.1605\n",
      "13/15, train_loss: 0.1529\n",
      "14/15, train_loss: 0.1492\n",
      "15/15, train_loss: 0.1203\n",
      "16/15, train_loss: 0.1267\n",
      "epoch 458 average loss: 0.1380\n",
      "----------\n",
      "epoch 459/500\n",
      "1/15, train_loss: 0.1115\n",
      "2/15, train_loss: 0.1261\n",
      "3/15, train_loss: 0.1227\n",
      "4/15, train_loss: 0.1224\n",
      "5/15, train_loss: 0.1322\n",
      "6/15, train_loss: 0.1466\n",
      "7/15, train_loss: 0.1229\n",
      "8/15, train_loss: 0.1709\n",
      "9/15, train_loss: 0.1409\n",
      "10/15, train_loss: 0.1146\n",
      "11/15, train_loss: 0.1312\n",
      "12/15, train_loss: 0.1381\n",
      "13/15, train_loss: 0.1377\n",
      "14/15, train_loss: 0.1337\n",
      "15/15, train_loss: 0.1667\n",
      "16/15, train_loss: 2.0103\n",
      "epoch 459 average loss: 0.2518\n",
      "----------\n",
      "epoch 460/500\n",
      "1/15, train_loss: 0.1646\n",
      "2/15, train_loss: 0.5340\n",
      "3/15, train_loss: 0.1235\n",
      "4/15, train_loss: 0.1200\n",
      "5/15, train_loss: 0.1668\n",
      "6/15, train_loss: 0.1396\n",
      "7/15, train_loss: 1.6965\n",
      "8/15, train_loss: 0.1516\n",
      "9/15, train_loss: 0.1925\n",
      "10/15, train_loss: 0.1251\n",
      "11/15, train_loss: 0.1534\n",
      "12/15, train_loss: 0.1530\n",
      "13/15, train_loss: 0.1326\n",
      "14/15, train_loss: 0.1257\n",
      "15/15, train_loss: 0.1574\n",
      "16/15, train_loss: 1.6646\n",
      "epoch 460 average loss: 0.3625\n",
      "----------\n",
      "epoch 461/500\n",
      "1/15, train_loss: 0.1288\n",
      "2/15, train_loss: 0.1316\n",
      "3/15, train_loss: 0.1193\n",
      "4/15, train_loss: 0.1136\n",
      "5/15, train_loss: 0.1382\n",
      "6/15, train_loss: 0.1415\n",
      "7/15, train_loss: 0.1255\n",
      "8/15, train_loss: 0.1313\n",
      "9/15, train_loss: 0.1390\n",
      "10/15, train_loss: 0.2033\n",
      "11/15, train_loss: 0.2366\n",
      "12/15, train_loss: 0.1325\n",
      "13/15, train_loss: 0.1090\n",
      "14/15, train_loss: 0.1179\n",
      "15/15, train_loss: 0.1419\n",
      "16/15, train_loss: 2.5033\n",
      "epoch 461 average loss: 0.2883\n",
      "----------\n",
      "epoch 462/500\n",
      "1/15, train_loss: 0.1375\n",
      "2/15, train_loss: 0.1106\n",
      "3/15, train_loss: 0.1614\n",
      "4/15, train_loss: 0.1723\n",
      "5/15, train_loss: 0.1276\n",
      "6/15, train_loss: 0.1274\n",
      "7/15, train_loss: 0.1288\n",
      "8/15, train_loss: 0.1211\n",
      "9/15, train_loss: 0.1405\n",
      "10/15, train_loss: 0.1317\n",
      "11/15, train_loss: 0.1283\n",
      "12/15, train_loss: 0.1309\n",
      "13/15, train_loss: 0.1343\n",
      "14/15, train_loss: 0.1365\n",
      "15/15, train_loss: 0.1492\n",
      "16/15, train_loss: 0.2043\n",
      "epoch 462 average loss: 0.1402\n",
      "----------\n",
      "epoch 463/500\n",
      "1/15, train_loss: 0.1100\n",
      "2/15, train_loss: 0.1320\n",
      "3/15, train_loss: 0.1330\n",
      "4/15, train_loss: 0.1209\n",
      "5/15, train_loss: 0.1585\n",
      "6/15, train_loss: 0.1299\n",
      "7/15, train_loss: 0.1151\n",
      "8/15, train_loss: 0.1558\n",
      "9/15, train_loss: 0.1104\n",
      "10/15, train_loss: 0.1367\n",
      "11/15, train_loss: 0.1300\n",
      "12/15, train_loss: 0.1273\n",
      "13/15, train_loss: 0.1379\n",
      "14/15, train_loss: 0.1638\n",
      "15/15, train_loss: 0.2087\n",
      "16/15, train_loss: 2.6967\n",
      "epoch 463 average loss: 0.2979\n",
      "----------\n",
      "epoch 464/500\n",
      "1/15, train_loss: 0.1241\n",
      "2/15, train_loss: 0.1501\n",
      "3/15, train_loss: 0.1196\n",
      "4/15, train_loss: 0.1268\n",
      "5/15, train_loss: 0.1166\n",
      "6/15, train_loss: 0.1210\n",
      "7/15, train_loss: 0.1284\n",
      "8/15, train_loss: 0.1124\n",
      "9/15, train_loss: 0.1370\n",
      "10/15, train_loss: 0.1325\n",
      "11/15, train_loss: 0.1443\n",
      "12/15, train_loss: 0.1463\n",
      "13/15, train_loss: 0.1271\n",
      "14/15, train_loss: 0.1234\n",
      "15/15, train_loss: 0.1331\n",
      "16/15, train_loss: 0.1538\n",
      "epoch 464 average loss: 0.1310\n",
      "----------\n",
      "epoch 465/500\n",
      "1/15, train_loss: 0.1341\n",
      "2/15, train_loss: 0.1477\n",
      "3/15, train_loss: 0.1571\n",
      "4/15, train_loss: 0.1212\n",
      "5/15, train_loss: 0.1184\n",
      "6/15, train_loss: 0.1345\n",
      "7/15, train_loss: 0.1260\n",
      "8/15, train_loss: 0.1989\n",
      "9/15, train_loss: 0.1462\n",
      "10/15, train_loss: 0.1420\n",
      "11/15, train_loss: 0.1554\n",
      "12/15, train_loss: 0.1313\n",
      "13/15, train_loss: 0.1776\n",
      "14/15, train_loss: 0.1209\n",
      "15/15, train_loss: 0.1095\n",
      "16/15, train_loss: 0.1211\n",
      "epoch 465 average loss: 0.1401\n",
      "----------\n",
      "epoch 466/500\n",
      "1/15, train_loss: 0.1184\n",
      "2/15, train_loss: 0.1353\n",
      "3/15, train_loss: 0.1124\n",
      "4/15, train_loss: 0.1176\n",
      "5/15, train_loss: 0.1242\n",
      "6/15, train_loss: 0.1329\n",
      "7/15, train_loss: 0.1796\n",
      "8/15, train_loss: 0.1098\n",
      "9/15, train_loss: 0.1716\n",
      "10/15, train_loss: 0.1265\n",
      "11/15, train_loss: 0.1520\n",
      "12/15, train_loss: 0.1897\n",
      "13/15, train_loss: 0.1363\n",
      "14/15, train_loss: 1.7849\n",
      "15/15, train_loss: 0.1367\n",
      "16/15, train_loss: 0.1253\n",
      "epoch 466 average loss: 0.2408\n",
      "----------\n",
      "epoch 467/500\n",
      "1/15, train_loss: 0.1168\n",
      "2/15, train_loss: 0.1462\n",
      "3/15, train_loss: 0.1682\n",
      "4/15, train_loss: 0.1447\n",
      "5/15, train_loss: 0.1236\n",
      "6/15, train_loss: 0.1322\n",
      "7/15, train_loss: 0.1438\n",
      "8/15, train_loss: 0.1621\n",
      "9/15, train_loss: 0.1311\n",
      "10/15, train_loss: 0.1247\n",
      "11/15, train_loss: 0.1434\n",
      "12/15, train_loss: 0.2127\n",
      "13/15, train_loss: 0.1393\n",
      "14/15, train_loss: 0.1303\n",
      "15/15, train_loss: 0.1754\n",
      "16/15, train_loss: 1.7815\n",
      "epoch 467 average loss: 0.2485\n",
      "----------\n",
      "epoch 468/500\n",
      "1/15, train_loss: 0.1252\n",
      "2/15, train_loss: 0.1297\n",
      "3/15, train_loss: 0.1266\n",
      "4/15, train_loss: 0.1679\n",
      "5/15, train_loss: 0.1228\n",
      "6/15, train_loss: 0.1626\n",
      "7/15, train_loss: 0.1284\n",
      "8/15, train_loss: 0.1208\n",
      "9/15, train_loss: 0.1182\n",
      "10/15, train_loss: 0.1386\n",
      "11/15, train_loss: 0.1350\n",
      "12/15, train_loss: 0.1377\n",
      "13/15, train_loss: 0.1403\n",
      "14/15, train_loss: 0.1279\n",
      "15/15, train_loss: 0.1428\n",
      "16/15, train_loss: 2.1169\n",
      "epoch 468 average loss: 0.2588\n",
      "----------\n",
      "epoch 469/500\n",
      "1/15, train_loss: 0.1111\n",
      "2/15, train_loss: 0.1587\n",
      "3/15, train_loss: 0.1286\n",
      "4/15, train_loss: 0.1321\n",
      "5/15, train_loss: 0.1391\n",
      "6/15, train_loss: 0.2828\n",
      "7/15, train_loss: 0.1207\n",
      "8/15, train_loss: 0.1241\n",
      "9/15, train_loss: 0.1925\n",
      "10/15, train_loss: 0.1228\n",
      "11/15, train_loss: 0.1195\n",
      "12/15, train_loss: 0.1296\n",
      "13/15, train_loss: 0.1186\n",
      "14/15, train_loss: 0.1842\n",
      "15/15, train_loss: 0.2428\n",
      "16/15, train_loss: 1.7002\n",
      "epoch 469 average loss: 0.2505\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 469 validation loss: 0.9926 dice_score: 0.8593 acc_metric: 0.7148 accuracy: 0.7148, f1score: 0.7177\n",
      " saved Best PMetric: 0.7870 at epoch: 469\n",
      "----------\n",
      "epoch 470/500\n",
      "1/15, train_loss: 0.1658\n",
      "2/15, train_loss: 0.1123\n",
      "3/15, train_loss: 0.1416\n",
      "4/15, train_loss: 0.1450\n",
      "5/15, train_loss: 0.1543\n",
      "6/15, train_loss: 0.1493\n",
      "7/15, train_loss: 0.1230\n",
      "8/15, train_loss: 0.1150\n",
      "9/15, train_loss: 0.1750\n",
      "10/15, train_loss: 0.1300\n",
      "11/15, train_loss: 0.1442\n",
      "12/15, train_loss: 0.1124\n",
      "13/15, train_loss: 0.1782\n",
      "14/15, train_loss: 0.1707\n",
      "15/15, train_loss: 0.1210\n",
      "16/15, train_loss: 0.1441\n",
      "epoch 470 average loss: 0.1426\n",
      "----------\n",
      "epoch 471/500\n",
      "1/15, train_loss: 0.1337\n",
      "2/15, train_loss: 0.1511\n",
      "3/15, train_loss: 0.1103\n",
      "4/15, train_loss: 0.1353\n",
      "5/15, train_loss: 0.1510\n",
      "6/15, train_loss: 0.1262\n",
      "7/15, train_loss: 0.1462\n",
      "8/15, train_loss: 0.1207\n",
      "9/15, train_loss: 0.1370\n",
      "10/15, train_loss: 0.1233\n",
      "11/15, train_loss: 0.1268\n",
      "12/15, train_loss: 0.1428\n",
      "13/15, train_loss: 0.1257\n",
      "14/15, train_loss: 0.1309\n",
      "15/15, train_loss: 0.2655\n",
      "16/15, train_loss: 0.1197\n",
      "epoch 471 average loss: 0.1404\n",
      "----------\n",
      "epoch 472/500\n",
      "1/15, train_loss: 0.1552\n",
      "2/15, train_loss: 0.1319\n",
      "3/15, train_loss: 0.1429\n",
      "4/15, train_loss: 0.1281\n",
      "5/15, train_loss: 0.2228\n",
      "6/15, train_loss: 0.1738\n",
      "7/15, train_loss: 0.1533\n",
      "8/15, train_loss: 0.1455\n",
      "9/15, train_loss: 0.2221\n",
      "10/15, train_loss: 0.1152\n",
      "11/15, train_loss: 0.1743\n",
      "12/15, train_loss: 0.1358\n",
      "13/15, train_loss: 0.1519\n",
      "14/15, train_loss: 0.1294\n",
      "15/15, train_loss: 0.1250\n",
      "16/15, train_loss: 2.9007\n",
      "epoch 472 average loss: 0.3255\n",
      "----------\n",
      "epoch 473/500\n",
      "1/15, train_loss: 0.1622\n",
      "2/15, train_loss: 0.1586\n",
      "3/15, train_loss: 0.1449\n",
      "4/15, train_loss: 0.1218\n",
      "5/15, train_loss: 0.1630\n",
      "6/15, train_loss: 0.1166\n",
      "7/15, train_loss: 0.1170\n",
      "8/15, train_loss: 0.1288\n",
      "9/15, train_loss: 0.1959\n",
      "10/15, train_loss: 0.1151\n",
      "11/15, train_loss: 0.1293\n",
      "12/15, train_loss: 1.5813\n",
      "13/15, train_loss: 0.1381\n",
      "14/15, train_loss: 0.1063\n",
      "15/15, train_loss: 0.1478\n",
      "16/15, train_loss: 0.2662\n",
      "epoch 473 average loss: 0.2371\n",
      "----------\n",
      "epoch 474/500\n",
      "1/15, train_loss: 0.1284\n",
      "2/15, train_loss: 0.1222\n",
      "3/15, train_loss: 0.1310\n",
      "4/15, train_loss: 0.1367\n",
      "5/15, train_loss: 0.1626\n",
      "6/15, train_loss: 0.1346\n",
      "7/15, train_loss: 0.1759\n",
      "8/15, train_loss: 0.1140\n",
      "9/15, train_loss: 0.1628\n",
      "10/15, train_loss: 0.1740\n",
      "11/15, train_loss: 0.1425\n",
      "12/15, train_loss: 0.1391\n",
      "13/15, train_loss: 0.1321\n",
      "14/15, train_loss: 0.1416\n",
      "15/15, train_loss: 0.1613\n",
      "16/15, train_loss: 2.2380\n",
      "epoch 474 average loss: 0.2748\n",
      "----------\n",
      "epoch 475/500\n",
      "1/15, train_loss: 0.1530\n",
      "2/15, train_loss: 0.1260\n",
      "3/15, train_loss: 0.1454\n",
      "4/15, train_loss: 0.1301\n",
      "5/15, train_loss: 0.1399\n",
      "6/15, train_loss: 0.1330\n",
      "7/15, train_loss: 0.1430\n",
      "8/15, train_loss: 0.1412\n",
      "9/15, train_loss: 0.1370\n",
      "10/15, train_loss: 0.1334\n",
      "11/15, train_loss: 0.1281\n",
      "12/15, train_loss: 0.1262\n",
      "13/15, train_loss: 0.1360\n",
      "14/15, train_loss: 0.1998\n",
      "15/15, train_loss: 0.1135\n",
      "16/15, train_loss: 0.1127\n",
      "epoch 475 average loss: 0.1374\n",
      "----------\n",
      "epoch 476/500\n",
      "1/15, train_loss: 0.1451\n",
      "2/15, train_loss: 0.1348\n",
      "3/15, train_loss: 0.1256\n",
      "4/15, train_loss: 0.1712\n",
      "5/15, train_loss: 0.1265\n",
      "6/15, train_loss: 0.1532\n",
      "7/15, train_loss: 0.1279\n",
      "8/15, train_loss: 0.1739\n",
      "9/15, train_loss: 0.1344\n",
      "10/15, train_loss: 0.1321\n",
      "11/15, train_loss: 0.1128\n",
      "12/15, train_loss: 0.1683\n",
      "13/15, train_loss: 0.1254\n",
      "14/15, train_loss: 0.1159\n",
      "15/15, train_loss: 0.1168\n",
      "16/15, train_loss: 0.1260\n",
      "epoch 476 average loss: 0.1369\n",
      "----------\n",
      "epoch 477/500\n",
      "1/15, train_loss: 0.1317\n",
      "2/15, train_loss: 0.1310\n",
      "3/15, train_loss: 0.1404\n",
      "4/15, train_loss: 0.1434\n",
      "5/15, train_loss: 0.1694\n",
      "6/15, train_loss: 0.1487\n",
      "7/15, train_loss: 0.1467\n",
      "8/15, train_loss: 0.1575\n",
      "9/15, train_loss: 0.5178\n",
      "10/15, train_loss: 0.1218\n",
      "11/15, train_loss: 0.1369\n",
      "12/15, train_loss: 0.1350\n",
      "13/15, train_loss: 0.1317\n",
      "14/15, train_loss: 0.1160\n",
      "15/15, train_loss: 0.1097\n",
      "16/15, train_loss: 0.1670\n",
      "epoch 477 average loss: 0.1628\n",
      "----------\n",
      "epoch 478/500\n",
      "1/15, train_loss: 0.1063\n",
      "2/15, train_loss: 0.1508\n",
      "3/15, train_loss: 0.1630\n",
      "4/15, train_loss: 0.1259\n",
      "5/15, train_loss: 0.1277\n",
      "6/15, train_loss: 0.1319\n",
      "7/15, train_loss: 0.1510\n",
      "8/15, train_loss: 0.1355\n",
      "9/15, train_loss: 0.1510\n",
      "10/15, train_loss: 0.1369\n",
      "11/15, train_loss: 0.1628\n",
      "12/15, train_loss: 0.1266\n",
      "13/15, train_loss: 0.1173\n",
      "14/15, train_loss: 0.1071\n",
      "15/15, train_loss: 0.1239\n",
      "16/15, train_loss: 2.5762\n",
      "epoch 478 average loss: 0.2871\n",
      "----------\n",
      "epoch 479/500\n",
      "1/15, train_loss: 0.1224\n",
      "2/15, train_loss: 0.1110\n",
      "3/15, train_loss: 0.1099\n",
      "4/15, train_loss: 0.1287\n",
      "5/15, train_loss: 0.1464\n",
      "6/15, train_loss: 0.1174\n",
      "7/15, train_loss: 0.1207\n",
      "8/15, train_loss: 0.1376\n",
      "9/15, train_loss: 0.1048\n",
      "10/15, train_loss: 0.1134\n",
      "11/15, train_loss: 0.1321\n",
      "12/15, train_loss: 0.1227\n",
      "13/15, train_loss: 0.1270\n",
      "14/15, train_loss: 0.1423\n",
      "15/15, train_loss: 0.1134\n",
      "16/15, train_loss: 0.1109\n",
      "epoch 479 average loss: 0.1225\n",
      "----------\n",
      "epoch 480/500\n",
      "1/15, train_loss: 0.1371\n",
      "2/15, train_loss: 0.1176\n",
      "3/15, train_loss: 0.1901\n",
      "4/15, train_loss: 0.1087\n",
      "5/15, train_loss: 0.1408\n",
      "6/15, train_loss: 0.1447\n",
      "7/15, train_loss: 0.1263\n",
      "8/15, train_loss: 0.1483\n",
      "9/15, train_loss: 0.1507\n",
      "10/15, train_loss: 0.1082\n",
      "11/15, train_loss: 0.1219\n",
      "12/15, train_loss: 0.1214\n",
      "13/15, train_loss: 0.1485\n",
      "14/15, train_loss: 0.1726\n",
      "15/15, train_loss: 0.1345\n",
      "16/15, train_loss: 0.3564\n",
      "epoch 480 average loss: 0.1517\n",
      "----------\n",
      "epoch 481/500\n",
      "1/15, train_loss: 0.2543\n",
      "2/15, train_loss: 0.1347\n",
      "3/15, train_loss: 0.1102\n",
      "4/15, train_loss: 0.1263\n",
      "5/15, train_loss: 0.1369\n",
      "6/15, train_loss: 0.1226\n",
      "7/15, train_loss: 0.1488\n",
      "8/15, train_loss: 2.2369\n",
      "9/15, train_loss: 0.1535\n",
      "10/15, train_loss: 0.1250\n",
      "11/15, train_loss: 0.1404\n",
      "12/15, train_loss: 0.1364\n",
      "13/15, train_loss: 0.1205\n",
      "14/15, train_loss: 0.1144\n",
      "15/15, train_loss: 0.1467\n",
      "16/15, train_loss: 0.1834\n",
      "epoch 481 average loss: 0.2744\n",
      "----------\n",
      "epoch 482/500\n",
      "1/15, train_loss: 0.1106\n",
      "2/15, train_loss: 0.1665\n",
      "3/15, train_loss: 0.1249\n",
      "4/15, train_loss: 0.1387\n",
      "5/15, train_loss: 0.1625\n",
      "6/15, train_loss: 0.1474\n",
      "7/15, train_loss: 0.1476\n",
      "8/15, train_loss: 0.1245\n",
      "9/15, train_loss: 0.1127\n",
      "10/15, train_loss: 0.1351\n",
      "11/15, train_loss: 0.1376\n",
      "12/15, train_loss: 0.1208\n",
      "13/15, train_loss: 0.1430\n",
      "14/15, train_loss: 0.1267\n",
      "15/15, train_loss: 0.1298\n",
      "16/15, train_loss: 2.5928\n",
      "epoch 482 average loss: 0.2888\n",
      "----------\n",
      "epoch 483/500\n",
      "1/15, train_loss: 0.1301\n",
      "2/15, train_loss: 0.1363\n",
      "3/15, train_loss: 0.1173\n",
      "4/15, train_loss: 0.1237\n",
      "5/15, train_loss: 0.1916\n",
      "6/15, train_loss: 0.1844\n",
      "7/15, train_loss: 0.1321\n",
      "8/15, train_loss: 0.1398\n",
      "9/15, train_loss: 0.1180\n",
      "10/15, train_loss: 0.1395\n",
      "11/15, train_loss: 0.1656\n",
      "12/15, train_loss: 0.1752\n",
      "13/15, train_loss: 0.1068\n",
      "14/15, train_loss: 0.1224\n",
      "15/15, train_loss: 0.1285\n",
      "16/15, train_loss: 0.1469\n",
      "epoch 483 average loss: 0.1411\n",
      "----------\n",
      "epoch 484/500\n",
      "1/15, train_loss: 0.1401\n",
      "2/15, train_loss: 0.1410\n",
      "3/15, train_loss: 0.3676\n",
      "4/15, train_loss: 0.1733\n",
      "5/15, train_loss: 0.1330\n",
      "6/15, train_loss: 0.1277\n",
      "7/15, train_loss: 0.6391\n",
      "8/15, train_loss: 0.1692\n",
      "9/15, train_loss: 0.1637\n",
      "10/15, train_loss: 0.1272\n",
      "11/15, train_loss: 0.1264\n",
      "12/15, train_loss: 0.1400\n",
      "13/15, train_loss: 0.1201\n",
      "14/15, train_loss: 0.1191\n",
      "15/15, train_loss: 0.1140\n",
      "16/15, train_loss: 2.4456\n",
      "epoch 484 average loss: 0.3279\n",
      "----------\n",
      "epoch 485/500\n",
      "1/15, train_loss: 0.1457\n",
      "2/15, train_loss: 0.1046\n",
      "3/15, train_loss: 0.1196\n",
      "4/15, train_loss: 0.1266\n",
      "5/15, train_loss: 0.1084\n",
      "6/15, train_loss: 0.1159\n",
      "7/15, train_loss: 0.1632\n",
      "8/15, train_loss: 0.1384\n",
      "9/15, train_loss: 0.1291\n",
      "10/15, train_loss: 0.1589\n",
      "11/15, train_loss: 0.1121\n",
      "12/15, train_loss: 0.1094\n",
      "13/15, train_loss: 0.1832\n",
      "14/15, train_loss: 0.1343\n",
      "15/15, train_loss: 0.1447\n",
      "16/15, train_loss: 2.1548\n",
      "epoch 485 average loss: 0.2593\n",
      "----------\n",
      "epoch 486/500\n",
      "1/15, train_loss: 0.1314\n",
      "2/15, train_loss: 0.1607\n",
      "3/15, train_loss: 0.1335\n",
      "4/15, train_loss: 0.1153\n",
      "5/15, train_loss: 0.1174\n",
      "6/15, train_loss: 0.4515\n",
      "7/15, train_loss: 0.1248\n",
      "8/15, train_loss: 0.1638\n",
      "9/15, train_loss: 0.1293\n",
      "10/15, train_loss: 0.1298\n",
      "11/15, train_loss: 0.1530\n",
      "12/15, train_loss: 0.1622\n",
      "13/15, train_loss: 0.1269\n",
      "14/15, train_loss: 0.1530\n",
      "15/15, train_loss: 0.1241\n",
      "16/15, train_loss: 0.1375\n",
      "epoch 486 average loss: 0.1571\n",
      "----------\n",
      "epoch 487/500\n",
      "1/15, train_loss: 0.1248\n",
      "2/15, train_loss: 0.1279\n",
      "3/15, train_loss: 0.2051\n",
      "4/15, train_loss: 0.1233\n",
      "5/15, train_loss: 0.1325\n",
      "6/15, train_loss: 0.1504\n",
      "7/15, train_loss: 0.1328\n",
      "8/15, train_loss: 0.1072\n",
      "9/15, train_loss: 0.1252\n",
      "10/15, train_loss: 0.1298\n",
      "11/15, train_loss: 0.1669\n",
      "12/15, train_loss: 0.1234\n",
      "13/15, train_loss: 0.1212\n",
      "14/15, train_loss: 0.1301\n",
      "15/15, train_loss: 0.1515\n",
      "16/15, train_loss: 0.2504\n",
      "epoch 487 average loss: 0.1439\n",
      "----------\n",
      "epoch 488/500\n",
      "1/15, train_loss: 0.1899\n",
      "2/15, train_loss: 0.1447\n",
      "3/15, train_loss: 0.1233\n",
      "4/15, train_loss: 0.1097\n",
      "5/15, train_loss: 0.1184\n",
      "6/15, train_loss: 0.1436\n",
      "7/15, train_loss: 0.1393\n",
      "8/15, train_loss: 0.1285\n",
      "9/15, train_loss: 0.1450\n",
      "10/15, train_loss: 0.1548\n",
      "11/15, train_loss: 0.1238\n",
      "12/15, train_loss: 0.1190\n",
      "13/15, train_loss: 0.1530\n",
      "14/15, train_loss: 0.1312\n",
      "15/15, train_loss: 0.1436\n",
      "16/15, train_loss: 2.7088\n",
      "epoch 488 average loss: 0.2985\n",
      "----------\n",
      "epoch 489/500\n",
      "1/15, train_loss: 0.1272\n",
      "2/15, train_loss: 0.1196\n",
      "3/15, train_loss: 0.1117\n",
      "4/15, train_loss: 0.1230\n",
      "5/15, train_loss: 0.1228\n",
      "6/15, train_loss: 0.1277\n",
      "7/15, train_loss: 0.1297\n",
      "8/15, train_loss: 0.2227\n",
      "9/15, train_loss: 0.1171\n",
      "10/15, train_loss: 0.1232\n",
      "11/15, train_loss: 0.1289\n",
      "12/15, train_loss: 0.1142\n",
      "13/15, train_loss: 0.1409\n",
      "14/15, train_loss: 0.1227\n",
      "15/15, train_loss: 0.1227\n",
      "16/15, train_loss: 2.3388\n",
      "epoch 489 average loss: 0.2683\n",
      "----------\n",
      "epoch 490/500\n",
      "1/15, train_loss: 0.1098\n",
      "2/15, train_loss: 0.1246\n",
      "3/15, train_loss: 0.1493\n",
      "4/15, train_loss: 0.1242\n",
      "5/15, train_loss: 0.1595\n",
      "6/15, train_loss: 0.1164\n",
      "7/15, train_loss: 0.1179\n",
      "8/15, train_loss: 0.1128\n",
      "9/15, train_loss: 0.1138\n",
      "10/15, train_loss: 0.1099\n",
      "11/15, train_loss: 0.1795\n",
      "12/15, train_loss: 0.1321\n",
      "13/15, train_loss: 0.1311\n",
      "14/15, train_loss: 0.1301\n",
      "15/15, train_loss: 0.1357\n",
      "16/15, train_loss: 1.7949\n",
      "epoch 490 average loss: 0.2338\n",
      "----------\n",
      "epoch 491/500\n",
      "1/15, train_loss: 0.1432\n",
      "2/15, train_loss: 0.1576\n",
      "3/15, train_loss: 0.1535\n",
      "4/15, train_loss: 0.1427\n",
      "5/15, train_loss: 0.1121\n",
      "6/15, train_loss: 0.1391\n",
      "7/15, train_loss: 0.1277\n",
      "8/15, train_loss: 0.1158\n",
      "9/15, train_loss: 0.1182\n",
      "10/15, train_loss: 0.1435\n",
      "11/15, train_loss: 0.1137\n",
      "12/15, train_loss: 0.1247\n",
      "13/15, train_loss: 0.1111\n",
      "14/15, train_loss: 0.1236\n",
      "15/15, train_loss: 0.1343\n",
      "16/15, train_loss: 0.1414\n",
      "epoch 491 average loss: 0.1314\n",
      "----------\n",
      "epoch 492/500\n",
      "1/15, train_loss: 0.1438\n",
      "2/15, train_loss: 0.1233\n",
      "3/15, train_loss: 0.1356\n",
      "4/15, train_loss: 0.1261\n",
      "5/15, train_loss: 0.1044\n",
      "6/15, train_loss: 0.1525\n",
      "7/15, train_loss: 0.1248\n",
      "8/15, train_loss: 0.1095\n",
      "9/15, train_loss: 0.1219\n",
      "10/15, train_loss: 0.1069\n",
      "11/15, train_loss: 0.1204\n",
      "12/15, train_loss: 0.1097\n",
      "13/15, train_loss: 0.1197\n",
      "14/15, train_loss: 0.1657\n",
      "15/15, train_loss: 0.1186\n",
      "16/15, train_loss: 0.1076\n",
      "epoch 492 average loss: 0.1244\n",
      "----------\n",
      "epoch 493/500\n",
      "1/15, train_loss: 0.1425\n",
      "2/15, train_loss: 0.1277\n",
      "3/15, train_loss: 0.1420\n",
      "4/15, train_loss: 0.1660\n",
      "5/15, train_loss: 0.1200\n",
      "6/15, train_loss: 0.1202\n",
      "7/15, train_loss: 0.1154\n",
      "8/15, train_loss: 0.1070\n",
      "9/15, train_loss: 0.1463\n",
      "10/15, train_loss: 0.1368\n",
      "11/15, train_loss: 0.1568\n",
      "12/15, train_loss: 0.1661\n",
      "13/15, train_loss: 0.1468\n",
      "14/15, train_loss: 0.1341\n",
      "15/15, train_loss: 0.1283\n",
      "16/15, train_loss: 0.1440\n",
      "epoch 493 average loss: 0.1375\n",
      "----------\n",
      "epoch 494/500\n",
      "1/15, train_loss: 0.1237\n",
      "2/15, train_loss: 0.1431\n",
      "3/15, train_loss: 0.3781\n",
      "4/15, train_loss: 0.1175\n",
      "5/15, train_loss: 0.1285\n",
      "6/15, train_loss: 0.1387\n",
      "7/15, train_loss: 0.1876\n",
      "8/15, train_loss: 0.1214\n",
      "9/15, train_loss: 0.1483\n",
      "10/15, train_loss: 0.1367\n",
      "11/15, train_loss: 0.1521\n",
      "12/15, train_loss: 0.1449\n",
      "13/15, train_loss: 0.1220\n",
      "14/15, train_loss: 0.1378\n",
      "15/15, train_loss: 0.1461\n",
      "16/15, train_loss: 0.1653\n",
      "epoch 494 average loss: 0.1557\n",
      "----------\n",
      "epoch 495/500\n",
      "1/15, train_loss: 0.1297\n",
      "2/15, train_loss: 0.1286\n",
      "3/15, train_loss: 0.1153\n",
      "4/15, train_loss: 0.1214\n",
      "5/15, train_loss: 0.1269\n",
      "6/15, train_loss: 0.1208\n",
      "7/15, train_loss: 0.1292\n",
      "8/15, train_loss: 0.1242\n",
      "9/15, train_loss: 0.1332\n",
      "10/15, train_loss: 0.1329\n",
      "11/15, train_loss: 0.1363\n",
      "12/15, train_loss: 0.1217\n",
      "13/15, train_loss: 0.1218\n",
      "14/15, train_loss: 0.1477\n",
      "15/15, train_loss: 0.1331\n",
      "16/15, train_loss: 0.1676\n",
      "epoch 495 average loss: 0.1307\n",
      "----------\n",
      "epoch 496/500\n",
      "1/15, train_loss: 0.1322\n",
      "2/15, train_loss: 0.1373\n",
      "3/15, train_loss: 0.1369\n",
      "4/15, train_loss: 0.1445\n",
      "5/15, train_loss: 0.1657\n",
      "6/15, train_loss: 0.1089\n",
      "7/15, train_loss: 0.1083\n",
      "8/15, train_loss: 0.1302\n",
      "9/15, train_loss: 0.1316\n",
      "10/15, train_loss: 0.1274\n",
      "11/15, train_loss: 0.1164\n",
      "12/15, train_loss: 0.1685\n",
      "13/15, train_loss: 0.1134\n",
      "14/15, train_loss: 0.1311\n",
      "15/15, train_loss: 0.1381\n",
      "16/15, train_loss: 0.1415\n",
      "epoch 496 average loss: 0.1333\n",
      "----------\n",
      "epoch 497/500\n",
      "1/15, train_loss: 0.1444\n",
      "2/15, train_loss: 0.1223\n",
      "3/15, train_loss: 0.1362\n",
      "4/15, train_loss: 0.1457\n",
      "5/15, train_loss: 0.1385\n",
      "6/15, train_loss: 0.1320\n",
      "7/15, train_loss: 0.1286\n",
      "8/15, train_loss: 0.1319\n",
      "9/15, train_loss: 0.1346\n",
      "10/15, train_loss: 0.2300\n",
      "11/15, train_loss: 0.1208\n",
      "12/15, train_loss: 0.1182\n",
      "13/15, train_loss: 0.1310\n",
      "14/15, train_loss: 0.1229\n",
      "15/15, train_loss: 0.1189\n",
      "16/15, train_loss: 0.1190\n",
      "epoch 497 average loss: 0.1359\n",
      "----------\n",
      "epoch 498/500\n",
      "1/15, train_loss: 0.1326\n",
      "2/15, train_loss: 0.1192\n",
      "3/15, train_loss: 0.1361\n",
      "4/15, train_loss: 0.1002\n",
      "5/15, train_loss: 0.1278\n",
      "6/15, train_loss: 0.1322\n",
      "7/15, train_loss: 0.1207\n",
      "8/15, train_loss: 0.1472\n",
      "9/15, train_loss: 0.1476\n",
      "10/15, train_loss: 0.1370\n",
      "11/15, train_loss: 0.1237\n",
      "12/15, train_loss: 0.1263\n",
      "13/15, train_loss: 0.1299\n",
      "14/15, train_loss: 0.1205\n",
      "15/15, train_loss: 0.1119\n",
      "16/15, train_loss: 2.1248\n",
      "epoch 498 average loss: 0.2524\n",
      "----------\n",
      "epoch 499/500\n",
      "1/15, train_loss: 0.1024\n",
      "2/15, train_loss: 0.1300\n",
      "3/15, train_loss: 0.1332\n",
      "4/15, train_loss: 0.1467\n",
      "5/15, train_loss: 0.1855\n",
      "6/15, train_loss: 0.1262\n",
      "7/15, train_loss: 0.1413\n",
      "8/15, train_loss: 0.1342\n",
      "9/15, train_loss: 0.1237\n",
      "10/15, train_loss: 0.1268\n",
      "11/15, train_loss: 0.1668\n",
      "12/15, train_loss: 0.1167\n",
      "13/15, train_loss: 0.1161\n",
      "14/15, train_loss: 0.1399\n",
      "15/15, train_loss: 0.1087\n",
      "16/15, train_loss: 2.5669\n",
      "epoch 499 average loss: 0.2853\n",
      "----------\n",
      "epoch 500/500\n",
      "1/15, train_loss: 0.1184\n",
      "2/15, train_loss: 0.1385\n",
      "3/15, train_loss: 0.1138\n",
      "4/15, train_loss: 0.1245\n",
      "5/15, train_loss: 0.1315\n",
      "6/15, train_loss: 0.2447\n",
      "7/15, train_loss: 0.1467\n",
      "8/15, train_loss: 0.1203\n",
      "9/15, train_loss: 0.1139\n",
      "10/15, train_loss: 0.1227\n",
      "11/15, train_loss: 0.1621\n",
      "12/15, train_loss: 0.1324\n",
      "13/15, train_loss: 0.1301\n",
      "14/15, train_loss: 0.1419\n",
      "15/15, train_loss: 0.1197\n",
      "16/15, train_loss: 0.1267\n",
      "epoch 500 average loss: 0.1368\n",
      "fold 1 Bacth Investigation, minimum batch size 4\n",
      "Ranger21 optimizer ready with following settings:\n",
      "\n",
      "Core optimizer = AdamW\n",
      "Learning rate of 0.0001\n",
      "\n",
      "Important - num_epochs of training = ** 500 epochs **\n",
      "please confirm this is correct or warmup and warmdown will be off\n",
      "\n",
      "Warm-up: linear warmup, over 2000 iterations\n",
      "\n",
      "Lookahead active, merging every 5 steps, with blend factor of 0.5\n",
      "Norm Loss active, factor = 0.0001\n",
      "Stable weight decay of 0.0001\n",
      "Gradient Centralization = On\n",
      "\n",
      "Adaptive Gradient Clipping = True\n",
      "\tclipping value of 0.01\n",
      "\tsteps for clipping = 0.001\n",
      "\n",
      "Warm-down: Linear warmdown, starting at 72.0%, iteration 5760 of 8000\n",
      "warm down will decay until 3e-05 lr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/500\n",
      "params size saved\n",
      "total param groups = 1\n",
      "total params in groups = 127\n",
      "1/15, train_loss: 2.3428\n",
      "2/15, train_loss: 2.4039\n",
      "3/15, train_loss: 2.4004\n",
      "4/15, train_loss: 2.3143\n",
      "5/15, train_loss: 2.3179\n",
      "6/15, train_loss: 2.3743\n",
      "7/15, train_loss: 2.3958\n",
      "8/15, train_loss: 2.4448\n",
      "9/15, train_loss: 2.3378\n",
      "10/15, train_loss: 2.3587\n",
      "11/15, train_loss: 2.3738\n",
      "12/15, train_loss: 2.4596\n",
      "13/15, train_loss: 2.4113\n",
      "14/15, train_loss: 2.4105\n",
      "15/15, train_loss: 2.4339\n",
      "16/15, train_loss: 2.3638\n",
      "epoch 1 average loss: 2.3840\n",
      "current fold: 1 current epoch: 1 dice_score: 0.3719 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525 epoch 1 average training loss: 2.3840 average validation loss: 1.5775\n",
      "----------\n",
      "epoch 2/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/15, train_loss: 2.3897\n",
      "2/15, train_loss: 2.3592\n",
      "3/15, train_loss: 2.4650\n",
      "4/15, train_loss: 2.3956\n",
      "5/15, train_loss: 2.3181\n",
      "6/15, train_loss: 2.4010\n",
      "7/15, train_loss: 2.3983\n",
      "8/15, train_loss: 2.5189\n",
      "9/15, train_loss: 2.3417\n",
      "10/15, train_loss: 2.2948\n",
      "11/15, train_loss: 2.3181\n",
      "12/15, train_loss: 2.4152\n",
      "13/15, train_loss: 2.3872\n",
      "14/15, train_loss: 2.3101\n",
      "15/15, train_loss: 2.2853\n",
      "16/15, train_loss: 2.3873\n",
      "epoch 2 average loss: 2.3741\n",
      "current fold: 1 current epoch: 2 dice_score: 0.3735 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525 epoch 2 average training loss: 2.3741 average validation loss: 1.5608\n",
      "----------\n",
      "epoch 3/500\n",
      "1/15, train_loss: 2.3130\n",
      "2/15, train_loss: 2.3397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/15, train_loss: 2.3584\n",
      "4/15, train_loss: 2.3447\n",
      "5/15, train_loss: 2.3472\n",
      "6/15, train_loss: 2.3341\n",
      "7/15, train_loss: 2.3550\n",
      "8/15, train_loss: 2.3804\n",
      "9/15, train_loss: 2.2744\n",
      "10/15, train_loss: 2.4454\n",
      "11/15, train_loss: 2.3683\n",
      "12/15, train_loss: 2.4343\n",
      "13/15, train_loss: 2.3390\n",
      "14/15, train_loss: 2.3959\n",
      "15/15, train_loss: 2.4056\n",
      "16/15, train_loss: 2.4792\n",
      "epoch 3 average loss: 2.3697\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 3 validation loss: 1.5763 dice_score: 0.3738 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4369 at epoch: 3\n",
      "----------\n",
      "epoch 4/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/15, train_loss: 2.4231\n",
      "2/15, train_loss: 2.4560\n",
      "3/15, train_loss: 2.2821\n",
      "4/15, train_loss: 2.4287\n",
      "5/15, train_loss: 2.3306\n",
      "6/15, train_loss: 2.3743\n",
      "7/15, train_loss: 2.3255\n",
      "8/15, train_loss: 2.3879\n",
      "9/15, train_loss: 2.3313\n",
      "10/15, train_loss: 2.4351\n",
      "11/15, train_loss: 2.3712\n",
      "12/15, train_loss: 2.3232\n",
      "13/15, train_loss: 2.2807\n",
      "14/15, train_loss: 2.3742\n",
      "15/15, train_loss: 2.3671\n",
      "16/15, train_loss: 2.2724\n",
      "epoch 4 average loss: 2.3602\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 4 validation loss: 1.5777 dice_score: 0.3743 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4371 at epoch: 4\n",
      "----------\n",
      "epoch 5/500\n",
      "1/15, train_loss: 2.3628\n",
      "2/15, train_loss: 2.4578\n",
      "3/15, train_loss: 2.2982\n",
      "4/15, train_loss: 2.3349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/15, train_loss: 2.2806\n",
      "6/15, train_loss: 2.4146\n",
      "7/15, train_loss: 2.4109\n",
      "8/15, train_loss: 2.3069\n",
      "9/15, train_loss: 2.3439\n",
      "10/15, train_loss: 2.3619\n",
      "11/15, train_loss: 2.3852\n",
      "12/15, train_loss: 2.2864\n",
      "13/15, train_loss: 2.3324\n",
      "14/15, train_loss: 2.3522\n",
      "15/15, train_loss: 2.4009\n",
      "16/15, train_loss: 2.4721\n",
      "epoch 5 average loss: 2.3626\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 5 validation loss: 1.5708 dice_score: 0.3753 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4376 at epoch: 5\n",
      "----------\n",
      "epoch 6/500\n",
      "1/15, train_loss: 2.3643\n",
      "2/15, train_loss: 2.3362\n",
      "3/15, train_loss: 2.3546\n",
      "4/15, train_loss: 2.3104\n",
      "5/15, train_loss: 2.3730\n",
      "6/15, train_loss: 2.4133\n",
      "7/15, train_loss: 2.3766\n",
      "8/15, train_loss: 2.3364\n",
      "9/15, train_loss: 2.3281\n",
      "10/15, train_loss: 2.3360\n",
      "11/15, train_loss: 2.3210\n",
      "12/15, train_loss: 2.4444\n",
      "13/15, train_loss: 2.3621\n",
      "14/15, train_loss: 2.3361\n",
      "15/15, train_loss: 2.2807\n",
      "16/15, train_loss: 2.2214\n",
      "epoch 6 average loss: 2.3434\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 6 validation loss: 1.5670 dice_score: 0.3758 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4379 at epoch: 6\n",
      "----------\n",
      "epoch 7/500\n",
      "1/15, train_loss: 2.3233\n",
      "2/15, train_loss: 2.4103\n",
      "3/15, train_loss: 2.3213\n",
      "4/15, train_loss: 2.3322\n",
      "5/15, train_loss: 2.2953\n",
      "6/15, train_loss: 2.3192\n",
      "7/15, train_loss: 2.4597\n",
      "8/15, train_loss: 2.4302\n",
      "9/15, train_loss: 2.2636\n",
      "10/15, train_loss: 2.4088\n",
      "11/15, train_loss: 2.3515\n",
      "12/15, train_loss: 2.3161\n",
      "13/15, train_loss: 2.3248\n",
      "14/15, train_loss: 2.2423\n",
      "15/15, train_loss: 2.3251\n",
      "16/15, train_loss: 2.3223\n",
      "epoch 7 average loss: 2.3404\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 7 validation loss: 1.5732 dice_score: 0.3774 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4387 at epoch: 7\n",
      "----------\n",
      "epoch 8/500\n",
      "1/15, train_loss: 2.2782\n",
      "2/15, train_loss: 2.4636\n",
      "3/15, train_loss: 2.4188\n",
      "4/15, train_loss: 2.3009\n",
      "5/15, train_loss: 2.2794\n",
      "6/15, train_loss: 2.3612\n",
      "7/15, train_loss: 2.2986\n",
      "8/15, train_loss: 2.3765\n",
      "9/15, train_loss: 2.3368\n",
      "10/15, train_loss: 2.2467\n",
      "11/15, train_loss: 2.3911\n",
      "12/15, train_loss: 2.2731\n",
      "13/15, train_loss: 2.2827\n",
      "14/15, train_loss: 2.3568\n",
      "15/15, train_loss: 2.3392\n",
      "16/15, train_loss: 2.2997\n",
      "epoch 8 average loss: 2.3315\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 8 validation loss: 1.5575 dice_score: 0.3792 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4396 at epoch: 8\n",
      "----------\n",
      "epoch 9/500\n",
      "1/15, train_loss: 2.4288\n",
      "2/15, train_loss: 2.3499\n",
      "3/15, train_loss: 2.3385\n",
      "4/15, train_loss: 2.3021\n",
      "5/15, train_loss: 2.3110\n",
      "6/15, train_loss: 2.2842\n",
      "7/15, train_loss: 2.3578\n",
      "8/15, train_loss: 2.3839\n",
      "9/15, train_loss: 2.2692\n",
      "10/15, train_loss: 2.3091\n",
      "11/15, train_loss: 2.2527\n",
      "12/15, train_loss: 2.3911\n",
      "13/15, train_loss: 2.2597\n",
      "14/15, train_loss: 2.3800\n",
      "15/15, train_loss: 2.3082\n",
      "16/15, train_loss: 2.3673\n",
      "epoch 9 average loss: 2.3309\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 9 validation loss: 1.5317 dice_score: 0.3812 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4406 at epoch: 9\n",
      "----------\n",
      "epoch 10/500\n",
      "1/15, train_loss: 2.4397\n",
      "2/15, train_loss: 2.2999\n",
      "3/15, train_loss: 2.2964\n",
      "4/15, train_loss: 2.3285\n",
      "5/15, train_loss: 2.2499\n",
      "6/15, train_loss: 2.2707\n",
      "7/15, train_loss: 2.3105\n",
      "8/15, train_loss: 2.3660\n",
      "9/15, train_loss: 2.3778\n",
      "10/15, train_loss: 2.2357\n",
      "11/15, train_loss: 2.2773\n",
      "12/15, train_loss: 2.2840\n",
      "13/15, train_loss: 2.3300\n",
      "14/15, train_loss: 2.3368\n",
      "15/15, train_loss: 2.3016\n",
      "16/15, train_loss: 2.2398\n",
      "epoch 10 average loss: 2.3090\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 10 validation loss: 1.5488 dice_score: 0.3833 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4416 at epoch: 10\n",
      "----------\n",
      "epoch 11/500\n",
      "1/15, train_loss: 2.2986\n",
      "2/15, train_loss: 2.2465\n",
      "3/15, train_loss: 2.3260\n",
      "4/15, train_loss: 2.4662\n",
      "5/15, train_loss: 2.3228\n",
      "6/15, train_loss: 2.2670\n",
      "7/15, train_loss: 2.2554\n",
      "8/15, train_loss: 2.2394\n",
      "9/15, train_loss: 2.4061\n",
      "10/15, train_loss: 2.2463\n",
      "11/15, train_loss: 2.3926\n",
      "12/15, train_loss: 2.3415\n",
      "13/15, train_loss: 2.3119\n",
      "14/15, train_loss: 2.3216\n",
      "15/15, train_loss: 2.3007\n",
      "16/15, train_loss: 2.2806\n",
      "epoch 11 average loss: 2.3140\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 11 validation loss: 1.5436 dice_score: 0.3850 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4425 at epoch: 11\n",
      "----------\n",
      "epoch 12/500\n",
      "1/15, train_loss: 2.2864\n",
      "2/15, train_loss: 2.3583\n",
      "3/15, train_loss: 2.3641\n",
      "4/15, train_loss: 2.2287\n",
      "5/15, train_loss: 2.3559\n",
      "6/15, train_loss: 2.2313\n",
      "7/15, train_loss: 2.2787\n",
      "8/15, train_loss: 2.2745\n",
      "9/15, train_loss: 2.1645\n",
      "10/15, train_loss: 2.3491\n",
      "11/15, train_loss: 2.2568\n",
      "12/15, train_loss: 2.2390\n",
      "13/15, train_loss: 2.1950\n",
      "14/15, train_loss: 2.2297\n",
      "15/15, train_loss: 2.2614\n",
      "16/15, train_loss: 2.2616\n",
      "epoch 12 average loss: 2.2709\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 12 validation loss: 1.5411 dice_score: 0.3872 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4436 at epoch: 12\n",
      "----------\n",
      "epoch 13/500\n",
      "1/15, train_loss: 2.2414\n",
      "2/15, train_loss: 2.2854\n",
      "3/15, train_loss: 2.3174\n",
      "4/15, train_loss: 2.2720\n",
      "5/15, train_loss: 2.3490\n",
      "6/15, train_loss: 2.2070\n",
      "7/15, train_loss: 2.2546\n",
      "8/15, train_loss: 2.1904\n",
      "9/15, train_loss: 2.1849\n",
      "10/15, train_loss: 2.2329\n",
      "11/15, train_loss: 2.2150\n",
      "12/15, train_loss: 2.2643\n",
      "13/15, train_loss: 2.2195\n",
      "14/15, train_loss: 2.4149\n",
      "15/15, train_loss: 2.1869\n",
      "16/15, train_loss: 2.2115\n",
      "epoch 13 average loss: 2.2529\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 13 validation loss: 1.5307 dice_score: 0.3905 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4453 at epoch: 13\n",
      "----------\n",
      "epoch 14/500\n",
      "1/15, train_loss: 2.2328\n",
      "2/15, train_loss: 2.1755\n",
      "3/15, train_loss: 2.3040\n",
      "4/15, train_loss: 2.3551\n",
      "5/15, train_loss: 2.1692\n",
      "6/15, train_loss: 2.2401\n",
      "7/15, train_loss: 2.2471\n",
      "8/15, train_loss: 2.3640\n",
      "9/15, train_loss: 2.2586\n",
      "10/15, train_loss: 2.1809\n",
      "11/15, train_loss: 2.1589\n",
      "12/15, train_loss: 2.1980\n",
      "13/15, train_loss: 2.1983\n",
      "14/15, train_loss: 2.1615\n",
      "15/15, train_loss: 2.1695\n",
      "16/15, train_loss: 2.2671\n",
      "epoch 14 average loss: 2.2300\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 14 validation loss: 1.5053 dice_score: 0.3945 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4473 at epoch: 14\n",
      "----------\n",
      "epoch 15/500\n",
      "1/15, train_loss: 2.3059\n",
      "2/15, train_loss: 2.2263\n",
      "3/15, train_loss: 2.2799\n",
      "4/15, train_loss: 2.1862\n",
      "5/15, train_loss: 2.3100\n",
      "6/15, train_loss: 2.1870\n",
      "7/15, train_loss: 2.2139\n",
      "8/15, train_loss: 2.2838\n",
      "9/15, train_loss: 2.2194\n",
      "10/15, train_loss: 2.1752\n",
      "11/15, train_loss: 2.2103\n",
      "12/15, train_loss: 2.1943\n",
      "13/15, train_loss: 2.1394\n",
      "14/15, train_loss: 2.2445\n",
      "15/15, train_loss: 2.2488\n",
      "16/15, train_loss: 2.2467\n",
      "epoch 15 average loss: 2.2295\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 15 validation loss: 1.4951 dice_score: 0.3991 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4496 at epoch: 15\n",
      "----------\n",
      "epoch 16/500\n",
      "1/15, train_loss: 2.2683\n",
      "2/15, train_loss: 2.3736\n",
      "3/15, train_loss: 2.2860\n",
      "4/15, train_loss: 2.2271\n",
      "5/15, train_loss: 2.2553\n",
      "6/15, train_loss: 2.3793\n",
      "7/15, train_loss: 2.1905\n",
      "8/15, train_loss: 2.1811\n",
      "9/15, train_loss: 2.2277\n",
      "10/15, train_loss: 2.1994\n",
      "11/15, train_loss: 2.1397\n",
      "12/15, train_loss: 2.2027\n",
      "13/15, train_loss: 2.1593\n",
      "14/15, train_loss: 2.2117\n",
      "15/15, train_loss: 2.1763\n",
      "16/15, train_loss: 2.2413\n",
      "epoch 16 average loss: 2.2325\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 16 validation loss: 1.4957 dice_score: 0.4052 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4526 at epoch: 16\n",
      "----------\n",
      "epoch 17/500\n",
      "1/15, train_loss: 2.2521\n",
      "2/15, train_loss: 2.2312\n",
      "3/15, train_loss: 2.1876\n",
      "4/15, train_loss: 2.1299\n",
      "5/15, train_loss: 2.2013\n",
      "6/15, train_loss: 2.2126\n",
      "7/15, train_loss: 2.1429\n",
      "8/15, train_loss: 2.1471\n",
      "9/15, train_loss: 2.1493\n",
      "10/15, train_loss: 2.2022\n",
      "11/15, train_loss: 2.2048\n",
      "12/15, train_loss: 2.1373\n",
      "13/15, train_loss: 2.1053\n",
      "14/15, train_loss: 2.1451\n",
      "15/15, train_loss: 2.2217\n",
      "16/15, train_loss: 2.1574\n",
      "epoch 17 average loss: 2.1767\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 17 validation loss: 1.4518 dice_score: 0.4136 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4568 at epoch: 17\n",
      "----------\n",
      "epoch 18/500\n",
      "1/15, train_loss: 2.2213\n",
      "2/15, train_loss: 2.2395\n",
      "3/15, train_loss: 2.2148\n",
      "4/15, train_loss: 2.2185\n",
      "5/15, train_loss: 2.1100\n",
      "6/15, train_loss: 2.1728\n",
      "7/15, train_loss: 2.1409\n",
      "8/15, train_loss: 2.2849\n",
      "9/15, train_loss: 2.1096\n",
      "10/15, train_loss: 2.1219\n",
      "11/15, train_loss: 2.1428\n",
      "12/15, train_loss: 2.0696\n",
      "13/15, train_loss: 2.1597\n",
      "14/15, train_loss: 2.1281\n",
      "15/15, train_loss: 2.2203\n",
      "16/15, train_loss: 2.1081\n",
      "epoch 18 average loss: 2.1664\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 18 validation loss: 1.4430 dice_score: 0.4228 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4614 at epoch: 18\n",
      "----------\n",
      "epoch 19/500\n",
      "1/15, train_loss: 2.1705\n",
      "2/15, train_loss: 2.1021\n",
      "3/15, train_loss: 2.1460\n",
      "4/15, train_loss: 2.2161\n",
      "5/15, train_loss: 2.1822\n",
      "6/15, train_loss: 2.1148\n",
      "7/15, train_loss: 2.1335\n",
      "8/15, train_loss: 2.1087\n",
      "9/15, train_loss: 2.1866\n",
      "10/15, train_loss: 2.1279\n",
      "11/15, train_loss: 2.1720\n",
      "12/15, train_loss: 2.1623\n",
      "13/15, train_loss: 2.1919\n",
      "14/15, train_loss: 2.1407\n",
      "15/15, train_loss: 2.1061\n",
      "16/15, train_loss: 2.1160\n",
      "epoch 19 average loss: 2.1486\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 19 validation loss: 1.4306 dice_score: 0.4318 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4659 at epoch: 19\n",
      "----------\n",
      "epoch 20/500\n",
      "1/15, train_loss: 2.1325\n",
      "2/15, train_loss: 2.0654\n",
      "3/15, train_loss: 2.1564\n",
      "4/15, train_loss: 2.0877\n",
      "5/15, train_loss: 2.1090\n",
      "6/15, train_loss: 2.0848\n",
      "7/15, train_loss: 2.1196\n",
      "8/15, train_loss: 2.1399\n",
      "9/15, train_loss: 2.1630\n",
      "10/15, train_loss: 2.1822\n",
      "11/15, train_loss: 2.1381\n",
      "12/15, train_loss: 2.0357\n",
      "13/15, train_loss: 2.2269\n",
      "14/15, train_loss: 2.2047\n",
      "15/15, train_loss: 2.0866\n",
      "16/15, train_loss: 2.0677\n",
      "epoch 20 average loss: 2.1250\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 20 validation loss: 1.4270 dice_score: 0.4377 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4689 at epoch: 20\n",
      "----------\n",
      "epoch 21/500\n",
      "1/15, train_loss: 2.1578\n",
      "2/15, train_loss: 2.3077\n",
      "3/15, train_loss: 2.0891\n",
      "4/15, train_loss: 2.2916\n",
      "5/15, train_loss: 2.1393\n",
      "6/15, train_loss: 2.0679\n",
      "7/15, train_loss: 2.0948\n",
      "8/15, train_loss: 1.9679\n",
      "9/15, train_loss: 2.0344\n",
      "10/15, train_loss: 2.2069\n",
      "11/15, train_loss: 2.0584\n",
      "12/15, train_loss: 2.0821\n",
      "13/15, train_loss: 2.2225\n",
      "14/15, train_loss: 2.0497\n",
      "15/15, train_loss: 2.2205\n",
      "16/15, train_loss: 2.1424\n",
      "epoch 21 average loss: 2.1333\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 21 validation loss: 1.4237 dice_score: 0.4432 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4716 at epoch: 21\n",
      "----------\n",
      "epoch 22/500\n",
      "1/15, train_loss: 2.1967\n",
      "2/15, train_loss: 2.1043\n",
      "3/15, train_loss: 1.9723\n",
      "4/15, train_loss: 2.2381\n",
      "5/15, train_loss: 2.0459\n",
      "6/15, train_loss: 2.1689\n",
      "7/15, train_loss: 2.0702\n",
      "8/15, train_loss: 2.2362\n",
      "9/15, train_loss: 1.9893\n",
      "10/15, train_loss: 2.1001\n",
      "11/15, train_loss: 2.1404\n",
      "12/15, train_loss: 2.0561\n",
      "13/15, train_loss: 2.1005\n",
      "14/15, train_loss: 2.1503\n",
      "15/15, train_loss: 2.0550\n",
      "16/15, train_loss: 2.0113\n",
      "epoch 22 average loss: 2.1022\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 22 validation loss: 1.3961 dice_score: 0.4529 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4765 at epoch: 22\n",
      "----------\n",
      "epoch 23/500\n",
      "1/15, train_loss: 2.0603\n",
      "2/15, train_loss: 2.2953\n",
      "3/15, train_loss: 2.1231\n",
      "4/15, train_loss: 2.1283\n",
      "5/15, train_loss: 2.1518\n",
      "6/15, train_loss: 2.2447\n",
      "7/15, train_loss: 2.0855\n",
      "8/15, train_loss: 2.0951\n",
      "9/15, train_loss: 2.1028\n",
      "10/15, train_loss: 2.0607\n",
      "11/15, train_loss: 2.0917\n",
      "12/15, train_loss: 2.0649\n",
      "13/15, train_loss: 2.1467\n",
      "14/15, train_loss: 2.0083\n",
      "15/15, train_loss: 2.1127\n",
      "16/15, train_loss: 2.1786\n",
      "epoch 23 average loss: 2.1219\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 23 validation loss: 1.3925 dice_score: 0.4586 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4793 at epoch: 23\n",
      "----------\n",
      "epoch 24/500\n",
      "1/15, train_loss: 2.1261\n",
      "2/15, train_loss: 2.1526\n",
      "3/15, train_loss: 2.1095\n",
      "4/15, train_loss: 2.0758\n",
      "5/15, train_loss: 2.0624\n",
      "6/15, train_loss: 1.9818\n",
      "7/15, train_loss: 2.1128\n",
      "8/15, train_loss: 2.0698\n",
      "9/15, train_loss: 2.0576\n",
      "10/15, train_loss: 2.0895\n",
      "11/15, train_loss: 2.0864\n",
      "12/15, train_loss: 2.0999\n",
      "13/15, train_loss: 2.0980\n",
      "14/15, train_loss: 2.0219\n",
      "15/15, train_loss: 2.0990\n",
      "16/15, train_loss: 1.9267\n",
      "epoch 24 average loss: 2.0731\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 24 validation loss: 1.3912 dice_score: 0.4720 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4860 at epoch: 24\n",
      "----------\n",
      "epoch 25/500\n",
      "1/15, train_loss: 2.1851\n",
      "2/15, train_loss: 2.0377\n",
      "3/15, train_loss: 2.0413\n",
      "4/15, train_loss: 2.0789\n",
      "5/15, train_loss: 2.1521\n",
      "6/15, train_loss: 1.9908\n",
      "7/15, train_loss: 2.1448\n",
      "8/15, train_loss: 2.0702\n",
      "9/15, train_loss: 2.1516\n",
      "10/15, train_loss: 2.0152\n",
      "11/15, train_loss: 2.0032\n",
      "12/15, train_loss: 2.1646\n",
      "13/15, train_loss: 2.0678\n",
      "14/15, train_loss: 2.1350\n",
      "15/15, train_loss: 2.0820\n",
      "16/15, train_loss: 2.2614\n",
      "epoch 25 average loss: 2.0988\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 25 validation loss: 1.4094 dice_score: 0.4708 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4854 at epoch: 25\n",
      "----------\n",
      "epoch 26/500\n",
      "1/15, train_loss: 2.0671\n",
      "2/15, train_loss: 1.9924\n",
      "3/15, train_loss: 2.0891\n",
      "4/15, train_loss: 2.1720\n",
      "5/15, train_loss: 2.0203\n",
      "6/15, train_loss: 2.0243\n",
      "7/15, train_loss: 2.3883\n",
      "8/15, train_loss: 1.9758\n",
      "9/15, train_loss: 2.0695\n",
      "10/15, train_loss: 2.1370\n",
      "11/15, train_loss: 2.0008\n",
      "12/15, train_loss: 2.0313\n",
      "13/15, train_loss: 2.0618\n",
      "14/15, train_loss: 1.9603\n",
      "15/15, train_loss: 2.0511\n",
      "16/15, train_loss: 2.0504\n",
      "epoch 26 average loss: 2.0682\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 26 validation loss: 1.4004 dice_score: 0.4752 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4876 at epoch: 26\n",
      "----------\n",
      "epoch 27/500\n",
      "1/15, train_loss: 2.0921\n",
      "2/15, train_loss: 2.0442\n",
      "3/15, train_loss: 2.0259\n",
      "4/15, train_loss: 2.2625\n",
      "5/15, train_loss: 2.0220\n",
      "6/15, train_loss: 2.0618\n",
      "7/15, train_loss: 2.1484\n",
      "8/15, train_loss: 2.0090\n",
      "9/15, train_loss: 2.1437\n",
      "10/15, train_loss: 2.0136\n",
      "11/15, train_loss: 2.0662\n",
      "12/15, train_loss: 2.0677\n",
      "13/15, train_loss: 2.1215\n",
      "14/15, train_loss: 1.9190\n",
      "15/15, train_loss: 2.0745\n",
      "16/15, train_loss: 2.0847\n",
      "epoch 27 average loss: 2.0723\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 27 validation loss: 1.3745 dice_score: 0.4854 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4927 at epoch: 27\n",
      "----------\n",
      "epoch 28/500\n",
      "1/15, train_loss: 2.0640\n",
      "2/15, train_loss: 2.1252\n",
      "3/15, train_loss: 1.9718\n",
      "4/15, train_loss: 2.0901\n",
      "5/15, train_loss: 1.9873\n",
      "6/15, train_loss: 2.1161\n",
      "7/15, train_loss: 1.9942\n",
      "8/15, train_loss: 1.9789\n",
      "9/15, train_loss: 1.9621\n",
      "10/15, train_loss: 2.0354\n",
      "11/15, train_loss: 2.0998\n",
      "12/15, train_loss: 2.0463\n",
      "13/15, train_loss: 2.0015\n",
      "14/15, train_loss: 1.9097\n",
      "15/15, train_loss: 1.9827\n",
      "16/15, train_loss: 2.1403\n",
      "epoch 28 average loss: 2.0316\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 28 validation loss: 1.3632 dice_score: 0.4981 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.4990 at epoch: 28\n",
      "----------\n",
      "epoch 29/500\n",
      "1/15, train_loss: 2.1650\n",
      "2/15, train_loss: 2.0255\n",
      "3/15, train_loss: 1.9436\n",
      "4/15, train_loss: 1.9699\n",
      "5/15, train_loss: 2.0722\n",
      "6/15, train_loss: 2.0784\n",
      "7/15, train_loss: 2.0643\n",
      "8/15, train_loss: 1.9991\n",
      "9/15, train_loss: 1.9550\n",
      "10/15, train_loss: 2.0866\n",
      "11/15, train_loss: 2.1345\n",
      "12/15, train_loss: 1.9403\n",
      "13/15, train_loss: 2.0530\n",
      "14/15, train_loss: 1.9719\n",
      "15/15, train_loss: 2.1000\n",
      "16/15, train_loss: 2.3349\n",
      "epoch 29 average loss: 2.0559\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 29 validation loss: 1.3720 dice_score: 0.5026 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.5013 at epoch: 29\n",
      "----------\n",
      "epoch 30/500\n",
      "1/15, train_loss: 1.8724\n",
      "2/15, train_loss: 1.9370\n",
      "3/15, train_loss: 2.0891\n",
      "4/15, train_loss: 2.0344\n",
      "5/15, train_loss: 1.9829\n",
      "6/15, train_loss: 2.1014\n",
      "7/15, train_loss: 1.9466\n",
      "8/15, train_loss: 1.9843\n",
      "9/15, train_loss: 1.9000\n",
      "10/15, train_loss: 2.0083\n",
      "11/15, train_loss: 1.9174\n",
      "12/15, train_loss: 2.0436\n",
      "13/15, train_loss: 1.9100\n",
      "14/15, train_loss: 2.0756\n",
      "15/15, train_loss: 2.0091\n",
      "16/15, train_loss: 1.9581\n",
      "epoch 30 average loss: 1.9856\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 30 validation loss: 1.3547 dice_score: 0.5070 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.5035 at epoch: 30\n",
      "----------\n",
      "epoch 31/500\n",
      "1/15, train_loss: 1.8313\n",
      "2/15, train_loss: 2.2956\n",
      "3/15, train_loss: 1.9610\n",
      "4/15, train_loss: 1.9189\n",
      "5/15, train_loss: 2.0805\n",
      "6/15, train_loss: 1.9311\n",
      "7/15, train_loss: 2.0682\n",
      "8/15, train_loss: 2.3689\n",
      "9/15, train_loss: 1.9469\n",
      "10/15, train_loss: 2.0820\n",
      "11/15, train_loss: 1.9213\n",
      "12/15, train_loss: 2.0516\n",
      "13/15, train_loss: 1.9762\n",
      "14/15, train_loss: 1.9150\n",
      "15/15, train_loss: 1.9681\n",
      "16/15, train_loss: 2.2185\n",
      "epoch 31 average loss: 2.0334\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 31 validation loss: 1.3329 dice_score: 0.5127 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.5063 at epoch: 31\n",
      "----------\n",
      "epoch 32/500\n",
      "1/15, train_loss: 1.9309\n",
      "2/15, train_loss: 1.9764\n",
      "3/15, train_loss: 1.9103\n",
      "4/15, train_loss: 2.0354\n",
      "5/15, train_loss: 1.7994\n",
      "6/15, train_loss: 1.9023\n",
      "7/15, train_loss: 1.9196\n",
      "8/15, train_loss: 1.9204\n",
      "9/15, train_loss: 1.9279\n",
      "10/15, train_loss: 1.9066\n",
      "11/15, train_loss: 1.8958\n",
      "12/15, train_loss: 2.0200\n",
      "13/15, train_loss: 1.9686\n",
      "14/15, train_loss: 2.0842\n",
      "15/15, train_loss: 1.9413\n",
      "16/15, train_loss: 1.8591\n",
      "epoch 32 average loss: 1.9374\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 32 validation loss: 1.3237 dice_score: 0.5292 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.5146 at epoch: 32\n",
      "----------\n",
      "epoch 33/500\n",
      "1/15, train_loss: 1.9084\n",
      "2/15, train_loss: 2.0690\n",
      "3/15, train_loss: 2.0220\n",
      "4/15, train_loss: 1.8885\n",
      "5/15, train_loss: 1.9543\n",
      "6/15, train_loss: 1.8082\n",
      "7/15, train_loss: 1.9613\n",
      "8/15, train_loss: 1.9775\n",
      "9/15, train_loss: 1.9048\n",
      "10/15, train_loss: 1.9299\n",
      "11/15, train_loss: 1.9296\n",
      "12/15, train_loss: 1.9956\n",
      "13/15, train_loss: 2.1571\n",
      "14/15, train_loss: 1.9541\n",
      "15/15, train_loss: 1.9396\n",
      "16/15, train_loss: 1.7850\n",
      "epoch 33 average loss: 1.9491\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 33 validation loss: 1.3273 dice_score: 0.5283 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.5142 at epoch: 33\n",
      "----------\n",
      "epoch 34/500\n",
      "1/15, train_loss: 1.8937\n",
      "2/15, train_loss: 1.7802\n",
      "3/15, train_loss: 1.9952\n",
      "4/15, train_loss: 2.0553\n",
      "5/15, train_loss: 1.9859\n",
      "6/15, train_loss: 1.9582\n",
      "7/15, train_loss: 1.9046\n",
      "8/15, train_loss: 1.9654\n",
      "9/15, train_loss: 1.8602\n",
      "10/15, train_loss: 1.9700\n",
      "11/15, train_loss: 1.8010\n",
      "12/15, train_loss: 1.8119\n",
      "13/15, train_loss: 1.9015\n",
      "14/15, train_loss: 1.8413\n",
      "15/15, train_loss: 1.8934\n",
      "16/15, train_loss: 2.2961\n",
      "epoch 34 average loss: 1.9321\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 34 validation loss: 1.3175 dice_score: 0.5360 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.5180 at epoch: 34\n",
      "----------\n",
      "epoch 35/500\n",
      "1/15, train_loss: 1.9430\n",
      "2/15, train_loss: 1.8225\n",
      "3/15, train_loss: 1.8177\n",
      "4/15, train_loss: 2.1412\n",
      "5/15, train_loss: 2.0918\n",
      "6/15, train_loss: 1.8041\n",
      "7/15, train_loss: 1.9127\n",
      "8/15, train_loss: 1.9358\n",
      "9/15, train_loss: 1.8916\n",
      "10/15, train_loss: 1.9558\n",
      "11/15, train_loss: 1.9183\n",
      "12/15, train_loss: 1.7613\n",
      "13/15, train_loss: 1.7634\n",
      "14/15, train_loss: 1.8858\n",
      "15/15, train_loss: 1.7665\n",
      "16/15, train_loss: 2.0448\n",
      "epoch 35 average loss: 1.9035\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 35 validation loss: 1.3059 dice_score: 0.5464 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.5232 at epoch: 35\n",
      "----------\n",
      "epoch 36/500\n",
      "1/15, train_loss: 1.8802\n",
      "2/15, train_loss: 1.9056\n",
      "3/15, train_loss: 1.9602\n",
      "4/15, train_loss: 1.8185\n",
      "5/15, train_loss: 1.7577\n",
      "6/15, train_loss: 1.8414\n",
      "7/15, train_loss: 1.9344\n",
      "8/15, train_loss: 1.8295\n",
      "9/15, train_loss: 1.9106\n",
      "10/15, train_loss: 1.8522\n",
      "11/15, train_loss: 1.7634\n",
      "12/15, train_loss: 2.0173\n",
      "13/15, train_loss: 1.9733\n",
      "14/15, train_loss: 1.9129\n",
      "15/15, train_loss: 1.8541\n",
      "16/15, train_loss: 1.7980\n",
      "epoch 36 average loss: 1.8756\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 36 validation loss: 1.3050 dice_score: 0.5584 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.5292 at epoch: 36\n",
      "----------\n",
      "epoch 37/500\n",
      "1/15, train_loss: 1.7898\n",
      "2/15, train_loss: 1.8465\n",
      "3/15, train_loss: 2.1335\n",
      "4/15, train_loss: 1.8843\n",
      "5/15, train_loss: 1.8153\n",
      "6/15, train_loss: 1.7415\n",
      "7/15, train_loss: 1.7892\n",
      "8/15, train_loss: 1.8194\n",
      "9/15, train_loss: 1.8634\n",
      "10/15, train_loss: 1.8736\n",
      "11/15, train_loss: 1.8699\n",
      "12/15, train_loss: 1.8615\n",
      "13/15, train_loss: 1.8473\n",
      "14/15, train_loss: 1.9879\n",
      "15/15, train_loss: 1.8273\n",
      "16/15, train_loss: 1.9303\n",
      "epoch 37 average loss: 1.8675\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 37 validation loss: 1.3135 dice_score: 0.5607 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.5304 at epoch: 37\n",
      "----------\n",
      "epoch 38/500\n",
      "1/15, train_loss: 1.8871\n",
      "2/15, train_loss: 1.7030\n",
      "3/15, train_loss: 1.7257\n",
      "4/15, train_loss: 1.7697\n",
      "5/15, train_loss: 1.7925\n",
      "6/15, train_loss: 1.7095\n",
      "7/15, train_loss: 1.7305\n",
      "8/15, train_loss: 1.7843\n",
      "9/15, train_loss: 1.7970\n",
      "10/15, train_loss: 1.9230\n",
      "11/15, train_loss: 1.7902\n",
      "12/15, train_loss: 1.7761\n",
      "13/15, train_loss: 1.9217\n",
      "14/15, train_loss: 1.6638\n",
      "15/15, train_loss: 1.8756\n",
      "16/15, train_loss: 1.9587\n",
      "epoch 38 average loss: 1.8005\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 38 validation loss: 1.2921 dice_score: 0.5676 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.5338 at epoch: 38\n",
      "----------\n",
      "epoch 39/500\n",
      "1/15, train_loss: 1.8593\n",
      "2/15, train_loss: 1.7126\n",
      "3/15, train_loss: 1.7180\n",
      "4/15, train_loss: 1.8814\n",
      "5/15, train_loss: 1.8561\n",
      "6/15, train_loss: 1.7528\n",
      "7/15, train_loss: 1.7908\n",
      "8/15, train_loss: 1.6539\n",
      "9/15, train_loss: 1.8238\n",
      "10/15, train_loss: 1.8616\n",
      "11/15, train_loss: 1.8921\n",
      "12/15, train_loss: 1.8613\n",
      "13/15, train_loss: 1.8157\n",
      "14/15, train_loss: 1.8963\n",
      "15/15, train_loss: 1.7914\n",
      "16/15, train_loss: 1.7290\n",
      "epoch 39 average loss: 1.8060\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 39 validation loss: 1.2868 dice_score: 0.5717 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3525\n",
      " saved Best PMetric: 0.5358 at epoch: 39\n",
      "----------\n",
      "epoch 40/500\n",
      "1/15, train_loss: 1.7703\n",
      "2/15, train_loss: 2.0608\n",
      "3/15, train_loss: 1.7721\n",
      "4/15, train_loss: 1.7334\n",
      "5/15, train_loss: 1.7602\n",
      "6/15, train_loss: 2.0090\n",
      "7/15, train_loss: 1.7755\n",
      "8/15, train_loss: 1.6974\n",
      "9/15, train_loss: 1.7028\n",
      "10/15, train_loss: 1.6907\n",
      "11/15, train_loss: 1.6869\n",
      "12/15, train_loss: 1.7005\n",
      "13/15, train_loss: 1.7766\n",
      "14/15, train_loss: 1.7015\n",
      "15/15, train_loss: 1.8379\n",
      "16/15, train_loss: 2.0032\n",
      "epoch 40 average loss: 1.7924\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 40 validation loss: 1.2891 dice_score: 0.5738 acc_metric: 0.4947 accuracy: 0.4947, f1score: 0.3525\n",
      " saved Best PMetric: 0.5343 at epoch: 40\n",
      "----------\n",
      "epoch 41/500\n",
      "1/15, train_loss: 1.7596\n",
      "2/15, train_loss: 1.6465\n",
      "3/15, train_loss: 1.7110\n",
      "4/15, train_loss: 1.7806\n",
      "5/15, train_loss: 1.7502\n",
      "6/15, train_loss: 1.7371\n",
      "7/15, train_loss: 2.0885\n",
      "8/15, train_loss: 1.8720\n",
      "9/15, train_loss: 1.8402\n",
      "10/15, train_loss: 1.6384\n",
      "11/15, train_loss: 1.7348\n",
      "12/15, train_loss: 1.9816\n",
      "13/15, train_loss: 1.8703\n",
      "14/15, train_loss: 1.7969\n",
      "15/15, train_loss: 1.5774\n",
      "16/15, train_loss: 1.8273\n",
      "epoch 41 average loss: 1.7883\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 41 validation loss: 1.2807 dice_score: 0.5754 acc_metric: 0.4947 accuracy: 0.4947, f1score: 0.3525\n",
      " saved Best PMetric: 0.5351 at epoch: 41\n",
      "----------\n",
      "epoch 42/500\n",
      "1/15, train_loss: 1.8969\n",
      "2/15, train_loss: 1.6804\n",
      "3/15, train_loss: 1.6552\n",
      "4/15, train_loss: 2.0186\n",
      "5/15, train_loss: 1.7108\n",
      "6/15, train_loss: 1.7532\n",
      "7/15, train_loss: 1.8589\n",
      "8/15, train_loss: 1.6523\n",
      "9/15, train_loss: 1.6517\n",
      "10/15, train_loss: 1.7762\n",
      "11/15, train_loss: 1.6659\n",
      "12/15, train_loss: 1.6407\n",
      "13/15, train_loss: 1.8623\n",
      "14/15, train_loss: 1.6366\n",
      "15/15, train_loss: 1.7454\n",
      "16/15, train_loss: 1.7305\n",
      "epoch 42 average loss: 1.7460\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 42 validation loss: 1.2734 dice_score: 0.5872 acc_metric: 0.4947 accuracy: 0.4947, f1score: 0.3525\n",
      " saved Best PMetric: 0.5410 at epoch: 42\n",
      "----------\n",
      "epoch 43/500\n",
      "1/15, train_loss: 1.8079\n",
      "2/15, train_loss: 1.7378\n",
      "3/15, train_loss: 1.7300\n",
      "4/15, train_loss: 1.5146\n",
      "5/15, train_loss: 1.5914\n",
      "6/15, train_loss: 1.8047\n",
      "7/15, train_loss: 1.9308\n",
      "8/15, train_loss: 1.7780\n",
      "9/15, train_loss: 1.7352\n",
      "10/15, train_loss: 1.6165\n",
      "11/15, train_loss: 1.5393\n",
      "12/15, train_loss: 1.6242\n",
      "13/15, train_loss: 1.8446\n",
      "14/15, train_loss: 1.6227\n",
      "15/15, train_loss: 1.5522\n",
      "16/15, train_loss: 1.6374\n",
      "epoch 43 average loss: 1.6917\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 43 validation loss: 1.2689 dice_score: 0.5851 acc_metric: 0.4947 accuracy: 0.4947, f1score: 0.3525\n",
      " saved Best PMetric: 0.5399 at epoch: 43\n",
      "----------\n",
      "epoch 44/500\n",
      "1/15, train_loss: 1.5584\n",
      "2/15, train_loss: 1.5079\n",
      "3/15, train_loss: 1.8185\n",
      "4/15, train_loss: 1.6044\n",
      "5/15, train_loss: 1.6614\n",
      "6/15, train_loss: 1.7971\n",
      "7/15, train_loss: 1.6879\n",
      "8/15, train_loss: 1.5864\n",
      "9/15, train_loss: 1.5385\n",
      "10/15, train_loss: 1.8556\n",
      "11/15, train_loss: 1.5972\n",
      "12/15, train_loss: 1.5574\n",
      "13/15, train_loss: 1.5656\n",
      "14/15, train_loss: 1.5828\n",
      "15/15, train_loss: 1.6496\n",
      "16/15, train_loss: 1.6046\n",
      "epoch 44 average loss: 1.6358\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 44 validation loss: 1.2459 dice_score: 0.6020 acc_metric: 0.4947 accuracy: 0.4947, f1score: 0.3525\n",
      " saved Best PMetric: 0.5484 at epoch: 44\n",
      "----------\n",
      "epoch 45/500\n",
      "1/15, train_loss: 1.7181\n",
      "2/15, train_loss: 1.6026\n",
      "3/15, train_loss: 1.6551\n",
      "4/15, train_loss: 1.3860\n",
      "5/15, train_loss: 1.8270\n",
      "6/15, train_loss: 1.5468\n",
      "7/15, train_loss: 2.0285\n",
      "8/15, train_loss: 1.6324\n",
      "9/15, train_loss: 1.4694\n",
      "10/15, train_loss: 1.6960\n",
      "11/15, train_loss: 1.4156\n",
      "12/15, train_loss: 1.6174\n",
      "13/15, train_loss: 1.5929\n",
      "14/15, train_loss: 1.6202\n",
      "15/15, train_loss: 1.5221\n",
      "16/15, train_loss: 1.8700\n",
      "epoch 45 average loss: 1.6375\n",
      "----------\n",
      "epoch 46/500\n",
      "1/15, train_loss: 1.5398\n",
      "2/15, train_loss: 1.5753\n",
      "3/15, train_loss: 1.7890\n",
      "4/15, train_loss: 1.5132\n",
      "5/15, train_loss: 1.8335\n",
      "6/15, train_loss: 1.5436\n",
      "7/15, train_loss: 1.4945\n",
      "8/15, train_loss: 1.3862\n",
      "9/15, train_loss: 1.5444\n",
      "10/15, train_loss: 1.6195\n",
      "11/15, train_loss: 1.4105\n",
      "12/15, train_loss: 1.9754\n",
      "13/15, train_loss: 1.5665\n",
      "14/15, train_loss: 1.6911\n",
      "15/15, train_loss: 1.3942\n",
      "16/15, train_loss: 1.4619\n",
      "epoch 46 average loss: 1.5837\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 46 validation loss: 1.2497 dice_score: 0.6028 acc_metric: 0.4831 accuracy: 0.4831, f1score: 0.3443\n",
      " saved Best PMetric: 0.5430 at epoch: 46\n",
      "----------\n",
      "epoch 47/500\n",
      "1/15, train_loss: 1.5696\n",
      "2/15, train_loss: 1.4507\n",
      "3/15, train_loss: 1.4867\n",
      "4/15, train_loss: 1.9537\n",
      "5/15, train_loss: 1.6588\n",
      "6/15, train_loss: 1.6621\n",
      "7/15, train_loss: 1.6383\n",
      "8/15, train_loss: 1.6114\n",
      "9/15, train_loss: 1.5147\n",
      "10/15, train_loss: 1.2865\n",
      "11/15, train_loss: 1.3164\n",
      "12/15, train_loss: 1.5683\n",
      "13/15, train_loss: 1.3871\n",
      "14/15, train_loss: 1.9287\n",
      "15/15, train_loss: 1.3896\n",
      "16/15, train_loss: 1.7781\n",
      "epoch 47 average loss: 1.5750\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 47 validation loss: 1.2429 dice_score: 0.6069 acc_metric: 0.4831 accuracy: 0.4831, f1score: 0.3443\n",
      " saved Best PMetric: 0.5450 at epoch: 47\n",
      "----------\n",
      "epoch 48/500\n",
      "1/15, train_loss: 1.2631\n",
      "2/15, train_loss: 1.3788\n",
      "3/15, train_loss: 1.3947\n",
      "4/15, train_loss: 1.4125\n",
      "5/15, train_loss: 1.7410\n",
      "6/15, train_loss: 1.3701\n",
      "7/15, train_loss: 1.4631\n",
      "8/15, train_loss: 1.4609\n",
      "9/15, train_loss: 1.3344\n",
      "10/15, train_loss: 1.5344\n",
      "11/15, train_loss: 1.3732\n",
      "12/15, train_loss: 1.3665\n",
      "13/15, train_loss: 1.5215\n",
      "14/15, train_loss: 1.6283\n",
      "15/15, train_loss: 1.5822\n",
      "16/15, train_loss: 1.4214\n",
      "epoch 48 average loss: 1.4529\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 48 validation loss: 1.2628 dice_score: 0.6153 acc_metric: 0.4798 accuracy: 0.4798, f1score: 0.3607\n",
      " saved Best PMetric: 0.5476 at epoch: 48\n",
      "----------\n",
      "epoch 49/500\n",
      "1/15, train_loss: 1.5417\n",
      "2/15, train_loss: 1.4731\n",
      "3/15, train_loss: 1.5313\n",
      "4/15, train_loss: 1.6861\n",
      "5/15, train_loss: 1.6963\n",
      "6/15, train_loss: 1.5923\n",
      "7/15, train_loss: 1.4216\n",
      "8/15, train_loss: 1.5088\n",
      "9/15, train_loss: 1.8712\n",
      "10/15, train_loss: 1.4540\n",
      "11/15, train_loss: 1.6300\n",
      "12/15, train_loss: 1.6583\n",
      "13/15, train_loss: 1.3816\n",
      "14/15, train_loss: 1.6226\n",
      "15/15, train_loss: 1.8457\n",
      "16/15, train_loss: 1.6298\n",
      "epoch 49 average loss: 1.5965\n",
      "----------\n",
      "epoch 50/500\n",
      "1/15, train_loss: 1.2929\n",
      "2/15, train_loss: 1.4937\n",
      "3/15, train_loss: 1.4130\n",
      "4/15, train_loss: 1.8957\n",
      "5/15, train_loss: 1.6295\n",
      "6/15, train_loss: 1.2405\n",
      "7/15, train_loss: 1.6982\n",
      "8/15, train_loss: 1.4461\n",
      "9/15, train_loss: 1.4911\n",
      "10/15, train_loss: 1.7684\n",
      "11/15, train_loss: 1.4953\n",
      "12/15, train_loss: 1.5037\n",
      "13/15, train_loss: 1.4267\n",
      "14/15, train_loss: 1.4126\n",
      "15/15, train_loss: 1.5140\n",
      "16/15, train_loss: 2.3043\n",
      "epoch 50 average loss: 1.5641\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 50 validation loss: 1.2379 dice_score: 0.6114 acc_metric: 0.4894 accuracy: 0.4894, f1score: 0.3525\n",
      " saved Best PMetric: 0.5504 at epoch: 50\n",
      "----------\n",
      "epoch 51/500\n",
      "1/15, train_loss: 1.3873\n",
      "2/15, train_loss: 1.3501\n",
      "3/15, train_loss: 1.2901\n",
      "4/15, train_loss: 1.3348\n",
      "5/15, train_loss: 1.4015\n",
      "6/15, train_loss: 1.3703\n",
      "7/15, train_loss: 1.3762\n",
      "8/15, train_loss: 1.3339\n",
      "9/15, train_loss: 1.2715\n",
      "10/15, train_loss: 1.3553\n",
      "11/15, train_loss: 1.2766\n",
      "12/15, train_loss: 1.4535\n",
      "13/15, train_loss: 1.2612\n",
      "14/15, train_loss: 1.3125\n",
      "15/15, train_loss: 1.2978\n",
      "16/15, train_loss: 1.5632\n",
      "epoch 51 average loss: 1.3522\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 51 validation loss: 1.2674 dice_score: 0.6287 acc_metric: 0.4735 accuracy: 0.4735, f1score: 0.3525\n",
      " saved Best PMetric: 0.5511 at epoch: 51\n",
      "----------\n",
      "epoch 52/500\n",
      "1/15, train_loss: 1.2719\n",
      "2/15, train_loss: 1.3796\n",
      "3/15, train_loss: 1.2524\n",
      "4/15, train_loss: 1.3352\n",
      "5/15, train_loss: 1.2606\n",
      "6/15, train_loss: 2.2988\n",
      "7/15, train_loss: 1.2680\n",
      "8/15, train_loss: 1.5381\n",
      "9/15, train_loss: 1.5193\n",
      "10/15, train_loss: 1.4961\n",
      "11/15, train_loss: 1.2688\n",
      "12/15, train_loss: 1.2272\n",
      "13/15, train_loss: 1.2864\n",
      "14/15, train_loss: 1.2566\n",
      "15/15, train_loss: 1.4115\n",
      "16/15, train_loss: 1.2879\n",
      "epoch 52 average loss: 1.3974\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 52 validation loss: 1.2353 dice_score: 0.6326 acc_metric: 0.5052 accuracy: 0.5052, f1score: 0.3934\n",
      " saved Best PMetric: 0.5689 at epoch: 52\n",
      "----------\n",
      "epoch 53/500\n",
      "1/15, train_loss: 1.3935\n",
      "2/15, train_loss: 1.6845\n",
      "3/15, train_loss: 1.6459\n",
      "4/15, train_loss: 1.4691\n",
      "5/15, train_loss: 1.5404\n",
      "6/15, train_loss: 2.0438\n",
      "7/15, train_loss: 1.2631\n",
      "8/15, train_loss: 1.5640\n",
      "9/15, train_loss: 1.1772\n",
      "10/15, train_loss: 1.6463\n",
      "11/15, train_loss: 1.3438\n",
      "12/15, train_loss: 1.4888\n",
      "13/15, train_loss: 1.2106\n",
      "14/15, train_loss: 1.2131\n",
      "15/15, train_loss: 1.5108\n",
      "16/15, train_loss: 1.2259\n",
      "epoch 53 average loss: 1.4638\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 53 validation loss: 1.2563 dice_score: 0.6406 acc_metric: 0.4617 accuracy: 0.4617, f1score: 0.3852\n",
      " saved Best PMetric: 0.5512 at epoch: 53\n",
      "----------\n",
      "epoch 54/500\n",
      "1/15, train_loss: 1.1029\n",
      "2/15, train_loss: 1.1904\n",
      "3/15, train_loss: 1.2585\n",
      "4/15, train_loss: 1.5742\n",
      "5/15, train_loss: 1.0975\n",
      "6/15, train_loss: 1.3288\n",
      "7/15, train_loss: 1.1634\n",
      "8/15, train_loss: 1.2020\n",
      "9/15, train_loss: 1.2979\n",
      "10/15, train_loss: 1.1590\n",
      "11/15, train_loss: 1.0556\n",
      "12/15, train_loss: 1.1969\n",
      "13/15, train_loss: 1.1668\n",
      "14/15, train_loss: 1.2676\n",
      "15/15, train_loss: 1.1738\n",
      "16/15, train_loss: 1.6032\n",
      "epoch 54 average loss: 1.2399\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 54 validation loss: 1.2210 dice_score: 0.6601 acc_metric: 0.4798 accuracy: 0.4798, f1score: 0.3607\n",
      " saved Best PMetric: 0.5699 at epoch: 54\n",
      "----------\n",
      "epoch 55/500\n",
      "1/15, train_loss: 1.2285\n",
      "2/15, train_loss: 1.2057\n",
      "3/15, train_loss: 1.3550\n",
      "4/15, train_loss: 1.1803\n",
      "5/15, train_loss: 1.3179\n",
      "6/15, train_loss: 1.2001\n",
      "7/15, train_loss: 1.7373\n",
      "8/15, train_loss: 1.4631\n",
      "9/15, train_loss: 1.1394\n",
      "10/15, train_loss: 1.2477\n",
      "11/15, train_loss: 1.2304\n",
      "12/15, train_loss: 1.2612\n",
      "13/15, train_loss: 1.4311\n",
      "14/15, train_loss: 1.2060\n",
      "15/15, train_loss: 1.7601\n",
      "16/15, train_loss: 1.2459\n",
      "epoch 55 average loss: 1.3256\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 55 validation loss: 1.2215 dice_score: 0.6616 acc_metric: 0.4882 accuracy: 0.4882, f1score: 0.3852\n",
      " saved Best PMetric: 0.5749 at epoch: 55\n",
      "----------\n",
      "epoch 56/500\n",
      "1/15, train_loss: 1.5727\n",
      "2/15, train_loss: 1.1870\n",
      "3/15, train_loss: 1.1231\n",
      "4/15, train_loss: 1.2505\n",
      "5/15, train_loss: 1.1807\n",
      "6/15, train_loss: 1.1723\n",
      "7/15, train_loss: 1.3577\n",
      "8/15, train_loss: 1.0567\n",
      "9/15, train_loss: 1.8934\n",
      "10/15, train_loss: 1.1358\n",
      "11/15, train_loss: 1.1239\n",
      "12/15, train_loss: 1.1789\n",
      "13/15, train_loss: 1.1408\n",
      "14/15, train_loss: 1.1935\n",
      "15/15, train_loss: 1.3557\n",
      "16/15, train_loss: 1.0104\n",
      "epoch 56 average loss: 1.2458\n",
      "----------\n",
      "epoch 57/500\n",
      "1/15, train_loss: 1.1323\n",
      "2/15, train_loss: 1.2714\n",
      "3/15, train_loss: 1.5217\n",
      "4/15, train_loss: 1.1022\n",
      "5/15, train_loss: 1.1732\n",
      "6/15, train_loss: 1.2225\n",
      "7/15, train_loss: 1.0888\n",
      "8/15, train_loss: 1.3763\n",
      "9/15, train_loss: 1.2694\n",
      "10/15, train_loss: 0.9606\n",
      "11/15, train_loss: 1.4264\n",
      "12/15, train_loss: 1.4196\n",
      "13/15, train_loss: 1.6445\n",
      "14/15, train_loss: 1.0345\n",
      "15/15, train_loss: 1.0766\n",
      "16/15, train_loss: 1.4799\n",
      "epoch 57 average loss: 1.2625\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 57 validation loss: 1.1992 dice_score: 0.6846 acc_metric: 0.4966 accuracy: 0.4966, f1score: 0.4098\n",
      " saved Best PMetric: 0.5906 at epoch: 57\n",
      "----------\n",
      "epoch 58/500\n",
      "1/15, train_loss: 1.0278\n",
      "2/15, train_loss: 1.1109\n",
      "3/15, train_loss: 1.2758\n",
      "4/15, train_loss: 1.0852\n",
      "5/15, train_loss: 1.7384\n",
      "6/15, train_loss: 1.2153\n",
      "7/15, train_loss: 2.0099\n",
      "8/15, train_loss: 0.9822\n",
      "9/15, train_loss: 1.1383\n",
      "10/15, train_loss: 1.8335\n",
      "11/15, train_loss: 2.2646\n",
      "12/15, train_loss: 1.2332\n",
      "13/15, train_loss: 1.1608\n",
      "14/15, train_loss: 1.0618\n",
      "15/15, train_loss: 1.1807\n",
      "16/15, train_loss: 1.0391\n",
      "epoch 58 average loss: 1.3348\n",
      "----------\n",
      "epoch 59/500\n",
      "1/15, train_loss: 1.4356\n",
      "2/15, train_loss: 1.2839\n",
      "3/15, train_loss: 1.0377\n",
      "4/15, train_loss: 1.0330\n",
      "5/15, train_loss: 0.9758\n",
      "6/15, train_loss: 1.2309\n",
      "7/15, train_loss: 0.9773\n",
      "8/15, train_loss: 1.0644\n",
      "9/15, train_loss: 1.2918\n",
      "10/15, train_loss: 1.0747\n",
      "11/15, train_loss: 1.0854\n",
      "12/15, train_loss: 0.9955\n",
      "13/15, train_loss: 1.2322\n",
      "14/15, train_loss: 1.1299\n",
      "15/15, train_loss: 1.2231\n",
      "16/15, train_loss: 1.8155\n",
      "epoch 59 average loss: 1.1804\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 59 validation loss: 1.2111 dice_score: 0.6946 acc_metric: 0.4626 accuracy: 0.4626, f1score: 0.4344\n",
      " saved Best PMetric: 0.5786 at epoch: 59\n",
      "----------\n",
      "epoch 60/500\n",
      "1/15, train_loss: 1.0859\n",
      "2/15, train_loss: 1.1069\n",
      "3/15, train_loss: 1.5197\n",
      "4/15, train_loss: 1.1767\n",
      "5/15, train_loss: 1.5168\n",
      "6/15, train_loss: 1.0093\n",
      "7/15, train_loss: 1.0262\n",
      "8/15, train_loss: 1.4121\n",
      "9/15, train_loss: 1.1811\n",
      "10/15, train_loss: 1.1302\n",
      "11/15, train_loss: 0.9628\n",
      "12/15, train_loss: 1.1604\n",
      "13/15, train_loss: 1.0661\n",
      "14/15, train_loss: 1.0599\n",
      "15/15, train_loss: 1.0520\n",
      "16/15, train_loss: 1.0650\n",
      "epoch 60 average loss: 1.1582\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 60 validation loss: 1.1959 dice_score: 0.7048 acc_metric: 0.4881 accuracy: 0.4881, f1score: 0.4262\n",
      " saved Best PMetric: 0.5964 at epoch: 60\n",
      "----------\n",
      "epoch 61/500\n",
      "1/15, train_loss: 1.4512\n",
      "2/15, train_loss: 1.1148\n",
      "3/15, train_loss: 1.0617\n",
      "4/15, train_loss: 1.3425\n",
      "5/15, train_loss: 0.9365\n",
      "6/15, train_loss: 1.0653\n",
      "7/15, train_loss: 1.5185\n",
      "8/15, train_loss: 0.9592\n",
      "9/15, train_loss: 1.5396\n",
      "10/15, train_loss: 1.0445\n",
      "11/15, train_loss: 1.1360\n",
      "12/15, train_loss: 1.4431\n",
      "13/15, train_loss: 1.2111\n",
      "14/15, train_loss: 1.2699\n",
      "15/15, train_loss: 1.0041\n",
      "16/15, train_loss: 1.4906\n",
      "epoch 61 average loss: 1.2243\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 61 validation loss: 1.2586 dice_score: 0.7113 acc_metric: 0.4987 accuracy: 0.4987, f1score: 0.4262\n",
      " saved Best PMetric: 0.6050 at epoch: 61\n",
      "----------\n",
      "epoch 62/500\n",
      "1/15, train_loss: 1.5571\n",
      "2/15, train_loss: 0.9168\n",
      "3/15, train_loss: 1.1487\n",
      "4/15, train_loss: 1.4151\n",
      "5/15, train_loss: 1.5039\n",
      "6/15, train_loss: 1.1217\n",
      "7/15, train_loss: 1.0867\n",
      "8/15, train_loss: 1.0082\n",
      "9/15, train_loss: 0.9673\n",
      "10/15, train_loss: 1.0155\n",
      "11/15, train_loss: 1.1399\n",
      "12/15, train_loss: 0.9653\n",
      "13/15, train_loss: 1.1566\n",
      "14/15, train_loss: 0.9734\n",
      "15/15, train_loss: 0.9826\n",
      "16/15, train_loss: 1.3321\n",
      "epoch 62 average loss: 1.1432\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 62 validation loss: 1.1704 dice_score: 0.7193 acc_metric: 0.5281 accuracy: 0.5281, f1score: 0.4918\n",
      " saved Best PMetric: 0.6237 at epoch: 62\n",
      "----------\n",
      "epoch 63/500\n",
      "1/15, train_loss: 0.9481\n",
      "2/15, train_loss: 1.1861\n",
      "3/15, train_loss: 1.1791\n",
      "4/15, train_loss: 0.9489\n",
      "5/15, train_loss: 1.0997\n",
      "6/15, train_loss: 0.9961\n",
      "7/15, train_loss: 1.0834\n",
      "8/15, train_loss: 0.9712\n",
      "9/15, train_loss: 0.9658\n",
      "10/15, train_loss: 1.1290\n",
      "11/15, train_loss: 0.9399\n",
      "12/15, train_loss: 1.0364\n",
      "13/15, train_loss: 1.0740\n",
      "14/15, train_loss: 0.9555\n",
      "15/15, train_loss: 1.1896\n",
      "16/15, train_loss: 0.9203\n",
      "epoch 63 average loss: 1.0389\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 63 validation loss: 1.1755 dice_score: 0.7255 acc_metric: 0.5428 accuracy: 0.5428, f1score: 0.5246\n",
      " saved Best PMetric: 0.6341 at epoch: 63\n",
      "----------\n",
      "epoch 64/500\n",
      "1/15, train_loss: 1.1211\n",
      "2/15, train_loss: 0.9807\n",
      "3/15, train_loss: 1.0116\n",
      "4/15, train_loss: 1.5293\n",
      "5/15, train_loss: 1.2136\n",
      "6/15, train_loss: 1.2739\n",
      "7/15, train_loss: 0.8927\n",
      "8/15, train_loss: 1.0344\n",
      "9/15, train_loss: 1.3072\n",
      "10/15, train_loss: 0.9462\n",
      "11/15, train_loss: 1.1266\n",
      "12/15, train_loss: 0.8994\n",
      "13/15, train_loss: 1.0425\n",
      "14/15, train_loss: 1.1220\n",
      "15/15, train_loss: 1.0470\n",
      "16/15, train_loss: 1.3274\n",
      "epoch 64 average loss: 1.1172\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 64 validation loss: 1.1427 dice_score: 0.7333 acc_metric: 0.6176 accuracy: 0.6176, f1score: 0.6557\n",
      " saved Best PMetric: 0.6755 at epoch: 64\n",
      "----------\n",
      "epoch 65/500\n",
      "1/15, train_loss: 1.1474\n",
      "2/15, train_loss: 0.9472\n",
      "3/15, train_loss: 1.2154\n",
      "4/15, train_loss: 0.9668\n",
      "5/15, train_loss: 0.9808\n",
      "6/15, train_loss: 0.8643\n",
      "7/15, train_loss: 1.1499\n",
      "8/15, train_loss: 1.1805\n",
      "9/15, train_loss: 0.9344\n",
      "10/15, train_loss: 1.1623\n",
      "11/15, train_loss: 1.6432\n",
      "12/15, train_loss: 1.1516\n",
      "13/15, train_loss: 1.1013\n",
      "14/15, train_loss: 0.9830\n",
      "15/15, train_loss: 0.9481\n",
      "16/15, train_loss: 0.8835\n",
      "epoch 65 average loss: 1.0787\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 65 validation loss: 1.1397 dice_score: 0.7381 acc_metric: 0.6072 accuracy: 0.6072, f1score: 0.6148\n",
      " saved Best PMetric: 0.6726 at epoch: 65\n",
      "----------\n",
      "epoch 66/500\n",
      "1/15, train_loss: 0.8773\n",
      "2/15, train_loss: 1.2085\n",
      "3/15, train_loss: 0.9710\n",
      "4/15, train_loss: 1.2825\n",
      "5/15, train_loss: 1.0590\n",
      "6/15, train_loss: 0.7845\n",
      "7/15, train_loss: 0.9601\n",
      "8/15, train_loss: 0.8567\n",
      "9/15, train_loss: 0.9735\n",
      "10/15, train_loss: 1.0038\n",
      "11/15, train_loss: 1.5052\n",
      "12/15, train_loss: 1.0083\n",
      "13/15, train_loss: 1.0064\n",
      "14/15, train_loss: 1.0543\n",
      "15/15, train_loss: 1.5111\n",
      "16/15, train_loss: 2.4237\n",
      "epoch 66 average loss: 1.1554\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 66 validation loss: 1.2052 dice_score: 0.7424 acc_metric: 0.6643 accuracy: 0.6643, f1score: 0.6475\n",
      " saved Best PMetric: 0.7033 at epoch: 66\n",
      "----------\n",
      "epoch 67/500\n",
      "1/15, train_loss: 1.0379\n",
      "2/15, train_loss: 0.8586\n",
      "3/15, train_loss: 1.1434\n",
      "4/15, train_loss: 0.8691\n",
      "5/15, train_loss: 0.8896\n",
      "6/15, train_loss: 1.3719\n",
      "7/15, train_loss: 0.9873\n",
      "8/15, train_loss: 1.3902\n",
      "9/15, train_loss: 0.9823\n",
      "10/15, train_loss: 1.2278\n",
      "11/15, train_loss: 1.1068\n",
      "12/15, train_loss: 0.8767\n",
      "13/15, train_loss: 1.0048\n",
      "14/15, train_loss: 0.8435\n",
      "15/15, train_loss: 0.9237\n",
      "16/15, train_loss: 1.0871\n",
      "epoch 67 average loss: 1.0376\n",
      "----------\n",
      "epoch 68/500\n",
      "1/15, train_loss: 0.9819\n",
      "2/15, train_loss: 1.1039\n",
      "3/15, train_loss: 0.9806\n",
      "4/15, train_loss: 0.8508\n",
      "5/15, train_loss: 1.2114\n",
      "6/15, train_loss: 0.8000\n",
      "7/15, train_loss: 1.4743\n",
      "8/15, train_loss: 0.9397\n",
      "9/15, train_loss: 0.9881\n",
      "10/15, train_loss: 1.2612\n",
      "11/15, train_loss: 1.0989\n",
      "12/15, train_loss: 1.1280\n",
      "13/15, train_loss: 0.9360\n",
      "14/15, train_loss: 1.1009\n",
      "15/15, train_loss: 0.8950\n",
      "16/15, train_loss: 0.8702\n",
      "epoch 68 average loss: 1.0388\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 68 validation loss: 1.1691 dice_score: 0.7540 acc_metric: 0.6441 accuracy: 0.6441, f1score: 0.6557\n",
      " saved Best PMetric: 0.6991 at epoch: 68\n",
      "----------\n",
      "epoch 69/500\n",
      "1/15, train_loss: 0.9496\n",
      "2/15, train_loss: 0.9207\n",
      "3/15, train_loss: 2.4101\n",
      "4/15, train_loss: 0.7509\n",
      "5/15, train_loss: 1.6174\n",
      "6/15, train_loss: 0.8987\n",
      "7/15, train_loss: 0.7980\n",
      "8/15, train_loss: 0.9307\n",
      "9/15, train_loss: 1.2342\n",
      "10/15, train_loss: 0.8137\n",
      "11/15, train_loss: 1.3202\n",
      "12/15, train_loss: 0.9586\n",
      "13/15, train_loss: 1.1157\n",
      "14/15, train_loss: 1.1210\n",
      "15/15, train_loss: 0.9250\n",
      "16/15, train_loss: 1.4356\n",
      "epoch 69 average loss: 1.1375\n",
      "----------\n",
      "epoch 70/500\n",
      "1/15, train_loss: 0.9051\n",
      "2/15, train_loss: 0.9638\n",
      "3/15, train_loss: 1.2562\n",
      "4/15, train_loss: 0.8587\n",
      "5/15, train_loss: 1.0123\n",
      "6/15, train_loss: 1.0173\n",
      "7/15, train_loss: 0.8599\n",
      "8/15, train_loss: 1.1939\n",
      "9/15, train_loss: 1.1502\n",
      "10/15, train_loss: 0.8778\n",
      "11/15, train_loss: 0.8149\n",
      "12/15, train_loss: 0.8515\n",
      "13/15, train_loss: 1.1523\n",
      "14/15, train_loss: 0.8768\n",
      "15/15, train_loss: 1.5849\n",
      "16/15, train_loss: 2.4504\n",
      "epoch 70 average loss: 1.1141\n",
      "----------\n",
      "epoch 71/500\n",
      "1/15, train_loss: 0.7825\n",
      "2/15, train_loss: 1.0355\n",
      "3/15, train_loss: 1.1797\n",
      "4/15, train_loss: 0.9828\n",
      "5/15, train_loss: 1.0416\n",
      "6/15, train_loss: 0.8802\n",
      "7/15, train_loss: 0.9895\n",
      "8/15, train_loss: 0.8142\n",
      "9/15, train_loss: 1.1018\n",
      "10/15, train_loss: 0.7542\n",
      "11/15, train_loss: 0.7554\n",
      "12/15, train_loss: 1.1589\n",
      "13/15, train_loss: 1.0786\n",
      "14/15, train_loss: 0.8737\n",
      "15/15, train_loss: 1.0385\n",
      "16/15, train_loss: 0.8963\n",
      "epoch 71 average loss: 0.9602\n",
      "----------\n",
      "epoch 72/500\n",
      "1/15, train_loss: 0.9079\n",
      "2/15, train_loss: 0.9133\n",
      "3/15, train_loss: 1.0806\n",
      "4/15, train_loss: 0.8915\n",
      "5/15, train_loss: 1.3260\n",
      "6/15, train_loss: 1.8431\n",
      "7/15, train_loss: 0.8217\n",
      "8/15, train_loss: 1.0874\n",
      "9/15, train_loss: 0.9032\n",
      "10/15, train_loss: 1.3066\n",
      "11/15, train_loss: 0.8711\n",
      "12/15, train_loss: 1.5255\n",
      "13/15, train_loss: 1.5486\n",
      "14/15, train_loss: 0.9543\n",
      "15/15, train_loss: 0.7879\n",
      "16/15, train_loss: 1.2256\n",
      "epoch 72 average loss: 1.1246\n",
      "----------\n",
      "epoch 73/500\n",
      "1/15, train_loss: 0.7888\n",
      "2/15, train_loss: 1.0287\n",
      "3/15, train_loss: 0.7700\n",
      "4/15, train_loss: 0.8402\n",
      "5/15, train_loss: 0.8802\n",
      "6/15, train_loss: 0.7788\n",
      "7/15, train_loss: 1.0800\n",
      "8/15, train_loss: 0.8501\n",
      "9/15, train_loss: 1.1032\n",
      "10/15, train_loss: 1.0246\n",
      "11/15, train_loss: 0.8283\n",
      "12/15, train_loss: 1.3948\n",
      "13/15, train_loss: 1.1298\n",
      "14/15, train_loss: 0.9803\n",
      "15/15, train_loss: 0.8769\n",
      "16/15, train_loss: 0.7903\n",
      "epoch 73 average loss: 0.9465\n",
      "----------\n",
      "epoch 74/500\n",
      "1/15, train_loss: 1.0014\n",
      "2/15, train_loss: 0.8054\n",
      "3/15, train_loss: 0.8558\n",
      "4/15, train_loss: 1.3800\n",
      "5/15, train_loss: 0.7909\n",
      "6/15, train_loss: 0.7917\n",
      "7/15, train_loss: 0.9948\n",
      "8/15, train_loss: 0.7938\n",
      "9/15, train_loss: 0.7977\n",
      "10/15, train_loss: 0.9975\n",
      "11/15, train_loss: 1.0722\n",
      "12/15, train_loss: 0.7745\n",
      "13/15, train_loss: 0.8892\n",
      "14/15, train_loss: 0.8619\n",
      "15/15, train_loss: 0.7514\n",
      "16/15, train_loss: 1.1685\n",
      "epoch 74 average loss: 0.9204\n",
      "----------\n",
      "epoch 75/500\n",
      "1/15, train_loss: 0.8835\n",
      "2/15, train_loss: 1.2193\n",
      "3/15, train_loss: 0.8965\n",
      "4/15, train_loss: 0.7083\n",
      "5/15, train_loss: 0.9775\n",
      "6/15, train_loss: 0.8073\n",
      "7/15, train_loss: 0.8527\n",
      "8/15, train_loss: 0.9763\n",
      "9/15, train_loss: 0.7534\n",
      "10/15, train_loss: 1.0198\n",
      "11/15, train_loss: 0.8973\n",
      "12/15, train_loss: 0.7675\n",
      "13/15, train_loss: 1.0626\n",
      "14/15, train_loss: 0.7480\n",
      "15/15, train_loss: 0.8638\n",
      "16/15, train_loss: 1.0107\n",
      "epoch 75 average loss: 0.9028\n",
      "----------\n",
      "epoch 76/500\n",
      "1/15, train_loss: 0.7704\n",
      "2/15, train_loss: 0.9132\n",
      "3/15, train_loss: 0.7745\n",
      "4/15, train_loss: 1.0186\n",
      "5/15, train_loss: 0.7348\n",
      "6/15, train_loss: 0.7529\n",
      "7/15, train_loss: 0.8493\n",
      "8/15, train_loss: 0.9430\n",
      "9/15, train_loss: 0.9720\n",
      "10/15, train_loss: 0.8340\n",
      "11/15, train_loss: 0.8265\n",
      "12/15, train_loss: 0.8740\n",
      "13/15, train_loss: 0.9119\n",
      "14/15, train_loss: 1.0327\n",
      "15/15, train_loss: 0.7994\n",
      "16/15, train_loss: 0.9646\n",
      "epoch 76 average loss: 0.8732\n",
      "----------\n",
      "epoch 77/500\n",
      "1/15, train_loss: 1.0229\n",
      "2/15, train_loss: 0.7713\n",
      "3/15, train_loss: 1.4776\n",
      "4/15, train_loss: 0.8192\n",
      "5/15, train_loss: 0.7275\n",
      "6/15, train_loss: 0.8835\n",
      "7/15, train_loss: 0.7174\n",
      "8/15, train_loss: 1.4046\n",
      "9/15, train_loss: 0.8758\n",
      "10/15, train_loss: 0.7217\n",
      "11/15, train_loss: 0.9611\n",
      "12/15, train_loss: 0.8347\n",
      "13/15, train_loss: 0.8166\n",
      "14/15, train_loss: 0.7297\n",
      "15/15, train_loss: 0.9321\n",
      "16/15, train_loss: 1.4495\n",
      "epoch 77 average loss: 0.9466\n",
      "----------\n",
      "epoch 78/500\n",
      "1/15, train_loss: 0.9370\n",
      "2/15, train_loss: 1.3262\n",
      "3/15, train_loss: 0.7698\n",
      "4/15, train_loss: 1.2991\n",
      "5/15, train_loss: 0.9290\n",
      "6/15, train_loss: 0.6956\n",
      "7/15, train_loss: 0.8642\n",
      "8/15, train_loss: 0.8774\n",
      "9/15, train_loss: 0.8417\n",
      "10/15, train_loss: 0.9975\n",
      "11/15, train_loss: 0.6868\n",
      "12/15, train_loss: 0.7831\n",
      "13/15, train_loss: 0.7591\n",
      "14/15, train_loss: 0.9084\n",
      "15/15, train_loss: 0.8278\n",
      "16/15, train_loss: 1.1104\n",
      "epoch 78 average loss: 0.9133\n",
      "----------\n",
      "epoch 79/500\n",
      "1/15, train_loss: 0.7775\n",
      "2/15, train_loss: 0.8305\n",
      "3/15, train_loss: 1.0640\n",
      "4/15, train_loss: 0.7724\n",
      "5/15, train_loss: 0.8016\n",
      "6/15, train_loss: 0.8467\n",
      "7/15, train_loss: 0.7560\n",
      "8/15, train_loss: 0.8575\n",
      "9/15, train_loss: 0.9531\n",
      "10/15, train_loss: 0.9452\n",
      "11/15, train_loss: 0.7572\n",
      "12/15, train_loss: 0.6949\n",
      "13/15, train_loss: 1.0784\n",
      "14/15, train_loss: 0.8405\n",
      "15/15, train_loss: 0.8416\n",
      "16/15, train_loss: 1.4963\n",
      "epoch 79 average loss: 0.8946\n",
      "----------\n",
      "epoch 80/500\n",
      "1/15, train_loss: 0.6765\n",
      "2/15, train_loss: 1.3066\n",
      "3/15, train_loss: 0.9717\n",
      "4/15, train_loss: 0.7819\n",
      "5/15, train_loss: 0.8285\n",
      "6/15, train_loss: 1.0181\n",
      "7/15, train_loss: 0.6787\n",
      "8/15, train_loss: 0.9649\n",
      "9/15, train_loss: 0.7987\n",
      "10/15, train_loss: 0.7526\n",
      "11/15, train_loss: 0.7893\n",
      "12/15, train_loss: 0.7414\n",
      "13/15, train_loss: 0.9787\n",
      "14/15, train_loss: 0.7200\n",
      "15/15, train_loss: 1.1236\n",
      "16/15, train_loss: 1.0540\n",
      "epoch 80 average loss: 0.8866\n",
      "----------\n",
      "epoch 81/500\n",
      "1/15, train_loss: 0.6949\n",
      "2/15, train_loss: 1.0085\n",
      "3/15, train_loss: 0.7661\n",
      "4/15, train_loss: 1.2779\n",
      "5/15, train_loss: 1.0480\n",
      "6/15, train_loss: 0.6819\n",
      "7/15, train_loss: 0.7197\n",
      "8/15, train_loss: 0.8067\n",
      "9/15, train_loss: 0.8785\n",
      "10/15, train_loss: 0.8551\n",
      "11/15, train_loss: 0.6487\n",
      "12/15, train_loss: 0.9506\n",
      "13/15, train_loss: 0.7551\n",
      "14/15, train_loss: 0.9399\n",
      "15/15, train_loss: 0.8137\n",
      "16/15, train_loss: 0.9749\n",
      "epoch 81 average loss: 0.8638\n",
      "----------\n",
      "epoch 82/500\n",
      "1/15, train_loss: 0.8670\n",
      "2/15, train_loss: 1.1151\n",
      "3/15, train_loss: 1.1363\n",
      "4/15, train_loss: 0.9169\n",
      "5/15, train_loss: 1.0525\n",
      "6/15, train_loss: 0.7988\n",
      "7/15, train_loss: 0.8750\n",
      "8/15, train_loss: 0.8000\n",
      "9/15, train_loss: 0.7539\n",
      "10/15, train_loss: 0.7349\n",
      "11/15, train_loss: 0.7961\n",
      "12/15, train_loss: 0.6871\n",
      "13/15, train_loss: 0.8435\n",
      "14/15, train_loss: 0.8369\n",
      "15/15, train_loss: 1.0105\n",
      "16/15, train_loss: 1.0890\n",
      "epoch 82 average loss: 0.8946\n",
      "----------\n",
      "epoch 83/500\n",
      "1/15, train_loss: 0.8967\n",
      "2/15, train_loss: 0.8144\n",
      "3/15, train_loss: 0.8707\n",
      "4/15, train_loss: 0.8872\n",
      "5/15, train_loss: 0.8640\n",
      "6/15, train_loss: 0.6826\n",
      "7/15, train_loss: 1.0407\n",
      "8/15, train_loss: 0.7345\n",
      "9/15, train_loss: 0.6891\n",
      "10/15, train_loss: 2.5306\n",
      "11/15, train_loss: 0.8420\n",
      "12/15, train_loss: 1.2267\n",
      "13/15, train_loss: 0.8921\n",
      "14/15, train_loss: 0.7913\n",
      "15/15, train_loss: 0.7496\n",
      "16/15, train_loss: 0.8300\n",
      "epoch 83 average loss: 0.9589\n",
      "----------\n",
      "epoch 84/500\n",
      "1/15, train_loss: 0.7339\n",
      "2/15, train_loss: 0.7229\n",
      "3/15, train_loss: 0.7699\n",
      "4/15, train_loss: 1.0054\n",
      "5/15, train_loss: 1.0626\n",
      "6/15, train_loss: 0.7970\n",
      "7/15, train_loss: 0.8610\n",
      "8/15, train_loss: 0.6398\n",
      "9/15, train_loss: 0.8096\n",
      "10/15, train_loss: 0.7970\n",
      "11/15, train_loss: 0.9316\n",
      "12/15, train_loss: 0.8404\n",
      "13/15, train_loss: 0.7512\n",
      "14/15, train_loss: 0.9130\n",
      "15/15, train_loss: 0.8227\n",
      "16/15, train_loss: 1.3275\n",
      "epoch 84 average loss: 0.8616\n",
      "----------\n",
      "epoch 85/500\n",
      "1/15, train_loss: 0.7848\n",
      "2/15, train_loss: 0.6622\n",
      "3/15, train_loss: 1.0391\n",
      "4/15, train_loss: 0.7200\n",
      "5/15, train_loss: 0.8826\n",
      "6/15, train_loss: 0.7491\n",
      "7/15, train_loss: 1.0295\n",
      "8/15, train_loss: 0.8954\n",
      "9/15, train_loss: 0.6800\n",
      "10/15, train_loss: 0.6977\n",
      "11/15, train_loss: 0.6936\n",
      "12/15, train_loss: 0.6488\n",
      "13/15, train_loss: 0.7824\n",
      "14/15, train_loss: 0.7842\n",
      "15/15, train_loss: 0.7928\n",
      "16/15, train_loss: 0.9605\n",
      "epoch 85 average loss: 0.8002\n",
      "----------\n",
      "epoch 86/500\n",
      "1/15, train_loss: 0.6605\n",
      "2/15, train_loss: 0.7911\n",
      "3/15, train_loss: 0.8518\n",
      "4/15, train_loss: 1.1231\n",
      "5/15, train_loss: 0.6668\n",
      "6/15, train_loss: 0.6646\n",
      "7/15, train_loss: 0.7786\n",
      "8/15, train_loss: 0.6791\n",
      "9/15, train_loss: 0.7632\n",
      "10/15, train_loss: 0.6217\n",
      "11/15, train_loss: 0.7068\n",
      "12/15, train_loss: 0.9144\n",
      "13/15, train_loss: 1.2827\n",
      "14/15, train_loss: 1.2709\n",
      "15/15, train_loss: 0.8323\n",
      "16/15, train_loss: 0.5789\n",
      "epoch 86 average loss: 0.8241\n",
      "----------\n",
      "epoch 87/500\n",
      "1/15, train_loss: 0.7627\n",
      "2/15, train_loss: 0.7272\n",
      "3/15, train_loss: 0.9497\n",
      "4/15, train_loss: 0.8644\n",
      "5/15, train_loss: 0.8466\n",
      "6/15, train_loss: 1.0876\n",
      "7/15, train_loss: 0.8018\n",
      "8/15, train_loss: 0.7757\n",
      "9/15, train_loss: 1.1832\n",
      "10/15, train_loss: 0.6734\n",
      "11/15, train_loss: 0.9431\n",
      "12/15, train_loss: 0.6334\n",
      "13/15, train_loss: 1.6156\n",
      "14/15, train_loss: 0.6620\n",
      "15/15, train_loss: 0.8649\n",
      "16/15, train_loss: 0.8524\n",
      "epoch 87 average loss: 0.8902\n",
      "----------\n",
      "epoch 88/500\n",
      "1/15, train_loss: 0.6682\n",
      "2/15, train_loss: 0.6837\n",
      "3/15, train_loss: 0.7008\n",
      "4/15, train_loss: 0.6557\n",
      "5/15, train_loss: 0.8812\n",
      "6/15, train_loss: 0.8301\n",
      "7/15, train_loss: 0.8593\n",
      "8/15, train_loss: 2.3394\n",
      "9/15, train_loss: 0.8530\n",
      "10/15, train_loss: 0.8420\n",
      "11/15, train_loss: 0.7170\n",
      "12/15, train_loss: 1.0826\n",
      "13/15, train_loss: 0.6734\n",
      "14/15, train_loss: 0.9389\n",
      "15/15, train_loss: 1.0295\n",
      "16/15, train_loss: 0.6065\n",
      "epoch 88 average loss: 0.8976\n",
      "----------\n",
      "epoch 89/500\n",
      "1/15, train_loss: 0.7234\n",
      "2/15, train_loss: 0.7326\n",
      "3/15, train_loss: 0.6415\n",
      "4/15, train_loss: 0.8057\n",
      "5/15, train_loss: 1.3102\n",
      "6/15, train_loss: 0.6821\n",
      "7/15, train_loss: 1.1392\n",
      "8/15, train_loss: 0.7768\n",
      "9/15, train_loss: 0.7388\n",
      "10/15, train_loss: 0.7490\n",
      "11/15, train_loss: 0.6955\n",
      "12/15, train_loss: 1.1902\n",
      "13/15, train_loss: 0.5913\n",
      "14/15, train_loss: 0.8571\n",
      "15/15, train_loss: 0.6442\n",
      "16/15, train_loss: 1.5763\n",
      "epoch 89 average loss: 0.8659\n",
      "----------\n",
      "epoch 90/500\n",
      "1/15, train_loss: 0.8283\n",
      "2/15, train_loss: 0.7164\n",
      "3/15, train_loss: 0.7287\n",
      "4/15, train_loss: 0.6301\n",
      "5/15, train_loss: 0.6791\n",
      "6/15, train_loss: 0.7046\n",
      "7/15, train_loss: 0.6255\n",
      "8/15, train_loss: 0.8350\n",
      "9/15, train_loss: 0.7470\n",
      "10/15, train_loss: 0.7558\n",
      "11/15, train_loss: 0.8850\n",
      "12/15, train_loss: 0.6553\n",
      "13/15, train_loss: 0.8650\n",
      "14/15, train_loss: 0.6867\n",
      "15/15, train_loss: 0.7048\n",
      "16/15, train_loss: 1.1551\n",
      "epoch 90 average loss: 0.7626\n",
      "----------\n",
      "epoch 91/500\n",
      "1/15, train_loss: 0.7491\n",
      "2/15, train_loss: 0.7184\n",
      "3/15, train_loss: 0.7147\n",
      "4/15, train_loss: 0.6443\n",
      "5/15, train_loss: 0.7057\n",
      "6/15, train_loss: 0.5780\n",
      "7/15, train_loss: 0.7184\n",
      "8/15, train_loss: 0.7100\n",
      "9/15, train_loss: 1.0239\n",
      "10/15, train_loss: 1.3487\n",
      "11/15, train_loss: 0.7065\n",
      "12/15, train_loss: 0.7979\n",
      "13/15, train_loss: 0.8076\n",
      "14/15, train_loss: 0.5991\n",
      "15/15, train_loss: 0.5979\n",
      "16/15, train_loss: 1.1366\n",
      "epoch 91 average loss: 0.7848\n",
      "----------\n",
      "epoch 92/500\n",
      "1/15, train_loss: 0.7917\n",
      "2/15, train_loss: 0.5976\n",
      "3/15, train_loss: 0.7505\n",
      "4/15, train_loss: 0.7377\n",
      "5/15, train_loss: 0.6880\n",
      "6/15, train_loss: 0.8607\n",
      "7/15, train_loss: 0.7056\n",
      "8/15, train_loss: 0.8725\n",
      "9/15, train_loss: 0.6980\n",
      "10/15, train_loss: 0.6237\n",
      "11/15, train_loss: 1.0342\n",
      "12/15, train_loss: 0.6495\n",
      "13/15, train_loss: 1.2630\n",
      "14/15, train_loss: 0.6527\n",
      "15/15, train_loss: 0.5859\n",
      "16/15, train_loss: 1.2770\n",
      "epoch 92 average loss: 0.7993\n",
      "----------\n",
      "epoch 93/500\n",
      "1/15, train_loss: 0.6342\n",
      "2/15, train_loss: 0.8285\n",
      "3/15, train_loss: 0.8016\n",
      "4/15, train_loss: 0.6520\n",
      "5/15, train_loss: 0.6555\n",
      "6/15, train_loss: 0.6512\n",
      "7/15, train_loss: 0.9795\n",
      "8/15, train_loss: 0.8050\n",
      "9/15, train_loss: 0.7575\n",
      "10/15, train_loss: 0.7718\n",
      "11/15, train_loss: 0.8087\n",
      "12/15, train_loss: 0.6486\n",
      "13/15, train_loss: 0.6065\n",
      "14/15, train_loss: 0.8967\n",
      "15/15, train_loss: 0.7388\n",
      "16/15, train_loss: 0.7805\n",
      "epoch 93 average loss: 0.7511\n",
      "----------\n",
      "epoch 94/500\n",
      "1/15, train_loss: 0.7535\n",
      "2/15, train_loss: 0.7305\n",
      "3/15, train_loss: 0.7650\n",
      "4/15, train_loss: 0.7922\n",
      "5/15, train_loss: 0.6003\n",
      "6/15, train_loss: 0.8343\n",
      "7/15, train_loss: 0.6189\n",
      "8/15, train_loss: 0.6023\n",
      "9/15, train_loss: 0.9145\n",
      "10/15, train_loss: 1.2719\n",
      "11/15, train_loss: 0.7106\n",
      "12/15, train_loss: 0.7236\n",
      "13/15, train_loss: 0.5638\n",
      "14/15, train_loss: 0.7013\n",
      "15/15, train_loss: 0.7739\n",
      "16/15, train_loss: 1.1804\n",
      "epoch 94 average loss: 0.7836\n",
      "----------\n",
      "epoch 95/500\n",
      "1/15, train_loss: 1.1073\n",
      "2/15, train_loss: 0.5918\n",
      "3/15, train_loss: 0.7316\n",
      "4/15, train_loss: 0.6361\n",
      "5/15, train_loss: 0.9972\n",
      "6/15, train_loss: 1.1046\n",
      "7/15, train_loss: 0.6532\n",
      "8/15, train_loss: 0.8201\n",
      "9/15, train_loss: 0.8009\n",
      "10/15, train_loss: 0.8669\n",
      "11/15, train_loss: 0.6968\n",
      "12/15, train_loss: 0.6115\n",
      "13/15, train_loss: 0.8646\n",
      "14/15, train_loss: 1.2805\n",
      "15/15, train_loss: 0.6199\n",
      "16/15, train_loss: 0.6775\n",
      "epoch 95 average loss: 0.8163\n",
      "----------\n",
      "epoch 96/500\n",
      "1/15, train_loss: 0.6403\n",
      "2/15, train_loss: 0.9159\n",
      "3/15, train_loss: 0.7366\n",
      "4/15, train_loss: 0.8404\n",
      "5/15, train_loss: 0.8268\n",
      "6/15, train_loss: 0.6397\n",
      "7/15, train_loss: 0.7772\n",
      "8/15, train_loss: 0.7976\n",
      "9/15, train_loss: 0.5955\n",
      "10/15, train_loss: 0.6162\n",
      "11/15, train_loss: 0.6564\n",
      "12/15, train_loss: 0.7459\n",
      "13/15, train_loss: 0.6480\n",
      "14/15, train_loss: 0.7590\n",
      "15/15, train_loss: 0.6843\n",
      "16/15, train_loss: 0.6619\n",
      "epoch 96 average loss: 0.7214\n",
      "----------\n",
      "epoch 97/500\n",
      "1/15, train_loss: 0.7420\n",
      "2/15, train_loss: 0.8510\n",
      "3/15, train_loss: 0.7439\n",
      "4/15, train_loss: 0.7860\n",
      "5/15, train_loss: 0.6570\n",
      "6/15, train_loss: 0.6881\n",
      "7/15, train_loss: 0.8378\n",
      "8/15, train_loss: 0.7200\n",
      "9/15, train_loss: 0.6121\n",
      "10/15, train_loss: 0.5658\n",
      "11/15, train_loss: 0.7661\n",
      "12/15, train_loss: 0.5719\n",
      "13/15, train_loss: 0.5994\n",
      "14/15, train_loss: 1.3497\n",
      "15/15, train_loss: 0.6164\n",
      "16/15, train_loss: 0.9500\n",
      "epoch 97 average loss: 0.7536\n",
      "----------\n",
      "epoch 98/500\n",
      "1/15, train_loss: 0.6473\n",
      "2/15, train_loss: 2.7166\n",
      "3/15, train_loss: 0.6162\n",
      "4/15, train_loss: 0.5953\n",
      "5/15, train_loss: 0.7901\n",
      "6/15, train_loss: 0.8124\n",
      "7/15, train_loss: 0.7527\n",
      "8/15, train_loss: 0.7780\n",
      "9/15, train_loss: 0.6315\n",
      "10/15, train_loss: 0.7254\n",
      "11/15, train_loss: 0.6755\n",
      "12/15, train_loss: 0.7226\n",
      "13/15, train_loss: 0.7320\n",
      "14/15, train_loss: 0.6658\n",
      "15/15, train_loss: 0.7715\n",
      "16/15, train_loss: 0.7126\n",
      "epoch 98 average loss: 0.8341\n",
      "----------\n",
      "epoch 99/500\n",
      "1/15, train_loss: 0.6882\n",
      "2/15, train_loss: 0.6133\n",
      "3/15, train_loss: 0.5583\n",
      "4/15, train_loss: 0.6465\n",
      "5/15, train_loss: 0.6824\n",
      "6/15, train_loss: 1.3809\n",
      "7/15, train_loss: 0.7461\n",
      "8/15, train_loss: 0.6125\n",
      "9/15, train_loss: 0.7413\n",
      "10/15, train_loss: 0.6660\n",
      "11/15, train_loss: 0.6459\n",
      "12/15, train_loss: 0.6260\n",
      "13/15, train_loss: 0.6141\n",
      "14/15, train_loss: 0.8061\n",
      "15/15, train_loss: 0.5906\n",
      "16/15, train_loss: 0.6100\n",
      "epoch 99 average loss: 0.7018\n",
      "----------\n",
      "epoch 100/500\n",
      "1/15, train_loss: 0.5538\n",
      "2/15, train_loss: 0.5962\n",
      "3/15, train_loss: 0.7193\n",
      "4/15, train_loss: 0.7749\n",
      "5/15, train_loss: 0.8490\n",
      "6/15, train_loss: 0.5866\n",
      "7/15, train_loss: 0.5695\n",
      "8/15, train_loss: 0.7122\n",
      "9/15, train_loss: 0.8345\n",
      "10/15, train_loss: 0.6364\n",
      "11/15, train_loss: 0.6435\n",
      "12/15, train_loss: 0.5643\n",
      "13/15, train_loss: 0.8541\n",
      "14/15, train_loss: 1.4892\n",
      "15/15, train_loss: 0.6780\n",
      "16/15, train_loss: 0.7246\n",
      "epoch 100 average loss: 0.7366\n",
      "----------\n",
      "epoch 101/500\n",
      "1/15, train_loss: 0.6189\n",
      "2/15, train_loss: 0.7433\n",
      "3/15, train_loss: 0.5624\n",
      "4/15, train_loss: 0.6292\n",
      "5/15, train_loss: 0.6628\n",
      "6/15, train_loss: 1.2709\n",
      "7/15, train_loss: 0.7915\n",
      "8/15, train_loss: 0.6849\n",
      "9/15, train_loss: 0.9451\n",
      "10/15, train_loss: 0.5994\n",
      "11/15, train_loss: 0.5735\n",
      "12/15, train_loss: 0.9419\n",
      "13/15, train_loss: 0.5657\n",
      "14/15, train_loss: 0.6858\n",
      "15/15, train_loss: 0.8073\n",
      "16/15, train_loss: 0.7232\n",
      "epoch 101 average loss: 0.7379\n",
      "----------\n",
      "epoch 102/500\n",
      "1/15, train_loss: 0.9692\n",
      "2/15, train_loss: 1.1627\n",
      "3/15, train_loss: 1.3308\n",
      "4/15, train_loss: 0.8495\n",
      "5/15, train_loss: 0.5458\n",
      "6/15, train_loss: 1.2803\n",
      "7/15, train_loss: 0.5919\n",
      "8/15, train_loss: 0.6390\n",
      "9/15, train_loss: 0.5772\n",
      "10/15, train_loss: 0.5445\n",
      "11/15, train_loss: 0.9352\n",
      "12/15, train_loss: 0.6627\n",
      "13/15, train_loss: 0.8126\n",
      "14/15, train_loss: 0.6372\n",
      "15/15, train_loss: 0.6115\n",
      "16/15, train_loss: 0.9470\n",
      "epoch 102 average loss: 0.8186\n",
      "----------\n",
      "epoch 103/500\n",
      "1/15, train_loss: 0.6912\n",
      "2/15, train_loss: 0.6752\n",
      "3/15, train_loss: 0.5416\n",
      "4/15, train_loss: 0.7278\n",
      "5/15, train_loss: 1.0553\n",
      "6/15, train_loss: 0.5269\n",
      "7/15, train_loss: 0.7842\n",
      "8/15, train_loss: 0.5916\n",
      "9/15, train_loss: 0.6460\n",
      "10/15, train_loss: 0.6333\n",
      "11/15, train_loss: 0.6060\n",
      "12/15, train_loss: 0.5991\n",
      "13/15, train_loss: 0.5483\n",
      "14/15, train_loss: 0.5756\n",
      "15/15, train_loss: 0.6603\n",
      "16/15, train_loss: 0.8120\n",
      "epoch 103 average loss: 0.6671\n",
      "----------\n",
      "epoch 104/500\n",
      "1/15, train_loss: 0.6643\n",
      "2/15, train_loss: 0.5764\n",
      "3/15, train_loss: 0.5583\n",
      "4/15, train_loss: 0.7257\n",
      "5/15, train_loss: 0.8109\n",
      "6/15, train_loss: 0.9757\n",
      "7/15, train_loss: 0.5519\n",
      "8/15, train_loss: 0.5232\n",
      "9/15, train_loss: 0.6349\n",
      "10/15, train_loss: 0.5841\n",
      "11/15, train_loss: 0.7261\n",
      "12/15, train_loss: 0.5441\n",
      "13/15, train_loss: 0.6041\n",
      "14/15, train_loss: 0.6435\n",
      "15/15, train_loss: 0.7266\n",
      "16/15, train_loss: 0.8819\n",
      "epoch 104 average loss: 0.6707\n",
      "----------\n",
      "epoch 105/500\n",
      "1/15, train_loss: 0.6100\n",
      "2/15, train_loss: 0.7284\n",
      "3/15, train_loss: 0.7286\n",
      "4/15, train_loss: 1.0390\n",
      "5/15, train_loss: 0.5626\n",
      "6/15, train_loss: 0.6292\n",
      "7/15, train_loss: 0.7493\n",
      "8/15, train_loss: 0.6569\n",
      "9/15, train_loss: 0.5165\n",
      "10/15, train_loss: 0.8330\n",
      "11/15, train_loss: 0.7739\n",
      "12/15, train_loss: 1.0254\n",
      "13/15, train_loss: 0.5931\n",
      "14/15, train_loss: 0.6708\n",
      "15/15, train_loss: 0.6909\n",
      "16/15, train_loss: 0.9087\n",
      "epoch 105 average loss: 0.7323\n",
      "----------\n",
      "epoch 106/500\n",
      "1/15, train_loss: 0.7762\n",
      "2/15, train_loss: 0.5894\n",
      "3/15, train_loss: 0.6736\n",
      "4/15, train_loss: 0.8643\n",
      "5/15, train_loss: 0.6557\n",
      "6/15, train_loss: 0.7189\n",
      "7/15, train_loss: 0.6773\n",
      "8/15, train_loss: 0.5521\n",
      "9/15, train_loss: 0.7540\n",
      "10/15, train_loss: 1.5134\n",
      "11/15, train_loss: 0.8440\n",
      "12/15, train_loss: 0.5537\n",
      "13/15, train_loss: 0.5720\n",
      "14/15, train_loss: 0.6062\n",
      "15/15, train_loss: 0.5800\n",
      "16/15, train_loss: 0.7524\n",
      "epoch 106 average loss: 0.7302\n",
      "----------\n",
      "epoch 107/500\n",
      "1/15, train_loss: 1.2060\n",
      "2/15, train_loss: 0.6013\n",
      "3/15, train_loss: 0.6250\n",
      "4/15, train_loss: 0.5830\n",
      "5/15, train_loss: 0.6198\n",
      "6/15, train_loss: 0.7400\n",
      "7/15, train_loss: 0.6753\n",
      "8/15, train_loss: 0.8224\n",
      "9/15, train_loss: 0.6016\n",
      "10/15, train_loss: 0.5461\n",
      "11/15, train_loss: 0.6467\n",
      "12/15, train_loss: 0.5011\n",
      "13/15, train_loss: 0.7188\n",
      "14/15, train_loss: 0.6393\n",
      "15/15, train_loss: 0.6272\n",
      "16/15, train_loss: 0.7442\n",
      "epoch 107 average loss: 0.6811\n",
      "----------\n",
      "epoch 108/500\n",
      "1/15, train_loss: 0.6060\n",
      "2/15, train_loss: 0.9581\n",
      "3/15, train_loss: 0.8484\n",
      "4/15, train_loss: 0.6397\n",
      "5/15, train_loss: 0.7174\n",
      "6/15, train_loss: 0.6121\n",
      "7/15, train_loss: 0.6844\n",
      "8/15, train_loss: 0.5647\n",
      "9/15, train_loss: 0.5969\n",
      "10/15, train_loss: 0.7245\n",
      "11/15, train_loss: 0.6590\n",
      "12/15, train_loss: 0.6305\n",
      "13/15, train_loss: 0.6093\n",
      "14/15, train_loss: 0.7467\n",
      "15/15, train_loss: 0.6042\n",
      "16/15, train_loss: 0.6837\n",
      "epoch 108 average loss: 0.6803\n",
      "----------\n",
      "epoch 109/500\n",
      "1/15, train_loss: 1.0088\n",
      "2/15, train_loss: 0.5265\n",
      "3/15, train_loss: 0.9661\n",
      "4/15, train_loss: 0.6319\n",
      "5/15, train_loss: 0.6565\n",
      "6/15, train_loss: 0.7909\n",
      "7/15, train_loss: 0.8531\n",
      "8/15, train_loss: 0.4928\n",
      "9/15, train_loss: 0.8833\n",
      "10/15, train_loss: 0.5152\n",
      "11/15, train_loss: 0.6086\n",
      "12/15, train_loss: 0.5374\n",
      "13/15, train_loss: 0.5941\n",
      "14/15, train_loss: 0.9933\n",
      "15/15, train_loss: 0.7656\n",
      "16/15, train_loss: 0.8128\n",
      "epoch 109 average loss: 0.7273\n",
      "----------\n",
      "epoch 110/500\n",
      "1/15, train_loss: 0.5792\n",
      "2/15, train_loss: 0.6852\n",
      "3/15, train_loss: 0.5746\n",
      "4/15, train_loss: 0.6830\n",
      "5/15, train_loss: 0.5533\n",
      "6/15, train_loss: 0.6966\n",
      "7/15, train_loss: 0.6194\n",
      "8/15, train_loss: 0.6336\n",
      "9/15, train_loss: 0.5385\n",
      "10/15, train_loss: 0.6932\n",
      "11/15, train_loss: 0.7049\n",
      "12/15, train_loss: 0.5030\n",
      "13/15, train_loss: 0.9260\n",
      "14/15, train_loss: 0.5774\n",
      "15/15, train_loss: 0.7314\n",
      "16/15, train_loss: 0.5470\n",
      "epoch 110 average loss: 0.6404\n",
      "----------\n",
      "epoch 111/500\n",
      "1/15, train_loss: 0.5380\n",
      "2/15, train_loss: 0.5876\n",
      "3/15, train_loss: 0.6271\n",
      "4/15, train_loss: 0.6806\n",
      "5/15, train_loss: 0.6677\n",
      "6/15, train_loss: 1.2426\n",
      "7/15, train_loss: 0.9268\n",
      "8/15, train_loss: 0.6115\n",
      "9/15, train_loss: 0.6683\n",
      "10/15, train_loss: 0.6366\n",
      "11/15, train_loss: 0.6729\n",
      "12/15, train_loss: 0.5479\n",
      "13/15, train_loss: 0.8533\n",
      "14/15, train_loss: 0.6377\n",
      "15/15, train_loss: 2.4729\n",
      "16/15, train_loss: 2.3360\n",
      "epoch 111 average loss: 0.9192\n",
      "----------\n",
      "epoch 112/500\n",
      "1/15, train_loss: 1.3145\n",
      "2/15, train_loss: 0.6764\n",
      "3/15, train_loss: 0.7466\n",
      "4/15, train_loss: 0.6107\n",
      "5/15, train_loss: 0.5014\n",
      "6/15, train_loss: 0.7440\n",
      "7/15, train_loss: 0.5118\n",
      "8/15, train_loss: 0.7474\n",
      "9/15, train_loss: 0.5459\n",
      "10/15, train_loss: 0.5939\n",
      "11/15, train_loss: 0.5654\n",
      "12/15, train_loss: 0.5487\n",
      "13/15, train_loss: 0.6306\n",
      "14/15, train_loss: 0.5059\n",
      "15/15, train_loss: 0.5196\n",
      "16/15, train_loss: 0.8448\n",
      "epoch 112 average loss: 0.6630\n",
      "----------\n",
      "epoch 113/500\n",
      "1/15, train_loss: 2.4496\n",
      "2/15, train_loss: 0.5573\n",
      "3/15, train_loss: 0.5459\n",
      "4/15, train_loss: 0.6174\n",
      "5/15, train_loss: 0.5907\n",
      "6/15, train_loss: 0.5843\n",
      "7/15, train_loss: 0.6631\n",
      "8/15, train_loss: 0.7461\n",
      "9/15, train_loss: 0.6045\n",
      "10/15, train_loss: 0.5805\n",
      "11/15, train_loss: 0.6019\n",
      "12/15, train_loss: 0.5686\n",
      "13/15, train_loss: 0.9129\n",
      "14/15, train_loss: 0.5337\n",
      "15/15, train_loss: 0.8827\n",
      "16/15, train_loss: 2.3998\n",
      "epoch 113 average loss: 0.8649\n",
      "----------\n",
      "epoch 114/500\n",
      "1/15, train_loss: 0.7174\n",
      "2/15, train_loss: 0.5163\n",
      "3/15, train_loss: 0.6117\n",
      "4/15, train_loss: 0.5506\n",
      "5/15, train_loss: 0.6879\n",
      "6/15, train_loss: 0.6332\n",
      "7/15, train_loss: 0.4969\n",
      "8/15, train_loss: 0.5047\n",
      "9/15, train_loss: 1.4235\n",
      "10/15, train_loss: 0.5731\n",
      "11/15, train_loss: 0.6212\n",
      "12/15, train_loss: 0.5366\n",
      "13/15, train_loss: 0.6755\n",
      "14/15, train_loss: 0.5746\n",
      "15/15, train_loss: 0.6228\n",
      "16/15, train_loss: 1.1695\n",
      "epoch 114 average loss: 0.6822\n",
      "----------\n",
      "epoch 115/500\n",
      "1/15, train_loss: 0.6604\n",
      "2/15, train_loss: 0.5473\n",
      "3/15, train_loss: 1.1079\n",
      "4/15, train_loss: 0.6342\n",
      "5/15, train_loss: 0.9353\n",
      "6/15, train_loss: 0.5511\n",
      "7/15, train_loss: 0.6409\n",
      "8/15, train_loss: 0.6958\n",
      "9/15, train_loss: 0.5146\n",
      "10/15, train_loss: 0.6102\n",
      "11/15, train_loss: 2.4421\n",
      "12/15, train_loss: 0.5258\n",
      "13/15, train_loss: 0.6086\n",
      "14/15, train_loss: 0.5782\n",
      "15/15, train_loss: 0.5423\n",
      "16/15, train_loss: 2.4706\n",
      "epoch 115 average loss: 0.8791\n",
      "----------\n",
      "epoch 116/500\n",
      "1/15, train_loss: 0.5009\n",
      "2/15, train_loss: 0.6820\n",
      "3/15, train_loss: 0.6344\n",
      "4/15, train_loss: 0.7799\n",
      "5/15, train_loss: 0.6445\n",
      "6/15, train_loss: 0.5172\n",
      "7/15, train_loss: 0.6549\n",
      "8/15, train_loss: 0.5627\n",
      "9/15, train_loss: 0.7199\n",
      "10/15, train_loss: 0.5189\n",
      "11/15, train_loss: 0.7717\n",
      "12/15, train_loss: 0.6541\n",
      "13/15, train_loss: 0.4964\n",
      "14/15, train_loss: 0.5923\n",
      "15/15, train_loss: 0.5689\n",
      "16/15, train_loss: 0.6662\n",
      "epoch 116 average loss: 0.6228\n",
      "----------\n",
      "epoch 117/500\n",
      "1/15, train_loss: 0.8317\n",
      "2/15, train_loss: 0.5766\n",
      "3/15, train_loss: 0.9210\n",
      "4/15, train_loss: 0.5237\n",
      "5/15, train_loss: 1.2338\n",
      "6/15, train_loss: 0.5569\n",
      "7/15, train_loss: 0.4808\n",
      "8/15, train_loss: 0.6140\n",
      "9/15, train_loss: 0.7310\n",
      "10/15, train_loss: 0.5714\n",
      "11/15, train_loss: 0.7418\n",
      "12/15, train_loss: 0.5180\n",
      "13/15, train_loss: 0.6016\n",
      "14/15, train_loss: 0.5120\n",
      "15/15, train_loss: 0.5867\n",
      "16/15, train_loss: 0.5517\n",
      "epoch 117 average loss: 0.6596\n",
      "----------\n",
      "epoch 118/500\n",
      "1/15, train_loss: 0.5380\n",
      "2/15, train_loss: 0.6857\n",
      "3/15, train_loss: 0.5336\n",
      "4/15, train_loss: 0.5629\n",
      "5/15, train_loss: 0.6647\n",
      "6/15, train_loss: 0.5387\n",
      "7/15, train_loss: 1.1064\n",
      "8/15, train_loss: 0.5602\n",
      "9/15, train_loss: 0.6506\n",
      "10/15, train_loss: 0.6008\n",
      "11/15, train_loss: 0.5477\n",
      "12/15, train_loss: 1.4281\n",
      "13/15, train_loss: 0.5652\n",
      "14/15, train_loss: 0.6804\n",
      "15/15, train_loss: 0.6357\n",
      "16/15, train_loss: 0.5023\n",
      "epoch 118 average loss: 0.6751\n",
      "----------\n",
      "epoch 119/500\n",
      "1/15, train_loss: 0.6800\n",
      "2/15, train_loss: 0.5863\n",
      "3/15, train_loss: 0.6826\n",
      "4/15, train_loss: 0.5362\n",
      "5/15, train_loss: 0.5286\n",
      "6/15, train_loss: 0.7062\n",
      "7/15, train_loss: 0.6296\n",
      "8/15, train_loss: 0.6821\n",
      "9/15, train_loss: 0.7980\n",
      "10/15, train_loss: 0.5009\n",
      "11/15, train_loss: 0.5454\n",
      "12/15, train_loss: 0.5839\n",
      "13/15, train_loss: 0.8822\n",
      "14/15, train_loss: 0.5751\n",
      "15/15, train_loss: 0.5728\n",
      "16/15, train_loss: 2.5983\n",
      "epoch 119 average loss: 0.7555\n",
      "----------\n",
      "epoch 120/500\n",
      "1/15, train_loss: 0.5763\n",
      "2/15, train_loss: 0.5686\n",
      "3/15, train_loss: 0.5679\n",
      "4/15, train_loss: 0.7934\n",
      "5/15, train_loss: 0.5491\n",
      "6/15, train_loss: 0.5292\n",
      "7/15, train_loss: 0.5218\n",
      "8/15, train_loss: 0.9831\n",
      "9/15, train_loss: 0.6191\n",
      "10/15, train_loss: 0.6737\n",
      "11/15, train_loss: 0.5586\n",
      "12/15, train_loss: 0.4647\n",
      "13/15, train_loss: 0.6091\n",
      "14/15, train_loss: 0.5189\n",
      "15/15, train_loss: 0.7215\n",
      "16/15, train_loss: 0.9903\n",
      "epoch 120 average loss: 0.6403\n",
      "----------\n",
      "epoch 121/500\n",
      "1/15, train_loss: 0.5549\n",
      "2/15, train_loss: 1.9242\n",
      "3/15, train_loss: 1.2173\n",
      "4/15, train_loss: 0.8714\n",
      "5/15, train_loss: 0.6879\n",
      "6/15, train_loss: 0.5256\n",
      "7/15, train_loss: 0.8352\n",
      "8/15, train_loss: 0.5669\n",
      "9/15, train_loss: 0.6000\n",
      "10/15, train_loss: 0.5840\n",
      "11/15, train_loss: 0.4903\n",
      "12/15, train_loss: 0.6778\n",
      "13/15, train_loss: 0.5602\n",
      "14/15, train_loss: 0.8235\n",
      "15/15, train_loss: 0.5576\n",
      "16/15, train_loss: 0.4854\n",
      "epoch 121 average loss: 0.7476\n",
      "----------\n",
      "epoch 122/500\n",
      "1/15, train_loss: 1.4869\n",
      "2/15, train_loss: 0.7330\n",
      "3/15, train_loss: 0.7657\n",
      "4/15, train_loss: 0.6037\n",
      "5/15, train_loss: 0.5446\n",
      "6/15, train_loss: 0.6359\n",
      "7/15, train_loss: 0.5294\n",
      "8/15, train_loss: 0.7285\n",
      "9/15, train_loss: 0.5112\n",
      "10/15, train_loss: 0.6147\n",
      "11/15, train_loss: 0.8374\n",
      "12/15, train_loss: 0.5692\n",
      "13/15, train_loss: 0.6434\n",
      "14/15, train_loss: 1.0567\n",
      "15/15, train_loss: 0.5628\n",
      "16/15, train_loss: 0.5453\n",
      "epoch 122 average loss: 0.7105\n",
      "----------\n",
      "epoch 123/500\n",
      "1/15, train_loss: 0.5031\n",
      "2/15, train_loss: 0.6733\n",
      "3/15, train_loss: 0.4847\n",
      "4/15, train_loss: 0.4613\n",
      "5/15, train_loss: 0.6137\n",
      "6/15, train_loss: 0.4714\n",
      "7/15, train_loss: 0.6289\n",
      "8/15, train_loss: 0.6504\n",
      "9/15, train_loss: 0.6955\n",
      "10/15, train_loss: 0.8085\n",
      "11/15, train_loss: 0.5488\n",
      "12/15, train_loss: 0.7936\n",
      "13/15, train_loss: 1.1907\n",
      "14/15, train_loss: 0.5487\n",
      "15/15, train_loss: 0.9578\n",
      "16/15, train_loss: 0.4926\n",
      "epoch 123 average loss: 0.6577\n",
      "----------\n",
      "epoch 124/500\n",
      "1/15, train_loss: 0.8235\n",
      "2/15, train_loss: 0.5567\n",
      "3/15, train_loss: 0.5321\n",
      "4/15, train_loss: 0.6390\n",
      "5/15, train_loss: 0.6382\n",
      "6/15, train_loss: 0.5439\n",
      "7/15, train_loss: 0.6469\n",
      "8/15, train_loss: 0.5418\n",
      "9/15, train_loss: 0.7760\n",
      "10/15, train_loss: 0.5314\n",
      "11/15, train_loss: 0.6181\n",
      "12/15, train_loss: 0.5548\n",
      "13/15, train_loss: 0.5077\n",
      "14/15, train_loss: 0.5327\n",
      "15/15, train_loss: 0.5044\n",
      "16/15, train_loss: 0.7365\n",
      "epoch 124 average loss: 0.6052\n",
      "----------\n",
      "epoch 125/500\n",
      "1/15, train_loss: 0.5884\n",
      "2/15, train_loss: 0.6102\n",
      "3/15, train_loss: 0.7734\n",
      "4/15, train_loss: 0.4952\n",
      "5/15, train_loss: 0.8947\n",
      "6/15, train_loss: 0.4981\n",
      "7/15, train_loss: 0.5481\n",
      "8/15, train_loss: 0.6123\n",
      "9/15, train_loss: 0.7669\n",
      "10/15, train_loss: 0.6022\n",
      "11/15, train_loss: 2.4620\n",
      "12/15, train_loss: 0.5471\n",
      "13/15, train_loss: 0.6059\n",
      "14/15, train_loss: 0.5518\n",
      "15/15, train_loss: 0.6073\n",
      "16/15, train_loss: 0.5365\n",
      "epoch 125 average loss: 0.7313\n",
      "----------\n",
      "epoch 126/500\n",
      "\n",
      "** Ranger21 update = Warmup complete - lr set to 0.0001\n",
      "\n",
      "1/15, train_loss: 0.5531\n",
      "2/15, train_loss: 0.5592\n",
      "3/15, train_loss: 0.7114\n",
      "4/15, train_loss: 0.5091\n",
      "5/15, train_loss: 0.5086\n",
      "6/15, train_loss: 0.5028\n",
      "7/15, train_loss: 0.6437\n",
      "8/15, train_loss: 0.4586\n",
      "9/15, train_loss: 0.7208\n",
      "10/15, train_loss: 0.5224\n",
      "11/15, train_loss: 0.6621\n",
      "12/15, train_loss: 0.9001\n",
      "13/15, train_loss: 0.5147\n",
      "14/15, train_loss: 0.6431\n",
      "15/15, train_loss: 0.4971\n",
      "16/15, train_loss: 2.5443\n",
      "epoch 126 average loss: 0.7157\n",
      "----------\n",
      "epoch 127/500\n",
      "1/15, train_loss: 0.6106\n",
      "2/15, train_loss: 0.5178\n",
      "3/15, train_loss: 1.0180\n",
      "4/15, train_loss: 0.5821\n",
      "5/15, train_loss: 0.4796\n",
      "6/15, train_loss: 0.5270\n",
      "7/15, train_loss: 0.5864\n",
      "8/15, train_loss: 0.5069\n",
      "9/15, train_loss: 0.5432\n",
      "10/15, train_loss: 0.5063\n",
      "11/15, train_loss: 1.4706\n",
      "12/15, train_loss: 0.6731\n",
      "13/15, train_loss: 0.4991\n",
      "14/15, train_loss: 0.4957\n",
      "15/15, train_loss: 0.6201\n",
      "16/15, train_loss: 2.4642\n",
      "epoch 127 average loss: 0.7563\n",
      "----------\n",
      "epoch 128/500\n",
      "1/15, train_loss: 0.5875\n",
      "2/15, train_loss: 0.7198\n",
      "3/15, train_loss: 0.4310\n",
      "4/15, train_loss: 0.5572\n",
      "5/15, train_loss: 0.6237\n",
      "6/15, train_loss: 0.5106\n",
      "7/15, train_loss: 0.5575\n",
      "8/15, train_loss: 0.4602\n",
      "9/15, train_loss: 0.4942\n",
      "10/15, train_loss: 0.5904\n",
      "11/15, train_loss: 0.4927\n",
      "12/15, train_loss: 0.5595\n",
      "13/15, train_loss: 0.5148\n",
      "14/15, train_loss: 0.5696\n",
      "15/15, train_loss: 0.6775\n",
      "16/15, train_loss: 0.5099\n",
      "epoch 128 average loss: 0.5535\n",
      "----------\n",
      "epoch 129/500\n",
      "1/15, train_loss: 0.7483\n",
      "2/15, train_loss: 0.4917\n",
      "3/15, train_loss: 0.4396\n",
      "4/15, train_loss: 0.5948\n",
      "5/15, train_loss: 0.4708\n",
      "6/15, train_loss: 1.2862\n",
      "7/15, train_loss: 0.4603\n",
      "8/15, train_loss: 2.3554\n",
      "9/15, train_loss: 1.0485\n",
      "10/15, train_loss: 0.4528\n",
      "11/15, train_loss: 0.6464\n",
      "12/15, train_loss: 0.5154\n",
      "13/15, train_loss: 0.5235\n",
      "14/15, train_loss: 1.2395\n",
      "15/15, train_loss: 0.4677\n",
      "16/15, train_loss: 1.0288\n",
      "epoch 129 average loss: 0.7981\n",
      "----------\n",
      "epoch 130/500\n",
      "1/15, train_loss: 0.4742\n",
      "2/15, train_loss: 0.9854\n",
      "3/15, train_loss: 0.8148\n",
      "4/15, train_loss: 0.5723\n",
      "5/15, train_loss: 0.5415\n",
      "6/15, train_loss: 0.4920\n",
      "7/15, train_loss: 0.5351\n",
      "8/15, train_loss: 0.6374\n",
      "9/15, train_loss: 0.5054\n",
      "10/15, train_loss: 0.5327\n",
      "11/15, train_loss: 0.4965\n",
      "12/15, train_loss: 0.5300\n",
      "13/15, train_loss: 0.6295\n",
      "14/15, train_loss: 0.5961\n",
      "15/15, train_loss: 1.1123\n",
      "16/15, train_loss: 0.5930\n",
      "epoch 130 average loss: 0.6280\n",
      "----------\n",
      "epoch 131/500\n",
      "1/15, train_loss: 0.4738\n",
      "2/15, train_loss: 0.5822\n",
      "3/15, train_loss: 0.6454\n",
      "4/15, train_loss: 0.4858\n",
      "5/15, train_loss: 0.4487\n",
      "6/15, train_loss: 0.4928\n",
      "7/15, train_loss: 0.7645\n",
      "8/15, train_loss: 0.4749\n",
      "9/15, train_loss: 0.7533\n",
      "10/15, train_loss: 0.4744\n",
      "11/15, train_loss: 0.5502\n",
      "12/15, train_loss: 0.5023\n",
      "13/15, train_loss: 0.5090\n",
      "14/15, train_loss: 0.6073\n",
      "15/15, train_loss: 0.6234\n",
      "16/15, train_loss: 0.5615\n",
      "epoch 131 average loss: 0.5594\n",
      "----------\n",
      "epoch 132/500\n",
      "1/15, train_loss: 0.7965\n",
      "2/15, train_loss: 0.5400\n",
      "3/15, train_loss: 0.6475\n",
      "4/15, train_loss: 0.4957\n",
      "5/15, train_loss: 0.7178\n",
      "6/15, train_loss: 0.5190\n",
      "7/15, train_loss: 0.4917\n",
      "8/15, train_loss: 0.5729\n",
      "9/15, train_loss: 0.6849\n",
      "10/15, train_loss: 0.4548\n",
      "11/15, train_loss: 0.7359\n",
      "12/15, train_loss: 0.4773\n",
      "13/15, train_loss: 0.5367\n",
      "14/15, train_loss: 0.6115\n",
      "15/15, train_loss: 0.6913\n",
      "16/15, train_loss: 0.8171\n",
      "epoch 132 average loss: 0.6119\n",
      "----------\n",
      "epoch 133/500\n",
      "1/15, train_loss: 0.4643\n",
      "2/15, train_loss: 0.7696\n",
      "3/15, train_loss: 1.3363\n",
      "4/15, train_loss: 0.4554\n",
      "5/15, train_loss: 0.9060\n",
      "6/15, train_loss: 0.5450\n",
      "7/15, train_loss: 0.6245\n",
      "8/15, train_loss: 0.5675\n",
      "9/15, train_loss: 0.4910\n",
      "10/15, train_loss: 0.5941\n",
      "11/15, train_loss: 0.9254\n",
      "12/15, train_loss: 0.5594\n",
      "13/15, train_loss: 0.5056\n",
      "14/15, train_loss: 0.5445\n",
      "15/15, train_loss: 0.6025\n",
      "16/15, train_loss: 0.6534\n",
      "epoch 133 average loss: 0.6590\n",
      "----------\n",
      "epoch 134/500\n",
      "1/15, train_loss: 0.5477\n",
      "2/15, train_loss: 0.5225\n",
      "3/15, train_loss: 0.4798\n",
      "4/15, train_loss: 0.5692\n",
      "5/15, train_loss: 0.5362\n",
      "6/15, train_loss: 0.6960\n",
      "7/15, train_loss: 0.5903\n",
      "8/15, train_loss: 1.0643\n",
      "9/15, train_loss: 0.5655\n",
      "10/15, train_loss: 0.4839\n",
      "11/15, train_loss: 0.6069\n",
      "12/15, train_loss: 0.5035\n",
      "13/15, train_loss: 0.6330\n",
      "14/15, train_loss: 0.7525\n",
      "15/15, train_loss: 0.6093\n",
      "16/15, train_loss: 0.7863\n",
      "epoch 134 average loss: 0.6217\n",
      "----------\n",
      "epoch 135/500\n",
      "1/15, train_loss: 0.4701\n",
      "2/15, train_loss: 0.4809\n",
      "3/15, train_loss: 0.6378\n",
      "4/15, train_loss: 0.6373\n",
      "5/15, train_loss: 0.5797\n",
      "6/15, train_loss: 0.6217\n",
      "7/15, train_loss: 0.9266\n",
      "8/15, train_loss: 0.6001\n",
      "9/15, train_loss: 0.5157\n",
      "10/15, train_loss: 2.4740\n",
      "11/15, train_loss: 0.4936\n",
      "12/15, train_loss: 0.5293\n",
      "13/15, train_loss: 0.6481\n",
      "14/15, train_loss: 0.5376\n",
      "15/15, train_loss: 0.4973\n",
      "16/15, train_loss: 0.7367\n",
      "epoch 135 average loss: 0.7117\n",
      "----------\n",
      "epoch 136/500\n",
      "1/15, train_loss: 0.7116\n",
      "2/15, train_loss: 0.5702\n",
      "3/15, train_loss: 0.5860\n",
      "4/15, train_loss: 0.8159\n",
      "5/15, train_loss: 0.4676\n",
      "6/15, train_loss: 0.5008\n",
      "7/15, train_loss: 0.5017\n",
      "8/15, train_loss: 0.5359\n",
      "9/15, train_loss: 0.5701\n",
      "10/15, train_loss: 0.5236\n",
      "11/15, train_loss: 0.4742\n",
      "12/15, train_loss: 0.6408\n",
      "13/15, train_loss: 0.5138\n",
      "14/15, train_loss: 0.9451\n",
      "15/15, train_loss: 0.4306\n",
      "16/15, train_loss: 2.3818\n",
      "epoch 136 average loss: 0.6981\n",
      "----------\n",
      "epoch 137/500\n",
      "1/15, train_loss: 0.6106\n",
      "2/15, train_loss: 0.7036\n",
      "3/15, train_loss: 0.4748\n",
      "4/15, train_loss: 0.5997\n",
      "5/15, train_loss: 0.4651\n",
      "6/15, train_loss: 0.5108\n",
      "7/15, train_loss: 0.4318\n",
      "8/15, train_loss: 1.2501\n",
      "9/15, train_loss: 0.4347\n",
      "10/15, train_loss: 0.6317\n",
      "11/15, train_loss: 0.6636\n",
      "12/15, train_loss: 0.5916\n",
      "13/15, train_loss: 0.5221\n",
      "14/15, train_loss: 0.5607\n",
      "15/15, train_loss: 0.5763\n",
      "16/15, train_loss: 0.4967\n",
      "epoch 137 average loss: 0.5952\n",
      "----------\n",
      "epoch 138/500\n",
      "1/15, train_loss: 0.6012\n",
      "2/15, train_loss: 0.5630\n",
      "3/15, train_loss: 0.7549\n",
      "4/15, train_loss: 0.4122\n",
      "5/15, train_loss: 0.5024\n",
      "6/15, train_loss: 0.4610\n",
      "7/15, train_loss: 0.8721\n",
      "8/15, train_loss: 0.7588\n",
      "9/15, train_loss: 0.4546\n",
      "10/15, train_loss: 0.4483\n",
      "11/15, train_loss: 0.6259\n",
      "12/15, train_loss: 0.7355\n",
      "13/15, train_loss: 1.0000\n",
      "14/15, train_loss: 0.4755\n",
      "15/15, train_loss: 0.6680\n",
      "16/15, train_loss: 2.4973\n",
      "epoch 138 average loss: 0.7394\n",
      "----------\n",
      "epoch 139/500\n",
      "1/15, train_loss: 0.5255\n",
      "2/15, train_loss: 0.6007\n",
      "3/15, train_loss: 0.4717\n",
      "4/15, train_loss: 0.5996\n",
      "5/15, train_loss: 0.8390\n",
      "6/15, train_loss: 0.8696\n",
      "7/15, train_loss: 0.7308\n",
      "8/15, train_loss: 0.4997\n",
      "9/15, train_loss: 1.1430\n",
      "10/15, train_loss: 0.4651\n",
      "11/15, train_loss: 0.6643\n",
      "12/15, train_loss: 0.4586\n",
      "13/15, train_loss: 0.5192\n",
      "14/15, train_loss: 0.5283\n",
      "15/15, train_loss: 0.4699\n",
      "16/15, train_loss: 0.7536\n",
      "epoch 139 average loss: 0.6337\n",
      "----------\n",
      "epoch 140/500\n",
      "1/15, train_loss: 0.4828\n",
      "2/15, train_loss: 0.4615\n",
      "3/15, train_loss: 0.6126\n",
      "4/15, train_loss: 0.5354\n",
      "5/15, train_loss: 0.5005\n",
      "6/15, train_loss: 0.6310\n",
      "7/15, train_loss: 0.4573\n",
      "8/15, train_loss: 0.5802\n",
      "9/15, train_loss: 1.1064\n",
      "10/15, train_loss: 0.6883\n",
      "11/15, train_loss: 0.5512\n",
      "12/15, train_loss: 0.6414\n",
      "13/15, train_loss: 0.5759\n",
      "14/15, train_loss: 0.5808\n",
      "15/15, train_loss: 0.5221\n",
      "16/15, train_loss: 0.5094\n",
      "epoch 140 average loss: 0.5898\n",
      "----------\n",
      "epoch 141/500\n",
      "1/15, train_loss: 0.4694\n",
      "2/15, train_loss: 0.5195\n",
      "3/15, train_loss: 0.5713\n",
      "4/15, train_loss: 0.6930\n",
      "5/15, train_loss: 0.4769\n",
      "6/15, train_loss: 0.9324\n",
      "7/15, train_loss: 0.6065\n",
      "8/15, train_loss: 0.4681\n",
      "9/15, train_loss: 0.5193\n",
      "10/15, train_loss: 0.4854\n",
      "11/15, train_loss: 0.6751\n",
      "12/15, train_loss: 0.5228\n",
      "13/15, train_loss: 0.6735\n",
      "14/15, train_loss: 0.5551\n",
      "15/15, train_loss: 0.5169\n",
      "16/15, train_loss: 0.4702\n",
      "epoch 141 average loss: 0.5722\n",
      "----------\n",
      "epoch 142/500\n",
      "1/15, train_loss: 0.5225\n",
      "2/15, train_loss: 0.8084\n",
      "3/15, train_loss: 0.7216\n",
      "4/15, train_loss: 0.4816\n",
      "5/15, train_loss: 0.4725\n",
      "6/15, train_loss: 1.1983\n",
      "7/15, train_loss: 0.7558\n",
      "8/15, train_loss: 0.6774\n",
      "9/15, train_loss: 0.4490\n",
      "10/15, train_loss: 0.4979\n",
      "11/15, train_loss: 0.5456\n",
      "12/15, train_loss: 0.5360\n",
      "13/15, train_loss: 0.6723\n",
      "14/15, train_loss: 0.4580\n",
      "15/15, train_loss: 0.5211\n",
      "16/15, train_loss: 0.4448\n",
      "epoch 142 average loss: 0.6102\n",
      "----------\n",
      "epoch 143/500\n",
      "1/15, train_loss: 0.5953\n",
      "2/15, train_loss: 0.6353\n",
      "3/15, train_loss: 0.4658\n",
      "4/15, train_loss: 0.4678\n",
      "5/15, train_loss: 0.4975\n",
      "6/15, train_loss: 0.5624\n",
      "7/15, train_loss: 0.9335\n",
      "8/15, train_loss: 0.4591\n",
      "9/15, train_loss: 0.5471\n",
      "10/15, train_loss: 0.5208\n",
      "11/15, train_loss: 0.4201\n",
      "12/15, train_loss: 0.5067\n",
      "13/15, train_loss: 0.4845\n",
      "14/15, train_loss: 0.4653\n",
      "15/15, train_loss: 0.6162\n",
      "16/15, train_loss: 2.6470\n",
      "epoch 143 average loss: 0.6765\n",
      "----------\n",
      "epoch 144/500\n",
      "1/15, train_loss: 0.6421\n",
      "2/15, train_loss: 0.4715\n",
      "3/15, train_loss: 0.4561\n",
      "4/15, train_loss: 0.4955\n",
      "5/15, train_loss: 0.7726\n",
      "6/15, train_loss: 0.6097\n",
      "7/15, train_loss: 0.4595\n",
      "8/15, train_loss: 0.4729\n",
      "9/15, train_loss: 0.5614\n",
      "10/15, train_loss: 0.4816\n",
      "11/15, train_loss: 0.6213\n",
      "12/15, train_loss: 0.5604\n",
      "13/15, train_loss: 0.5242\n",
      "14/15, train_loss: 0.4510\n",
      "15/15, train_loss: 0.4514\n",
      "16/15, train_loss: 0.6046\n",
      "epoch 144 average loss: 0.5397\n",
      "----------\n",
      "epoch 145/500\n",
      "1/15, train_loss: 0.5068\n",
      "2/15, train_loss: 0.4458\n",
      "3/15, train_loss: 0.5772\n",
      "4/15, train_loss: 0.4534\n",
      "5/15, train_loss: 0.5187\n",
      "6/15, train_loss: 0.4511\n",
      "7/15, train_loss: 0.5591\n",
      "8/15, train_loss: 0.5675\n",
      "9/15, train_loss: 0.5865\n",
      "10/15, train_loss: 0.6548\n",
      "11/15, train_loss: 0.5034\n",
      "12/15, train_loss: 1.0247\n",
      "13/15, train_loss: 0.6018\n",
      "14/15, train_loss: 0.5599\n",
      "15/15, train_loss: 0.4939\n",
      "16/15, train_loss: 2.2699\n",
      "epoch 145 average loss: 0.6734\n",
      "----------\n",
      "epoch 146/500\n",
      "1/15, train_loss: 0.7626\n",
      "2/15, train_loss: 0.5115\n",
      "3/15, train_loss: 0.4378\n",
      "4/15, train_loss: 0.4720\n",
      "5/15, train_loss: 0.4501\n",
      "6/15, train_loss: 0.6523\n",
      "7/15, train_loss: 0.5313\n",
      "8/15, train_loss: 0.5139\n",
      "9/15, train_loss: 0.5079\n",
      "10/15, train_loss: 0.4929\n",
      "11/15, train_loss: 0.4435\n",
      "12/15, train_loss: 0.6111\n",
      "13/15, train_loss: 0.4954\n",
      "14/15, train_loss: 0.5538\n",
      "15/15, train_loss: 0.5437\n",
      "16/15, train_loss: 2.3201\n",
      "epoch 146 average loss: 0.6437\n",
      "----------\n",
      "epoch 147/500\n",
      "1/15, train_loss: 0.5901\n",
      "2/15, train_loss: 0.4869\n",
      "3/15, train_loss: 0.5482\n",
      "4/15, train_loss: 0.4362\n",
      "5/15, train_loss: 0.4940\n",
      "6/15, train_loss: 0.5293\n",
      "7/15, train_loss: 0.4687\n",
      "8/15, train_loss: 0.5199\n",
      "9/15, train_loss: 0.4881\n",
      "10/15, train_loss: 0.5087\n",
      "11/15, train_loss: 0.5702\n",
      "12/15, train_loss: 0.4961\n",
      "13/15, train_loss: 0.5207\n",
      "14/15, train_loss: 0.4779\n",
      "15/15, train_loss: 0.4213\n",
      "16/15, train_loss: 0.6124\n",
      "epoch 147 average loss: 0.5105\n",
      "----------\n",
      "epoch 148/500\n",
      "1/15, train_loss: 0.5299\n",
      "2/15, train_loss: 0.6443\n",
      "3/15, train_loss: 0.6854\n",
      "4/15, train_loss: 0.4753\n",
      "5/15, train_loss: 0.5132\n",
      "6/15, train_loss: 0.4901\n",
      "7/15, train_loss: 0.6482\n",
      "8/15, train_loss: 0.4866\n",
      "9/15, train_loss: 0.7740\n",
      "10/15, train_loss: 0.5577\n",
      "11/15, train_loss: 0.4710\n",
      "12/15, train_loss: 0.5139\n",
      "13/15, train_loss: 0.6220\n",
      "14/15, train_loss: 0.4875\n",
      "15/15, train_loss: 0.6620\n",
      "16/15, train_loss: 2.3981\n",
      "epoch 148 average loss: 0.6849\n",
      "----------\n",
      "epoch 149/500\n",
      "1/15, train_loss: 0.5059\n",
      "2/15, train_loss: 0.4731\n",
      "3/15, train_loss: 0.5546\n",
      "4/15, train_loss: 0.4949\n",
      "5/15, train_loss: 0.4461\n",
      "6/15, train_loss: 0.5017\n",
      "7/15, train_loss: 0.4975\n",
      "8/15, train_loss: 0.4537\n",
      "9/15, train_loss: 0.5453\n",
      "10/15, train_loss: 0.5763\n",
      "11/15, train_loss: 0.5535\n",
      "12/15, train_loss: 0.4476\n",
      "13/15, train_loss: 0.5193\n",
      "14/15, train_loss: 0.6591\n",
      "15/15, train_loss: 0.5954\n",
      "16/15, train_loss: 0.7853\n",
      "epoch 149 average loss: 0.5381\n",
      "----------\n",
      "epoch 150/500\n",
      "1/15, train_loss: 0.4728\n",
      "2/15, train_loss: 0.4810\n",
      "3/15, train_loss: 0.5402\n",
      "4/15, train_loss: 0.5775\n",
      "5/15, train_loss: 0.5002\n",
      "6/15, train_loss: 0.4255\n",
      "7/15, train_loss: 0.5097\n",
      "8/15, train_loss: 1.0858\n",
      "9/15, train_loss: 0.4495\n",
      "10/15, train_loss: 0.6837\n",
      "11/15, train_loss: 0.4638\n",
      "12/15, train_loss: 0.4837\n",
      "13/15, train_loss: 0.4691\n",
      "14/15, train_loss: 0.5208\n",
      "15/15, train_loss: 0.4287\n",
      "16/15, train_loss: 0.8522\n",
      "epoch 150 average loss: 0.5590\n",
      "----------\n",
      "epoch 151/500\n",
      "1/15, train_loss: 0.4901\n",
      "2/15, train_loss: 0.5765\n",
      "3/15, train_loss: 0.4119\n",
      "4/15, train_loss: 0.4926\n",
      "5/15, train_loss: 0.3968\n",
      "6/15, train_loss: 0.4303\n",
      "7/15, train_loss: 0.5479\n",
      "8/15, train_loss: 0.4774\n",
      "9/15, train_loss: 0.4942\n",
      "10/15, train_loss: 0.4479\n",
      "11/15, train_loss: 0.6607\n",
      "12/15, train_loss: 2.4756\n",
      "13/15, train_loss: 0.4452\n",
      "14/15, train_loss: 0.6944\n",
      "15/15, train_loss: 0.4529\n",
      "16/15, train_loss: 2.1454\n",
      "epoch 151 average loss: 0.7275\n",
      "----------\n",
      "epoch 152/500\n",
      "1/15, train_loss: 0.4648\n",
      "2/15, train_loss: 0.9425\n",
      "3/15, train_loss: 0.4702\n",
      "4/15, train_loss: 0.4553\n",
      "5/15, train_loss: 0.4525\n",
      "6/15, train_loss: 0.5223\n",
      "7/15, train_loss: 0.4026\n",
      "8/15, train_loss: 0.4259\n",
      "9/15, train_loss: 0.4170\n",
      "10/15, train_loss: 0.4269\n",
      "11/15, train_loss: 0.5136\n",
      "12/15, train_loss: 0.5175\n",
      "13/15, train_loss: 0.4754\n",
      "14/15, train_loss: 0.4108\n",
      "15/15, train_loss: 0.4547\n",
      "16/15, train_loss: 0.8856\n",
      "epoch 152 average loss: 0.5149\n",
      "----------\n",
      "epoch 153/500\n",
      "1/15, train_loss: 2.5442\n",
      "2/15, train_loss: 0.4707\n",
      "3/15, train_loss: 0.4458\n",
      "4/15, train_loss: 0.5685\n",
      "5/15, train_loss: 0.4467\n",
      "6/15, train_loss: 0.3694\n",
      "7/15, train_loss: 0.5079\n",
      "8/15, train_loss: 0.4650\n",
      "9/15, train_loss: 0.5259\n",
      "10/15, train_loss: 0.5787\n",
      "11/15, train_loss: 0.5624\n",
      "12/15, train_loss: 1.2426\n",
      "13/15, train_loss: 0.4301\n",
      "14/15, train_loss: 0.5959\n",
      "15/15, train_loss: 0.4901\n",
      "16/15, train_loss: 2.2844\n",
      "epoch 153 average loss: 0.7830\n",
      "----------\n",
      "epoch 154/500\n",
      "1/15, train_loss: 0.4233\n",
      "2/15, train_loss: 0.5708\n",
      "3/15, train_loss: 0.4260\n",
      "4/15, train_loss: 0.5157\n",
      "5/15, train_loss: 0.9492\n",
      "6/15, train_loss: 0.5620\n",
      "7/15, train_loss: 0.6049\n",
      "8/15, train_loss: 0.6152\n",
      "9/15, train_loss: 0.6544\n",
      "10/15, train_loss: 0.4813\n",
      "11/15, train_loss: 0.5707\n",
      "12/15, train_loss: 0.5298\n",
      "13/15, train_loss: 0.4168\n",
      "14/15, train_loss: 0.5108\n",
      "15/15, train_loss: 0.5663\n",
      "16/15, train_loss: 2.4230\n",
      "epoch 154 average loss: 0.6763\n",
      "----------\n",
      "epoch 155/500\n",
      "1/15, train_loss: 0.5630\n",
      "2/15, train_loss: 1.1208\n",
      "3/15, train_loss: 0.5624\n",
      "4/15, train_loss: 0.4526\n",
      "5/15, train_loss: 0.4313\n",
      "6/15, train_loss: 0.5877\n",
      "7/15, train_loss: 0.4270\n",
      "8/15, train_loss: 0.9037\n",
      "9/15, train_loss: 0.4352\n",
      "10/15, train_loss: 0.4215\n",
      "11/15, train_loss: 0.6972\n",
      "12/15, train_loss: 0.4624\n",
      "13/15, train_loss: 0.4541\n",
      "14/15, train_loss: 0.4935\n",
      "15/15, train_loss: 0.6984\n",
      "16/15, train_loss: 0.6058\n",
      "epoch 155 average loss: 0.5823\n",
      "----------\n",
      "epoch 156/500\n",
      "1/15, train_loss: 0.4648\n",
      "2/15, train_loss: 0.4568\n",
      "3/15, train_loss: 0.4680\n",
      "4/15, train_loss: 0.4450\n",
      "5/15, train_loss: 0.4210\n",
      "6/15, train_loss: 0.4398\n",
      "7/15, train_loss: 1.3965\n",
      "8/15, train_loss: 0.5474\n",
      "9/15, train_loss: 0.6116\n",
      "10/15, train_loss: 0.5945\n",
      "11/15, train_loss: 0.5254\n",
      "12/15, train_loss: 0.6216\n",
      "13/15, train_loss: 0.4810\n",
      "14/15, train_loss: 0.5896\n",
      "15/15, train_loss: 0.4765\n",
      "16/15, train_loss: 0.4884\n",
      "epoch 156 average loss: 0.5642\n",
      "----------\n",
      "epoch 157/500\n",
      "1/15, train_loss: 0.5699\n",
      "2/15, train_loss: 0.6082\n",
      "3/15, train_loss: 0.4764\n",
      "4/15, train_loss: 0.8164\n",
      "5/15, train_loss: 0.3938\n",
      "6/15, train_loss: 0.5450\n",
      "7/15, train_loss: 0.5263\n",
      "8/15, train_loss: 0.8256\n",
      "9/15, train_loss: 0.4763\n",
      "10/15, train_loss: 0.4856\n",
      "11/15, train_loss: 0.4926\n",
      "12/15, train_loss: 0.4317\n",
      "13/15, train_loss: 0.4461\n",
      "14/15, train_loss: 0.4502\n",
      "15/15, train_loss: 0.3907\n",
      "16/15, train_loss: 0.7343\n",
      "epoch 157 average loss: 0.5418\n",
      "----------\n",
      "epoch 158/500\n",
      "1/15, train_loss: 0.4082\n",
      "2/15, train_loss: 0.4432\n",
      "3/15, train_loss: 0.4325\n",
      "4/15, train_loss: 0.4177\n",
      "5/15, train_loss: 0.6482\n",
      "6/15, train_loss: 0.5011\n",
      "7/15, train_loss: 0.6098\n",
      "8/15, train_loss: 0.6443\n",
      "9/15, train_loss: 0.3944\n",
      "10/15, train_loss: 0.9498\n",
      "11/15, train_loss: 0.4392\n",
      "12/15, train_loss: 0.6490\n",
      "13/15, train_loss: 0.4900\n",
      "14/15, train_loss: 0.5137\n",
      "15/15, train_loss: 0.8099\n",
      "16/15, train_loss: 0.5423\n",
      "epoch 158 average loss: 0.5558\n",
      "----------\n",
      "epoch 159/500\n",
      "1/15, train_loss: 0.6623\n",
      "2/15, train_loss: 0.5185\n",
      "3/15, train_loss: 0.3911\n",
      "4/15, train_loss: 0.8290\n",
      "5/15, train_loss: 0.3951\n",
      "6/15, train_loss: 0.4621\n",
      "7/15, train_loss: 0.4496\n",
      "8/15, train_loss: 0.3840\n",
      "9/15, train_loss: 0.5354\n",
      "10/15, train_loss: 0.4021\n",
      "11/15, train_loss: 0.6230\n",
      "12/15, train_loss: 0.4925\n",
      "13/15, train_loss: 0.4198\n",
      "14/15, train_loss: 0.5859\n",
      "15/15, train_loss: 0.5343\n",
      "16/15, train_loss: 0.5517\n",
      "epoch 159 average loss: 0.5148\n",
      "----------\n",
      "epoch 160/500\n",
      "1/15, train_loss: 0.4272\n",
      "2/15, train_loss: 0.3975\n",
      "3/15, train_loss: 0.4926\n",
      "4/15, train_loss: 0.3837\n",
      "5/15, train_loss: 0.6400\n",
      "6/15, train_loss: 0.5924\n",
      "7/15, train_loss: 0.5524\n",
      "8/15, train_loss: 0.6467\n",
      "9/15, train_loss: 0.4103\n",
      "10/15, train_loss: 0.5366\n",
      "11/15, train_loss: 0.3934\n",
      "12/15, train_loss: 0.6699\n",
      "13/15, train_loss: 0.5503\n",
      "14/15, train_loss: 2.2379\n",
      "15/15, train_loss: 1.0600\n",
      "16/15, train_loss: 1.0722\n",
      "epoch 160 average loss: 0.6914\n",
      "----------\n",
      "epoch 161/500\n",
      "1/15, train_loss: 0.3805\n",
      "2/15, train_loss: 0.4231\n",
      "3/15, train_loss: 0.6099\n",
      "4/15, train_loss: 0.4734\n",
      "5/15, train_loss: 0.4880\n",
      "6/15, train_loss: 0.4628\n",
      "7/15, train_loss: 0.9106\n",
      "8/15, train_loss: 0.7139\n",
      "9/15, train_loss: 0.4808\n",
      "10/15, train_loss: 0.4135\n",
      "11/15, train_loss: 0.4595\n",
      "12/15, train_loss: 0.4539\n",
      "13/15, train_loss: 0.4026\n",
      "14/15, train_loss: 0.4764\n",
      "15/15, train_loss: 0.3983\n",
      "16/15, train_loss: 0.3881\n",
      "epoch 161 average loss: 0.4960\n",
      "----------\n",
      "epoch 162/500\n",
      "1/15, train_loss: 0.5191\n",
      "2/15, train_loss: 0.3867\n",
      "3/15, train_loss: 0.4565\n",
      "4/15, train_loss: 0.4054\n",
      "5/15, train_loss: 0.4639\n",
      "6/15, train_loss: 0.5507\n",
      "7/15, train_loss: 0.4795\n",
      "8/15, train_loss: 0.4819\n",
      "9/15, train_loss: 0.7251\n",
      "10/15, train_loss: 0.5838\n",
      "11/15, train_loss: 0.5539\n",
      "12/15, train_loss: 1.3190\n",
      "13/15, train_loss: 0.5253\n",
      "14/15, train_loss: 0.3939\n",
      "15/15, train_loss: 0.7379\n",
      "16/15, train_loss: 0.4283\n",
      "epoch 162 average loss: 0.5632\n",
      "----------\n",
      "epoch 163/500\n",
      "1/15, train_loss: 0.4270\n",
      "2/15, train_loss: 0.4955\n",
      "3/15, train_loss: 0.4983\n",
      "4/15, train_loss: 0.4399\n",
      "5/15, train_loss: 0.4273\n",
      "6/15, train_loss: 0.5039\n",
      "7/15, train_loss: 0.5130\n",
      "8/15, train_loss: 0.3915\n",
      "9/15, train_loss: 0.3904\n",
      "10/15, train_loss: 0.4678\n",
      "11/15, train_loss: 0.3896\n",
      "12/15, train_loss: 0.5013\n",
      "13/15, train_loss: 0.4436\n",
      "14/15, train_loss: 0.4030\n",
      "15/15, train_loss: 0.6796\n",
      "16/15, train_loss: 0.6439\n",
      "epoch 163 average loss: 0.4760\n",
      "----------\n",
      "epoch 164/500\n",
      "1/15, train_loss: 0.4521\n",
      "2/15, train_loss: 0.4632\n",
      "3/15, train_loss: 0.4168\n",
      "4/15, train_loss: 0.8722\n",
      "5/15, train_loss: 0.4647\n",
      "6/15, train_loss: 0.5345\n",
      "7/15, train_loss: 0.4499\n",
      "8/15, train_loss: 0.4258\n",
      "9/15, train_loss: 0.4834\n",
      "10/15, train_loss: 0.4284\n",
      "11/15, train_loss: 0.5278\n",
      "12/15, train_loss: 0.3934\n",
      "13/15, train_loss: 0.4471\n",
      "14/15, train_loss: 0.7897\n",
      "15/15, train_loss: 0.4725\n",
      "16/15, train_loss: 0.5478\n",
      "epoch 164 average loss: 0.5106\n",
      "----------\n",
      "epoch 165/500\n",
      "1/15, train_loss: 0.3593\n",
      "2/15, train_loss: 0.5328\n",
      "3/15, train_loss: 0.4650\n",
      "4/15, train_loss: 0.5245\n",
      "5/15, train_loss: 0.3952\n",
      "6/15, train_loss: 0.7775\n",
      "7/15, train_loss: 0.4303\n",
      "8/15, train_loss: 0.5091\n",
      "9/15, train_loss: 0.4459\n",
      "10/15, train_loss: 0.5129\n",
      "11/15, train_loss: 0.3887\n",
      "12/15, train_loss: 0.4262\n",
      "13/15, train_loss: 0.4969\n",
      "14/15, train_loss: 0.3865\n",
      "15/15, train_loss: 0.4885\n",
      "16/15, train_loss: 0.6490\n",
      "epoch 165 average loss: 0.4868\n",
      "----------\n",
      "epoch 166/500\n",
      "1/15, train_loss: 0.4357\n",
      "2/15, train_loss: 0.7527\n",
      "3/15, train_loss: 0.3895\n",
      "4/15, train_loss: 0.5962\n",
      "5/15, train_loss: 0.4740\n",
      "6/15, train_loss: 0.7504\n",
      "7/15, train_loss: 0.4666\n",
      "8/15, train_loss: 0.4550\n",
      "9/15, train_loss: 0.4440\n",
      "10/15, train_loss: 0.4125\n",
      "11/15, train_loss: 0.9186\n",
      "12/15, train_loss: 0.4413\n",
      "13/15, train_loss: 0.4570\n",
      "14/15, train_loss: 0.5345\n",
      "15/15, train_loss: 0.4670\n",
      "16/15, train_loss: 0.3961\n",
      "epoch 166 average loss: 0.5244\n",
      "----------\n",
      "epoch 167/500\n",
      "1/15, train_loss: 0.4201\n",
      "2/15, train_loss: 0.3885\n",
      "3/15, train_loss: 0.4559\n",
      "4/15, train_loss: 0.4088\n",
      "5/15, train_loss: 0.4258\n",
      "6/15, train_loss: 0.5020\n",
      "7/15, train_loss: 1.5042\n",
      "8/15, train_loss: 0.5002\n",
      "9/15, train_loss: 0.4424\n",
      "10/15, train_loss: 0.5344\n",
      "11/15, train_loss: 0.7290\n",
      "12/15, train_loss: 0.3677\n",
      "13/15, train_loss: 0.4807\n",
      "14/15, train_loss: 0.4078\n",
      "15/15, train_loss: 0.5242\n",
      "16/15, train_loss: 2.4350\n",
      "epoch 167 average loss: 0.6579\n",
      "----------\n",
      "epoch 168/500\n",
      "1/15, train_loss: 0.3644\n",
      "2/15, train_loss: 0.4502\n",
      "3/15, train_loss: 0.4302\n",
      "4/15, train_loss: 0.6841\n",
      "5/15, train_loss: 0.4669\n",
      "6/15, train_loss: 0.4028\n",
      "7/15, train_loss: 0.4456\n",
      "8/15, train_loss: 0.4056\n",
      "9/15, train_loss: 0.4188\n",
      "10/15, train_loss: 0.4523\n",
      "11/15, train_loss: 0.4987\n",
      "12/15, train_loss: 0.4718\n",
      "13/15, train_loss: 0.4483\n",
      "14/15, train_loss: 0.4885\n",
      "15/15, train_loss: 0.4902\n",
      "16/15, train_loss: 0.3799\n",
      "epoch 168 average loss: 0.4561\n",
      "----------\n",
      "epoch 169/500\n",
      "1/15, train_loss: 0.4111\n",
      "2/15, train_loss: 0.4320\n",
      "3/15, train_loss: 0.5207\n",
      "4/15, train_loss: 0.4334\n",
      "5/15, train_loss: 0.4976\n",
      "6/15, train_loss: 0.5112\n",
      "7/15, train_loss: 0.3618\n",
      "8/15, train_loss: 0.6215\n",
      "9/15, train_loss: 0.4665\n",
      "10/15, train_loss: 0.4606\n",
      "11/15, train_loss: 0.3889\n",
      "12/15, train_loss: 0.4359\n",
      "13/15, train_loss: 0.3751\n",
      "14/15, train_loss: 0.5675\n",
      "15/15, train_loss: 0.3419\n",
      "16/15, train_loss: 1.5023\n",
      "epoch 169 average loss: 0.5205\n",
      "----------\n",
      "epoch 170/500\n",
      "1/15, train_loss: 0.5275\n",
      "2/15, train_loss: 0.6729\n",
      "3/15, train_loss: 0.3975\n",
      "4/15, train_loss: 0.4480\n",
      "5/15, train_loss: 0.6637\n",
      "6/15, train_loss: 0.4724\n",
      "7/15, train_loss: 0.4098\n",
      "8/15, train_loss: 0.4264\n",
      "9/15, train_loss: 0.4206\n",
      "10/15, train_loss: 0.8199\n",
      "11/15, train_loss: 0.3845\n",
      "12/15, train_loss: 0.4822\n",
      "13/15, train_loss: 0.4427\n",
      "14/15, train_loss: 0.4329\n",
      "15/15, train_loss: 0.6141\n",
      "16/15, train_loss: 0.6605\n",
      "epoch 170 average loss: 0.5172\n",
      "----------\n",
      "epoch 171/500\n",
      "1/15, train_loss: 0.4264\n",
      "2/15, train_loss: 0.5231\n",
      "3/15, train_loss: 0.4811\n",
      "4/15, train_loss: 1.1150\n",
      "5/15, train_loss: 0.6060\n",
      "6/15, train_loss: 0.5402\n",
      "7/15, train_loss: 0.5020\n",
      "8/15, train_loss: 0.5192\n",
      "9/15, train_loss: 0.4190\n",
      "10/15, train_loss: 0.3906\n",
      "11/15, train_loss: 0.4617\n",
      "12/15, train_loss: 0.3944\n",
      "13/15, train_loss: 0.5211\n",
      "14/15, train_loss: 0.4166\n",
      "15/15, train_loss: 0.4074\n",
      "16/15, train_loss: 0.4681\n",
      "epoch 171 average loss: 0.5120\n",
      "----------\n",
      "epoch 172/500\n",
      "1/15, train_loss: 0.4996\n",
      "2/15, train_loss: 0.3939\n",
      "3/15, train_loss: 0.4591\n",
      "4/15, train_loss: 0.4629\n",
      "5/15, train_loss: 0.5097\n",
      "6/15, train_loss: 0.5625\n",
      "7/15, train_loss: 0.3569\n",
      "8/15, train_loss: 0.4594\n",
      "9/15, train_loss: 0.5337\n",
      "10/15, train_loss: 0.4958\n",
      "11/15, train_loss: 0.6038\n",
      "12/15, train_loss: 0.4103\n",
      "13/15, train_loss: 1.3586\n",
      "14/15, train_loss: 2.1814\n",
      "15/15, train_loss: 0.5069\n",
      "16/15, train_loss: 0.4095\n",
      "epoch 172 average loss: 0.6378\n",
      "----------\n",
      "epoch 173/500\n",
      "1/15, train_loss: 0.3758\n",
      "2/15, train_loss: 0.4051\n",
      "3/15, train_loss: 0.5116\n",
      "4/15, train_loss: 0.3694\n",
      "5/15, train_loss: 0.5976\n",
      "6/15, train_loss: 0.4014\n",
      "7/15, train_loss: 0.4339\n",
      "8/15, train_loss: 0.8931\n",
      "9/15, train_loss: 0.3762\n",
      "10/15, train_loss: 0.4874\n",
      "11/15, train_loss: 0.5292\n",
      "12/15, train_loss: 0.4145\n",
      "13/15, train_loss: 0.4124\n",
      "14/15, train_loss: 0.4237\n",
      "15/15, train_loss: 0.4738\n",
      "16/15, train_loss: 1.0637\n",
      "epoch 173 average loss: 0.5106\n",
      "----------\n",
      "epoch 174/500\n",
      "1/15, train_loss: 0.5927\n",
      "2/15, train_loss: 0.5178\n",
      "3/15, train_loss: 0.4825\n",
      "4/15, train_loss: 0.3969\n",
      "5/15, train_loss: 0.4031\n",
      "6/15, train_loss: 0.4517\n",
      "7/15, train_loss: 0.4307\n",
      "8/15, train_loss: 0.4527\n",
      "9/15, train_loss: 0.4376\n",
      "10/15, train_loss: 0.3784\n",
      "11/15, train_loss: 0.3764\n",
      "12/15, train_loss: 0.3737\n",
      "13/15, train_loss: 0.8640\n",
      "14/15, train_loss: 0.4407\n",
      "15/15, train_loss: 0.4533\n",
      "16/15, train_loss: 0.4206\n",
      "epoch 174 average loss: 0.4670\n",
      "----------\n",
      "epoch 175/500\n",
      "1/15, train_loss: 0.4714\n",
      "2/15, train_loss: 0.6727\n",
      "3/15, train_loss: 0.5279\n",
      "4/15, train_loss: 0.4043\n",
      "5/15, train_loss: 0.4490\n",
      "6/15, train_loss: 0.4066\n",
      "7/15, train_loss: 0.4261\n",
      "8/15, train_loss: 0.5772\n",
      "9/15, train_loss: 0.4724\n",
      "10/15, train_loss: 0.4439\n",
      "11/15, train_loss: 0.4008\n",
      "12/15, train_loss: 0.4892\n",
      "13/15, train_loss: 0.4478\n",
      "14/15, train_loss: 0.3848\n",
      "15/15, train_loss: 0.5039\n",
      "16/15, train_loss: 0.5192\n",
      "epoch 175 average loss: 0.4748\n",
      "----------\n",
      "epoch 176/500\n",
      "1/15, train_loss: 0.4487\n",
      "2/15, train_loss: 0.4260\n",
      "3/15, train_loss: 0.3602\n",
      "4/15, train_loss: 0.3816\n",
      "5/15, train_loss: 0.3999\n",
      "6/15, train_loss: 0.4036\n",
      "7/15, train_loss: 0.4849\n",
      "8/15, train_loss: 0.4203\n",
      "9/15, train_loss: 0.4693\n",
      "10/15, train_loss: 0.5549\n",
      "11/15, train_loss: 0.3778\n",
      "12/15, train_loss: 0.4446\n",
      "13/15, train_loss: 0.4023\n",
      "14/15, train_loss: 0.5195\n",
      "15/15, train_loss: 0.3965\n",
      "16/15, train_loss: 0.4520\n",
      "epoch 176 average loss: 0.4339\n",
      "----------\n",
      "epoch 177/500\n",
      "1/15, train_loss: 0.5861\n",
      "2/15, train_loss: 0.4237\n",
      "3/15, train_loss: 0.4637\n",
      "4/15, train_loss: 0.3921\n",
      "5/15, train_loss: 0.3906\n",
      "6/15, train_loss: 0.4199\n",
      "7/15, train_loss: 0.3841\n",
      "8/15, train_loss: 0.4359\n",
      "9/15, train_loss: 0.3967\n",
      "10/15, train_loss: 0.3634\n",
      "11/15, train_loss: 0.4489\n",
      "12/15, train_loss: 0.3692\n",
      "13/15, train_loss: 0.5043\n",
      "14/15, train_loss: 0.4140\n",
      "15/15, train_loss: 0.3670\n",
      "16/15, train_loss: 2.3292\n",
      "epoch 177 average loss: 0.5431\n",
      "----------\n",
      "epoch 178/500\n",
      "1/15, train_loss: 0.4039\n",
      "2/15, train_loss: 0.4596\n",
      "3/15, train_loss: 0.8646\n",
      "4/15, train_loss: 0.6010\n",
      "5/15, train_loss: 0.5165\n",
      "6/15, train_loss: 0.4115\n",
      "7/15, train_loss: 0.6121\n",
      "8/15, train_loss: 0.3614\n",
      "9/15, train_loss: 0.3868\n",
      "10/15, train_loss: 0.3709\n",
      "11/15, train_loss: 0.3729\n",
      "12/15, train_loss: 0.4127\n",
      "13/15, train_loss: 0.4389\n",
      "14/15, train_loss: 0.3858\n",
      "15/15, train_loss: 0.4166\n",
      "16/15, train_loss: 0.4507\n",
      "epoch 178 average loss: 0.4666\n",
      "----------\n",
      "epoch 179/500\n",
      "1/15, train_loss: 0.5400\n",
      "2/15, train_loss: 0.3926\n",
      "3/15, train_loss: 0.3432\n",
      "4/15, train_loss: 0.5365\n",
      "5/15, train_loss: 0.3452\n",
      "6/15, train_loss: 0.4925\n",
      "7/15, train_loss: 0.4298\n",
      "8/15, train_loss: 0.4557\n",
      "9/15, train_loss: 0.4522\n",
      "10/15, train_loss: 0.3567\n",
      "11/15, train_loss: 0.4365\n",
      "12/15, train_loss: 0.3972\n",
      "13/15, train_loss: 0.3494\n",
      "14/15, train_loss: 0.4739\n",
      "15/15, train_loss: 0.3871\n",
      "16/15, train_loss: 0.6285\n",
      "epoch 179 average loss: 0.4386\n",
      "----------\n",
      "epoch 180/500\n",
      "1/15, train_loss: 0.5109\n",
      "2/15, train_loss: 0.4038\n",
      "3/15, train_loss: 0.8452\n",
      "4/15, train_loss: 0.3638\n",
      "5/15, train_loss: 0.4466\n",
      "6/15, train_loss: 0.3715\n",
      "7/15, train_loss: 0.9102\n",
      "8/15, train_loss: 0.4006\n",
      "9/15, train_loss: 0.7037\n",
      "10/15, train_loss: 0.4291\n",
      "11/15, train_loss: 0.3847\n",
      "12/15, train_loss: 0.3891\n",
      "13/15, train_loss: 0.3913\n",
      "14/15, train_loss: 0.3980\n",
      "15/15, train_loss: 0.4172\n",
      "16/15, train_loss: 0.4127\n",
      "epoch 180 average loss: 0.4862\n",
      "----------\n",
      "epoch 181/500\n",
      "1/15, train_loss: 0.6141\n",
      "2/15, train_loss: 0.4757\n",
      "3/15, train_loss: 0.6543\n",
      "4/15, train_loss: 0.5071\n",
      "5/15, train_loss: 0.5263\n",
      "6/15, train_loss: 0.8386\n",
      "7/15, train_loss: 0.4891\n",
      "8/15, train_loss: 0.3648\n",
      "9/15, train_loss: 0.3797\n",
      "10/15, train_loss: 0.4780\n",
      "11/15, train_loss: 0.4063\n",
      "12/15, train_loss: 0.4213\n",
      "13/15, train_loss: 0.7093\n",
      "14/15, train_loss: 0.4891\n",
      "15/15, train_loss: 0.4729\n",
      "16/15, train_loss: 0.4886\n",
      "epoch 181 average loss: 0.5197\n",
      "----------\n",
      "epoch 182/500\n",
      "1/15, train_loss: 0.4339\n",
      "2/15, train_loss: 0.5534\n",
      "3/15, train_loss: 0.3948\n",
      "4/15, train_loss: 0.4872\n",
      "5/15, train_loss: 0.4809\n",
      "6/15, train_loss: 0.4872\n",
      "7/15, train_loss: 0.5850\n",
      "8/15, train_loss: 0.5181\n",
      "9/15, train_loss: 0.3967\n",
      "10/15, train_loss: 0.3948\n",
      "11/15, train_loss: 0.4149\n",
      "12/15, train_loss: 0.4242\n",
      "13/15, train_loss: 0.4613\n",
      "14/15, train_loss: 0.6227\n",
      "15/15, train_loss: 0.4086\n",
      "16/15, train_loss: 0.3633\n",
      "epoch 182 average loss: 0.4642\n",
      "----------\n",
      "epoch 183/500\n",
      "1/15, train_loss: 0.4773\n",
      "2/15, train_loss: 0.3941\n",
      "3/15, train_loss: 0.4470\n",
      "4/15, train_loss: 0.3362\n",
      "5/15, train_loss: 0.3588\n",
      "6/15, train_loss: 0.4013\n",
      "7/15, train_loss: 0.3773\n",
      "8/15, train_loss: 0.4142\n",
      "9/15, train_loss: 0.4517\n",
      "10/15, train_loss: 0.3354\n",
      "11/15, train_loss: 0.4846\n",
      "12/15, train_loss: 0.3856\n",
      "13/15, train_loss: 0.5111\n",
      "14/15, train_loss: 0.3704\n",
      "15/15, train_loss: 0.3420\n",
      "16/15, train_loss: 2.4085\n",
      "epoch 183 average loss: 0.5310\n",
      "----------\n",
      "epoch 184/500\n",
      "1/15, train_loss: 0.3828\n",
      "2/15, train_loss: 0.3850\n",
      "3/15, train_loss: 1.2266\n",
      "4/15, train_loss: 0.4128\n",
      "5/15, train_loss: 0.4223\n",
      "6/15, train_loss: 0.3867\n",
      "7/15, train_loss: 0.4448\n",
      "8/15, train_loss: 0.3550\n",
      "9/15, train_loss: 0.5164\n",
      "10/15, train_loss: 0.6229\n",
      "11/15, train_loss: 0.3683\n",
      "12/15, train_loss: 0.4427\n",
      "13/15, train_loss: 0.4206\n",
      "14/15, train_loss: 0.4199\n",
      "15/15, train_loss: 0.4574\n",
      "16/15, train_loss: 2.2717\n",
      "epoch 184 average loss: 0.5960\n",
      "----------\n",
      "epoch 185/500\n",
      "1/15, train_loss: 0.3827\n",
      "2/15, train_loss: 0.4607\n",
      "3/15, train_loss: 0.4113\n",
      "4/15, train_loss: 0.3956\n",
      "5/15, train_loss: 0.4124\n",
      "6/15, train_loss: 0.3903\n",
      "7/15, train_loss: 0.3862\n",
      "8/15, train_loss: 0.4025\n",
      "9/15, train_loss: 0.5228\n",
      "10/15, train_loss: 0.3592\n",
      "11/15, train_loss: 0.4272\n",
      "12/15, train_loss: 0.3776\n",
      "13/15, train_loss: 0.3953\n",
      "14/15, train_loss: 0.4906\n",
      "15/15, train_loss: 0.3893\n",
      "16/15, train_loss: 0.4134\n",
      "epoch 185 average loss: 0.4136\n",
      "----------\n",
      "epoch 186/500\n",
      "1/15, train_loss: 0.8091\n",
      "2/15, train_loss: 0.3697\n",
      "3/15, train_loss: 0.4525\n",
      "4/15, train_loss: 0.6567\n",
      "5/15, train_loss: 0.7843\n",
      "6/15, train_loss: 0.3534\n",
      "7/15, train_loss: 0.6785\n",
      "8/15, train_loss: 0.6340\n",
      "9/15, train_loss: 0.3555\n",
      "10/15, train_loss: 0.3786\n",
      "11/15, train_loss: 0.3921\n",
      "12/15, train_loss: 0.3548\n",
      "13/15, train_loss: 0.6115\n",
      "14/15, train_loss: 0.3718\n",
      "15/15, train_loss: 0.4418\n",
      "16/15, train_loss: 0.6413\n",
      "epoch 186 average loss: 0.5178\n",
      "----------\n",
      "epoch 187/500\n",
      "1/15, train_loss: 0.3768\n",
      "2/15, train_loss: 0.5394\n",
      "3/15, train_loss: 0.4751\n",
      "4/15, train_loss: 0.8066\n",
      "5/15, train_loss: 0.4645\n",
      "6/15, train_loss: 0.3679\n",
      "7/15, train_loss: 0.3450\n",
      "8/15, train_loss: 0.3368\n",
      "9/15, train_loss: 0.4219\n",
      "10/15, train_loss: 0.4375\n",
      "11/15, train_loss: 0.3532\n",
      "12/15, train_loss: 0.3979\n",
      "13/15, train_loss: 0.3839\n",
      "14/15, train_loss: 0.3670\n",
      "15/15, train_loss: 0.3514\n",
      "16/15, train_loss: 0.3773\n",
      "epoch 187 average loss: 0.4251\n",
      "----------\n",
      "epoch 188/500\n",
      "1/15, train_loss: 0.6360\n",
      "2/15, train_loss: 0.4640\n",
      "3/15, train_loss: 0.3933\n",
      "4/15, train_loss: 0.4012\n",
      "5/15, train_loss: 0.4084\n",
      "6/15, train_loss: 0.4660\n",
      "7/15, train_loss: 0.3849\n",
      "8/15, train_loss: 0.4052\n",
      "9/15, train_loss: 0.7191\n",
      "10/15, train_loss: 0.4029\n",
      "11/15, train_loss: 0.4928\n",
      "12/15, train_loss: 0.3394\n",
      "13/15, train_loss: 0.4003\n",
      "14/15, train_loss: 0.3297\n",
      "15/15, train_loss: 0.3603\n",
      "16/15, train_loss: 0.4940\n",
      "epoch 188 average loss: 0.4436\n",
      "----------\n",
      "epoch 189/500\n",
      "1/15, train_loss: 0.4544\n",
      "2/15, train_loss: 0.5347\n",
      "3/15, train_loss: 0.5265\n",
      "4/15, train_loss: 0.4229\n",
      "5/15, train_loss: 0.3779\n",
      "6/15, train_loss: 0.4415\n",
      "7/15, train_loss: 0.3648\n",
      "8/15, train_loss: 0.3781\n",
      "9/15, train_loss: 0.4053\n",
      "10/15, train_loss: 0.4341\n",
      "11/15, train_loss: 0.3450\n",
      "12/15, train_loss: 0.5666\n",
      "13/15, train_loss: 0.3986\n",
      "14/15, train_loss: 0.3805\n",
      "15/15, train_loss: 0.3872\n",
      "16/15, train_loss: 0.4793\n",
      "epoch 189 average loss: 0.4311\n",
      "----------\n",
      "epoch 190/500\n",
      "1/15, train_loss: 0.5507\n",
      "2/15, train_loss: 0.3961\n",
      "3/15, train_loss: 0.3566\n",
      "4/15, train_loss: 0.4063\n",
      "5/15, train_loss: 0.3736\n",
      "6/15, train_loss: 0.4249\n",
      "7/15, train_loss: 0.3319\n",
      "8/15, train_loss: 0.3316\n",
      "9/15, train_loss: 0.4593\n",
      "10/15, train_loss: 0.3581\n",
      "11/15, train_loss: 0.4449\n",
      "12/15, train_loss: 0.3274\n",
      "13/15, train_loss: 0.5121\n",
      "14/15, train_loss: 0.5511\n",
      "15/15, train_loss: 0.4183\n",
      "16/15, train_loss: 0.6662\n",
      "epoch 190 average loss: 0.4318\n",
      "----------\n",
      "epoch 191/500\n",
      "1/15, train_loss: 0.3639\n",
      "2/15, train_loss: 0.3708\n",
      "3/15, train_loss: 0.4504\n",
      "4/15, train_loss: 0.3516\n",
      "5/15, train_loss: 0.5072\n",
      "6/15, train_loss: 0.5643\n",
      "7/15, train_loss: 0.3395\n",
      "8/15, train_loss: 0.4476\n",
      "9/15, train_loss: 0.3708\n",
      "10/15, train_loss: 0.4099\n",
      "11/15, train_loss: 0.3901\n",
      "12/15, train_loss: 0.4775\n",
      "13/15, train_loss: 0.4148\n",
      "14/15, train_loss: 0.4129\n",
      "15/15, train_loss: 0.5414\n",
      "16/15, train_loss: 0.3659\n",
      "epoch 191 average loss: 0.4237\n",
      "saved new best metric model\n",
      "current fold: 1 current epoch: 191 validation loss: 0.9970 dice_score: 0.8348 acc_metric: 0.5677 accuracy: 0.5677, f1score: 0.6803\n",
      " saved Best PMetric: 0.7012 at epoch: 191\n",
      "----------\n",
      "epoch 192/500\n",
      "1/15, train_loss: 0.3914\n",
      "2/15, train_loss: 0.5106\n",
      "3/15, train_loss: 0.4498\n",
      "4/15, train_loss: 0.3797\n",
      "5/15, train_loss: 0.5002\n",
      "6/15, train_loss: 0.3645\n",
      "7/15, train_loss: 0.5161\n",
      "8/15, train_loss: 0.3321\n",
      "9/15, train_loss: 0.4566\n",
      "10/15, train_loss: 0.3220\n",
      "11/15, train_loss: 0.3429\n",
      "12/15, train_loss: 0.4356\n",
      "13/15, train_loss: 0.3773\n",
      "14/15, train_loss: 0.4214\n",
      "15/15, train_loss: 0.4193\n",
      "16/15, train_loss: 0.5043\n",
      "epoch 192 average loss: 0.4202\n",
      "----------\n",
      "epoch 193/500\n",
      "1/15, train_loss: 0.4122\n",
      "2/15, train_loss: 0.3288\n",
      "3/15, train_loss: 0.4655\n",
      "4/15, train_loss: 0.3886\n",
      "5/15, train_loss: 0.3602\n",
      "6/15, train_loss: 0.4322\n",
      "7/15, train_loss: 0.4720\n",
      "8/15, train_loss: 0.4485\n",
      "9/15, train_loss: 0.4188\n",
      "10/15, train_loss: 0.3425\n",
      "11/15, train_loss: 0.3694\n",
      "12/15, train_loss: 0.7381\n",
      "13/15, train_loss: 0.4367\n",
      "14/15, train_loss: 2.1543\n",
      "15/15, train_loss: 0.4006\n",
      "16/15, train_loss: 0.3520\n",
      "epoch 193 average loss: 0.5325\n",
      "----------\n",
      "epoch 194/500\n",
      "1/15, train_loss: 0.4066\n",
      "2/15, train_loss: 0.4203\n",
      "3/15, train_loss: 0.4020\n",
      "4/15, train_loss: 0.3449\n",
      "5/15, train_loss: 0.4155\n",
      "6/15, train_loss: 0.5812\n",
      "7/15, train_loss: 0.4452\n",
      "8/15, train_loss: 0.4085\n",
      "9/15, train_loss: 0.4735\n",
      "10/15, train_loss: 0.4389\n",
      "11/15, train_loss: 0.3290\n",
      "12/15, train_loss: 0.3949\n",
      "13/15, train_loss: 0.3809\n",
      "14/15, train_loss: 0.3369\n",
      "15/15, train_loss: 0.3762\n",
      "16/15, train_loss: 2.3618\n",
      "epoch 194 average loss: 0.5323\n",
      "----------\n",
      "epoch 195/500\n",
      "1/15, train_loss: 0.4902\n",
      "2/15, train_loss: 0.3495\n",
      "3/15, train_loss: 0.3519\n",
      "4/15, train_loss: 0.4920\n",
      "5/15, train_loss: 0.3123\n",
      "6/15, train_loss: 0.3292\n",
      "7/15, train_loss: 0.3606\n",
      "8/15, train_loss: 0.3526\n",
      "9/15, train_loss: 0.3599\n",
      "10/15, train_loss: 2.3751\n",
      "11/15, train_loss: 0.3414\n",
      "12/15, train_loss: 0.3294\n",
      "13/15, train_loss: 0.3189\n",
      "14/15, train_loss: 0.4279\n",
      "15/15, train_loss: 2.2555\n",
      "16/15, train_loss: 0.3856\n",
      "epoch 195 average loss: 0.6145\n",
      "----------\n",
      "epoch 196/500\n",
      "1/15, train_loss: 0.3142\n",
      "2/15, train_loss: 0.4757\n",
      "3/15, train_loss: 0.5368\n",
      "4/15, train_loss: 0.3629\n",
      "5/15, train_loss: 0.3840\n",
      "6/15, train_loss: 0.3943\n",
      "7/15, train_loss: 0.3867\n",
      "8/15, train_loss: 0.3805\n",
      "9/15, train_loss: 0.3497\n",
      "10/15, train_loss: 0.3668\n",
      "11/15, train_loss: 0.3997\n",
      "12/15, train_loss: 0.3236\n",
      "13/15, train_loss: 0.3888\n",
      "14/15, train_loss: 0.3266\n",
      "15/15, train_loss: 0.5098\n",
      "16/15, train_loss: 0.5874\n",
      "epoch 196 average loss: 0.4055\n",
      "----------\n",
      "epoch 197/500\n",
      "1/15, train_loss: 0.3764\n",
      "2/15, train_loss: 0.4036\n",
      "3/15, train_loss: 0.3932\n",
      "4/15, train_loss: 0.4531\n",
      "5/15, train_loss: 0.4600\n",
      "6/15, train_loss: 2.2962\n",
      "7/15, train_loss: 0.5875\n",
      "8/15, train_loss: 0.3550\n",
      "9/15, train_loss: 0.4535\n",
      "10/15, train_loss: 0.3371\n",
      "11/15, train_loss: 2.1799\n",
      "12/15, train_loss: 1.0323\n",
      "13/15, train_loss: 0.3406\n",
      "14/15, train_loss: 0.4065\n",
      "15/15, train_loss: 0.3402\n",
      "16/15, train_loss: 0.3326\n",
      "epoch 197 average loss: 0.6717\n",
      "----------\n",
      "epoch 198/500\n",
      "1/15, train_loss: 0.4574\n",
      "2/15, train_loss: 0.7396\n",
      "3/15, train_loss: 0.3967\n",
      "4/15, train_loss: 0.4813\n",
      "5/15, train_loss: 0.3120\n",
      "6/15, train_loss: 0.4395\n",
      "7/15, train_loss: 0.3304\n",
      "8/15, train_loss: 0.4082\n",
      "9/15, train_loss: 0.3201\n",
      "10/15, train_loss: 0.3705\n",
      "11/15, train_loss: 0.4796\n",
      "12/15, train_loss: 0.3168\n",
      "13/15, train_loss: 0.5368\n",
      "14/15, train_loss: 0.3875\n",
      "15/15, train_loss: 0.3274\n",
      "16/15, train_loss: 2.2679\n",
      "epoch 198 average loss: 0.5357\n",
      "----------\n",
      "epoch 199/500\n",
      "1/15, train_loss: 0.3656\n",
      "2/15, train_loss: 0.3973\n",
      "3/15, train_loss: 0.3791\n",
      "4/15, train_loss: 0.6716\n",
      "5/15, train_loss: 0.3310\n",
      "6/15, train_loss: 0.3135\n",
      "7/15, train_loss: 0.4129\n",
      "8/15, train_loss: 0.3959\n",
      "9/15, train_loss: 0.3589\n",
      "10/15, train_loss: 0.3513\n",
      "11/15, train_loss: 0.4072\n",
      "12/15, train_loss: 0.4193\n",
      "13/15, train_loss: 0.3256\n",
      "14/15, train_loss: 1.0548\n",
      "15/15, train_loss: 0.5127\n",
      "16/15, train_loss: 0.4112\n",
      "epoch 199 average loss: 0.4442\n",
      "----------\n",
      "epoch 200/500\n",
      "1/15, train_loss: 0.4058\n",
      "2/15, train_loss: 0.3805\n",
      "3/15, train_loss: 0.3535\n",
      "4/15, train_loss: 0.3993\n",
      "5/15, train_loss: 0.3775\n",
      "6/15, train_loss: 0.4060\n",
      "7/15, train_loss: 0.3407\n",
      "8/15, train_loss: 0.5751\n",
      "9/15, train_loss: 0.5312\n",
      "10/15, train_loss: 0.3438\n",
      "11/15, train_loss: 0.5355\n",
      "12/15, train_loss: 0.3303\n",
      "13/15, train_loss: 0.3867\n",
      "14/15, train_loss: 0.3815\n",
      "15/15, train_loss: 0.3434\n",
      "16/15, train_loss: 0.3938\n",
      "epoch 200 average loss: 0.4053\n",
      "----------\n",
      "epoch 201/500\n",
      "1/15, train_loss: 0.3448\n",
      "2/15, train_loss: 0.7747\n",
      "3/15, train_loss: 0.4119\n",
      "4/15, train_loss: 0.4046\n",
      "5/15, train_loss: 0.5542\n",
      "6/15, train_loss: 0.3769\n",
      "7/15, train_loss: 0.3873\n",
      "8/15, train_loss: 0.3577\n",
      "9/15, train_loss: 0.3209\n",
      "10/15, train_loss: 0.4396\n",
      "11/15, train_loss: 0.3859\n",
      "12/15, train_loss: 0.3725\n",
      "13/15, train_loss: 0.3846\n",
      "14/15, train_loss: 0.4187\n",
      "15/15, train_loss: 0.3436\n",
      "16/15, train_loss: 0.4855\n",
      "epoch 201 average loss: 0.4227\n",
      "----------\n",
      "epoch 202/500\n",
      "1/15, train_loss: 0.5742\n",
      "2/15, train_loss: 0.6296\n",
      "3/15, train_loss: 0.3882\n",
      "4/15, train_loss: 1.0942\n",
      "5/15, train_loss: 0.6609\n",
      "6/15, train_loss: 0.7029\n",
      "7/15, train_loss: 0.4574\n",
      "8/15, train_loss: 0.3569\n",
      "9/15, train_loss: 0.3693\n",
      "10/15, train_loss: 0.5529\n",
      "11/15, train_loss: 0.3029\n",
      "12/15, train_loss: 0.3409\n",
      "13/15, train_loss: 0.3231\n",
      "14/15, train_loss: 0.3762\n",
      "15/15, train_loss: 0.3293\n",
      "16/15, train_loss: 0.4913\n",
      "epoch 202 average loss: 0.4969\n",
      "----------\n",
      "epoch 203/500\n",
      "1/15, train_loss: 0.4876\n",
      "2/15, train_loss: 0.4736\n",
      "3/15, train_loss: 0.3597\n",
      "4/15, train_loss: 0.3845\n",
      "5/15, train_loss: 0.3954\n",
      "6/15, train_loss: 0.7457\n",
      "7/15, train_loss: 0.3081\n",
      "8/15, train_loss: 0.3733\n",
      "9/15, train_loss: 0.4653\n",
      "10/15, train_loss: 0.3609\n",
      "11/15, train_loss: 0.3314\n",
      "12/15, train_loss: 0.3538\n",
      "13/15, train_loss: 0.3640\n",
      "14/15, train_loss: 0.3050\n",
      "15/15, train_loss: 0.3798\n",
      "16/15, train_loss: 0.4173\n",
      "epoch 203 average loss: 0.4066\n",
      "----------\n",
      "epoch 204/500\n",
      "1/15, train_loss: 0.6264\n",
      "2/15, train_loss: 0.3411\n",
      "3/15, train_loss: 0.3602\n",
      "4/15, train_loss: 0.6941\n",
      "5/15, train_loss: 0.3508\n",
      "6/15, train_loss: 0.4407\n",
      "7/15, train_loss: 0.3362\n",
      "8/15, train_loss: 0.5673\n",
      "9/15, train_loss: 0.2910\n",
      "10/15, train_loss: 0.5635\n",
      "11/15, train_loss: 0.3096\n",
      "12/15, train_loss: 0.4364\n",
      "13/15, train_loss: 0.3624\n",
      "14/15, train_loss: 0.4113\n",
      "15/15, train_loss: 0.3622\n",
      "16/15, train_loss: 2.1035\n",
      "epoch 204 average loss: 0.5348\n",
      "----------\n",
      "epoch 205/500\n",
      "1/15, train_loss: 0.3663\n",
      "2/15, train_loss: 0.5011\n",
      "3/15, train_loss: 0.3614\n",
      "4/15, train_loss: 0.4527\n",
      "5/15, train_loss: 0.3697\n",
      "6/15, train_loss: 0.5375\n",
      "7/15, train_loss: 0.3471\n",
      "8/15, train_loss: 0.7437\n",
      "9/15, train_loss: 0.3830\n",
      "10/15, train_loss: 0.3288\n",
      "11/15, train_loss: 0.3757\n",
      "12/15, train_loss: 0.4529\n",
      "13/15, train_loss: 0.4199\n",
      "14/15, train_loss: 0.3365\n",
      "15/15, train_loss: 0.3683\n",
      "16/15, train_loss: 0.3615\n",
      "epoch 205 average loss: 0.4191\n",
      "----------\n",
      "epoch 206/500\n",
      "1/15, train_loss: 0.3909\n",
      "2/15, train_loss: 0.3252\n",
      "3/15, train_loss: 0.7945\n",
      "4/15, train_loss: 0.6550\n",
      "5/15, train_loss: 0.4997\n",
      "6/15, train_loss: 0.3240\n",
      "7/15, train_loss: 0.3146\n",
      "8/15, train_loss: 0.3598\n",
      "9/15, train_loss: 0.3457\n",
      "10/15, train_loss: 0.3474\n",
      "11/15, train_loss: 0.3100\n",
      "12/15, train_loss: 0.3412\n",
      "13/15, train_loss: 0.3516\n",
      "14/15, train_loss: 0.4878\n",
      "15/15, train_loss: 0.4144\n",
      "16/15, train_loss: 0.3939\n",
      "epoch 206 average loss: 0.4160\n",
      "----------\n",
      "epoch 207/500\n",
      "1/15, train_loss: 0.4109\n",
      "2/15, train_loss: 0.8126\n",
      "3/15, train_loss: 0.3600\n",
      "4/15, train_loss: 0.3880\n",
      "5/15, train_loss: 0.3693\n",
      "6/15, train_loss: 0.3051\n",
      "7/15, train_loss: 0.3510\n",
      "8/15, train_loss: 0.3451\n",
      "9/15, train_loss: 0.5693\n",
      "10/15, train_loss: 0.3998\n",
      "11/15, train_loss: 0.3011\n",
      "12/15, train_loss: 0.3866\n",
      "13/15, train_loss: 0.4309\n",
      "14/15, train_loss: 0.3543\n",
      "15/15, train_loss: 0.3642\n",
      "16/15, train_loss: 0.3803\n",
      "epoch 207 average loss: 0.4080\n",
      "----------\n",
      "epoch 208/500\n",
      "1/15, train_loss: 0.3705\n",
      "2/15, train_loss: 0.3748\n",
      "3/15, train_loss: 0.4030\n",
      "4/15, train_loss: 0.3336\n",
      "5/15, train_loss: 0.3395\n",
      "6/15, train_loss: 2.5644\n",
      "7/15, train_loss: 0.3477\n",
      "8/15, train_loss: 0.3505\n",
      "9/15, train_loss: 0.3139\n",
      "10/15, train_loss: 0.3300\n",
      "11/15, train_loss: 0.7862\n",
      "12/15, train_loss: 0.3203\n",
      "13/15, train_loss: 0.3585\n",
      "14/15, train_loss: 0.2998\n",
      "15/15, train_loss: 0.3576\n",
      "16/15, train_loss: 0.5781\n",
      "epoch 208 average loss: 0.5268\n",
      "----------\n",
      "epoch 209/500\n",
      "1/15, train_loss: 0.3701\n",
      "2/15, train_loss: 0.3827\n",
      "3/15, train_loss: 0.3693\n",
      "4/15, train_loss: 0.3678\n",
      "5/15, train_loss: 0.4030\n",
      "6/15, train_loss: 0.3249\n",
      "7/15, train_loss: 0.5741\n",
      "8/15, train_loss: 0.3387\n",
      "9/15, train_loss: 0.3338\n",
      "10/15, train_loss: 0.4365\n",
      "11/15, train_loss: 0.3547\n",
      "12/15, train_loss: 0.5591\n",
      "13/15, train_loss: 0.3121\n",
      "14/15, train_loss: 0.3218\n",
      "15/15, train_loss: 0.5386\n",
      "16/15, train_loss: 0.8187\n",
      "epoch 209 average loss: 0.4254\n",
      "----------\n",
      "epoch 210/500\n",
      "1/15, train_loss: 0.3417\n",
      "2/15, train_loss: 0.3898\n",
      "3/15, train_loss: 0.3274\n",
      "4/15, train_loss: 0.5397\n",
      "5/15, train_loss: 0.4441\n",
      "6/15, train_loss: 0.3719\n",
      "7/15, train_loss: 0.4879\n",
      "8/15, train_loss: 0.3317\n",
      "9/15, train_loss: 0.5143\n",
      "10/15, train_loss: 0.3399\n",
      "11/15, train_loss: 0.3423\n",
      "12/15, train_loss: 0.3348\n",
      "13/15, train_loss: 0.4453\n",
      "14/15, train_loss: 0.4028\n",
      "15/15, train_loss: 0.3154\n",
      "16/15, train_loss: 2.2957\n",
      "epoch 210 average loss: 0.5140\n",
      "----------\n",
      "epoch 211/500\n",
      "1/15, train_loss: 0.3713\n",
      "2/15, train_loss: 0.3133\n",
      "3/15, train_loss: 0.4236\n",
      "4/15, train_loss: 0.3207\n",
      "5/15, train_loss: 0.4048\n",
      "6/15, train_loss: 0.3696\n",
      "7/15, train_loss: 0.3160\n",
      "8/15, train_loss: 0.8332\n",
      "9/15, train_loss: 0.3294\n",
      "10/15, train_loss: 0.3842\n",
      "11/15, train_loss: 0.3575\n",
      "12/15, train_loss: 0.3961\n",
      "13/15, train_loss: 0.3759\n",
      "14/15, train_loss: 0.3436\n",
      "15/15, train_loss: 0.3905\n",
      "16/15, train_loss: 0.3861\n",
      "epoch 211 average loss: 0.3947\n",
      "----------\n",
      "epoch 212/500\n",
      "1/15, train_loss: 0.3016\n",
      "2/15, train_loss: 0.4372\n",
      "3/15, train_loss: 0.3747\n",
      "4/15, train_loss: 0.3436\n",
      "5/15, train_loss: 0.3467\n",
      "6/15, train_loss: 0.3434\n",
      "7/15, train_loss: 0.3329\n",
      "8/15, train_loss: 0.4109\n",
      "9/15, train_loss: 0.4440\n",
      "10/15, train_loss: 0.3137\n",
      "11/15, train_loss: 1.0478\n",
      "12/15, train_loss: 0.4905\n",
      "13/15, train_loss: 0.3186\n",
      "14/15, train_loss: 0.3704\n",
      "15/15, train_loss: 0.7380\n",
      "16/15, train_loss: 0.3811\n",
      "epoch 212 average loss: 0.4372\n",
      "----------\n",
      "epoch 213/500\n",
      "1/15, train_loss: 0.3963\n",
      "2/15, train_loss: 0.3654\n",
      "3/15, train_loss: 0.4782\n",
      "4/15, train_loss: 0.3602\n",
      "5/15, train_loss: 2.1800\n",
      "6/15, train_loss: 0.4221\n",
      "7/15, train_loss: 0.3802\n",
      "8/15, train_loss: 0.3553\n",
      "9/15, train_loss: 0.3095\n",
      "10/15, train_loss: 0.3425\n",
      "11/15, train_loss: 0.3153\n",
      "12/15, train_loss: 0.3224\n",
      "13/15, train_loss: 0.3943\n",
      "14/15, train_loss: 0.3031\n",
      "15/15, train_loss: 0.3872\n",
      "16/15, train_loss: 0.4804\n",
      "epoch 213 average loss: 0.4870\n",
      "----------\n",
      "epoch 214/500\n",
      "1/15, train_loss: 0.4941\n",
      "2/15, train_loss: 0.3517\n",
      "3/15, train_loss: 0.3480\n",
      "4/15, train_loss: 0.3465\n",
      "5/15, train_loss: 0.4374\n",
      "6/15, train_loss: 0.4141\n",
      "7/15, train_loss: 0.4209\n",
      "8/15, train_loss: 0.8144\n",
      "9/15, train_loss: 0.5629\n",
      "10/15, train_loss: 0.9753\n",
      "11/15, train_loss: 0.3366\n",
      "12/15, train_loss: 0.3317\n",
      "13/15, train_loss: 0.3186\n",
      "14/15, train_loss: 0.5421\n",
      "15/15, train_loss: 0.4141\n",
      "16/15, train_loss: 0.2840\n",
      "epoch 214 average loss: 0.4620\n",
      "----------\n",
      "epoch 215/500\n",
      "1/15, train_loss: 0.3752\n",
      "2/15, train_loss: 0.3024\n",
      "3/15, train_loss: 0.2934\n",
      "4/15, train_loss: 0.3329\n",
      "5/15, train_loss: 0.3472\n",
      "6/15, train_loss: 0.4779\n",
      "7/15, train_loss: 0.3616\n",
      "8/15, train_loss: 0.3637\n",
      "9/15, train_loss: 0.4510\n",
      "10/15, train_loss: 0.2981\n",
      "11/15, train_loss: 0.3194\n",
      "12/15, train_loss: 0.3048\n",
      "13/15, train_loss: 0.3098\n",
      "14/15, train_loss: 0.3771\n",
      "15/15, train_loss: 0.3227\n",
      "16/15, train_loss: 0.3754\n",
      "epoch 215 average loss: 0.3508\n",
      "----------\n",
      "epoch 216/500\n",
      "1/15, train_loss: 0.4969\n",
      "2/15, train_loss: 0.5901\n",
      "3/15, train_loss: 0.4235\n",
      "4/15, train_loss: 0.3213\n",
      "5/15, train_loss: 0.3322\n",
      "6/15, train_loss: 0.4485\n",
      "7/15, train_loss: 0.3483\n",
      "8/15, train_loss: 0.3923\n",
      "9/15, train_loss: 0.3239\n",
      "10/15, train_loss: 0.3835\n",
      "11/15, train_loss: 0.3127\n",
      "12/15, train_loss: 0.3224\n",
      "13/15, train_loss: 0.4432\n",
      "14/15, train_loss: 0.4280\n",
      "15/15, train_loss: 0.3845\n",
      "16/15, train_loss: 0.3370\n",
      "epoch 216 average loss: 0.3930\n",
      "----------\n",
      "epoch 217/500\n",
      "1/15, train_loss: 0.3281\n",
      "2/15, train_loss: 0.4346\n",
      "3/15, train_loss: 0.3212\n",
      "4/15, train_loss: 0.4977\n",
      "5/15, train_loss: 0.7388\n",
      "6/15, train_loss: 0.3610\n",
      "7/15, train_loss: 0.3228\n",
      "8/15, train_loss: 0.3125\n",
      "9/15, train_loss: 0.3376\n",
      "10/15, train_loss: 0.3009\n",
      "11/15, train_loss: 0.3078\n",
      "12/15, train_loss: 0.4836\n",
      "13/15, train_loss: 0.4400\n",
      "14/15, train_loss: 0.6840\n",
      "15/15, train_loss: 0.2979\n",
      "16/15, train_loss: 0.3381\n",
      "epoch 217 average loss: 0.4067\n",
      "----------\n",
      "epoch 218/500\n",
      "1/15, train_loss: 0.4156\n",
      "2/15, train_loss: 0.3700\n",
      "3/15, train_loss: 0.3731\n",
      "4/15, train_loss: 0.8589\n",
      "5/15, train_loss: 0.4594\n",
      "6/15, train_loss: 0.3619\n",
      "7/15, train_loss: 0.3453\n",
      "8/15, train_loss: 0.3709\n",
      "9/15, train_loss: 0.3184\n",
      "10/15, train_loss: 0.3324\n",
      "11/15, train_loss: 0.3418\n",
      "12/15, train_loss: 0.3442\n",
      "13/15, train_loss: 0.3839\n",
      "14/15, train_loss: 0.4263\n",
      "15/15, train_loss: 0.4333\n",
      "16/15, train_loss: 0.4987\n",
      "epoch 218 average loss: 0.4146\n",
      "----------\n",
      "epoch 219/500\n",
      "1/15, train_loss: 0.4050\n",
      "2/15, train_loss: 0.3178\n",
      "3/15, train_loss: 0.3823\n",
      "4/15, train_loss: 0.3604\n",
      "5/15, train_loss: 0.3526\n",
      "6/15, train_loss: 0.3830\n",
      "7/15, train_loss: 0.3747\n",
      "8/15, train_loss: 0.2952\n",
      "9/15, train_loss: 0.3125\n",
      "10/15, train_loss: 0.3697\n",
      "11/15, train_loss: 0.3266\n",
      "12/15, train_loss: 0.4248\n",
      "13/15, train_loss: 0.3229\n",
      "14/15, train_loss: 0.3201\n",
      "15/15, train_loss: 0.3704\n",
      "16/15, train_loss: 0.4626\n",
      "epoch 219 average loss: 0.3613\n",
      "----------\n",
      "epoch 220/500\n",
      "1/15, train_loss: 0.3761\n",
      "2/15, train_loss: 0.2941\n",
      "3/15, train_loss: 0.4432\n",
      "4/15, train_loss: 0.4095\n",
      "5/15, train_loss: 0.3376\n",
      "6/15, train_loss: 0.4535\n",
      "7/15, train_loss: 2.3206\n",
      "8/15, train_loss: 0.3239\n",
      "9/15, train_loss: 0.3691\n",
      "10/15, train_loss: 0.3995\n",
      "11/15, train_loss: 0.3485\n",
      "12/15, train_loss: 0.3456\n",
      "13/15, train_loss: 0.3185\n",
      "14/15, train_loss: 0.3001\n",
      "15/15, train_loss: 0.3353\n",
      "16/15, train_loss: 0.6530\n",
      "epoch 220 average loss: 0.5018\n",
      "----------\n",
      "epoch 221/500\n",
      "1/15, train_loss: 0.3317\n",
      "2/15, train_loss: 0.3409\n",
      "3/15, train_loss: 0.3534\n",
      "4/15, train_loss: 0.3291\n",
      "5/15, train_loss: 0.8346\n",
      "6/15, train_loss: 0.5845\n",
      "7/15, train_loss: 0.3098\n",
      "8/15, train_loss: 0.4165\n",
      "9/15, train_loss: 0.3306\n",
      "10/15, train_loss: 0.3860\n",
      "11/15, train_loss: 0.3710\n",
      "12/15, train_loss: 0.5679\n",
      "13/15, train_loss: 0.3315\n",
      "14/15, train_loss: 0.3025\n",
      "15/15, train_loss: 0.3537\n",
      "16/15, train_loss: 0.2977\n",
      "epoch 221 average loss: 0.4026\n",
      "----------\n",
      "epoch 222/500\n",
      "1/15, train_loss: 0.3322\n",
      "2/15, train_loss: 0.3322\n",
      "3/15, train_loss: 0.8905\n",
      "4/15, train_loss: 0.3108\n",
      "5/15, train_loss: 0.5716\n",
      "6/15, train_loss: 0.4126\n",
      "7/15, train_loss: 0.3600\n",
      "8/15, train_loss: 0.3673\n",
      "9/15, train_loss: 0.3681\n",
      "10/15, train_loss: 0.3275\n",
      "11/15, train_loss: 0.3675\n",
      "12/15, train_loss: 0.2889\n",
      "13/15, train_loss: 0.3806\n",
      "14/15, train_loss: 0.2848\n",
      "15/15, train_loss: 0.4129\n",
      "16/15, train_loss: 0.2849\n",
      "epoch 222 average loss: 0.3933\n",
      "----------\n",
      "epoch 223/500\n",
      "1/15, train_loss: 0.3812\n",
      "2/15, train_loss: 0.3174\n",
      "3/15, train_loss: 0.4480\n",
      "4/15, train_loss: 0.3122\n",
      "5/15, train_loss: 0.3627\n",
      "6/15, train_loss: 0.3634\n",
      "7/15, train_loss: 0.3604\n",
      "8/15, train_loss: 0.3565\n",
      "9/15, train_loss: 0.3626\n",
      "10/15, train_loss: 0.3255\n",
      "11/15, train_loss: 0.4233\n",
      "12/15, train_loss: 0.4116\n",
      "13/15, train_loss: 0.3615\n",
      "14/15, train_loss: 0.3334\n",
      "15/15, train_loss: 0.4352\n",
      "16/15, train_loss: 0.3887\n",
      "epoch 223 average loss: 0.3715\n",
      "----------\n",
      "epoch 224/500\n",
      "1/15, train_loss: 0.6396\n",
      "2/15, train_loss: 0.4044\n",
      "3/15, train_loss: 0.3424\n",
      "4/15, train_loss: 0.4156\n",
      "5/15, train_loss: 0.6517\n",
      "6/15, train_loss: 0.3565\n",
      "7/15, train_loss: 0.3308\n",
      "8/15, train_loss: 0.2967\n",
      "9/15, train_loss: 0.3044\n",
      "10/15, train_loss: 0.4513\n",
      "11/15, train_loss: 0.3068\n",
      "12/15, train_loss: 0.2973\n",
      "13/15, train_loss: 0.3520\n",
      "14/15, train_loss: 0.3039\n",
      "15/15, train_loss: 0.3647\n",
      "16/15, train_loss: 2.1394\n",
      "epoch 224 average loss: 0.4973\n",
      "----------\n",
      "epoch 225/500\n",
      "1/15, train_loss: 0.3526\n",
      "2/15, train_loss: 0.3124\n",
      "3/15, train_loss: 0.3402\n",
      "4/15, train_loss: 0.3254\n",
      "5/15, train_loss: 0.2955\n",
      "6/15, train_loss: 0.5614\n",
      "7/15, train_loss: 0.2705\n",
      "8/15, train_loss: 0.3056\n",
      "9/15, train_loss: 0.3557\n",
      "10/15, train_loss: 0.2813\n",
      "11/15, train_loss: 0.3763\n",
      "12/15, train_loss: 0.4953\n",
      "13/15, train_loss: 0.3144\n",
      "14/15, train_loss: 0.3754\n",
      "15/15, train_loss: 0.4321\n",
      "16/15, train_loss: 0.3051\n",
      "epoch 225 average loss: 0.3562\n",
      "----------\n",
      "epoch 226/500\n",
      "1/15, train_loss: 0.8204\n",
      "2/15, train_loss: 0.2953\n",
      "3/15, train_loss: 0.3534\n",
      "4/15, train_loss: 0.3927\n",
      "5/15, train_loss: 0.3483\n",
      "6/15, train_loss: 0.3929\n",
      "7/15, train_loss: 0.3785\n",
      "8/15, train_loss: 0.4523\n",
      "9/15, train_loss: 0.5360\n",
      "10/15, train_loss: 0.8261\n",
      "11/15, train_loss: 0.3580\n",
      "12/15, train_loss: 0.3963\n",
      "13/15, train_loss: 0.3132\n",
      "14/15, train_loss: 0.4592\n",
      "15/15, train_loss: 0.5832\n",
      "16/15, train_loss: 0.3665\n",
      "epoch 226 average loss: 0.4545\n",
      "----------\n",
      "epoch 227/500\n",
      "1/15, train_loss: 0.3017\n",
      "2/15, train_loss: 0.3565\n",
      "3/15, train_loss: 0.3061\n",
      "4/15, train_loss: 0.3943\n",
      "5/15, train_loss: 0.3681\n",
      "6/15, train_loss: 0.2821\n",
      "7/15, train_loss: 0.5062\n",
      "8/15, train_loss: 0.3035\n",
      "9/15, train_loss: 0.4430\n",
      "10/15, train_loss: 0.3076\n",
      "11/15, train_loss: 0.3484\n",
      "12/15, train_loss: 0.3442\n",
      "13/15, train_loss: 0.3186\n",
      "14/15, train_loss: 0.3917\n",
      "15/15, train_loss: 0.3328\n",
      "16/15, train_loss: 0.3908\n",
      "epoch 227 average loss: 0.3560\n",
      "----------\n",
      "epoch 228/500\n",
      "1/15, train_loss: 0.4115\n",
      "2/15, train_loss: 0.6183\n",
      "3/15, train_loss: 0.3626\n",
      "4/15, train_loss: 0.2922\n",
      "5/15, train_loss: 0.3307\n",
      "6/15, train_loss: 0.3710\n",
      "7/15, train_loss: 0.3501\n",
      "8/15, train_loss: 0.4496\n",
      "9/15, train_loss: 0.2818\n",
      "10/15, train_loss: 0.5991\n",
      "11/15, train_loss: 0.3139\n",
      "12/15, train_loss: 0.3765\n",
      "13/15, train_loss: 0.2998\n",
      "14/15, train_loss: 0.4311\n",
      "15/15, train_loss: 0.3041\n",
      "16/15, train_loss: 2.3914\n",
      "epoch 228 average loss: 0.5115\n",
      "----------\n",
      "epoch 229/500\n",
      "1/15, train_loss: 0.3785\n",
      "2/15, train_loss: 0.4651\n",
      "3/15, train_loss: 0.7900\n",
      "4/15, train_loss: 0.3025\n",
      "5/15, train_loss: 0.3982\n",
      "6/15, train_loss: 0.3999\n",
      "7/15, train_loss: 0.3862\n",
      "8/15, train_loss: 0.3086\n",
      "9/15, train_loss: 0.2966\n",
      "10/15, train_loss: 0.3385\n",
      "11/15, train_loss: 0.3181\n",
      "12/15, train_loss: 0.4766\n",
      "13/15, train_loss: 0.3340\n",
      "14/15, train_loss: 0.4059\n",
      "15/15, train_loss: 0.3740\n",
      "16/15, train_loss: 0.3193\n",
      "epoch 229 average loss: 0.3932\n",
      "----------\n",
      "epoch 230/500\n",
      "1/15, train_loss: 0.4533\n",
      "2/15, train_loss: 0.3073\n",
      "3/15, train_loss: 0.3855\n",
      "4/15, train_loss: 0.3350\n",
      "5/15, train_loss: 0.3859\n",
      "6/15, train_loss: 0.3270\n",
      "7/15, train_loss: 0.5224\n",
      "8/15, train_loss: 0.3336\n",
      "9/15, train_loss: 0.3867\n",
      "10/15, train_loss: 0.3196\n",
      "11/15, train_loss: 0.3260\n",
      "12/15, train_loss: 0.3104\n",
      "13/15, train_loss: 0.3278\n",
      "14/15, train_loss: 0.3306\n",
      "15/15, train_loss: 0.2986\n",
      "16/15, train_loss: 0.2905\n",
      "epoch 230 average loss: 0.3525\n",
      "----------\n",
      "epoch 231/500\n",
      "1/15, train_loss: 0.3185\n",
      "2/15, train_loss: 0.2841\n",
      "3/15, train_loss: 0.3667\n",
      "4/15, train_loss: 0.6506\n",
      "5/15, train_loss: 0.9255\n",
      "6/15, train_loss: 0.3200\n",
      "7/15, train_loss: 0.2991\n",
      "8/15, train_loss: 0.3436\n",
      "9/15, train_loss: 0.3171\n",
      "10/15, train_loss: 0.3486\n",
      "11/15, train_loss: 0.3291\n",
      "12/15, train_loss: 0.3099\n",
      "13/15, train_loss: 0.3886\n",
      "14/15, train_loss: 0.3507\n",
      "15/15, train_loss: 0.3194\n",
      "16/15, train_loss: 0.4062\n",
      "epoch 231 average loss: 0.3924\n",
      "----------\n",
      "epoch 232/500\n",
      "1/15, train_loss: 0.2939\n",
      "2/15, train_loss: 0.3468\n",
      "3/15, train_loss: 0.3545\n",
      "4/15, train_loss: 0.3503\n",
      "5/15, train_loss: 0.3467\n",
      "6/15, train_loss: 0.3637\n",
      "7/15, train_loss: 0.3065\n",
      "8/15, train_loss: 0.3428\n",
      "9/15, train_loss: 0.3282\n",
      "10/15, train_loss: 0.3886\n",
      "11/15, train_loss: 0.4024\n",
      "12/15, train_loss: 0.3523\n",
      "13/15, train_loss: 0.2843\n",
      "14/15, train_loss: 0.3050\n",
      "15/15, train_loss: 0.3484\n",
      "16/15, train_loss: 0.4412\n",
      "epoch 232 average loss: 0.3472\n",
      "----------\n",
      "epoch 233/500\n",
      "1/15, train_loss: 0.3554\n",
      "2/15, train_loss: 0.2953\n",
      "3/15, train_loss: 0.3758\n",
      "4/15, train_loss: 0.3173\n",
      "5/15, train_loss: 0.2966\n",
      "6/15, train_loss: 0.6875\n",
      "7/15, train_loss: 0.3120\n",
      "8/15, train_loss: 0.3383\n",
      "9/15, train_loss: 0.3072\n",
      "10/15, train_loss: 0.2843\n",
      "11/15, train_loss: 0.2951\n",
      "12/15, train_loss: 0.2978\n",
      "13/15, train_loss: 0.4599\n",
      "14/15, train_loss: 0.3292\n",
      "15/15, train_loss: 0.3112\n",
      "16/15, train_loss: 0.3164\n",
      "epoch 233 average loss: 0.3487\n",
      "----------\n",
      "epoch 234/500\n",
      "1/15, train_loss: 0.2918\n",
      "2/15, train_loss: 0.8053\n",
      "3/15, train_loss: 0.3756\n",
      "4/15, train_loss: 0.3103\n",
      "5/15, train_loss: 0.4277\n",
      "6/15, train_loss: 0.3996\n",
      "7/15, train_loss: 0.3321\n",
      "8/15, train_loss: 0.3269\n",
      "9/15, train_loss: 0.3266\n",
      "10/15, train_loss: 0.3296\n",
      "11/15, train_loss: 0.3323\n",
      "12/15, train_loss: 0.4031\n",
      "13/15, train_loss: 0.3145\n",
      "14/15, train_loss: 0.3215\n",
      "15/15, train_loss: 0.3906\n",
      "16/15, train_loss: 0.3513\n",
      "epoch 234 average loss: 0.3774\n",
      "----------\n",
      "epoch 235/500\n",
      "1/15, train_loss: 0.3380\n",
      "2/15, train_loss: 0.3243\n",
      "3/15, train_loss: 0.4323\n",
      "4/15, train_loss: 0.3943\n",
      "5/15, train_loss: 0.5266\n",
      "6/15, train_loss: 0.3084\n",
      "7/15, train_loss: 0.3127\n",
      "8/15, train_loss: 0.2793\n",
      "9/15, train_loss: 0.5520\n",
      "10/15, train_loss: 0.3813\n",
      "11/15, train_loss: 0.3783\n",
      "12/15, train_loss: 0.2915\n",
      "13/15, train_loss: 0.2826\n",
      "14/15, train_loss: 0.3434\n",
      "15/15, train_loss: 0.2790\n",
      "16/15, train_loss: 0.4624\n",
      "epoch 235 average loss: 0.3679\n",
      "----------\n",
      "epoch 236/500\n",
      "1/15, train_loss: 0.2737\n",
      "2/15, train_loss: 0.3037\n",
      "3/15, train_loss: 0.3414\n",
      "4/15, train_loss: 0.3010\n",
      "5/15, train_loss: 0.3917\n",
      "6/15, train_loss: 0.3062\n",
      "7/15, train_loss: 0.3667\n",
      "8/15, train_loss: 0.4410\n",
      "9/15, train_loss: 0.3512\n",
      "10/15, train_loss: 0.3922\n",
      "11/15, train_loss: 0.2932\n",
      "12/15, train_loss: 0.2723\n",
      "13/15, train_loss: 0.4254\n",
      "14/15, train_loss: 0.2812\n",
      "15/15, train_loss: 0.3574\n",
      "16/15, train_loss: 0.4194\n",
      "epoch 236 average loss: 0.3448\n",
      "----------\n",
      "epoch 237/500\n",
      "1/15, train_loss: 0.4175\n",
      "2/15, train_loss: 0.3415\n",
      "3/15, train_loss: 0.2967\n",
      "4/15, train_loss: 0.4064\n",
      "5/15, train_loss: 0.2847\n",
      "6/15, train_loss: 0.3338\n",
      "7/15, train_loss: 0.3081\n",
      "8/15, train_loss: 0.2633\n",
      "9/15, train_loss: 0.3193\n",
      "10/15, train_loss: 0.3331\n",
      "11/15, train_loss: 0.2978\n",
      "12/15, train_loss: 0.4231\n",
      "13/15, train_loss: 0.3176\n",
      "14/15, train_loss: 0.4796\n",
      "15/15, train_loss: 0.3031\n",
      "16/15, train_loss: 0.2618\n",
      "epoch 237 average loss: 0.3367\n",
      "----------\n",
      "epoch 238/500\n",
      "1/15, train_loss: 0.3674\n",
      "2/15, train_loss: 0.3974\n",
      "3/15, train_loss: 0.3032\n",
      "4/15, train_loss: 0.3257\n",
      "5/15, train_loss: 0.4902\n",
      "6/15, train_loss: 0.3128\n",
      "7/15, train_loss: 0.3120\n",
      "8/15, train_loss: 0.2990\n",
      "9/15, train_loss: 0.3503\n",
      "10/15, train_loss: 0.3375\n",
      "11/15, train_loss: 0.3117\n",
      "12/15, train_loss: 0.3505\n",
      "13/15, train_loss: 0.3751\n",
      "14/15, train_loss: 0.3007\n",
      "15/15, train_loss: 0.3296\n",
      "16/15, train_loss: 0.4661\n",
      "epoch 238 average loss: 0.3518\n",
      "----------\n",
      "epoch 239/500\n",
      "1/15, train_loss: 0.2880\n",
      "2/15, train_loss: 0.3269\n",
      "3/15, train_loss: 0.2735\n",
      "4/15, train_loss: 0.2935\n",
      "5/15, train_loss: 0.3089\n",
      "6/15, train_loss: 0.2773\n",
      "7/15, train_loss: 0.3424\n",
      "8/15, train_loss: 0.5066\n",
      "9/15, train_loss: 0.4066\n",
      "10/15, train_loss: 0.3421\n",
      "11/15, train_loss: 0.3101\n",
      "12/15, train_loss: 0.3578\n",
      "13/15, train_loss: 0.3536\n",
      "14/15, train_loss: 0.2736\n",
      "15/15, train_loss: 0.3804\n",
      "16/15, train_loss: 2.0884\n",
      "epoch 239 average loss: 0.4456\n",
      "----------\n",
      "epoch 240/500\n",
      "1/15, train_loss: 0.2985\n",
      "2/15, train_loss: 0.2931\n",
      "3/15, train_loss: 0.3047\n",
      "4/15, train_loss: 0.3519\n",
      "5/15, train_loss: 0.3721\n",
      "6/15, train_loss: 0.3870\n",
      "7/15, train_loss: 0.2903\n",
      "8/15, train_loss: 0.2983\n",
      "9/15, train_loss: 0.3645\n",
      "10/15, train_loss: 0.8255\n",
      "11/15, train_loss: 0.4743\n",
      "12/15, train_loss: 0.2809\n",
      "13/15, train_loss: 0.2552\n",
      "14/15, train_loss: 0.3100\n",
      "15/15, train_loss: 0.2674\n",
      "16/15, train_loss: 0.3223\n",
      "epoch 240 average loss: 0.3560\n",
      "----------\n",
      "epoch 241/500\n",
      "1/15, train_loss: 0.3113\n",
      "2/15, train_loss: 0.3289\n",
      "3/15, train_loss: 0.3323\n",
      "4/15, train_loss: 0.3269\n",
      "5/15, train_loss: 0.7002\n",
      "6/15, train_loss: 0.3049\n",
      "7/15, train_loss: 0.3387\n",
      "8/15, train_loss: 0.3304\n",
      "9/15, train_loss: 0.2793\n",
      "10/15, train_loss: 0.3479\n",
      "11/15, train_loss: 0.2861\n",
      "12/15, train_loss: 0.3019\n",
      "13/15, train_loss: 0.4124\n",
      "14/15, train_loss: 0.2830\n",
      "15/15, train_loss: 0.3074\n",
      "16/15, train_loss: 0.4291\n",
      "epoch 241 average loss: 0.3513\n",
      "----------\n",
      "epoch 242/500\n",
      "1/15, train_loss: 0.2846\n",
      "2/15, train_loss: 0.4249\n",
      "3/15, train_loss: 0.3258\n",
      "4/15, train_loss: 0.3548\n",
      "5/15, train_loss: 0.2942\n",
      "6/15, train_loss: 0.2857\n",
      "7/15, train_loss: 0.3155\n",
      "8/15, train_loss: 0.2858\n",
      "9/15, train_loss: 0.6005\n",
      "10/15, train_loss: 0.3211\n",
      "11/15, train_loss: 0.3377\n",
      "12/15, train_loss: 0.3173\n",
      "13/15, train_loss: 0.5359\n",
      "14/15, train_loss: 0.2999\n",
      "15/15, train_loss: 0.2992\n",
      "16/15, train_loss: 0.3357\n",
      "epoch 242 average loss: 0.3512\n",
      "----------\n",
      "epoch 243/500\n",
      "1/15, train_loss: 0.3874\n",
      "2/15, train_loss: 0.3071\n",
      "3/15, train_loss: 0.4293\n",
      "4/15, train_loss: 0.3072\n",
      "5/15, train_loss: 0.3901\n",
      "6/15, train_loss: 0.6418\n",
      "7/15, train_loss: 0.3462\n",
      "8/15, train_loss: 0.3012\n",
      "9/15, train_loss: 0.2915\n",
      "10/15, train_loss: 0.3329\n",
      "11/15, train_loss: 0.2965\n",
      "12/15, train_loss: 0.2869\n",
      "13/15, train_loss: 0.3990\n",
      "14/15, train_loss: 2.2922\n",
      "15/15, train_loss: 0.3501\n",
      "16/15, train_loss: 0.2995\n",
      "epoch 243 average loss: 0.4787\n",
      "----------\n",
      "epoch 244/500\n",
      "1/15, train_loss: 0.3448\n",
      "2/15, train_loss: 0.3013\n",
      "3/15, train_loss: 0.3146\n",
      "4/15, train_loss: 0.2894\n",
      "5/15, train_loss: 0.3296\n",
      "6/15, train_loss: 0.2849\n",
      "7/15, train_loss: 0.5510\n",
      "8/15, train_loss: 0.5894\n",
      "9/15, train_loss: 0.2539\n",
      "10/15, train_loss: 0.6862\n",
      "11/15, train_loss: 0.3216\n",
      "12/15, train_loss: 0.4046\n",
      "13/15, train_loss: 0.2697\n",
      "14/15, train_loss: 0.3178\n",
      "15/15, train_loss: 0.2728\n",
      "16/15, train_loss: 0.3974\n",
      "epoch 244 average loss: 0.3706\n",
      "----------\n",
      "epoch 245/500\n",
      "1/15, train_loss: 0.3375\n",
      "2/15, train_loss: 0.2968\n",
      "3/15, train_loss: 0.3801\n",
      "4/15, train_loss: 0.4384\n",
      "5/15, train_loss: 0.3247\n",
      "6/15, train_loss: 0.2825\n",
      "7/15, train_loss: 0.2836\n",
      "8/15, train_loss: 0.3299\n",
      "9/15, train_loss: 0.3605\n",
      "10/15, train_loss: 0.2936\n",
      "11/15, train_loss: 0.3444\n",
      "12/15, train_loss: 0.3035\n",
      "13/15, train_loss: 0.2974\n",
      "14/15, train_loss: 0.3778\n",
      "15/15, train_loss: 0.3529\n",
      "16/15, train_loss: 0.6951\n",
      "epoch 245 average loss: 0.3562\n",
      "----------\n",
      "epoch 246/500\n",
      "1/15, train_loss: 0.3087\n",
      "2/15, train_loss: 0.2933\n",
      "3/15, train_loss: 0.3100\n",
      "4/15, train_loss: 0.2920\n",
      "5/15, train_loss: 0.3238\n",
      "6/15, train_loss: 0.3929\n",
      "7/15, train_loss: 0.2469\n",
      "8/15, train_loss: 0.3587\n",
      "9/15, train_loss: 0.3036\n",
      "10/15, train_loss: 0.3510\n",
      "11/15, train_loss: 0.3500\n",
      "12/15, train_loss: 0.3127\n",
      "13/15, train_loss: 0.4706\n",
      "14/15, train_loss: 0.3620\n",
      "15/15, train_loss: 0.3651\n",
      "16/15, train_loss: 0.3050\n",
      "epoch 246 average loss: 0.3341\n",
      "----------\n",
      "epoch 247/500\n",
      "1/15, train_loss: 0.2618\n",
      "2/15, train_loss: 0.2775\n",
      "3/15, train_loss: 0.3203\n",
      "4/15, train_loss: 0.3948\n",
      "5/15, train_loss: 0.2830\n",
      "6/15, train_loss: 0.4469\n",
      "7/15, train_loss: 0.4848\n",
      "8/15, train_loss: 0.3440\n",
      "9/15, train_loss: 0.3134\n",
      "10/15, train_loss: 0.3019\n",
      "11/15, train_loss: 0.3194\n",
      "12/15, train_loss: 0.2894\n",
      "13/15, train_loss: 0.3118\n",
      "14/15, train_loss: 0.2965\n",
      "15/15, train_loss: 0.2689\n",
      "16/15, train_loss: 0.3560\n",
      "epoch 247 average loss: 0.3294\n",
      "----------\n",
      "epoch 248/500\n",
      "1/15, train_loss: 0.3516\n",
      "2/15, train_loss: 0.2925\n",
      "3/15, train_loss: 0.3158\n",
      "4/15, train_loss: 0.3028\n",
      "5/15, train_loss: 0.3021\n",
      "6/15, train_loss: 0.3078\n",
      "7/15, train_loss: 0.2891\n",
      "8/15, train_loss: 0.4032\n",
      "9/15, train_loss: 2.2465\n",
      "10/15, train_loss: 0.2677\n",
      "11/15, train_loss: 0.3251\n",
      "12/15, train_loss: 0.3956\n",
      "13/15, train_loss: 0.3211\n",
      "14/15, train_loss: 0.2977\n",
      "15/15, train_loss: 0.2600\n",
      "16/15, train_loss: 0.3918\n",
      "epoch 248 average loss: 0.4419\n",
      "----------\n",
      "epoch 249/500\n",
      "1/15, train_loss: 0.2736\n",
      "2/15, train_loss: 0.2954\n",
      "3/15, train_loss: 0.3061\n",
      "4/15, train_loss: 0.3597\n",
      "5/15, train_loss: 0.3580\n",
      "6/15, train_loss: 0.4849\n",
      "7/15, train_loss: 0.3189\n",
      "8/15, train_loss: 2.1342\n",
      "9/15, train_loss: 0.3353\n",
      "10/15, train_loss: 0.3516\n",
      "11/15, train_loss: 2.2278\n",
      "12/15, train_loss: 0.2918\n",
      "13/15, train_loss: 0.2807\n",
      "14/15, train_loss: 0.2507\n",
      "15/15, train_loss: 0.3455\n",
      "16/15, train_loss: 0.2973\n",
      "epoch 249 average loss: 0.5570\n",
      "----------\n",
      "epoch 250/500\n",
      "1/15, train_loss: 0.2975\n",
      "2/15, train_loss: 0.2760\n",
      "3/15, train_loss: 0.3120\n",
      "4/15, train_loss: 0.3189\n",
      "5/15, train_loss: 0.3228\n",
      "6/15, train_loss: 1.6293\n",
      "7/15, train_loss: 0.2575\n",
      "8/15, train_loss: 0.3667\n",
      "9/15, train_loss: 0.4393\n",
      "10/15, train_loss: 0.2753\n",
      "11/15, train_loss: 0.3001\n",
      "12/15, train_loss: 0.6265\n",
      "13/15, train_loss: 0.2843\n",
      "14/15, train_loss: 0.3586\n",
      "15/15, train_loss: 0.3420\n",
      "16/15, train_loss: 0.4957\n",
      "epoch 250 average loss: 0.4314\n",
      "----------\n",
      "epoch 251/500\n",
      "1/15, train_loss: 0.3458\n",
      "2/15, train_loss: 0.3328\n",
      "3/15, train_loss: 0.4469\n",
      "4/15, train_loss: 0.3178\n",
      "5/15, train_loss: 0.2993\n",
      "6/15, train_loss: 0.3020\n",
      "7/15, train_loss: 0.3352\n",
      "8/15, train_loss: 0.3337\n",
      "9/15, train_loss: 0.3846\n",
      "10/15, train_loss: 0.4119\n",
      "11/15, train_loss: 0.6132\n",
      "12/15, train_loss: 0.2534\n",
      "13/15, train_loss: 0.3231\n",
      "14/15, train_loss: 0.3716\n",
      "15/15, train_loss: 0.2913\n",
      "16/15, train_loss: 0.3533\n",
      "epoch 251 average loss: 0.3572\n",
      "----------\n",
      "epoch 252/500\n",
      "1/15, train_loss: 0.2761\n",
      "2/15, train_loss: 0.2591\n",
      "3/15, train_loss: 0.2784\n",
      "4/15, train_loss: 0.2848\n",
      "5/15, train_loss: 0.3194\n",
      "6/15, train_loss: 0.3014\n",
      "7/15, train_loss: 0.3530\n",
      "8/15, train_loss: 0.3069\n",
      "9/15, train_loss: 2.3830\n",
      "10/15, train_loss: 0.2880\n",
      "11/15, train_loss: 0.2545\n",
      "12/15, train_loss: 0.4427\n",
      "13/15, train_loss: 0.3690\n",
      "14/15, train_loss: 0.2880\n",
      "15/15, train_loss: 0.6263\n",
      "16/15, train_loss: 0.3622\n",
      "epoch 252 average loss: 0.4621\n",
      "----------\n",
      "epoch 253/500\n",
      "1/15, train_loss: 0.2942\n",
      "2/15, train_loss: 0.2759\n",
      "3/15, train_loss: 0.3197\n",
      "4/15, train_loss: 0.2571\n",
      "5/15, train_loss: 0.2845\n",
      "6/15, train_loss: 0.3576\n",
      "7/15, train_loss: 0.3767\n",
      "8/15, train_loss: 0.2927\n",
      "9/15, train_loss: 0.4411\n",
      "10/15, train_loss: 0.2631\n",
      "11/15, train_loss: 0.2959\n",
      "12/15, train_loss: 0.2712\n",
      "13/15, train_loss: 0.2939\n",
      "14/15, train_loss: 0.3123\n",
      "15/15, train_loss: 0.2587\n",
      "16/15, train_loss: 0.3916\n",
      "epoch 253 average loss: 0.3116\n",
      "----------\n",
      "epoch 254/500\n",
      "1/15, train_loss: 0.3555\n",
      "2/15, train_loss: 0.3359\n",
      "3/15, train_loss: 0.2492\n",
      "4/15, train_loss: 0.2710\n",
      "5/15, train_loss: 0.3606\n",
      "6/15, train_loss: 0.2806\n",
      "7/15, train_loss: 0.4173\n",
      "8/15, train_loss: 0.2974\n",
      "9/15, train_loss: 0.3759\n",
      "10/15, train_loss: 0.3475\n",
      "11/15, train_loss: 0.5049\n",
      "12/15, train_loss: 0.3029\n",
      "13/15, train_loss: 0.2704\n",
      "14/15, train_loss: 0.2650\n",
      "15/15, train_loss: 0.3578\n",
      "16/15, train_loss: 0.2612\n",
      "epoch 254 average loss: 0.3283\n",
      "----------\n",
      "epoch 255/500\n",
      "1/15, train_loss: 0.3375\n",
      "2/15, train_loss: 0.3124\n",
      "3/15, train_loss: 0.3066\n",
      "4/15, train_loss: 0.3364\n",
      "5/15, train_loss: 0.3378\n",
      "6/15, train_loss: 0.2946\n",
      "7/15, train_loss: 0.2565\n",
      "8/15, train_loss: 0.2718\n",
      "9/15, train_loss: 0.3883\n",
      "10/15, train_loss: 0.3434\n",
      "11/15, train_loss: 0.3421\n",
      "12/15, train_loss: 0.2944\n",
      "13/15, train_loss: 0.4203\n",
      "14/15, train_loss: 0.2906\n",
      "15/15, train_loss: 0.2571\n",
      "16/15, train_loss: 0.3616\n",
      "epoch 255 average loss: 0.3220\n",
      "----------\n",
      "epoch 256/500\n",
      "1/15, train_loss: 0.3052\n",
      "2/15, train_loss: 0.2747\n",
      "3/15, train_loss: 0.4559\n",
      "4/15, train_loss: 0.2616\n",
      "5/15, train_loss: 0.3085\n",
      "6/15, train_loss: 0.2757\n",
      "7/15, train_loss: 0.2771\n",
      "8/15, train_loss: 2.2332\n",
      "9/15, train_loss: 0.3169\n",
      "10/15, train_loss: 0.3359\n",
      "11/15, train_loss: 0.2703\n",
      "12/15, train_loss: 0.2726\n",
      "13/15, train_loss: 0.3290\n",
      "14/15, train_loss: 0.3654\n",
      "15/15, train_loss: 0.3145\n",
      "16/15, train_loss: 2.2868\n",
      "epoch 256 average loss: 0.5552\n",
      "----------\n",
      "epoch 257/500\n",
      "1/15, train_loss: 0.2850\n",
      "2/15, train_loss: 0.3007\n",
      "3/15, train_loss: 0.2623\n",
      "4/15, train_loss: 0.3377\n",
      "5/15, train_loss: 0.3103\n",
      "6/15, train_loss: 0.2715\n",
      "7/15, train_loss: 0.3344\n",
      "8/15, train_loss: 0.3330\n",
      "9/15, train_loss: 0.3301\n",
      "10/15, train_loss: 0.2603\n",
      "11/15, train_loss: 0.2418\n",
      "12/15, train_loss: 0.2873\n",
      "13/15, train_loss: 0.2562\n",
      "14/15, train_loss: 0.2982\n",
      "15/15, train_loss: 0.2453\n",
      "16/15, train_loss: 0.2462\n",
      "epoch 257 average loss: 0.2875\n",
      "----------\n",
      "epoch 258/500\n",
      "1/15, train_loss: 0.5144\n",
      "2/15, train_loss: 0.2746\n",
      "3/15, train_loss: 0.3208\n",
      "4/15, train_loss: 0.2884\n",
      "5/15, train_loss: 0.2725\n",
      "6/15, train_loss: 0.4021\n",
      "7/15, train_loss: 0.2698\n",
      "8/15, train_loss: 0.2927\n",
      "9/15, train_loss: 0.3175\n",
      "10/15, train_loss: 0.2740\n",
      "11/15, train_loss: 0.3542\n",
      "12/15, train_loss: 0.3316\n",
      "13/15, train_loss: 0.3324\n",
      "14/15, train_loss: 0.2688\n",
      "15/15, train_loss: 0.3493\n",
      "16/15, train_loss: 0.3846\n",
      "epoch 258 average loss: 0.3280\n",
      "----------\n",
      "epoch 259/500\n",
      "1/15, train_loss: 0.2572\n",
      "2/15, train_loss: 0.2865\n",
      "3/15, train_loss: 0.2596\n",
      "4/15, train_loss: 0.6171\n",
      "5/15, train_loss: 0.3597\n",
      "6/15, train_loss: 0.2833\n",
      "7/15, train_loss: 0.2811\n",
      "8/15, train_loss: 0.2654\n",
      "9/15, train_loss: 0.3406\n",
      "10/15, train_loss: 0.3164\n",
      "11/15, train_loss: 0.2903\n",
      "12/15, train_loss: 0.3107\n",
      "13/15, train_loss: 0.2711\n",
      "14/15, train_loss: 0.2820\n",
      "15/15, train_loss: 0.2976\n",
      "16/15, train_loss: 0.3389\n",
      "epoch 259 average loss: 0.3161\n",
      "----------\n",
      "epoch 260/500\n",
      "1/15, train_loss: 0.3841\n",
      "2/15, train_loss: 0.2408\n",
      "3/15, train_loss: 0.2500\n",
      "4/15, train_loss: 0.4655\n",
      "5/15, train_loss: 0.3472\n",
      "6/15, train_loss: 0.4247\n",
      "7/15, train_loss: 0.3214\n",
      "8/15, train_loss: 0.2797\n",
      "9/15, train_loss: 0.2857\n",
      "10/15, train_loss: 2.1630\n",
      "11/15, train_loss: 0.3733\n",
      "12/15, train_loss: 0.4393\n",
      "13/15, train_loss: 0.2509\n",
      "14/15, train_loss: 0.3094\n",
      "15/15, train_loss: 0.3878\n",
      "16/15, train_loss: 2.4089\n",
      "epoch 260 average loss: 0.5832\n",
      "----------\n",
      "epoch 261/500\n",
      "1/15, train_loss: 0.2521\n",
      "2/15, train_loss: 0.2862\n",
      "3/15, train_loss: 0.2725\n",
      "4/15, train_loss: 0.3102\n",
      "5/15, train_loss: 0.3065\n",
      "6/15, train_loss: 0.2693\n",
      "7/15, train_loss: 0.2467\n",
      "8/15, train_loss: 0.3014\n",
      "9/15, train_loss: 0.3040\n",
      "10/15, train_loss: 0.3240\n",
      "11/15, train_loss: 0.2641\n",
      "12/15, train_loss: 0.3582\n",
      "13/15, train_loss: 0.4079\n",
      "14/15, train_loss: 0.2737\n",
      "15/15, train_loss: 0.3062\n",
      "16/15, train_loss: 0.6812\n",
      "epoch 261 average loss: 0.3228\n",
      "----------\n",
      "epoch 262/500\n",
      "1/15, train_loss: 0.3109\n",
      "2/15, train_loss: 0.3139\n",
      "3/15, train_loss: 0.2455\n",
      "4/15, train_loss: 0.3178\n",
      "5/15, train_loss: 0.3857\n",
      "6/15, train_loss: 0.2389\n",
      "7/15, train_loss: 0.3712\n",
      "8/15, train_loss: 0.3037\n",
      "9/15, train_loss: 0.3033\n",
      "10/15, train_loss: 0.2454\n",
      "11/15, train_loss: 0.3367\n",
      "12/15, train_loss: 0.5072\n",
      "13/15, train_loss: 0.3258\n",
      "14/15, train_loss: 0.5927\n",
      "15/15, train_loss: 0.2899\n",
      "16/15, train_loss: 0.2588\n",
      "epoch 262 average loss: 0.3342\n",
      "----------\n",
      "epoch 263/500\n",
      "1/15, train_loss: 0.2975\n",
      "2/15, train_loss: 0.2846\n",
      "3/15, train_loss: 0.2680\n",
      "4/15, train_loss: 0.2616\n",
      "5/15, train_loss: 0.2941\n",
      "6/15, train_loss: 0.2890\n",
      "7/15, train_loss: 0.2753\n",
      "8/15, train_loss: 0.2967\n",
      "9/15, train_loss: 0.2843\n",
      "10/15, train_loss: 0.2823\n",
      "11/15, train_loss: 0.2785\n",
      "12/15, train_loss: 0.2431\n",
      "13/15, train_loss: 0.3317\n",
      "14/15, train_loss: 0.5842\n",
      "15/15, train_loss: 0.2974\n",
      "16/15, train_loss: 0.4359\n",
      "epoch 263 average loss: 0.3128\n",
      "----------\n",
      "epoch 264/500\n",
      "1/15, train_loss: 0.2777\n",
      "2/15, train_loss: 0.2799\n",
      "3/15, train_loss: 0.2602\n",
      "4/15, train_loss: 0.2578\n",
      "5/15, train_loss: 0.2554\n",
      "6/15, train_loss: 0.2941\n",
      "7/15, train_loss: 0.2649\n",
      "8/15, train_loss: 0.3179\n",
      "9/15, train_loss: 0.4146\n",
      "10/15, train_loss: 0.2768\n",
      "11/15, train_loss: 0.2512\n",
      "12/15, train_loss: 0.2829\n",
      "13/15, train_loss: 0.2477\n",
      "14/15, train_loss: 0.2931\n",
      "15/15, train_loss: 0.3171\n",
      "16/15, train_loss: 0.2590\n",
      "epoch 264 average loss: 0.2844\n",
      "----------\n",
      "epoch 265/500\n",
      "1/15, train_loss: 0.3367\n",
      "2/15, train_loss: 0.3925\n",
      "3/15, train_loss: 0.3940\n",
      "4/15, train_loss: 0.3010\n",
      "5/15, train_loss: 0.3156\n",
      "6/15, train_loss: 0.3061\n",
      "7/15, train_loss: 0.2809\n",
      "8/15, train_loss: 0.2611\n",
      "9/15, train_loss: 0.2882\n",
      "10/15, train_loss: 0.3513\n",
      "11/15, train_loss: 0.3204\n",
      "12/15, train_loss: 0.2694\n",
      "13/15, train_loss: 0.2823\n",
      "14/15, train_loss: 0.3823\n",
      "15/15, train_loss: 0.3359\n",
      "16/15, train_loss: 0.2397\n",
      "epoch 265 average loss: 0.3161\n",
      "----------\n",
      "epoch 266/500\n",
      "1/15, train_loss: 0.3462\n",
      "2/15, train_loss: 0.2935\n",
      "3/15, train_loss: 0.2864\n",
      "4/15, train_loss: 0.2639\n",
      "5/15, train_loss: 0.7098\n",
      "6/15, train_loss: 0.3376\n",
      "7/15, train_loss: 0.2718\n",
      "8/15, train_loss: 0.2768\n",
      "9/15, train_loss: 0.2565\n",
      "10/15, train_loss: 0.2856\n",
      "11/15, train_loss: 0.2703\n",
      "12/15, train_loss: 0.3023\n",
      "13/15, train_loss: 0.3188\n",
      "14/15, train_loss: 0.2849\n",
      "15/15, train_loss: 0.3067\n",
      "16/15, train_loss: 2.3194\n",
      "epoch 266 average loss: 0.4457\n",
      "----------\n",
      "epoch 267/500\n",
      "1/15, train_loss: 0.2591\n",
      "2/15, train_loss: 0.3344\n",
      "3/15, train_loss: 0.2758\n",
      "4/15, train_loss: 0.2692\n",
      "5/15, train_loss: 0.2874\n",
      "6/15, train_loss: 0.3109\n",
      "7/15, train_loss: 0.3199\n",
      "8/15, train_loss: 0.2867\n",
      "9/15, train_loss: 0.2819\n",
      "10/15, train_loss: 0.5218\n",
      "11/15, train_loss: 0.2496\n",
      "12/15, train_loss: 0.2825\n",
      "13/15, train_loss: 0.3098\n",
      "14/15, train_loss: 0.2683\n",
      "15/15, train_loss: 0.2780\n",
      "16/15, train_loss: 0.2442\n",
      "epoch 267 average loss: 0.2987\n",
      "----------\n",
      "epoch 268/500\n",
      "1/15, train_loss: 0.4155\n",
      "2/15, train_loss: 0.2485\n",
      "3/15, train_loss: 0.3034\n",
      "4/15, train_loss: 0.2988\n",
      "5/15, train_loss: 0.2606\n",
      "6/15, train_loss: 0.3088\n",
      "7/15, train_loss: 0.2975\n",
      "8/15, train_loss: 0.3324\n",
      "9/15, train_loss: 0.3858\n",
      "10/15, train_loss: 0.2866\n",
      "11/15, train_loss: 0.3896\n",
      "12/15, train_loss: 0.2881\n",
      "13/15, train_loss: 0.3273\n",
      "14/15, train_loss: 0.3158\n",
      "15/15, train_loss: 0.2756\n",
      "16/15, train_loss: 0.2512\n",
      "epoch 268 average loss: 0.3116\n",
      "----------\n",
      "epoch 269/500\n",
      "1/15, train_loss: 0.2545\n",
      "2/15, train_loss: 0.4052\n",
      "3/15, train_loss: 0.2230\n",
      "4/15, train_loss: 0.2785\n",
      "5/15, train_loss: 0.3819\n",
      "6/15, train_loss: 0.2429\n",
      "7/15, train_loss: 0.2531\n",
      "8/15, train_loss: 0.3797\n",
      "9/15, train_loss: 0.3157\n",
      "10/15, train_loss: 0.3265\n",
      "11/15, train_loss: 0.2736\n",
      "12/15, train_loss: 0.3797\n",
      "13/15, train_loss: 0.2614\n",
      "14/15, train_loss: 0.2419\n",
      "15/15, train_loss: 0.4105\n",
      "16/15, train_loss: 0.2847\n",
      "epoch 269 average loss: 0.3071\n",
      "----------\n",
      "epoch 270/500\n",
      "1/15, train_loss: 0.4200\n",
      "2/15, train_loss: 0.2389\n",
      "3/15, train_loss: 0.2484\n",
      "4/15, train_loss: 0.2923\n",
      "5/15, train_loss: 0.2388\n",
      "6/15, train_loss: 0.3032\n",
      "7/15, train_loss: 0.2869\n",
      "8/15, train_loss: 0.2778\n",
      "9/15, train_loss: 0.3556\n",
      "10/15, train_loss: 0.2573\n",
      "11/15, train_loss: 0.3064\n",
      "12/15, train_loss: 0.3640\n",
      "13/15, train_loss: 0.2615\n",
      "14/15, train_loss: 0.2906\n",
      "15/15, train_loss: 0.2476\n",
      "16/15, train_loss: 0.3305\n",
      "epoch 270 average loss: 0.2950\n",
      "----------\n",
      "epoch 271/500\n",
      "1/15, train_loss: 0.2777\n",
      "2/15, train_loss: 0.2567\n",
      "3/15, train_loss: 0.3052\n",
      "4/15, train_loss: 0.2494\n",
      "5/15, train_loss: 0.5034\n",
      "6/15, train_loss: 0.3149\n",
      "7/15, train_loss: 0.2627\n",
      "8/15, train_loss: 0.2888\n",
      "9/15, train_loss: 0.2315\n",
      "10/15, train_loss: 0.2920\n",
      "11/15, train_loss: 0.3172\n",
      "12/15, train_loss: 0.2514\n",
      "13/15, train_loss: 0.2839\n",
      "14/15, train_loss: 0.3312\n",
      "15/15, train_loss: 0.5449\n",
      "16/15, train_loss: 0.3092\n",
      "epoch 271 average loss: 0.3138\n",
      "----------\n",
      "epoch 272/500\n",
      "1/15, train_loss: 0.3124\n",
      "2/15, train_loss: 0.2985\n",
      "3/15, train_loss: 0.2659\n",
      "4/15, train_loss: 0.4707\n",
      "5/15, train_loss: 0.3157\n",
      "6/15, train_loss: 0.2808\n",
      "7/15, train_loss: 0.2847\n",
      "8/15, train_loss: 0.2773\n",
      "9/15, train_loss: 0.3296\n",
      "10/15, train_loss: 0.2894\n",
      "11/15, train_loss: 0.2296\n",
      "12/15, train_loss: 0.3424\n",
      "13/15, train_loss: 0.2330\n",
      "14/15, train_loss: 0.2960\n",
      "15/15, train_loss: 0.3453\n",
      "16/15, train_loss: 0.3378\n",
      "epoch 272 average loss: 0.3068\n"
     ]
    }
   ],
   "source": [
    "n_splits = 3\n",
    "if start_training:\n",
    "    #### Running 10 folds\n",
    "    for i in range(0, n_splits):\n",
    "        \n",
    "        #train_files_fld, train_files_fld_IDH_label, val_files_fld, val_files_fld_IDH_label  = copy.deepcopy(train_folds[f'fold{i}']), copy.deepcopy(train_folds[f'fold{i}_IDH_label']),\\\n",
    "        #copy.deepcopy(val_folds[f'fold{i}']), copy.deepcopy(val_folds[f'fold{i}_IDH_label'])\n",
    "             \n",
    "        train_files_fld, val_files_fld  = copy.deepcopy(BraTS20SubjectsIDHTrainDCT[f'fold{i}']), copy.deepcopy(BraTS20SubjectsIDHValDCT[f'fold{i}'])\n",
    "        train_files_fld_IDH_label, val_files_fld_IDH_label = None, None\n",
    "        batch_size=8\n",
    "        ### Need to change batch size if minimux training batch size == 1\n",
    "        print('fold', i, \"Bacth Investigation, minimum batch size\", len(train_files_fld)%batch_size)        \n",
    "        #train(train_files_fld, train_files_fld_IDH_label, val_files_fld, val_files_fld_IDH_label, batch_size=batch_size, epochs = 200, cfold = i)\n",
    "        train(train_files_fld, train_files_fld_IDH_label, val_files_fld, val_files_fld_IDH_label, batch_size=batch_size, epochs = 500, cfold = i, transfer_modelPath = None)\n",
    "    \n",
    "    start_training = False\n",
    "else:\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb943e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_multilabel_classification\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# X, y = make_multilabel_classification(random_state=0, n_classes=2)\n",
    "# inner_clf = LogisticRegression(solver=\"liblinear\", random_state=0)\n",
    "# clf = MultiOutputClassifier(inner_clf).fit(X, y)\n",
    "# y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)])\n",
    "# roc_auc_score(y, y_score, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb97120",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2480ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferWithTA(data_loader,listmodels, prediction_folder=\"./\", topk=1, num_channels = 4,\\\n",
    "                orientation=\"LPS\", withoptimizer = False, softmaxEnsemble=False, save_inference = False, tta = False):\n",
    "    \"\"\"\n",
    "    run inference, the output folder will be \"./output\"\n",
    "    \"\"\"        \n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    listmodels = listmodels[0:topk]\n",
    "    for x in listmodels:\n",
    "        print(f\"available model file: {x}.\")\n",
    "        \n",
    "    channel_nums =  monai.utils.misc.first(data_loader)['image'].shape[1] ##next(iter(val_loader[\"image\"])).shape[1]\n",
    "    channelNums = f\"{channel_nums} channels\"\n",
    "    keys = ('image',)\n",
    "    patch_size = (32, 32, 32)\n",
    "    \n",
    "    post_trans_sigbin = Compose([EnsureType(), Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "    post_trans_bin = Compose([EnsureType(), AsDiscrete(threshold=0.5)])\n",
    "    post_trans_sig = Compose([EnsureType(), Activations(sigmoid=True)])\n",
    "    \n",
    "    auc_metric = ROCAUCMetric()\n",
    "    dice_metric = monai.metrics.DiceMetric(include_background=True, reduction='mean', get_not_nans=False)\n",
    "    dice_metric_batch = monai.metrics.DiceMetric(include_background=True, reduction='mean_batch', get_not_nans=False)\n",
    "    \n",
    "    \n",
    "    HD_metric = HausdorffDistanceMetric(include_background=True, percentile = 95., reduction='mean', get_not_nans=False)\n",
    "    HD_metric_batch = HausdorffDistanceMetric(include_background=True, percentile = 95., reduction='mean_batch', get_not_nans=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    post_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])  \n",
    "    \n",
    "    def one_hot_permute(x):\n",
    "        return F.one_hot(x.squeeze(dim=0).long(), num_classes=num_classes).permute(3, 0, 1, 2)\n",
    "    \n",
    "    def get_binarize_tensor(x, dim=1):\n",
    "        x_chlist = torch.unbind(x, dim = dim)\n",
    "        bin_x = torch.zeros_like(x_chlist[0])\n",
    "        for x_i in x_chlist:\n",
    "            bin_x = torch.logical_or(x_i, bin_x)\n",
    "        return bin_x.unsqueeze(dim=dim).to(torch.float32)\n",
    "        \n",
    "    def get_segclass(x, dim = 1):\n",
    "        xdvc = x.device\n",
    "        x_chlist = torch.unbind(x, dim = dim)\n",
    "        xclassNoList = []\n",
    "        xvalueList = []\n",
    "\n",
    "        for x_i in x_chlist:\n",
    "\n",
    "            xv, xc = torch.unique(x_i, return_counts  = True)\n",
    "\n",
    "            if xc.shape[0]==1:\n",
    "                if xv==0:\n",
    "                    xclassNoList.append(-1)\n",
    "                    xvalueList.append(xv[0].item())\n",
    "                elif xv==1:\n",
    "                    xclassNoList.append(xc[0].item())\n",
    "                    xvalueList.append(xv[0].item())\n",
    "                else:\n",
    "                    print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "\n",
    "\n",
    "            elif xc.shape[0]==2:\n",
    "                    if torch.any(torch.eq(xv, 1)):\n",
    "                        xclassNoList.append(xc[1].item())\n",
    "                        xvalueList.append(xv[1].item())\n",
    "                    else:\n",
    "                        print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "            else:\n",
    "                print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "\n",
    "        #pdb.set_trace()\n",
    "        #if torch.any(torch.eq(torch.tensor(xvalueList), 1)):\n",
    "\n",
    "        if xclassNoList[0]!=xclassNoList[1]: \n",
    "            xclass = torch.argmax(torch.tensor(xclassNoList).to(xdvc))\n",
    "        else:\n",
    "            xclass = torch.tensor(float('NaN')).to(xdvc)\n",
    "\n",
    "        #else:\n",
    "            '''If all uniques class values are 0, we are assigning nan values as a class'''\n",
    "        #    xclass = torch.tensor(float('NaN')).to(xdvc)\n",
    "\n",
    "\n",
    "        return xclass\n",
    "    \n",
    "    \n",
    "    \n",
    "    keys = (\"image\",)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "        y = torch.tensor([], dtype=torch.long, device=device)\n",
    "    \n",
    "        for infindx, infer_data in enumerate(tqdm(data_loader)):\n",
    "\n",
    "            \n",
    "            val_inputs, val_labels, val_IDH_labels = (\n",
    "                infer_data['image'].to(device),\n",
    "                infer_data['label'].to(device),\n",
    "                infer_data['IDH_label'].to(device),\n",
    "            )\n",
    "                \n",
    "\n",
    "            n_class = 2\n",
    "            val_outputsAll = torch.zeros(val_inputs.shape[0], n_class, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]).to(device)\n",
    "            n_model = 0.\n",
    "            \n",
    "            #for mdlindx in (number+1 for number in range(topk)):\n",
    "            for mdlindx in range(topk):\n",
    "    \n",
    "                print(f'Model {mdlindx}, {listmodels[mdlindx]} is running now')\n",
    "                model = None        \n",
    "    \n",
    "                model = DynUNet(\n",
    "                    spatial_dims=3,\n",
    "                    in_channels=4,\n",
    "                    out_channels=n_class,\n",
    "                    kernel_size=kernels,\n",
    "                    strides=strides,\n",
    "                    upsample_kernel_size=strides[1:],\n",
    "                    norm_name=\"batch\",\n",
    "                    filters = filters,\n",
    "                    deep_supervision=True,\n",
    "                    #res_block=True,\n",
    "                    deep_supr_num=2,\n",
    "                ).to(device)\n",
    "                \n",
    "            \n",
    "                if withoptimizer ==True:\n",
    "                    \n",
    "                    state_dictsAll = torch.load(listmodels[mdlindx], map_location=device)\n",
    "                    model.load_state_dict(state_dictsAll[\"model_state_dict\"])\n",
    "                    model.eval()\n",
    "                \n",
    "                else:    \n",
    "                    model.load_state_dict(torch.load(listmodels[mdlindx], map_location=device))\n",
    "                    model.eval()\n",
    "                \n",
    "                \n",
    "                n = 1.0\n",
    "                roi_size = patch_size #(32, 32, 32)\n",
    "                sw_batch_size = 8\n",
    "                val_overlap = 0.5\n",
    "                mode=\"gaussian\"\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "\n",
    "                    preds = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device)\n",
    "                    \n",
    "                flip_val_inputs = torch.flip(val_inputs, dims=(2, 3, 4))\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    \n",
    "                    mfpred = sliding_window_inference(flip_val_inputs, roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device)\n",
    "                 \n",
    "                flip_pred = torch.flip(mfpred, dims=(2, 3, 4))\n",
    "                preds  = preds + flip_pred\n",
    "                n = n + 1.0\n",
    "                \n",
    "                if tta:\n",
    "                    \n",
    "                    for _ in range(4):\n",
    "                        # test time augmentations\n",
    "                        _img = RandGaussianNoised(keys[0], prob=1.0, std=0.01)(infer_data)[keys[0]]\n",
    "\n",
    "\n",
    "                        with torch.cuda.amp.autocast():\n",
    "\n",
    "                            #val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model, sw_device = device, device = device)\n",
    "                            _img_pred = sliding_window_inference(_img.to(device), roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device)\n",
    "                            preds = preds + _img_pred\n",
    "                            n = n + 1.0\n",
    "\n",
    "\n",
    "                        _img_flip = torch.flip(_img, dims=(2, 3, 4)) \n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            _mf_flip_pred = sliding_window_inference(_img_flip.to(device), roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device)\n",
    "\n",
    "                        _img_flip_pred = torch.flip(_mf_flip_pred, dims=(2, 3, 4))\n",
    "                        preds = preds + _img_flip_pred\n",
    "                        n = n + 1.0\n",
    "                 \n",
    "                \n",
    "                preds = preds / n\n",
    "                \n",
    "                if softmaxEnsemble:\n",
    "                    preds = torch.stack([post_trans_sig(i) for i in torch.unbind(preds, dim = 0)], dim = 0)\n",
    "                val_outputsAll = val_outputsAll + preds\n",
    "                n_model = n_model+1.0\n",
    "                \n",
    "                # Free up GPU memory after training\n",
    "                model = None\n",
    "                del model\n",
    "                #train_loader, val_loader = None, None        \n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                           \n",
    "            val_outputsAll = val_outputsAll / n_model\n",
    "            \n",
    "            val_outputs = post_trans_bin(val_outputsAll) if softmaxEnsemble else post_trans_sigbin(val_outputsAll)\n",
    "            '''Sigmoid or logit'''\n",
    "            val_outputsSig = val_outputsAll if softmaxEnsemble else post_trans_sig(val_outputsAll)\n",
    "            #val_outputsSig = val_outputsAll\n",
    "            \n",
    "            \n",
    "\n",
    "            #val_labels2hot = torch.stack([one_hot_permute(i) for i in torch.unbind(val_labels, dim = 0)], dim = 0)\n",
    "\n",
    "\n",
    "            val_labels_bin = get_binarize_tensor(val_labels, dim=1)\n",
    "            val_outputs_bin = get_binarize_tensor(val_outputs, dim=1)\n",
    "\n",
    "            dice_metric(y_pred=val_outputs_bin, y=val_labels_bin)\n",
    "\n",
    "            klcc = KeepLargestConnectedComponent(applied_labels = [0, 1], is_onehot = True)  ##is_onehot=True or None by default\n",
    "            #val_labels = klcc(val_labels.squeeze(dim=0)).unsqueeze(dim=0)\n",
    "            val_labels = torch.stack([klcc(i) for i in torch.unbind(val_labels, dim = 0)], dim = 0)\n",
    "\n",
    "            val_label4mSeg_C = get_segclass(val_outputs)\n",
    "            #val_surv_labels = val_surv_labels.squeeze(dim=1)  ###Squeezing from B, 1 to B if needed\n",
    "            y_pred = torch.cat([y_pred, val_label4mSeg_C.view(1)], dim=0)\n",
    "            y = torch.cat([y, val_IDH_labels], dim=0)\n",
    "                \n",
    "                        \n",
    "            \n",
    "        mdice_value = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()        \n",
    "        \n",
    "        \n",
    "        y_pred, y = y_pred.cpu(), y.cpu()\n",
    "\n",
    "        if torch.all(torch.isnan(y_pred))==True:\n",
    "\n",
    "            auc_result, accscore, f1score = np.nan, np.nan, np.nan\n",
    "            #print('acc_metric#', np.nan, ', auc#', np.nan, ', f1#', np.nan, '\\n')\n",
    "            num_nanvalues = len(y_pred)\n",
    "\n",
    "        else:\n",
    "\n",
    "            num_nanvalues = torch.isnan(y_pred).sum().item()\n",
    "            not_nanmask = torch.logical_not(torch.isnan(y_pred))\n",
    "            y = y[not_nanmask]\n",
    "            y_pred = y_pred[not_nanmask]\n",
    "\n",
    "\n",
    "            acc_value = torch.eq(y_pred, y)\n",
    "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "\n",
    "            '''auc metric'''\n",
    "            auc_metric(y_pred, y)\n",
    "            auc_result = auc_metric.aggregate()\n",
    "            auc_metric.reset()\n",
    "\n",
    "            '''balanced accuracy and f1 score'''\n",
    "            accscore = balanced_accuracy_score(y, y_pred)\n",
    "            f1score = f1_score(y, y_pred, average='micro')\n",
    "            #print('acc_metric#', acc_metric, ', auc#', auc_result, ', f1#', f1score, '\\n')\n",
    "\n",
    "\n",
    "        del y, y_pred\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        aDCT = {\"dice_score\": mdice_value,  'acc_metric':auc_result,  \"accuracy\": accscore,  'f1score':f1score, 'NanSubjectNos':num_nanvalues}\n",
    "        \n",
    "                    \n",
    "#     dfET = pd.DataFrame({\"BraTS21ID\":Infer_idLst, \"Model\": [\"DynUnet\"]* len(ETdices), \"Channels\":[channelNums]*len(ETdices), \"Tumor regions\": [\"ET\"]*len(ETdices), \"Dice score\": ETdices, \"HD95\": ETHD95s})\n",
    "#     dfTC = pd.DataFrame({\"BraTS21ID\":Infer_idLst, \"Model\": [\"DynUnet\"]* len(TCdices), \"Channels\":[channelNums]*len(TCdices), \"Tumor regions\": [\"TC\"]*len(TCdices), \"Dice score\": TCdices, \"HD95\": TCHD95s})\n",
    "#     dfWT = pd.DataFrame({\"BraTS21ID\":Infer_idLst,\"Model\": [\"DynUnet\"]* len(WTdices), \"Channels\":[channelNums]*len(WTdices), \"Tumor regions\": [\"WT\"]*len(WTdices), \"Dice score\": WTdices, \"HD95\": WTHD95s})\n",
    "#     dfWT_TC_ET = pd.DataFrame({\"BraTS21ID\":Infer_idLst,\"Model\": [\"DynUnet\"]* len(WTdices), \"Channels\":[channelNums]*len(WTdices), \"Tumor regions\": [\"WT_TC_ET_Regions\"]*len(WTdices), \"Dice score\": Alldices, \"HD95\": AllHD95s})\n",
    "    \n",
    "#     dfRegions = pd.concat([dfTC, dfWT, dfET, dfWT_TC_ET])\n",
    "#     return dfRegions\n",
    "\n",
    "\n",
    "    return aDCT\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7dbb0c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fold0': ['/raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth'],\n",
       " 'fold1': ['/raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth'],\n",
       " 'fold2': ['/raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth']}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# listmodels = glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_1PatchEp500V2_Fold0_0.8371_epoch207.pt*') \n",
    "# listmodels.extend(glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_1PatchEp500V2_Fold1_0.8944_epoch75.pt*')) \n",
    "# listmodels.extend(glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_1PatchEp500V2_Fold2_0.8719_epoch482.pt*'))\n",
    "# print(listmodels)\n",
    "\n",
    "prediction_folder = f'{save_dir}/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand' \n",
    "# modelDCTList = {'fold0': [glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pt*')[0],\n",
    "#                           glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth*')[0]],\\\n",
    "#                'fold1':[glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pt*')[0],\n",
    "#                         glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pt*')[0]],\\\n",
    "#                'fold2':[glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold2_0.8901_epoch122.pt*')[0],\n",
    "#                         glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold2_0.8862_epoch132.pt*')[0]]}\n",
    "\n",
    "modelDCTList = {'fold0': glob.glob(f'{save_dir}/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pt*'),\\\n",
    "               'fold1':glob.glob(f'{save_dir}/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pt*'),\\\n",
    "               'fold2':glob.glob(f'{save_dir}/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pt*')}\n",
    "\n",
    "modelDCTList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7494725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3eabe587",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_inference = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58c00066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available model file: /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n",
      "  0%|                                                                                                                                                        | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–ˆâ–ˆ                                                                                                                                              | 1/71 [00:07<08:43,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                            | 2/71 [00:13<07:35,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                          | 3/71 [00:18<06:37,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                        | 4/71 [00:22<05:47,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                     | 5/71 [00:26<05:04,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                   | 6/71 [00:32<05:36,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                 | 7/71 [00:36<05:06,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                               | 8/71 [00:41<05:02,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                             | 9/71 [00:45<04:44,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                          | 10/71 [00:49<04:29,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                        | 11/71 [00:53<04:23,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                      | 12/71 [00:57<04:14,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                    | 13/71 [01:02<04:15,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                  | 14/71 [01:07<04:15,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                | 15/71 [01:12<04:20,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                              | 16/71 [01:16<04:11,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                            | 17/71 [01:21<04:18,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                          | 18/71 [01:27<04:25,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                        | 19/71 [01:32<04:15,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                      | 20/71 [01:37<04:16,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                    | 21/71 [01:41<03:54,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                  | 22/71 [01:46<03:58,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                | 23/71 [01:50<03:36,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                              | 24/71 [01:54<03:28,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                            | 25/71 [01:58<03:18,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                          | 26/71 [02:03<03:15,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                        | 27/71 [02:10<03:51,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                      | 28/71 [02:14<03:34,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                    | 29/71 [02:19<03:20,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                  | 30/71 [02:24<03:21,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                | 31/71 [02:28<03:10,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                              | 32/71 [02:36<03:38,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                            | 33/71 [02:41<03:32,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                          | 34/71 [02:46<03:16,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                        | 35/71 [02:52<03:15,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                      | 36/71 [02:58<03:22,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                    | 37/71 [03:04<03:19,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                  | 38/71 [03:09<02:56,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                | 39/71 [03:14<02:50,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                              | 40/71 [03:18<02:34,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                            | 41/71 [03:24<02:36,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                          | 42/71 [03:29<02:33,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                        | 43/71 [03:34<02:25,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                      | 44/71 [03:38<02:08,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                    | 45/71 [03:43<02:05,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                  | 46/71 [03:49<02:07,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                | 47/71 [03:52<01:52,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                              | 48/71 [03:57<01:50,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                            | 49/71 [04:03<01:51,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                          | 50/71 [04:09<01:50,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                        | 51/71 [04:13<01:37,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                      | 52/71 [04:17<01:31,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 53/71 [04:23<01:28,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 54/71 [04:27<01:18,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 55/71 [04:32<01:16,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 56/71 [04:37<01:12,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 57/71 [04:41<01:05,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 58/71 [04:46<01:01,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 59/71 [04:51<00:56,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 60/71 [04:54<00:48,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 61/71 [04:58<00:41,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 62/71 [05:02<00:36,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 63/71 [05:07<00:34,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 64/71 [05:12<00:32,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 65/71 [05:17<00:27,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 66/71 [05:21<00:22,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 67/71 [05:24<00:16,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 68/71 [05:29<00:12,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 69/71 [05:34<00:09,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 70/71 [05:38<00:04,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold0_0.8544_epoch279.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [05:43<00:00,  4.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available model file: /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n",
      "  0%|                                                                                                                                                        | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–ˆâ–ˆ                                                                                                                                              | 1/72 [00:03<04:08,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                            | 2/72 [00:10<06:46,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                          | 3/72 [00:17<06:52,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                        | 4/72 [00:23<06:48,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                      | 5/72 [00:26<05:44,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                    | 6/72 [00:30<05:13,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                  | 7/72 [00:34<04:57,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                | 8/72 [00:38<04:38,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                              | 9/72 [00:44<04:51,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                           | 10/72 [00:48<04:47,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                         | 11/72 [00:53<04:42,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                       | 12/72 [00:58<04:51,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                     | 13/72 [01:04<05:09,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                   | 14/72 [01:10<05:18,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                 | 15/72 [01:15<04:48,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                               | 16/72 [01:20<04:42,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                             | 17/72 [01:25<04:42,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                           | 18/72 [01:30<04:33,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                         | 19/72 [01:36<04:40,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                       | 20/72 [01:40<04:20,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                     | 21/72 [01:45<04:19,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                   | 22/72 [01:50<04:11,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                 | 23/72 [01:54<03:46,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                               | 24/72 [02:00<04:03,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                             | 25/72 [02:06<04:06,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                           | 26/72 [02:09<03:42,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                         | 27/72 [02:14<03:30,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                       | 28/72 [02:18<03:24,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                     | 29/72 [02:25<03:40,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                   | 30/72 [02:28<03:17,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                 | 31/72 [02:32<03:04,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                               | 32/72 [02:36<02:54,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                             | 33/72 [02:40<02:39,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                           | 34/72 [02:44<02:40,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                         | 35/72 [02:48<02:31,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 36/72 [02:52<02:25,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                     | 37/72 [02:56<02:24,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                   | 38/72 [03:00<02:16,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                 | 39/72 [03:05<02:21,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                               | 40/72 [03:09<02:16,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                             | 41/72 [03:14<02:12,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                           | 42/72 [03:19<02:17,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                         | 43/72 [03:24<02:19,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                       | 44/72 [03:30<02:19,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                     | 45/72 [03:35<02:20,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                   | 46/72 [03:40<02:11,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                 | 47/72 [03:44<01:59,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                               | 48/72 [03:49<01:54,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                             | 49/72 [03:54<01:54,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 50/72 [03:59<01:49,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                         | 51/72 [04:07<02:01,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                       | 52/72 [04:11<01:46,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                     | 53/72 [04:15<01:30,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                   | 54/72 [04:21<01:33,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 55/72 [04:27<01:31,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 56/72 [04:30<01:16,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 57/72 [04:38<01:25,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 58/72 [04:42<01:11,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 59/72 [04:47<01:06,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 60/72 [04:51<00:58,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 61/72 [04:56<00:54,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 62/72 [05:00<00:46,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 63/72 [05:05<00:41,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 64/72 [05:09<00:36,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 65/72 [05:13<00:31,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 66/72 [05:18<00:27,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 67/72 [05:22<00:21,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 68/72 [05:28<00:18,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 69/72 [05:31<00:12,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 70/72 [05:40<00:11,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 71/72 [05:45<00:05,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold1_0.8246_epoch165.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [05:50<00:00,  4.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available model file: /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n",
      "  0%|                                                                                                                                                        | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–ˆâ–ˆ                                                                                                                                              | 1/71 [00:03<04:39,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                            | 2/71 [00:07<04:29,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                          | 3/71 [00:12<04:59,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                        | 4/71 [00:17<05:10,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                     | 5/71 [00:22<05:15,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                   | 6/71 [00:27<05:08,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                 | 7/71 [00:31<04:56,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                               | 8/71 [00:36<04:41,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                             | 9/71 [00:41<04:47,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                          | 10/71 [00:47<05:12,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                        | 11/71 [00:53<05:28,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                      | 12/71 [00:58<05:10,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                    | 13/71 [01:04<05:18,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                  | 14/71 [01:08<04:46,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                | 15/71 [01:12<04:20,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                              | 16/71 [01:17<04:26,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                            | 17/71 [01:21<04:16,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                          | 18/71 [01:26<04:07,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                        | 19/71 [01:32<04:20,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                      | 20/71 [01:37<04:15,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                    | 21/71 [01:45<05:06,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                  | 22/71 [01:51<04:54,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                | 23/71 [01:56<04:32,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                              | 24/71 [02:00<04:04,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                            | 25/71 [02:04<03:42,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                          | 26/71 [02:10<03:45,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                        | 27/71 [02:14<03:35,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                      | 28/71 [02:20<03:43,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                    | 29/71 [02:25<03:40,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                  | 30/71 [02:29<03:18,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                | 31/71 [02:33<03:03,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                              | 32/71 [02:40<03:18,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                            | 33/71 [02:45<03:20,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                          | 34/71 [02:52<03:25,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                        | 35/71 [02:57<03:22,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                      | 36/71 [03:02<03:04,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                    | 37/71 [03:07<02:57,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                  | 38/71 [03:12<02:54,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                | 39/71 [03:16<02:37,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                              | 40/71 [03:24<02:53,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                            | 41/71 [03:27<02:33,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                          | 42/71 [03:33<02:31,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                        | 43/71 [03:37<02:16,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                      | 44/71 [03:44<02:31,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                    | 45/71 [03:50<02:22,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                  | 46/71 [03:54<02:10,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                | 47/71 [03:58<01:56,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                              | 48/71 [04:07<02:20,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                            | 49/71 [04:12<02:07,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                          | 50/71 [04:18<02:00,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                        | 51/71 [04:25<02:05,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                      | 52/71 [04:30<01:47,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 53/71 [04:34<01:36,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 54/71 [04:38<01:23,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 55/71 [04:44<01:24,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 56/71 [04:49<01:18,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 57/71 [04:55<01:13,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 58/71 [04:58<01:01,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 59/71 [05:05<01:04,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 60/71 [05:09<00:53,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 61/71 [05:15<00:52,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 62/71 [05:19<00:42,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 63/71 [05:23<00:36,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 64/71 [05:26<00:30,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 65/71 [05:31<00:25,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 66/71 [05:34<00:20,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 67/71 [05:41<00:18,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 68/71 [05:45<00:14,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 69/71 [05:50<00:09,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 70/71 [05:57<00:05,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/PlainDynUNet_BratsTCGA_3CV_4ChnlsMorePatch_Infer1PatchSWIRngr21_2nRatioclass_HistStand_Fold2_0.8731_epoch417.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [06:01<00:00,  5.09s/it]\n"
     ]
    }
   ],
   "source": [
    "n_splits = 3\n",
    "aDCTResultList = list()\n",
    "if start_inference:\n",
    "    #### Running 10 folds\n",
    "    for i in range(0, n_splits):\n",
    "        \n",
    "\n",
    "        \n",
    "        infer_files_fld = copy.deepcopy(BraTS20SubjectsIDHTestDCT[f'fold{i}'])\n",
    "        \n",
    "        infer_dataset_fld = monai.data.Dataset(data=infer_files_fld, transform=val_transforms)\n",
    "       \n",
    "        infer_loader_fld = monai.data.DataLoader(infer_dataset_fld, batch_size=1, shuffle=False) #num_workers=2, pin_memory=True\n",
    "        \n",
    "        aDCTResult = inferWithTA(data_loader = infer_loader_fld, listmodels=modelDCTList[f'fold{i}'], prediction_folder=prediction_folder, topk=len(modelDCTList[f'fold{i}']), num_channels=4,\\\n",
    "                    orientation=\"LPS\", withoptimizer = False, softmaxEnsemble= True, tta = True)\n",
    "        aDCTResult['TestSplitName'] = f\"split{i}\"\n",
    "        aDCTResultList.append(aDCTResult.copy())\n",
    "        \n",
    "    \n",
    "    start_inference = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1ef284a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on the testset (3 testsest from 3 splits)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dice_score</th>\n",
       "      <th>acc_metric</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1score</th>\n",
       "      <th>NanSubjectNos</th>\n",
       "      <th>TestSplitName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858957</td>\n",
       "      <td>0.885484</td>\n",
       "      <td>0.885484</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0</td>\n",
       "      <td>split0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.875124</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0</td>\n",
       "      <td>split1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.866398</td>\n",
       "      <td>0.803226</td>\n",
       "      <td>0.803226</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>0</td>\n",
       "      <td>split2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dice_score  acc_metric  accuracy   f1score  NanSubjectNos TestSplitName\n",
       "0    0.858957    0.885484  0.885484  0.887324              0        split0\n",
       "1    0.875124    0.787500  0.787500  0.791667              0        split1\n",
       "2    0.866398    0.803226  0.803226  0.802817              0        split2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dice_score</th>\n",
       "      <th>acc_metric</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1score</th>\n",
       "      <th>NanSubjectNos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.866826</td>\n",
       "      <td>0.825403</td>\n",
       "      <td>0.825403</td>\n",
       "      <td>0.827269</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.008092</td>\n",
       "      <td>0.052622</td>\n",
       "      <td>0.052622</td>\n",
       "      <td>0.052307</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.858957</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.862677</td>\n",
       "      <td>0.795363</td>\n",
       "      <td>0.795363</td>\n",
       "      <td>0.797242</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.866398</td>\n",
       "      <td>0.803226</td>\n",
       "      <td>0.803226</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.870761</td>\n",
       "      <td>0.844355</td>\n",
       "      <td>0.844355</td>\n",
       "      <td>0.845070</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.875124</td>\n",
       "      <td>0.885484</td>\n",
       "      <td>0.885484</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dice_score  acc_metric  accuracy   f1score  NanSubjectNos\n",
       "count    3.000000    3.000000  3.000000  3.000000            3.0\n",
       "mean     0.866826    0.825403  0.825403  0.827269            0.0\n",
       "std      0.008092    0.052622  0.052622  0.052307            0.0\n",
       "min      0.858957    0.787500  0.787500  0.791667            0.0\n",
       "25%      0.862677    0.795363  0.795363  0.797242            0.0\n",
       "50%      0.866398    0.803226  0.803226  0.802817            0.0\n",
       "75%      0.870761    0.844355  0.844355  0.845070            0.0\n",
       "max      0.875124    0.885484  0.885484  0.887324            0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Result on the testset (3 testsest from 3 splits)')\n",
    "DFResult = pd.DataFrame.from_dict(aDCTResultList)\n",
    "display(DFResult)\n",
    "DFResult.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0faf05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_score(y, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26583b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_permute(x):\n",
    "    return F.one_hot(x.squeeze(dim=0).long(), num_classes=3).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa5e021e",
   "metadata": {},
   "source": [
    "atensor = torch.tensor([[[0, 2, 0, 0],[0, 0, 0, 2],[0, 2, 0, 0]]])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87c97a61",
   "metadata": {},
   "source": [
    "one_hot_permute(atensor)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fdbcf0c",
   "metadata": {},
   "source": [
    "atensor = torch.tensor([[[0, 1, 0, 0],[0, 0, 0, 1],[0, 1, 0, 0]]])\n",
    "one_hot_permute(atensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc= torch.tensor([0, 1, 0, 0, 2, 3, 10])\n",
    "if torch.any(torch.eq(xc, 11)):\n",
    "    print('Do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc = torch.tensor([0, 100, 500, 10000, 5])\n",
    "torch.argmax(xc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2fd5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argsort(xc)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "is_onehot = True\n",
    "img = torch.ones((1, 64, 64, 64))\n",
    "is_onehot2 = img.shape[0] > 1 if is_onehot is None else is_onehot\n",
    "is_onehot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = np.array([[1.]])\n",
    "dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = np.stack([dd, dd], axis = 0)\n",
    "dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([dx, dx], axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fae7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "P1=torch.tensor([[0, 1, 1, 1,  0, 0, 0, 0, 0]])\n",
    "P2=torch.tensor([[0, 1, 1, 1,   0, 0, 1, 1, 0]])\n",
    "P3=torch.tensor([[1, 0, 1, 1,   0, 0, 1, 1, 1]])\n",
    "P4=torch.tensor([[1, 0, 1, 1,   0, 0, 1, 1, 0]])\n",
    "P5=torch.tensor([[1, 0, 1, 1,   0, 0, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329fc235",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = torch.mode(torch.cat((P1, P2, P3, P4, P5), dim=0), dim=0, keepdim=True)[0]\n",
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5a8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "P1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8028f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "P00=torch.tensor([1])\n",
    "torch.mode(P00)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbf0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mode(P1)[0].view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "atnsr = torch.tensor((0, 0, 1, 1))\n",
    "xv, xc = torch.unique(atnsr, return_counts  = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058497b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc, xv[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac9697",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.any(torch.eq(xv, 1))\n",
    "xv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4528d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(torch.tensor([1000, 2000, 3000, float('NaN'), 5, 600]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3cfac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mode(torch.tensor([[5, 3, 3, float('NaN'),  float('NaN'), float('NaN'), float('NaN'), 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0243312",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mode(torch.tensor([60, 60, 50, 50, 60]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(float('NaN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b8aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdvc = device\n",
    "def get_bin_tensor(xbatchclass):\n",
    "   \n",
    "    if torch.all(torch.isnan(xbatchclass))==True:\n",
    "            \n",
    "            return torch.tensor(float('NaN')).to(xdvc)\n",
    "        \n",
    "    else:\n",
    "\n",
    "        num_xbatchnanvalues = torch.isnan(xbatchclass).sum().item()\n",
    "        not_xbatchnanmask = torch.logical_not(torch.isnan(xbatchclass))\n",
    "        xbatchclass = xbatchclass[not_xbatchnanmask]\n",
    "\n",
    "        xclassVal_01, xclassCnt_01 =xbatchclass.unique(return_counts = True)\n",
    "\n",
    "        if xclassCnt_01.shape[0]==1:\n",
    "            return xclassVal_01[0].to(xdvc)\n",
    "\n",
    "\n",
    "        if xclassCnt_01.shape[0]==2:\n",
    "            if xclassCnt_01[0]!=xclassCnt_01[1]:\n",
    "                ''' xclassCnt_01 will always be two values converting [7, 8] to 1; [8, 7] to 0'''\n",
    "                return torch.argmax(xclassCnt_01).to(xdvc)  \n",
    "\n",
    "            else:\n",
    "                return torch.tensor(float('NaN')).to(xdvc)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1635c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bin_tensor(torch.tensor([float('NaN'), float('NaN'), float('NaN'), 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b43a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(torch.tensor([0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "xclassVal_01, xclassCnt_01 =torch.tensor([0, 0, 0, 1]).unique(return_counts = True)\n",
    "xclassVal_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e45be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(xclassCnt_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877512a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(torch.tensor([7, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ecd9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segclass(x_chlist):\n",
    "    \n",
    "    xclassNoList = list()\n",
    "    xvalueList = list()\n",
    "    \n",
    "    for x_i in x_chlist:\n",
    "    \n",
    "        xv, xc = torch.unique(x_i, return_counts  = True)\n",
    "\n",
    "        if xc.shape[0]==1:\n",
    "            if xv==0:\n",
    "                xclassNoList.append(-1)\n",
    "                xvalueList.append(xv[0].item())\n",
    "            elif xv==1:\n",
    "                xclassNoList.append(xc[0].item())\n",
    "                xvalueList.append(xv[0].item())\n",
    "            else:\n",
    "                print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "                \n",
    "\n",
    "        elif xc.shape[0]==2:\n",
    "                if torch.any(torch.eq(xv, 1)):\n",
    "                    xclassNoList.append(xc[1].item())\n",
    "                    xvalueList.append(xv[1].item())\n",
    "                else:\n",
    "                    print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "        else:\n",
    "            print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "\n",
    "    #pdb.set_trace()\n",
    "    #if torch.any(torch.eq(torch.tensor(xvalueList), 1)):\n",
    "        \n",
    "    if xclassNoList[0]!=xclassNoList[1]: \n",
    "        xclass = torch.argmax(torch.tensor(xclassNoList).to(xdvc))\n",
    "    else:\n",
    "        xclass = torch.tensor(float('NaN')).to(xdvc)\n",
    "            \n",
    "    #else:\n",
    "        '''If all uniques class values are 0, we are assigning nan values as a class'''\n",
    "    #    xclass = torch.tensor(float('NaN')).to(xdvc)\n",
    "    \n",
    "    \n",
    "    return xclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_segclass([torch.tensor([[1,1,1,1,1],[1,1,1,1,0]]), torch.tensor([[1,1,1,1,1], [1,1,1,1,0]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb591cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([0, 0, 0, float('NaN'), 0]).unique(return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d3329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffd12c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
