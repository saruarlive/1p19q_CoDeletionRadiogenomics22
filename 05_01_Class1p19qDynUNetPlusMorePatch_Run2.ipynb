{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73da9fd6",
   "metadata": {},
   "source": [
    "# MRI based brain tumor IDH classification with MONAI (3D multiparametric MRI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb61652",
   "metadata": {},
   "source": [
    "This tutorial shows how to construct a training workflow of binary classification task.  \n",
    "And it contains below features:\n",
    "1. Transforms for Monai dictionary format data.\n",
    "2. Define a new transform according MONAI transform API.\n",
    "3. Load Nifti image with metadata, load a list of images and stack them.\n",
    "5. 3D Voxel DynUNet model, Dice loss, cross entropy loss function for IDH classification task.\n",
    "6. Deterministic training for reproducibility.\n",
    "\n",
    "The Brain tumor dataset can be downloaded from \n",
    "https://ipp.cbica.upenn.edu/ and  http://medicaldecathlon.com/.  \n",
    "\n",
    "Target: IDH classification based on whole brain, tumour core, whole tumor, and enhancing tumor from MRI \n",
    "Modality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  \n",
    "training: 135 3D MRI \\\n",
    "validation:  \\\n",
    "testing: Not revealed\n",
    "\n",
    "Source: BRATS 2020/2021 datasets.  \n",
    "Challenge: RSNA-MICCAI Brain Tumor Radiogenomic Classification\n",
    "\n",
    "Below figure shows image patches with the tumor sub-regions that are annotated in the different modalities (top left) and the final labels for the whole dataset (right). (Figure taken from the [BraTS IEEE TMI paper](https://ieeexplore.ieee.org/document/6975210/))  \n",
    "![image](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/42/7283692/6975210/6975210-fig-3-source-large.gif)\n",
    "\n",
    "The image patches show from left to right:\n",
    "1. the whole tumor (yellow) visible in T2-FLAIR (Fig.A).\n",
    "2. the tumor core (red) visible in T2 (Fig.B).\n",
    "3. the enhancing tumor structures (light blue) visible in T1Gd, surrounding the cystic/necrotic components of the core (green) (Fig. C).\n",
    "4. The segmentations are used to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f847e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.9.0\n",
      "Numpy version: 1.22.3\n",
      "Pytorch version: 1.10.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: af0e0e9f757558d144b655c63afcea3a4e0a06f5\n",
      "MONAI __file__: /home/mmiv-ml/anaconda3/envs/sa_tumorseg22/lib/python3.9/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.9\n",
      "Nibabel version: 4.0.1\n",
      "scikit-image version: 0.19.3\n",
      "Pillow version: 9.0.1\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.11.2\n",
      "tqdm version: 4.64.0\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.1\n",
      "pandas version: 1.4.2\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import sys\n",
    "import gc\n",
    "import logging\n",
    "import copy\n",
    "import pdb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.networks.nets import DynUNet, EfficientNetBN, DenseNet121, SegResNet, SegResNetVAE\n",
    "from monai.data import CacheDataset, Dataset, DataLoader, ThreadDataLoader, list_data_collate\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    CastToTyped,\n",
    "    Compose, \n",
    "    CropForegroundd,\n",
    "    ResizeWithPadOrCrop,\n",
    "    ResizeWithPadOrCropd,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    Resized,\n",
    "    EnsureChannelFirstd, \n",
    "    Orientationd,\n",
    "    LoadImaged,\n",
    "    CopyItemsd,\n",
    "    NormalizeIntensity,\n",
    "    HistogramNormalize,\n",
    "    NormalizeIntensityd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandCropByLabelClassesd,\n",
    "    RandAffined,\n",
    "    RandFlipd,\n",
    "    Flipd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandGibbsNoised,\n",
    "    RandStdShiftIntensityd,\n",
    "    RandScaleIntensityd,\n",
    "    RandZoomd, \n",
    "    SpatialCrop, \n",
    "    SpatialPadd, \n",
    "    MapTransform,\n",
    "    CastToType,\n",
    "    ToTensord,\n",
    "    AddChanneld,\n",
    "    MapTransform,\n",
    "    Orientationd,\n",
    "    ScaleIntensityd,\n",
    "    ScaleIntensity,\n",
    "    ScaleIntensityRangePercentilesd,\n",
    "    KeepLargestConnectedComponentd,\n",
    "    KeepLargestConnectedComponent,\n",
    "    ScaleIntensityRange,\n",
    "    RandShiftIntensityd,\n",
    "    RandAdjustContrastd,\n",
    "    AdjustContrastd,\n",
    "    Rotated,\n",
    "    ToNumpyd,\n",
    "    ToDeviced,\n",
    "    EnsureType,\n",
    "    EnsureTyped,\n",
    "    DataStatsd,\n",
    ")\n",
    "\n",
    "from monai.config import KeysCollection\n",
    "from monai.transforms.compose import MapTransform, Randomizable\n",
    "from collections.abc import Iterable\n",
    "from typing import Any, Dict, Hashable, Mapping, Optional, Sequence, Tuple, Union\n",
    "from monai.utils import set_determinism\n",
    "from monai.utils import (\n",
    "    ensure_tuple,\n",
    "    ensure_tuple_rep,\n",
    "    ensure_tuple_size,\n",
    ")\n",
    "\n",
    "from monai.optimizers import LearningRateFinder\n",
    "\n",
    "from monai.transforms.compose import MapTransform\n",
    "from monai.transforms.utils import generate_spatial_bounding_box\n",
    "from skimage.transform import resize\n",
    "from monai.losses import DiceCELoss, DiceLoss\n",
    "from monai.utils import set_determinism\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "\n",
    "from monai.metrics import DiceMetric, ROCAUCMetric, HausdorffDistanceMetric\n",
    "from monai.data import decollate_batch\n",
    "import glob\n",
    "import monai\n",
    "from monai.metrics import compute_meandice\n",
    "import random\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from typing import Sequence, Optional\n",
    "import ipywidgets as widgets\n",
    "from itertools import compress\n",
    "import SimpleITK as sitk\n",
    "import torchio as tio\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, accuracy_score, recall_score, \\\n",
    "accuracy_score, precision_score, f1_score, make_scorer, balanced_accuracy_score \n",
    "\n",
    "from monai.utils import ensure_tuple_rep\n",
    "from monai.networks.layers.factories import Conv, Dropout, Norm, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from ranger21 import Ranger21\n",
    "\n",
    "### monai and ignite based imports\n",
    "import logging\n",
    "import sys\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.contrib.handlers import FastaiLRFinder, ProgressBar\n",
    "from ignite.engine import (\n",
    "    Events,\n",
    "    _prepare_batch,\n",
    "    create_supervised_evaluator,\n",
    "    create_supervised_trainer,\n",
    ")\n",
    "from ignite.handlers import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itkwidgets import view\n",
    "import random\n",
    "monai.config.print_config()\n",
    "#from sliding_window_inference_classes import sliding_window_inference_classes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0ea9ce8",
   "metadata": {},
   "source": [
    "# Pipelines implemented here\n",
    "#[image](ProposedArchImgPath =250x250)\\\n",
    "<img src=\"assets/ProposedIDHClass.png\" align=\"left\" width=\"1024\" height=\"1800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c66f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b53541",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_THREADS =2\n",
    "sitk.ProcessObject.SetGlobalDefaultNumberOfThreads(MAX_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f3556a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  4 00:37:37 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.172.01   Driver Version: 450.172.01   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    41W / 300W |    108MiB / 32505MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    53W / 300W |   7832MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    37W / 300W |     13MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    50W / 300W |  12909MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2917      G   /usr/lib/xorg/Xorg                 86MiB |\n",
      "|    0   N/A  N/A      3284      G   /usr/bin/gnome-shell               16MiB |\n",
      "|    1   N/A  N/A      2917      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A      2917      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A      2917      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A   3650176      C   .../sa_tumorseg22/bin/python     4567MiB |\n",
      "|    3   N/A  N/A   3708809      C   .../sa_tumorseg22/bin/python     8327MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "seeds = 40961024\n",
    "set_determinism(seed=seeds)\n",
    "##np.random.seed(seeds) np random seed does not work here\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa052ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#patch_size = (128, 128, 128)\n",
    "spacing = (1.0, 1.0, 1.0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"2\"\n",
    "device = torch.device('cuda:0')\n",
    "deviceName = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d70ea20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "data_rpath = '/home/mmiv-ml/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "581468ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BraTS2021</th>\n",
       "      <th>t1wPath</th>\n",
       "      <th>t1cwPath</th>\n",
       "      <th>t2wPath</th>\n",
       "      <th>flairPath</th>\n",
       "      <th>segPath</th>\n",
       "      <th>brain_maskPath</th>\n",
       "      <th>brain_mask_ch2Path</th>\n",
       "      <th>brain_mask_ch1Path</th>\n",
       "      <th>BraTS2020</th>\n",
       "      <th>...</th>\n",
       "      <th>Histology</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Data Collection</th>\n",
       "      <th>IDH</th>\n",
       "      <th>1p19q_co_deletion_bin</th>\n",
       "      <th>IDH</th>\n",
       "      <th>1p19q_co_deletion</th>\n",
       "      <th>CV_group</th>\n",
       "      <th>is_merged_2</th>\n",
       "      <th>is_merged_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BraTS2021_00140</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_00140/ROI_BraTS2021_00140.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_00140/BraTS2021_00140_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_00140/BraTS2021_00140_BrainROI.nii.gz</td>\n",
       "      <td>BraTS20_Training_233</td>\n",
       "      <td>...</td>\n",
       "      <td>glioblastoma</td>\n",
       "      <td>G4</td>\n",
       "      <td>HGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non-codel</td>\n",
       "      <td>3.0</td>\n",
       "      <td>both</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BraTS2021_01283</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01283/ROI_BraTS2021_01283.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01283/BraTS2021_01283_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01283/BraTS2021_01283_BrainROI.nii.gz</td>\n",
       "      <td>BraTS20_Training_165</td>\n",
       "      <td>...</td>\n",
       "      <td>glioblastoma</td>\n",
       "      <td>G4</td>\n",
       "      <td>HGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non-codel</td>\n",
       "      <td>2.0</td>\n",
       "      <td>both</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BraTS2021_01528</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01528/ROI_BraTS2021_01528.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01528/BraTS2021_01528_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01528/BraTS2021_01528_BrainROI.nii.gz</td>\n",
       "      <td>BraTS20_Training_323</td>\n",
       "      <td>...</td>\n",
       "      <td>oligoastrocytoma</td>\n",
       "      <td>G2</td>\n",
       "      <td>LGG</td>\n",
       "      <td>Mutant</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IDH1</td>\n",
       "      <td>non-codel</td>\n",
       "      <td>3.0</td>\n",
       "      <td>both</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BraTS2021_01503</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01503/ROI_BraTS2021_01503.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01503/BraTS2021_01503_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01503/BraTS2021_01503_BrainROI.nii.gz</td>\n",
       "      <td>BraTS20_Training_298</td>\n",
       "      <td>...</td>\n",
       "      <td>oligodendroglioma</td>\n",
       "      <td>G3</td>\n",
       "      <td>LGG</td>\n",
       "      <td>Mutant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>IDH1</td>\n",
       "      <td>codel</td>\n",
       "      <td>3.0</td>\n",
       "      <td>both</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BraTS2021_01453</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01453/ROI_BraTS2021_01453.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01453/BraTS2021_01453_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01453/BraTS2021_01453_BrainROI.nii.gz</td>\n",
       "      <td>BraTS20_Training_206</td>\n",
       "      <td>...</td>\n",
       "      <td>glioblastoma</td>\n",
       "      <td>G4</td>\n",
       "      <td>HGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non-codel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>both</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t2.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-651/LGG-651_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-651/LGG-651_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-651/LGG-651_BrainROI.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Oligodendroglioma</td>\n",
       "      <td>G2</td>\n",
       "      <td>LGG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>codel</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t2.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-658/LGG-658_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-658/LGG-658_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-658/LGG-658_BrainROI.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Oligodendroglioma</td>\n",
       "      <td>G3</td>\n",
       "      <td>LGG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>codel</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t2.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-659/LGG-659_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-659/LGG-659_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-659/LGG-659_BrainROI.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Oligoastrocytoma</td>\n",
       "      <td>G2</td>\n",
       "      <td>LGG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>codel</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t2.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-660/LGG-660_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-660/LGG-660_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-660/LGG-660_BrainROI.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Oligoastrocytoma</td>\n",
       "      <td>G2</td>\n",
       "      <td>LGG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>codel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t2.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-766/LGG-766_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-766/LGG-766_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-766/LGG-766_BrainROI.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Oligoastrocytoma</td>\n",
       "      <td>G2</td>\n",
       "      <td>LGG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non-codel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>368 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           BraTS2021  \\\n",
       "0    BraTS2021_00140   \n",
       "1    BraTS2021_01283   \n",
       "2    BraTS2021_01528   \n",
       "3    BraTS2021_01503   \n",
       "4    BraTS2021_01453   \n",
       "..               ...   \n",
       "363              NaN   \n",
       "364              NaN   \n",
       "365              NaN   \n",
       "366              NaN   \n",
       "367              NaN   \n",
       "\n",
       "                                                                                               t1wPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1.nii.gz   \n",
       "..                                                                                                 ...   \n",
       "363                                                                                                NaN   \n",
       "364                                                                                                NaN   \n",
       "365                                                                                                NaN   \n",
       "366                                                                                                NaN   \n",
       "367                                                                                                NaN   \n",
       "\n",
       "                                                                                                t1cwPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1ce.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1ce.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1ce.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1ce.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1ce.nii.gz   \n",
       "..                                                                                                   ...   \n",
       "363     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t1Gd.nii.gz   \n",
       "364     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t1Gd.nii.gz   \n",
       "365     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t1Gd.nii.gz   \n",
       "366     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t1Gd.nii.gz   \n",
       "367     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t1Gd.nii.gz   \n",
       "\n",
       "                                                                                               t2wPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t2.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t2.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t2.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t2.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t2.nii.gz   \n",
       "..                                                                                                 ...   \n",
       "363     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t2.nii.gz   \n",
       "364     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t2.nii.gz   \n",
       "365     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t2.nii.gz   \n",
       "366     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t2.nii.gz   \n",
       "367     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t2.nii.gz   \n",
       "\n",
       "                                                                                                flairPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_flair.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_flair.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_flair.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_flair.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_flair.nii.gz   \n",
       "..                                                                                                    ...   \n",
       "363                                                                                                   NaN   \n",
       "364                                                                                                   NaN   \n",
       "365                                                                                                   NaN   \n",
       "366                                                                                                   NaN   \n",
       "367                                                                                                   NaN   \n",
       "\n",
       "                                                                                                segPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_seg.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_seg.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_seg.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_seg.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_seg.nii.gz   \n",
       "..                                                                                                  ...   \n",
       "363            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-651/LGG-651_pred.nii.gz   \n",
       "364            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-658/LGG-658_pred.nii.gz   \n",
       "365            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-659/LGG-659_pred.nii.gz   \n",
       "366            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-660/LGG-660_pred.nii.gz   \n",
       "367            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-766/LGG-766_pred.nii.gz   \n",
       "\n",
       "                                                                 brain_maskPath  \\\n",
       "0    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_00140/ROI_BraTS2021_00140.nii.gz   \n",
       "1    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01283/ROI_BraTS2021_01283.nii.gz   \n",
       "2    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01528/ROI_BraTS2021_01528.nii.gz   \n",
       "3    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01503/ROI_BraTS2021_01503.nii.gz   \n",
       "4    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01453/ROI_BraTS2021_01453.nii.gz   \n",
       "..                                                                          ...   \n",
       "363                                                                         NaN   \n",
       "364                                                                         NaN   \n",
       "365                                                                         NaN   \n",
       "366                                                                         NaN   \n",
       "367                                                                         NaN   \n",
       "\n",
       "                                                                                         brain_mask_ch2Path  \\\n",
       "0    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_00140/BraTS2021_00140_BrainROIT1cwx2.nii.gz   \n",
       "1    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01283/BraTS2021_01283_BrainROIT1cwx2.nii.gz   \n",
       "2    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01528/BraTS2021_01528_BrainROIT1cwx2.nii.gz   \n",
       "3    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01503/BraTS2021_01503_BrainROIT1cwx2.nii.gz   \n",
       "4    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01453/BraTS2021_01453_BrainROIT1cwx2.nii.gz   \n",
       "..                                                                                                      ...   \n",
       "363      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-651/LGG-651_BrainROIT1cwx2.nii.gz   \n",
       "364      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-658/LGG-658_BrainROIT1cwx2.nii.gz   \n",
       "365      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-659/LGG-659_BrainROIT1cwx2.nii.gz   \n",
       "366      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-660/LGG-660_BrainROIT1cwx2.nii.gz   \n",
       "367      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-766/LGG-766_BrainROIT1cwx2.nii.gz   \n",
       "\n",
       "                                                                                   brain_mask_ch1Path  \\\n",
       "0    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_00140/BraTS2021_00140_BrainROI.nii.gz   \n",
       "1    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01283/BraTS2021_01283_BrainROI.nii.gz   \n",
       "2    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01528/BraTS2021_01528_BrainROI.nii.gz   \n",
       "3    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01503/BraTS2021_01503_BrainROI.nii.gz   \n",
       "4    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01453/BraTS2021_01453_BrainROI.nii.gz   \n",
       "..                                                                                                ...   \n",
       "363      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-651/LGG-651_BrainROI.nii.gz   \n",
       "364      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-658/LGG-658_BrainROI.nii.gz   \n",
       "365      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-659/LGG-659_BrainROI.nii.gz   \n",
       "366      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-660/LGG-660_BrainROI.nii.gz   \n",
       "367      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-766/LGG-766_BrainROI.nii.gz   \n",
       "\n",
       "                BraTS2020  ...          Histology Grade Data Collection  \\\n",
       "0    BraTS20_Training_233  ...       glioblastoma    G4             HGG   \n",
       "1    BraTS20_Training_165  ...       glioblastoma    G4             HGG   \n",
       "2    BraTS20_Training_323  ...   oligoastrocytoma    G2             LGG   \n",
       "3    BraTS20_Training_298  ...  oligodendroglioma    G3             LGG   \n",
       "4    BraTS20_Training_206  ...       glioblastoma    G4             HGG   \n",
       "..                    ...  ...                ...   ...             ...   \n",
       "363                   NaN  ...  Oligodendroglioma    G2             LGG   \n",
       "364                   NaN  ...  Oligodendroglioma    G3             LGG   \n",
       "365                   NaN  ...   Oligoastrocytoma    G2             LGG   \n",
       "366                   NaN  ...   Oligoastrocytoma    G2             LGG   \n",
       "367                   NaN  ...   Oligoastrocytoma    G2             LGG   \n",
       "\n",
       "        IDH  1p19q_co_deletion_bin   IDH 1p19q_co_deletion  CV_group  \\\n",
       "0        WT                    0.0   NaN         non-codel       3.0   \n",
       "1        WT                    0.0   NaN         non-codel       2.0   \n",
       "2    Mutant                    0.0  IDH1         non-codel       3.0   \n",
       "3    Mutant                    1.0  IDH1             codel       3.0   \n",
       "4        WT                    0.0   NaN         non-codel       1.0   \n",
       "..      ...                    ...   ...               ...       ...   \n",
       "363     NaN                    1.0   NaN             codel       3.0   \n",
       "364     NaN                    1.0   NaN             codel       3.0   \n",
       "365     NaN                    1.0   NaN             codel       2.0   \n",
       "366     NaN                    1.0   NaN             codel       1.0   \n",
       "367     NaN                    0.0   NaN         non-codel       1.0   \n",
       "\n",
       "    is_merged_2 is_merged_0  \n",
       "0          both         NaN  \n",
       "1          both         NaN  \n",
       "2          both         NaN  \n",
       "3          both         NaN  \n",
       "4          both         NaN  \n",
       "..          ...         ...  \n",
       "363         NaN        both  \n",
       "364         NaN        both  \n",
       "365         NaN        both  \n",
       "366         NaN        both  \n",
       "367         NaN        both  \n",
       "\n",
       "[368 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BraTS20Subjectsp1q19WithMetaDF  = pd.read_csv('assets/BraTS_TCGA_LGG_GBM_LGG_1p19qDFMetaFew.csv')\n",
    "BraTS20Subjectsp1q19WithMetaDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80470792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BraTS2021</th>\n",
       "      <th>t1wPath</th>\n",
       "      <th>t1cwPath</th>\n",
       "      <th>t2wPath</th>\n",
       "      <th>flairPath</th>\n",
       "      <th>segPath</th>\n",
       "      <th>brain_maskPath</th>\n",
       "      <th>brain_mask_ch2Path</th>\n",
       "      <th>brain_mask_ch1Path</th>\n",
       "      <th>BraTS2020</th>\n",
       "      <th>...</th>\n",
       "      <th>Histology</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Data Collection</th>\n",
       "      <th>IDH</th>\n",
       "      <th>1p19q_co_deletion_bin</th>\n",
       "      <th>IDH</th>\n",
       "      <th>1p19q_co_deletion</th>\n",
       "      <th>CV_group</th>\n",
       "      <th>is_merged_2</th>\n",
       "      <th>is_merged_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BraTS2021_00140</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_00140/ROI_BraTS2021_00140.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_00140/BraTS2021_00140_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_00140/BraTS2021_00140_BrainROI.nii.gz</td>\n",
       "      <td>BraTS20_Training_233</td>\n",
       "      <td>...</td>\n",
       "      <td>glioblastoma</td>\n",
       "      <td>G4</td>\n",
       "      <td>HGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non-codel</td>\n",
       "      <td>3.0</td>\n",
       "      <td>both</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BraTS2021_01283</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01283/ROI_BraTS2021_01283.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01283/BraTS2021_01283_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01283/BraTS2021_01283_BrainROI.nii.gz</td>\n",
       "      <td>BraTS20_Training_165</td>\n",
       "      <td>...</td>\n",
       "      <td>glioblastoma</td>\n",
       "      <td>G4</td>\n",
       "      <td>HGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non-codel</td>\n",
       "      <td>2.0</td>\n",
       "      <td>both</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BraTS2021_01528</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01528/ROI_BraTS2021_01528.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01528/BraTS2021_01528_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01528/BraTS2021_01528_BrainROI.nii.gz</td>\n",
       "      <td>BraTS20_Training_323</td>\n",
       "      <td>...</td>\n",
       "      <td>oligoastrocytoma</td>\n",
       "      <td>G2</td>\n",
       "      <td>LGG</td>\n",
       "      <td>Mutant</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IDH1</td>\n",
       "      <td>non-codel</td>\n",
       "      <td>3.0</td>\n",
       "      <td>both</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BraTS2021_01503</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01503/ROI_BraTS2021_01503.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01503/BraTS2021_01503_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01503/BraTS2021_01503_BrainROI.nii.gz</td>\n",
       "      <td>BraTS20_Training_298</td>\n",
       "      <td>...</td>\n",
       "      <td>oligodendroglioma</td>\n",
       "      <td>G3</td>\n",
       "      <td>LGG</td>\n",
       "      <td>Mutant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>IDH1</td>\n",
       "      <td>codel</td>\n",
       "      <td>3.0</td>\n",
       "      <td>both</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BraTS2021_01453</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1ce.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t2.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_flair.nii.gz</td>\n",
       "      <td>/raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_seg.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01453/ROI_BraTS2021_01453.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01453/BraTS2021_01453_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01453/BraTS2021_01453_BrainROI.nii.gz</td>\n",
       "      <td>BraTS20_Training_206</td>\n",
       "      <td>...</td>\n",
       "      <td>glioblastoma</td>\n",
       "      <td>G4</td>\n",
       "      <td>HGG</td>\n",
       "      <td>WT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non-codel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>both</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t2.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-651/LGG-651_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-651/LGG-651_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-651/LGG-651_BrainROI.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Oligodendroglioma</td>\n",
       "      <td>G2</td>\n",
       "      <td>LGG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>codel</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t2.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-658/LGG-658_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-658/LGG-658_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-658/LGG-658_BrainROI.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Oligodendroglioma</td>\n",
       "      <td>G3</td>\n",
       "      <td>LGG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>codel</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t2.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-659/LGG-659_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-659/LGG-659_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-659/LGG-659_BrainROI.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Oligoastrocytoma</td>\n",
       "      <td>G2</td>\n",
       "      <td>LGG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>codel</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t2.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-660/LGG-660_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-660/LGG-660_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-660/LGG-660_BrainROI.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Oligoastrocytoma</td>\n",
       "      <td>G2</td>\n",
       "      <td>LGG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>codel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t1Gd.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t2.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-766/LGG-766_pred.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-766/LGG-766_BrainROIT1cwx2.nii.gz</td>\n",
       "      <td>/raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-766/LGG-766_BrainROI.nii.gz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Oligoastrocytoma</td>\n",
       "      <td>G2</td>\n",
       "      <td>LGG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non-codel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>368 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           BraTS2021  \\\n",
       "0    BraTS2021_00140   \n",
       "1    BraTS2021_01283   \n",
       "2    BraTS2021_01528   \n",
       "3    BraTS2021_01503   \n",
       "4    BraTS2021_01453   \n",
       "..               ...   \n",
       "363              NaN   \n",
       "364              NaN   \n",
       "365              NaN   \n",
       "366              NaN   \n",
       "367              NaN   \n",
       "\n",
       "                                                                                               t1wPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1.nii.gz   \n",
       "..                                                                                                 ...   \n",
       "363                                                                                                NaN   \n",
       "364                                                                                                NaN   \n",
       "365                                                                                                NaN   \n",
       "366                                                                                                NaN   \n",
       "367                                                                                                NaN   \n",
       "\n",
       "                                                                                                t1cwPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t1ce.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t1ce.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t1ce.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t1ce.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t1ce.nii.gz   \n",
       "..                                                                                                   ...   \n",
       "363     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t1Gd.nii.gz   \n",
       "364     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t1Gd.nii.gz   \n",
       "365     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t1Gd.nii.gz   \n",
       "366     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t1Gd.nii.gz   \n",
       "367     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t1Gd.nii.gz   \n",
       "\n",
       "                                                                                               t2wPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_t2.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_t2.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_t2.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_t2.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_t2.nii.gz   \n",
       "..                                                                                                 ...   \n",
       "363     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-651/LGG-651_t2.nii.gz   \n",
       "364     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-658/LGG-658_t2.nii.gz   \n",
       "365     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-659/LGG-659_t2.nii.gz   \n",
       "366     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-660/LGG-660_t2.nii.gz   \n",
       "367     /raid/brats2021/LGG_1p19q_rawNifti/LGG_1p19q_BraTSLikeProcess_mnibet/LGG-766/LGG-766_t2.nii.gz   \n",
       "\n",
       "                                                                                                flairPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_flair.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_flair.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_flair.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_flair.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_flair.nii.gz   \n",
       "..                                                                                                    ...   \n",
       "363                                                                                                   NaN   \n",
       "364                                                                                                   NaN   \n",
       "365                                                                                                   NaN   \n",
       "366                                                                                                   NaN   \n",
       "367                                                                                                   NaN   \n",
       "\n",
       "                                                                                                segPath  \\\n",
       "0    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_00140/BraTS2021_00140_seg.nii.gz   \n",
       "1    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01283/BraTS2021_01283_seg.nii.gz   \n",
       "2    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01528/BraTS2021_01528_seg.nii.gz   \n",
       "3    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01503/BraTS2021_01503_seg.nii.gz   \n",
       "4    /raid/brats2021/RSNA_ASNR_MICCAI_BraTS2021_TrainingData/BraTS2021_01453/BraTS2021_01453_seg.nii.gz   \n",
       "..                                                                                                  ...   \n",
       "363            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-651/LGG-651_pred.nii.gz   \n",
       "364            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-658/LGG-658_pred.nii.gz   \n",
       "365            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-659/LGG-659_pred.nii.gz   \n",
       "366            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-660/LGG-660_pred.nii.gz   \n",
       "367            /raid/brats2021/LGG_1p19q_rawNifti/4Ensemble_LGG_1p19q_Infer/LGG-766/LGG-766_pred.nii.gz   \n",
       "\n",
       "                                                                 brain_maskPath  \\\n",
       "0    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_00140/ROI_BraTS2021_00140.nii.gz   \n",
       "1    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01283/ROI_BraTS2021_01283.nii.gz   \n",
       "2    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01528/ROI_BraTS2021_01528.nii.gz   \n",
       "3    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01503/ROI_BraTS2021_01503.nii.gz   \n",
       "4    /raid/brats2021/T1wx4Brain_ROIs/BraTS2021_01453/ROI_BraTS2021_01453.nii.gz   \n",
       "..                                                                          ...   \n",
       "363                                                                         NaN   \n",
       "364                                                                         NaN   \n",
       "365                                                                         NaN   \n",
       "366                                                                         NaN   \n",
       "367                                                                         NaN   \n",
       "\n",
       "                                                                                         brain_mask_ch2Path  \\\n",
       "0    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_00140/BraTS2021_00140_BrainROIT1cwx2.nii.gz   \n",
       "1    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01283/BraTS2021_01283_BrainROIT1cwx2.nii.gz   \n",
       "2    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01528/BraTS2021_01528_BrainROIT1cwx2.nii.gz   \n",
       "3    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01503/BraTS2021_01503_BrainROIT1cwx2.nii.gz   \n",
       "4    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01453/BraTS2021_01453_BrainROIT1cwx2.nii.gz   \n",
       "..                                                                                                      ...   \n",
       "363      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-651/LGG-651_BrainROIT1cwx2.nii.gz   \n",
       "364      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-658/LGG-658_BrainROIT1cwx2.nii.gz   \n",
       "365      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-659/LGG-659_BrainROIT1cwx2.nii.gz   \n",
       "366      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-660/LGG-660_BrainROIT1cwx2.nii.gz   \n",
       "367      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-766/LGG-766_BrainROIT1cwx2.nii.gz   \n",
       "\n",
       "                                                                                   brain_mask_ch1Path  \\\n",
       "0    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_00140/BraTS2021_00140_BrainROI.nii.gz   \n",
       "1    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01283/BraTS2021_01283_BrainROI.nii.gz   \n",
       "2    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01528/BraTS2021_01528_BrainROI.nii.gz   \n",
       "3    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01503/BraTS2021_01503_BrainROI.nii.gz   \n",
       "4    /raid/brats2021/T1wx2Brain_ROIs_BraTS21_Training/BraTS2021_01453/BraTS2021_01453_BrainROI.nii.gz   \n",
       "..                                                                                                ...   \n",
       "363      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-651/LGG-651_BrainROI.nii.gz   \n",
       "364      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-658/LGG-658_BrainROI.nii.gz   \n",
       "365      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-659/LGG-659_BrainROI.nii.gz   \n",
       "366      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-660/LGG-660_BrainROI.nii.gz   \n",
       "367      /raid/brats2021/LGG_1p19q_rawNifti/T1wx2Brain_ROIs_LGG_1p19q/LGG-766/LGG-766_BrainROI.nii.gz   \n",
       "\n",
       "                BraTS2020  ...          Histology Grade Data Collection  \\\n",
       "0    BraTS20_Training_233  ...       glioblastoma    G4             HGG   \n",
       "1    BraTS20_Training_165  ...       glioblastoma    G4             HGG   \n",
       "2    BraTS20_Training_323  ...   oligoastrocytoma    G2             LGG   \n",
       "3    BraTS20_Training_298  ...  oligodendroglioma    G3             LGG   \n",
       "4    BraTS20_Training_206  ...       glioblastoma    G4             HGG   \n",
       "..                    ...  ...                ...   ...             ...   \n",
       "363                   NaN  ...  Oligodendroglioma    G2             LGG   \n",
       "364                   NaN  ...  Oligodendroglioma    G3             LGG   \n",
       "365                   NaN  ...   Oligoastrocytoma    G2             LGG   \n",
       "366                   NaN  ...   Oligoastrocytoma    G2             LGG   \n",
       "367                   NaN  ...   Oligoastrocytoma    G2             LGG   \n",
       "\n",
       "        IDH  1p19q_co_deletion_bin   IDH 1p19q_co_deletion  CV_group  \\\n",
       "0        WT                    0.0   NaN         non-codel       3.0   \n",
       "1        WT                    0.0   NaN         non-codel       2.0   \n",
       "2    Mutant                    0.0  IDH1         non-codel       3.0   \n",
       "3    Mutant                    1.0  IDH1             codel       3.0   \n",
       "4        WT                    0.0   NaN         non-codel       1.0   \n",
       "..      ...                    ...   ...               ...       ...   \n",
       "363     NaN                    1.0   NaN             codel       3.0   \n",
       "364     NaN                    1.0   NaN             codel       3.0   \n",
       "365     NaN                    1.0   NaN             codel       2.0   \n",
       "366     NaN                    1.0   NaN             codel       1.0   \n",
       "367     NaN                    0.0   NaN         non-codel       1.0   \n",
       "\n",
       "    is_merged_2 is_merged_0  \n",
       "0          both         NaN  \n",
       "1          both         NaN  \n",
       "2          both         NaN  \n",
       "3          both         NaN  \n",
       "4          both         NaN  \n",
       "..          ...         ...  \n",
       "363         NaN        both  \n",
       "364         NaN        both  \n",
       "365         NaN        both  \n",
       "366         NaN        both  \n",
       "367         NaN        both  \n",
       "\n",
       "[368 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BraTS20Subjectsp1q19WithMetaDF = BraTS20Subjectsp1q19WithMetaDF.apply(pd.to_numeric, errors = 'ignore')\n",
    "BraTS20Subjectsp1q19WithMetaDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d99d4717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([238, 130]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(BraTS20Subjectsp1q19WithMetaDF['1p19q_co_deletion_bin'].values, return_counts = True)\n",
    "#BraTS20SubjectsIDHWithMetaDF['1p19q_co_deletion_bin'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c9b8f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BraTS2021                          object\n",
       "t1wPath                            object\n",
       "t1cwPath                           object\n",
       "t2wPath                            object\n",
       "flairPath                          object\n",
       "segPath                            object\n",
       "brain_maskPath                     object\n",
       "brain_mask_ch2Path                 object\n",
       "brain_mask_ch1Path                 object\n",
       "BraTS2020                          object\n",
       "BraTS2019                          object\n",
       "BraTS2018                          object\n",
       "BraTS2017                          object\n",
       "CohortName                         object\n",
       "SiteNo_Originating_Institution    float64\n",
       "TCGA_ID                            object\n",
       "is_merged_1                        object\n",
       "Age                               float64\n",
       "Gender                             object\n",
       "Histology                          object\n",
       "Grade                              object\n",
       "Data Collection                    object\n",
       "   IDH                             object\n",
       "1p19q_co_deletion_bin             float64\n",
       "IDH                                object\n",
       "1p19q_co_deletion                  object\n",
       "CV_group                          float64\n",
       "is_merged_2                        object\n",
       "is_merged_0                        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BraTS20Subjectsp1q19WithMetaDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79b5ab",
   "metadata": {},
   "source": [
    "## Cearing a list of dictionaries in order to feed into Monai's Dataset\n",
    "Keys:\n",
    "- ***image:*** T1, T1c, T2, and flair image\n",
    "- ***label:*** Segmented mask GT\n",
    "- ***brain_mask:*** Whole brain area (brain area=1 and Non brain area=0)\n",
    "- ***IDH_value:*** IDH class corresponding to the subject/images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674cc46f",
   "metadata": {},
   "source": [
    "### Creating/extracting 3 splits for cross validaion (3 cross validaion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813c8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_df(BraTS20SubjectsIDHWithMetaDF):\n",
    "    \n",
    "    \n",
    "    BraTS20SubjectsIDHTrainDCT = {}\n",
    "    BraTS20SubjectsIDHValDCT = {}\n",
    "    BraTS20SubjectsIDHTestDCT = {}\n",
    "    \n",
    "    aDCT = {'fold0':[1, 2, 3], 'fold1':[2, 3, 1], 'fold3': [3, 1, 2]}\n",
    "    \n",
    "    for indx, (akey, aval) in enumerate(aDCT.items()):\n",
    "        \n",
    "    \n",
    "        BraTS20SubjectsIDHWithMetaDFTrain = BraTS20SubjectsIDHWithMetaDF.loc[BraTS20SubjectsIDHWithMetaDF['CV_group']==aval[0]]\n",
    "        BraTS20SubjectsIDHWithMetaDFVal = BraTS20SubjectsIDHWithMetaDF.loc[BraTS20SubjectsIDHWithMetaDF['CV_group']==aval[1]]\n",
    "        BraTS20SubjectsIDHWithMetaDFTest = BraTS20SubjectsIDHWithMetaDF.loc[BraTS20SubjectsIDHWithMetaDF['CV_group']==aval[2]]\n",
    "\n",
    "        train_files = [{'image': (image_nameT1ce, image_nameT2), 'label': label_name, 'brain_mask':brain_mask, 'IDH_label':np.array(IDH_label_name).astype(np.float32)} \n",
    "                       for image_nameT1ce, image_nameT2, label_name, brain_mask, IDH_label_name \n",
    "                       in zip(BraTS20SubjectsIDHWithMetaDFTrain['t1cwPath'], BraTS20SubjectsIDHWithMetaDFTrain['t2wPath'], BraTS20SubjectsIDHWithMetaDFTrain['segPath'], \\\n",
    "                              BraTS20SubjectsIDHWithMetaDFTrain['brain_mask_ch2Path'], BraTS20SubjectsIDHWithMetaDFTrain['1p19q_co_deletion_bin'].values)]\n",
    "        \n",
    "        val_files =[{'image': (image_nameT1ce, image_nameT2), 'label': label_name, 'brain_mask':brain_mask, 'IDH_label':np.array(IDH_label_name).astype(np.float32)} \n",
    "                    for image_nameT1ce, image_nameT2, label_name, brain_mask, IDH_label_name \n",
    "                    in zip(BraTS20SubjectsIDHWithMetaDFTrain['t1cwPath'], BraTS20SubjectsIDHWithMetaDFVal['t2wPath'],BraTS20SubjectsIDHWithMetaDFVal['segPath'],\\\n",
    "                           BraTS20SubjectsIDHWithMetaDFVal['brain_mask_ch2Path'], BraTS20SubjectsIDHWithMetaDFVal['1p19q_co_deletion_bin'].values)]\n",
    "        \n",
    "        test_files = [{'image': (image_nameT1ce, image_nameT2), 'label': label_name, 'brain_mask':brain_mask, 'IDH_label':np.array(IDH_label_name).astype(np.float32)} \n",
    "                      for image_nameT1ce, image_nameT2, label_name, brain_mask, IDH_label_name \n",
    "                      in zip(BraTS20SubjectsIDHWithMetaDFTrain['t1cwPath'], BraTS20SubjectsIDHWithMetaDFTest['t2wPath'], BraTS20SubjectsIDHWithMetaDFTest['segPath'], \\\n",
    "                             BraTS20SubjectsIDHWithMetaDFTest['brain_mask_ch2Path'], BraTS20SubjectsIDHWithMetaDFTest['1p19q_co_deletion_bin'].values)]\n",
    "        \n",
    "        \n",
    "        BraTS20SubjectsIDHTrainDCT[f'fold{indx}'] = copy.deepcopy(train_files)\n",
    "        BraTS20SubjectsIDHValDCT[f'fold{indx}'] = copy.deepcopy(val_files)\n",
    "        BraTS20SubjectsIDHTestDCT[f'fold{indx}'] = copy.deepcopy(test_files)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return BraTS20SubjectsIDHTrainDCT, BraTS20SubjectsIDHValDCT, BraTS20SubjectsIDHTestDCT\n",
    "        \n",
    "        \n",
    "        \n",
    "BraTS20SubjectsIDHTrainDCT, BraTS20SubjectsIDHValDCT, BraTS20SubjectsIDHTestDCT =  get_train_val_test_df(BraTS20Subjectsp1q19WithMetaDF)      \n",
    "        \n",
    "        \n",
    "\n",
    "# train_files_image = [(image_nameT1, image_nameT1ce, image_nameT2, image_nameFl) \n",
    "#                      for image_nameT1,image_nameT1ce, image_nameT2, image_nameFl \n",
    "#                      in zip(dfTrainLbl['t1wPath'], dfTrainLbl['t1cwPath'], dfTrainLbl['T2wPath'], dfTrainLbl['FlairPath'])]\n",
    "# train_files_label = dfTrainLbl['segPath'].tolist()\n",
    "# train_files_brain_mask = dfTrainLbl['brain_maskPath'].tolist()\n",
    "# train_files_IDH_label = dfTrainLbl['IDH_value'].values.ravel().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1472abda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9454b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_splits = 3\n",
    "# #train_index = np.linspace(0, train_features.shape[0]-1, num = train_features.shape[0], dtype = np.uint16, endpoint=True)\n",
    "# #partition_data = monai.data.utils.partition_dataset_classes(train_index, train_labels.values.ravel().tolist(), shuffle=True, num_partitions=n_splits) \n",
    "# #partition_data = monai.data.utils.partition_dataset_classes(train_files, dfTrainLbl['IDH_value'].values.ravel().tolist(), shuffle=True, num_partitions=n_splits)\n",
    "# partition_data = monai.data.partition_dataset_classes(train_files, BraTS20SubjectsIDHWithMetaDF['IDH_value'].values.ravel().tolist(), shuffle=True, num_partitions=n_splits)\n",
    "# print(len(partition_data), len(partition_data[0]), len(partition_data[1]), len(partition_data[2]))\n",
    "\n",
    "\n",
    "# # val_folds = {}\n",
    "# # train_folds = {}\n",
    "# # flds = np.linspace(0, n_splits, num=n_splits, dtype = np.int8)\n",
    "# # for cfold in range(n_splits):\n",
    "# #     not_cfold = np.delete(flds, cfold)\n",
    "# #     val_folds[cfold] = partition_data[cfold]\n",
    "# # #     train_folds[cfold] = \n",
    "# # # sub_flds = flds[..., ~0]   \n",
    "# # # sub_flds\n",
    "\n",
    "# val_folds = {}\n",
    "# train_folds = {}\n",
    "# flds = np.linspace(0, n_splits, num=n_splits, dtype = np.uint8)\n",
    "# for cfold in range(n_splits):\n",
    "#     #val_folds[f\"fold{cfold}\"] = train_features.values[partition_data[cfold],:]\n",
    "#     #train_folds[f\"fold{cfold}\"] = np.delete(train_features.values, partition_data[cfold], axis=0)\n",
    "#     #not_cfold = np.delete(flds, cfold)\n",
    "    \n",
    "#     val_folds[f\"fold{cfold}\"] = copy.deepcopy(partition_data[cfold])\n",
    "#     val_folds[f\"fold{cfold}_IDH_label\"] = copy.deepcopy([adct['IDH_label'].item() for adct in partition_data[cfold]])\n",
    "#     train_folds_masks = [1]*n_splits\n",
    "#     train_folds_masks[cfold] = 0\n",
    "#     partition_data_non_cfold = list()\n",
    "#     for aDctLstitem in compress(partition_data, train_folds_masks):\n",
    "#         partition_data_non_cfold.extend(aDctLstitem)\n",
    "        \n",
    "        \n",
    "#     train_folds[f\"fold{cfold}\"] = copy.deepcopy(partition_data_non_cfold)\n",
    "#     train_folds[f\"fold{cfold}_IDH_label\"] = copy.deepcopy([adct['IDH_label'].item() for adct in partition_data_non_cfold])\n",
    "\n",
    "# for i in range(n_splits):\n",
    "#     print('val: ', len(val_folds[f'fold{i}']), 'train: ', len(train_folds[f'fold{i}']), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb14b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_folds[\"fold0\"]), len(train_files)\n",
    "\n",
    "# for i_cv in range(n_splits):\n",
    "#     print('Training classes\\n')\n",
    "#     print(np.unique([train_folds[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(train_folds[f'fold{i_cv}']))], return_counts = True))\n",
    "#     print('\\nValidation classes\\n')\n",
    "#     print(np.unique([val_folds[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(val_folds[f'fold{i_cv}']))], return_counts = True))\n",
    "#     print('#'*4, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b222291e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classes\n",
      "\n",
      "(array([0., 1.]), array([79, 43]))\n",
      "\n",
      "Validation classes\n",
      "\n",
      "(array([0., 1.]), array([80, 42]))\n",
      "\n",
      "Testing classes\n",
      "\n",
      "(array([0., 1.]), array([79, 43]))\n",
      "######################################## \n",
      "\n",
      "\n",
      "Training classes\n",
      "\n",
      "(array([0., 1.]), array([80, 44]))\n",
      "\n",
      "Validation classes\n",
      "\n",
      "(array([0., 1.]), array([79, 43]))\n",
      "\n",
      "Testing classes\n",
      "\n",
      "(array([0., 1.]), array([79, 43]))\n",
      "######################################## \n",
      "\n",
      "\n",
      "Training classes\n",
      "\n",
      "(array([0., 1.]), array([79, 43]))\n",
      "\n",
      "Validation classes\n",
      "\n",
      "(array([0., 1.]), array([79, 43]))\n",
      "\n",
      "Testing classes\n",
      "\n",
      "(array([0., 1.]), array([80, 42]))\n",
      "######################################## \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_splits = 3\n",
    "dfFolds = BraTS20SubjectsIDHTrainDCT\n",
    "for i_cv in range(n_splits):\n",
    "    print('Training classes\\n')\n",
    "    print(np.unique([BraTS20SubjectsIDHTrainDCT[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(BraTS20SubjectsIDHTrainDCT[f'fold{i_cv}']))], return_counts = True))\n",
    "    print('\\nValidation classes\\n')\n",
    "    print(np.unique([BraTS20SubjectsIDHValDCT[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(BraTS20SubjectsIDHValDCT[f'fold{i_cv}']))], return_counts = True))\n",
    "    #print('#'*4, '\\n')\n",
    "    print('\\nTesting classes\\n')\n",
    "    print(np.unique([BraTS20SubjectsIDHTestDCT[f'fold{i_cv}'][i]['IDH_label'].item() for i in range(len(BraTS20SubjectsIDHTestDCT[f'fold{i_cv}']))], return_counts = True))\n",
    "    print('#'*40, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e363dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0f581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8941664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_folds['fold0'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9853822",
   "metadata": {},
   "source": [
    "***HistogramStandardization***\n",
    "\n",
    "Implementing histogram standardization from [torchIO](https://github.com/fepegar/torchio) library\n",
    "\n",
    "Bases: [torchio.transforms.preprocessing.intensity.normalization_transform.NormalizationTransform](https://torchio.readthedocs.io/transforms/preprocessing.html#torchio.transforms.preprocessing.intensity.NormalizationTransform)\n",
    "\n",
    "Perform histogram standardization of intensity values.\n",
    "\n",
    "Implementation of [New variants of a method of MRI scale standardization](https://ieeexplore.ieee.org/document/836373).\n",
    "\n",
    "We can visit in [torchio.transforms.HistogramStandardization.train()]((https://torchio.readthedocs.io/transforms/preprocessing.html#torchio.transforms.HistogramStandardization.train)) for more details.\n",
    "\n",
    "PARAMETERS\n",
    "landmarks â€“ Dictionary (or path to a PyTorch file with .pt or .pth extension in which a dictionary has been saved) whose keys are image names in the subject and values are NumPy arrays or paths to NumPy arrays defining the landmarks after training with [torchio.transforms.HistogramStandardization.train()](https://torchio.readthedocs.io/transforms/preprocessing.html#torchio.transforms.HistogramStandardization.train).\n",
    "\n",
    "Here, ***save_dir*** is a path where the trained histogram files for four channels (T1w, T1cw, T2w, and Flair), and trained model's weights will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd397f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_prefix = 'ConvEffNet_Brats21_1SplitV0'\n",
    "# savedirname = 'ConvEffNet_Brats21'\n",
    "# save_dir = os.path.join('/raid/brats2021/pthBraTS2021Radiogenomics', savedirname)\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "\n",
    "# train_images20T1 = dfTrainLbl['t1wPath'].values\n",
    "# train_images20T1ce = dfTrainLbl['t1cwPath'].values\n",
    "# train_images20T2 = dfTrainLbl['T2wPath'].values\n",
    "# train_images20Flair = dfTrainLbl['FlairPath'].values\n",
    "\n",
    "# hiseq_t1npyfile = os.path.join(save_dir, f\"histeq_t1w_{file_prefix}.npy\")\n",
    "# t1w_landmarks = (hiseq_t1npyfile if os.path.isfile(hiseq_t1npyfile) else \\\n",
    "#                  tio.HistogramStandardization.train(train_images20T1, output_path = hiseq_t1npyfile))\n",
    "# # #torch.save(t1w_landmarks, hiseq_t1npyfile)\n",
    "\n",
    "# hiseq_t1cnpyfile =  os.path.join(save_dir, f\"histeq_t1cw_{file_prefix}.npy\")\n",
    "# t1cw_landmarks = (hiseq_t1cnpyfile if os.path.isfile(hiseq_t1cnpyfile) else \\\n",
    "#                   tio.HistogramStandardization.train(train_images20T1ce, output_path = hiseq_t1cnpyfile))\n",
    "# #torch.save(t1cw_landmarks, hiseq_t1cnpyfile)\n",
    "\n",
    "\n",
    "# hiseq_t2npyfile = os.path.join(save_dir, f\"histeq_t2w_{file_prefix}.npy\")\n",
    "# t2w_landmarks = (hiseq_t2npyfile if os.path.isfile(hiseq_t2npyfile) else \\\n",
    "#                  tio.HistogramStandardization.train(train_images20T2, output_path = hiseq_t2npyfile))\n",
    "# #torch.save(t2w_landmarks, hiseq_t2npyfile)\n",
    "\n",
    "# hiseq_flairnpyfile = os.path.join(save_dir, f\"histeq_flair_{file_prefix}.npy\")\n",
    "# flair_landmarks = (hiseq_flairnpyfile if os.path.isfile(hiseq_flairnpyfile) else \\\n",
    "#                    tio.HistogramStandardization.train(train_images20Flair, output_path = hiseq_flairnpyfile))\n",
    "# #torch.save(flair_landmarks, hiseq_flairnpyfile)\n",
    "\n",
    "\n",
    "    \n",
    "file_prefix = 'DyUNetPlus_BratsTCGA_1p19q_3CV_2Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Run2'\n",
    "savedirname = 'DynUNetVariants_TCGA'\n",
    "save_dir = os.path.join('/raid/brats2021/pthTCGA_1p19q_CoDeletion', savedirname)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6456489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "169c9281",
   "metadata": {},
   "source": [
    "## Classes for Monai/Pytorch compose class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfb1cc",
   "metadata": {},
   "source": [
    "A class to rearrange label mask array as \n",
    "- [0, :, :, :] = the multi class mask (class labels: 0 (background), 1, 2, and 4)\n",
    "- [1, :, :, :] = the whole tumor mask (class labels: 0 (background), and 1)\\\n",
    "Not using here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c46abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelPlusWT(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            \n",
    "            d[key]=np.squeeze(d[key], axis = 0) # Converting 1, H, W, D to H, W, D\n",
    "            result.append(d[key])\n",
    "\n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            result.append(\n",
    "                np.logical_or(\n",
    "                    np.logical_or(d[key] == 2, d[key] == 4), d[key] == 1\n",
    "                )\n",
    "            )\n",
    "            ## merge label 1 and label 4 to construct TC\n",
    "            #result.append(np.logical_or(d[key] == 1, d[key] == 4))\n",
    "            ## label 4 is ET\n",
    "            #result.append(d[key] == 4)\n",
    "            d[key] = np.stack(result, axis=0).astype(np.uint8)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2610e7",
   "metadata": {},
   "source": [
    "#### Define a new transform to convert brain tumor labels\n",
    "Here we convert the multi-classes labels into multi-labels segmentation task in One-Hot format.\\\n",
    "Not using here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7629b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            \n",
    "            d[key]=np.squeeze(d[key], axis = 0) # Converting 1, H, W, D to H, W, D\n",
    "\n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            result.append(\n",
    "                np.logical_or(\n",
    "                    np.logical_or(d[key] == 2, d[key] == 4), d[key] == 1\n",
    "                )\n",
    "            )\n",
    "            # merge label 1 and label 4 to construct TC\n",
    "            result.append(np.logical_or(d[key] == 1, d[key] == 4))\n",
    "            # label 4 is ET\n",
    "            result.append(d[key] == 4)\n",
    "            d[key] = np.stack(result, axis=0).astype(np.float32)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0492ce",
   "metadata": {},
   "source": [
    "#### A class to add new key having the tumor mask (GT) to the existing data dictionary\n",
    "The new key, ***label_mask*** will have the same dimension (size: 4,x,x,x) with image array (size: 4,x,x,x)\\\n",
    "Using in ***compose*** class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df6a4f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToIDHLabel2WTd(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, IDH_label_key:str = 'IDH_label') -> None:\n",
    "\n",
    "        super().__init__(keys)\n",
    "        self.IDH_label_key = IDH_label_key\n",
    "       \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            #WT = np.logical_or(np.logical_or(d[key] == 2, d[key] == 4), d[key] == 1).astype(np.uint8)\n",
    "            result = []\n",
    "            WT = np.squeeze(d[key], axis = 0)\n",
    "            if d[self.IDH_label_key].item() == 1:\n",
    "                WT=np.multiply(WT, 2)\n",
    "                #WT = 2*WT\n",
    "            \n",
    "            result.append(WT==1)\n",
    "            result.append(WT==2)\n",
    "            \n",
    "            d[key] = np.stack(result, axis = 0).astype(np.float32)\n",
    "        \n",
    "        monai.data.utils.remove_keys(d, self.IDH_label_key)\n",
    "            \n",
    "    \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b739e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convert2WTd(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            output_classes = 2\n",
    "            \n",
    "            # merge labels 1, 2 and 4 to construct WT\n",
    "            WT = np.logical_or(np.logical_or(d[key] == 2, d[key] == 4), d[key] == 1).astype(np.float32)\n",
    "            d[f'{key}'] = WT\n",
    "         \n",
    "            WT = np.expand_dims(ndimage.binary_dilation(np.squeeze(WT, axis=0), iterations=2), axis = 0).astype(np.float32)\n",
    "            #WT = np.stack(tuple([ndimage.binary_dilation((np.squeeze(WT, axis = 0)==_k).astype(WT.dtype), iterations=5).astype(WT.dtype) for _k in range(output_classes)]), axis = 0)\n",
    "            d[f'{key}_mask'] = WT\n",
    "            d[f'{key}_mask_meta_dict'] = copy.deepcopy(d[f\"{key}_meta_dict\"])\n",
    "            \n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12365c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialCropWTCOMd(MapTransform):\n",
    "    \n",
    "    \"\"\"\n",
    "     GD-enhancing tumor (ET â€” label 4), \n",
    "     the peritumoral edema (ED â€” label 2), and \n",
    "     the necrotic and non-enhancing tumor core (NCR/NET â€” label 1)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, keys: KeysCollection, roi_size, COM_label_key:str = 'label_mask') -> None:\n",
    "\n",
    "        super().__init__(keys)\n",
    "        self.COM_label_key = COM_label_key\n",
    "        self.roi_size = roi_size\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            Coms = np.array([ndimage.measurements.center_of_mass(lbl) for lbl in list(d[self.COM_label_key])])\n",
    "            Coms[np.isnan(Coms)] = 70\n",
    "            Coms=Coms[0].astype(np.uint16).tolist()\n",
    "        \n",
    "            sc_com= SpatialCrop(roi_center= Coms, roi_size=self.roi_size)\n",
    "            d[key] = sc_com(d[key])\n",
    "                \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5563b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatLabelBrainmaskd(MapTransform):\n",
    "    \"\"\"\n",
    "          we do not need labels as it is a generative problem\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, image_key = 'image', label_key = 'label', \n",
    "                 brain_mask_key = 'brain_mask') -> None:\n",
    "        \n",
    "        super().__init__(keys)\n",
    "        self.brain_mask_key = brain_mask_key\n",
    "        self.image_key = image_key\n",
    "        self.label_key = label_key\n",
    "    \n",
    "    \n",
    "    def __call__(self, data):\n",
    "     \n",
    "        d = dict(data)\n",
    "        #d[self.image_key] = np.concatenate((d[self.image_key], d[self.label_key], d[self.brain_mask_key][0:1]), axis = 0)\n",
    "        d[self.image_key] = np.concatenate((d[self.image_key], d[self.label_key][0:1]), axis = 0)\n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47d730",
   "metadata": {},
   "source": [
    "### Implementing channelwise histogram normalization\n",
    "(Not using here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56907fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistogramNormalizeChannelWised(MapTransform):\n",
    "    \"\"\"\n",
    "          we do not need labels as it is a generative problem\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, keys: KeysCollection, brain_mask_key = 'brain_mask', min=0, max=255) -> None:\n",
    "        \n",
    "        super().__init__(keys)\n",
    "        self.brain_mask_key = brain_mask_key\n",
    "        self.histnorms = HistogramNormalize(num_bins=256, min=min, max=max)\n",
    "    \n",
    "    \n",
    "    def __call__(self, data):\n",
    "     \n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            nchnl = d[key].shape[0]\n",
    "            for ch in range(nchnl):\n",
    "                d[key][ch] = self.histnorms(d[key][ch], d[self.brain_mask_key][ch])\n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014dc88",
   "metadata": {},
   "source": [
    "### Defining traning and validation transforms\n",
    "\n",
    "- Training transform includes:\n",
    "    - LoadImaged\n",
    "    - EnsureChannelFirstd\n",
    "    - HistogramNormalizeChannelWised: Histogram normalization channel wise (custom class defined aboove)\n",
    "    - NormalizeIntensityd\n",
    "    - RandRotate90d\n",
    "    - RandZoomd\n",
    "    - ConvertToIDHLabel2WTd (custom class defined above)\n",
    "    - CropForegroundd: Cropping foreground based on the whole tumor mask (WT GT)\n",
    "    - RandCropByPosNegLabeld: Randomly cropping 8 patches based on 3: 1 (WT : non tumor tissus) ratio\n",
    "    - RandGaussianNoised\n",
    "    - RandStdShiftIntensityd\n",
    "    - RandFlipd\n",
    "    \n",
    "- validation transform includes:\n",
    "    - LoadImaged\n",
    "    - EnsureChannelFirstd\n",
    "    - HistogramNormalizeChannelWised: Histogram normalization channel wise (custom class defined aboove)\n",
    "    - NormalizeIntensityd\n",
    "    - ConvertToIDHLabel2WTd (custom class defined above)\n",
    "    \n",
    "Most of transfroms are implemented using [Monai](https://docs.monai.io/en/latest/transforms.html#dictionary-transforms) library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e097589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c10dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_foreground(x):\n",
    "    # threshold at not equal to 0\n",
    "    return x == 1\n",
    "\n",
    "\n",
    "#Resized(keys=keys[0:-1], spatial_size=patch_size, mode = ('area','nearest','nearest')),\n",
    "\n",
    "# ConvertToMultiChannelBasedOnBratsClassesd(keys = ['label']),\n",
    "# ConcatLabelBrainmaskd(keys = None, image_key = 'image', label_key = 'label', brain_mask_key = 'brain_mask'),\n",
    "# CropForegroundd(keys=keys[0:-1], source_key=\"brain_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord'),\n",
    "# SpatialPadd(keys=keys[0:-1], spatial_size=patch_size),\n",
    "\n",
    "# RandGaussianSmoothd(\n",
    "#     keys=[\"image\"],\n",
    "#     sigma_x=(0.5, 1.15),\n",
    "#     sigma_y=(0.5, 1.15),\n",
    "#     sigma_z=(0.5, 1.15),\n",
    "#     prob=0.3,\n",
    "# ),\n",
    "\n",
    "# RandScaleIntensityd(keys=[\"image\"], factors=0.3, prob=0.3),\n",
    "# RandGibbsNoised(keys=[\"image\"], prob=0.3, alpha=(0.1, 0.5), as_tensor_output=False),\n",
    "\n",
    "          \n",
    "# DataStatsd(keys=keys[0:-1], prefix=\"Data\", data_type=True, data_shape=True, value_range=True, data_value=False),\n",
    "#DataStatsd(keys=keysExt[0:-1], prefix=\"Data\", data_type=True, data_shape=True, value_range=True, data_value=False),\n",
    "\n",
    "\n",
    "def get_task_transforms(patch_size, task='train', pos_sample_num=1, neg_sample_num=1, num_samples=1):\n",
    "    \n",
    "    #spatial_size=(30, 30, 30)\n",
    "    orig_img_size = (240, 240, 155)\n",
    "\n",
    "    if task=='train':\n",
    "        keys = [\"image\", 'label', 'brain_mask', 'IDH_label']\n",
    "        keysExt = [\"image\", 'label', 'brain_mask', 'label_mask', 'IDH_label']\n",
    "        \n",
    "        all_transform = [\n",
    "            \n",
    "            LoadImaged(keys=keys[0:-1], reader = \"NibabelReader\"),\n",
    "            EnsureChannelFirstd(keys=keys[0:-1]),\n",
    "            #adapter_tioChannelWise2monai(tiofn = tio.HistogramStandardization, mode = 'train',landmarks = landmarks_dict),\n",
    "            #HistogramNormalizeChannelWised(keys = ['image'], brain_mask_key = 'brain_mask', min = 1, max = 65535),\n",
    "            \n",
    "            RandAffined(\n",
    "                keys = keys[0:-1],\n",
    "                prob=0.2,\n",
    "                spatial_size= orig_img_size, #(240, 240, 155),\n",
    "                rotate_range=np.pi/9,\n",
    "                scale_range=(0.1, 0.1),\n",
    "                mode=(\"bilinear\", \"nearest\", \"nearest\"),\n",
    "                as_tensor_output=False,\n",
    "                padding_mode = (\"zeros\", \"zeros\", \"zeros\"),\n",
    "            ),\n",
    "            \n",
    "            RandRotate90d(keys=keys[0:-1], prob=0.3, spatial_axes=[0, 2]),\n",
    "\n",
    "            RandZoomd(\n",
    "                keys=keys[0:-1],\n",
    "                min_zoom=0.9,\n",
    "                max_zoom=1.1,\n",
    "                mode=(\"trilinear\", \"nearest\", \"nearest\"),\n",
    "                align_corners=(True, None, None),\n",
    "                prob=0.3,\n",
    "            ),\n",
    "            \n",
    "            #ConvertToIDHLabel2WTd(keys = [\"label\"]),\n",
    "            CopyItemsd(keys=[\"label\"], names=[\"label_mask\"], times=1),\n",
    "            Convert2WTd(keys = [\"label\"]),\n",
    "            ConvertToIDHLabel2WTd(keys = [\"label\"], IDH_label_key = 'IDH_label'),\n",
    "            CropForegroundd(keys=keysExt[0:-1], source_key=\"brain_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord'),\n",
    "            #Spacingd(keys = keysExt[0:-1], pixdim=(1.25, 1.25, 1.25), mode = ('bilinear','nearest', 'nearest', 'nearest')),\n",
    "            NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
    "            \n",
    "            #ResizeWithPadOrCropd(keys = keysExt[0:-1], spatial_size = (128, 160, 128)),\n",
    "            RandGaussianNoised(keys=[\"image\"], std=0.01, prob=0.3),\n",
    "            RandStdShiftIntensityd(keys = [\"image\"], factors=0.3, nonzero=True, channel_wise=True, prob=0.3), \n",
    "            RandFlipd(keys=keysExt[0:-1], prob=0.5, spatial_axis=0),\n",
    "            RandFlipd(keys=keysExt[0:-1], prob=0.5, spatial_axis=1),\n",
    "            RandFlipd(keys=keysExt[0:-1], prob=0.5, spatial_axis=2),\n",
    "            \n",
    "            CropForegroundd(keys=keysExt[0:-1], source_key=\"label_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord', margin=2),\n",
    "            #ResizeWithPadOrCropd(keys = keysExt[0:-1], spatial_size = patch_size),\n",
    "            SpatialPadd(keys = keysExt[0:-1], spatial_size = patch_size),\n",
    "#             RandCropByLabelClassesd(\n",
    "#                 keys=keysExt[0:-1],            \n",
    "#                 label_key = \"label_mask\",\n",
    "#                 spatial_size = patch_size,    \n",
    "#                 ratios= [1, 3],\n",
    "#                 #ratios=[1,] * 2,\n",
    "#                 num_classes=2,              \n",
    "#                 num_samples=4,\n",
    "#                 image_key=\"brain_mask\",\n",
    "#                 image_threshold=0.0,\n",
    "#                 #allow_smaller = True,\n",
    "#             ),\n",
    "            \n",
    "            RandCropByPosNegLabeld(\n",
    "                keys=keysExt[0:-1],\n",
    "                label_key=\"label_mask\",\n",
    "                spatial_size=patch_size,\n",
    "                pos=pos_sample_num,\n",
    "                neg=neg_sample_num,\n",
    "                num_samples=num_samples,\n",
    "                #image_key=None,\n",
    "                image_key=\"brain_mask\",\n",
    "                image_threshold=0.,\n",
    "            ),\n",
    "            \n",
    "            #SpatialCropWTCOMd(keys=keysExt[0:-1], roi_size=patch_size, COM_label_key = \"label_mask\"),\n",
    "            SpatialPadd(keys = keysExt[0:-1], spatial_size = patch_size),\n",
    "        \n",
    "            #CastToTyped(keys=keysExt, dtype=(np.float32, np.uint8, np.uint8, np.uint8, np.float32)),\n",
    "            CastToTyped(keys=keysExt[0:-1], dtype=(np.float32, np.float32, np.float32, np.float32)), #np.float32\n",
    "            #ToTensord(keys=keysExt, dtype = (torch.float32, torch.float32, torch.float32, torch.float32, torch.float32)),\n",
    "            #EnsureTyped(keys=keysExt[0:-1], data_type = \"tensor\", wrap_sequence=True),\n",
    "#             #ToDeviced(keys = keys, device = deviceName),\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif task=='validation':\n",
    "    \n",
    "        keys = [\"image\", 'label', 'brain_mask', 'IDH_label']\n",
    "        keysExt = [\"image\", 'label', 'brain_mask', 'label_mask', 'IDH_label']\n",
    "        all_transform = [\n",
    "            \n",
    "            LoadImaged(keys=keys[0:-1], reader = \"NibabelReader\"),\n",
    "            EnsureChannelFirstd(keys=keys[0:-1]),\n",
    "            #adapter_tioChannelWise2monai(tiofn = tio.HistogramStandardization, mode = 'train',landmarks = landmarks_dict),\n",
    "            #HistogramNormalizeChannelWised(keys = ['image'], brain_mask_key = 'brain_mask', min = 1, max = 65535),\n",
    "          \n",
    "            CopyItemsd(keys=[\"label\"], names=[\"label_mask\"], times=1),\n",
    "            Convert2WTd(keys = [\"label\"]),\n",
    "            ConvertToIDHLabel2WTd(keys = [\"label\"], IDH_label_key = 'IDH_label'),\n",
    "            CropForegroundd(keys=keysExt[0:-1], source_key=\"brain_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord'),\n",
    "            #Spacingd(keys = keysExt[0:-1], pixdim=(1.25, 1.25, 1.25), mode = ('bilinear','nearest', 'nearest', 'nearest')),\n",
    "            NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
    "            #ResizeWithPadOrCropd(keys = keysExt[0:-1], spatial_size = (128, 160, 128)),\n",
    "            CropForegroundd(keys=keysExt[0:-1], source_key=\"label_mask\", select_fn = threshold_foreground, start_coord_key='fg_start_coord', end_coord_key='fg_end_coord', margin=2),\n",
    "            #SpatialCropWTCOMd(keys=keysExt[0:-1], roi_size=patch_size, COM_label_key = \"label_mask\"),\n",
    "            SpatialPadd(keys = keysExt[0:-1], spatial_size = patch_size),\n",
    "#             RandCropByPosNegLabeld(\n",
    "#                 keys=keysExt[0:-1],\n",
    "#                 label_key=\"label_mask\",\n",
    "#                 spatial_size=patch_size,\n",
    "#                 pos=pos_sample_num,\n",
    "#                 neg=neg_sample_num,\n",
    "#                 num_samples=num_samples,\n",
    "#                 image_key=\"brain_mask\",\n",
    "#                 image_threshold=0,\n",
    "#             ),\n",
    "\n",
    "            #CastToTyped(keys=keysExt, dtype=(np.float32, np.uint8, np.uint8, np.uint8, np.float32)),\n",
    "            CastToTyped(keys=keysExt[0:-1], dtype=(np.float32, np.float32, np.float32, np.float32)), # np.float32\n",
    "            #ToTensord(keys=keysExt, dtype = (torch.float32, torch.float32, torch.float32, torch.float32, torch.float32)),\n",
    "            #ToTensord(keys=keysExt),\n",
    "            #EnsureTyped(keys=keysExt[0:-1], data_type = \"tensor\", wrap_sequence=True),\n",
    "            #ToDeviced(keys = keys, device = deviceName),\n",
    "        ]\n",
    "        \n",
    "    else:\n",
    "        print('print task either train or validation here')\n",
    "\n",
    "\n",
    "    return Compose(all_transform)\n",
    "\n",
    "# def create_cachedir(cache_dir):\n",
    "#     if not os.path.exists(cache_dir):\n",
    "#         os.makedirs(cache_dir)\n",
    "#     return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7bb91f",
   "metadata": {},
   "source": [
    "### Section for visual inspection and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8d8332d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#patch_size=(128, 160, 128)\n",
    "#patch_size=(64, 80, 64)\n",
    "patch_size=(32, 32, 32)\n",
    "train_transforms = get_task_transforms(patch_size, task='train', pos_sample_num=3, neg_sample_num=1, num_samples=16)\n",
    "val_transforms = get_task_transforms(patch_size, task='validation', pos_sample_num=1, neg_sample_num=1, num_samples=1)\n",
    "len(train_transforms), len(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4259a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f21b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cf6ce56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_dataset = monai.data.Dataset(data=BraTS20SubjectsIDHTrainDCT[\"fold0\"], transform=train_transforms)\n",
    "len(all_train_dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ffbf5e0",
   "metadata": {},
   "source": [
    "#all_train_dataset = Dataset(data=BraTS20SubjectsIDHTrainDCT[\"fold0\"], transform=val_transforms)\n",
    "all_train_dataset = monai.data.Dataset(data=BraTS20SubjectsIDHTrainDCT[\"fold0\"][40:], transform=train_transforms)\n",
    "print(all_train_dataset[0][0]['image'].shape)\n",
    "for cnt, kj in enumerate(all_train_dataset):\n",
    "    #asub = kj\n",
    "    num_samples=4\n",
    "    pindx = random.choice(np.arange(num_samples, dtype = np.uint8))\n",
    "    kj = kj[pindx]\n",
    "    print('Counts: ', cnt, ' image shape: ', kj['image'].shape)\n",
    "    print('p1q19_label:', kj['IDH_label'])\n",
    "    print(kj['label'].shape, np.unique(kj['label'], return_counts = True))\n",
    "    print(kj['label_mask'].shape, np.unique(kj['label_mask'], return_counts = True))\n",
    "    print(kj['brain_mask'].shape, np.unique(kj['brain_mask'], return_counts = True))\n",
    "    #unque = kj['label_mask'].unique(return_counts = True)[1]\n",
    "    #print('\\n Background ratio:', unque[0]/unque.sum(), ' Tumor(=1): ',  unque[1]/unque.sum())\n",
    "    print('#'*100)\n",
    "#print(asub['image'].shape)\n",
    "# print(asub['image'][0].min(),asub['image'][0].max(), asub['image'][0].mean())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f893ed5",
   "metadata": {},
   "source": [
    "all_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a09a02dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#investifiles = copy.deepcopy(BraTS20SubjectsIDHTrainDCT[\"fold0\"])\n",
    "\n",
    "# for i in range(len(investifiles)):\n",
    "#     investifiles[i]['IDH_label'] = investifiles[i]['IDH_label'].astype(np.float32) \n",
    "# all_train_dataset = monai.data.Dataset(data=investifiles, transform=train_transforms)\n",
    "#all_train_dataset[10][3]['IDH_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9222201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cfold in tqdm(range(len(BraTS20SubjectsIDHTrainDCT))):\n",
    "#     all_train_dataset = monai.data.Dataset(data=copy.deepcopy(BraTS20SubjectsIDHTrainDCT[f\"fold{cfold}\"]), transform=train_transforms)\n",
    "#     dls = monai.data.DataLoader(all_train_dataset, batch_size=8, shuffle=False, collate_fn=list_data_collate)\n",
    "#     # abatch = next(iter(dls))\n",
    "#     # print(abatch['image'].shape)\n",
    "#     # print(abatch['label'].shape)\n",
    "#     for epoch in range(5):\n",
    "#         for abatch in dls:\n",
    "#             print(abatch['image'].shape)\n",
    "#             print(abatch['label'].shape)\n",
    "#             print(abatch['IDH_label'].shape)\n",
    "#             print(abatch['IDH_label'], '\\n', '###'*10, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94749be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12089293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#few_train_dataset = Dataset(data=train_files[0:10], transform=train_transforms)\n",
    "# asub = few_train_dataset[5] \n",
    "# view(image = asub['image'][3].cpu(), label_image = asub['label_mask'][0].cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982256b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dc7b51f",
   "metadata": {},
   "source": [
    "### Few investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c855baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# afold_train_dataset = monai.data.Dataset(data=train_folds['fold0'], transform=train_transforms)\n",
    "# #train_folds['fold0_IDH_label']\n",
    "# uval, ucnt = np.unique(train_folds['fold0_IDH_label'], return_counts=True)\n",
    "# weight = 1./(ucnt/ucnt.min())\n",
    "# #weight = np.array([0.55, 0.45])\n",
    "# sample_weights = np.array([weight[int(t)] for t in train_folds['fold0_IDH_label']])\n",
    "# sample_weights = torch.from_numpy(sample_weights)\n",
    "# weight, ucnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa313b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f0465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb84b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14253e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5955d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#afold_train_dataset[200]['IDH_label'], afold_train_dataset[200]['label'].unique(return_counts = True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "251aefbb",
   "metadata": {},
   "source": [
    "#sampler = WeightedRandomSampler(sample_weights, num_samples= len(samples_weight))\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples= len(sample_weights), replacement=True)\n",
    "\n",
    "afold_train_loader = monai.data.DataLoader(afold_train_dataset, batch_size=32, sampler=sampler)\n",
    "#afold_train_loader = torch.utils.data.DataLoader(afold_train_dataset, batch_size=32, sampler=sampler)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c5b56d3",
   "metadata": {},
   "source": [
    "for ibatch in afold_train_loader:\n",
    "    ibatch_IDH = ibatch['IDH_label']\n",
    "    print(torch.eq(ibatch_IDH, 0).sum(), torch.eq(ibatch_IDH, 1).sum())\n",
    "    print('#'*50)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0e88f8d",
   "metadata": {},
   "source": [
    "numDataPoints = 1000\n",
    "data_dim = 5\n",
    "bs = 100\n",
    "\n",
    "# Create dummy data with class imbalance 9 to 1\n",
    "data = torch.FloatTensor(numDataPoints, data_dim)\n",
    "target = np.hstack((np.zeros(int(numDataPoints * 0.9), dtype=np.int32),\n",
    "                    np.ones(int(numDataPoints * 0.1), dtype=np.int32)))\n",
    "\n",
    "print('target train 0/1: {}/{}'.format(len(np.where(target == 0)[0]), len(np.where(target == 1)[0])))\n",
    "\n",
    "class_sample_count = np.array(\n",
    "    [len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in target])\n",
    "samples_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551a33d",
   "metadata": {},
   "source": [
    "## Custom editing of SegResNetVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9983ead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]] \n",
      " [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]]\n",
      "strides length 5\n",
      "Filters: [32, 64, 128, 256, 320]\n"
     ]
    }
   ],
   "source": [
    "def get_kernels_strides(sizes, spacings):\n",
    "    #sizes, spacings = patch_size[task_id], spacing[task_id]\n",
    "    strides, kernels = [], []\n",
    "\n",
    "    while True:\n",
    "        spacing_ratio = [sp / min(spacings) for sp in spacings]\n",
    "        stride = [\n",
    "            2 if ratio <= 2 and size >= 8 else 1\n",
    "            for (ratio, size) in zip(spacing_ratio, sizes)\n",
    "        ]\n",
    "        kernel = [3 if ratio <= 2 else 1 for ratio in spacing_ratio]\n",
    "        if all(s == 1 for s in stride):\n",
    "            break\n",
    "        sizes = [i / j for i, j in zip(sizes, stride)]\n",
    "        spacings = [i * j for i, j in zip(spacings, stride)]\n",
    "        kernels.append(kernel)\n",
    "        strides.append(stride)\n",
    "    strides.insert(0, len(spacings) * [1])\n",
    "    kernels.append(len(spacings) * [3])\n",
    "    return kernels, strides\n",
    "#task_id = \"01\"\n",
    "kernels, strides = get_kernels_strides(patch_size, spacing)\n",
    "kernels.append([3, 3, 3])\n",
    "strides.append([2, 2, 2])\n",
    "\n",
    "print(kernels,'\\n', strides)\n",
    "\n",
    "print('strides length', len(strides))\n",
    "#filters = [64, 96, 128, 192, 256, 384, 512, 768, 1024][: len(strides)]\n",
    "#filters = [16, 32, 64, 128, 160, 160][: len(strides)]\n",
    "#filters = [64, 128, 256, 384, 512, 768, 1024][: len(strides)]\n",
    "\n",
    "filters = [32, 64, 128, 256, 320, 384, 512][: len(strides)]\n",
    "print(\"Filters:\", filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "742344c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernels, strides = get_kernels_strides((128, 128, 128), spacing)\n",
    "# #kernels, strides\n",
    "# kernels = [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 6, 3]]\n",
    "# strides = [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43898b12",
   "metadata": {},
   "source": [
    "num_classes = 2\n",
    "\n",
    "model = DynUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=2,\n",
    "    out_channels=num_classes,\n",
    "    kernel_size=kernels,\n",
    "    strides=strides,\n",
    "    upsample_kernel_size=strides[1:],\n",
    "    norm_name=\"batch\",\n",
    "    filters = filters,\n",
    "    #dropout=0.2,\n",
    "    deep_supervision=True,\n",
    "    res_block=True,\n",
    "    deep_supr_num=2,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf72bb09",
   "metadata": {},
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35f2318b",
   "metadata": {},
   "source": [
    "inps = torch.randn(3, 1, 32, 32, 32).to(device)\n",
    "x = model(inps)\n",
    "# # model\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebcefe70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "13f9aaca",
   "metadata": {},
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (4, 32, 32, 32))\n",
    "# # inps = torch.randn(3, 4, 48, 64, 48)\n",
    "# # litConv = nn.Conv3d(4, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "# # litConv(inps).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c0976",
   "metadata": {},
   "source": [
    "### Defining loss functions\n",
    "- ***CrossEntropyLogitLoss*** Cross entropy logit loss from [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)\n",
    "- ***DiceCELoss*** Dice + Cross entropy loss from Monai\n",
    "https://docs.monai.io/en/latest/losses.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ecb5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyInstWLogitLoss(nn.Module):\n",
    "    def __init__(self, is_smooth=False, label_smoothing = 0.1):\n",
    "        super().__init__()\n",
    "        self.is_smooth = is_smooth\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "       \n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        if self.is_smooth == True:\n",
    "            y_true = y_true.float() * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "\n",
    "        y_true=y_true.type_as(y_pred)   ### y_pred, and y_true should be same size and same data type\n",
    "        \n",
    "        \n",
    "        #deviceidx = y_pred.get_device()\n",
    "        #device = torch.device('cpu') if deviceidx == -1 else torch.device(f'cuda:{deviceidx}')\n",
    "        #loss = F.binary_cross_entropy_with_logits(y_pred.to(device), y_true.to(device), pos_weight = weight.to(device))  ##pos_weight = weight \n",
    "        loss = F.binary_cross_entropy_with_logits(y_pred, y_true) \n",
    "        return loss\n",
    "\n",
    "\n",
    "# class DeepDiceCELogitInstLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         #self.volweight = torch.softmax(torch.tensor([0.12, 0.33, 0.55]), dim = 0)\n",
    "#         self.dice = DiceLoss(include_background=False, to_onehot_y=True, softmax=True, squared_pred=True, batch = False)  # reduction = \"none\", batch = True\n",
    "#         #self.smcross_entropy = CrossEntropyInstLoss()  ### was none torch.Tensor([0.66, 0.33, 1]), torch.tensor(self.volweight)\n",
    "\n",
    "#     def forward(self, y_pred, y_true):\n",
    "        \n",
    "#         y_true = y_true.unsqueeze(dim=0).expand(y_pred.shape[1],-1,-1,-1,-1, -1)\n",
    "#         #return sum([0.5 ** i * ((self.dice(p, l)) + self.smcross_entropy(p, l)) \\\n",
    "#         #            for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "#         return sum([0.5 ** i * self.dice(p, l) for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeepDiceCELogitInstLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.volweight = torch.softmax(torch.tensor([0.12, 0.33, 0.55]), dim = 0)\n",
    "        self.dice = DiceLoss(to_onehot_y=False, sigmoid=True, squared_pred=True, batch = True, smooth_nr=0, smooth_dr=1e-5)  # reduction = \"none\", False\n",
    "        self.logitcross_entropy = CrossEntropyInstWLogitLoss()  ### was none torch.Tensor([0.66, 0.33, 1]), torch.tensor(self.volweight)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_true = y_true.unsqueeze(dim=0).expand(y_pred.shape[1],-1,-1,-1,-1, -1)\n",
    "        return sum([0.5 ** i * ((self.dice(p, l)) + self.logitcross_entropy(p, l)) \\\n",
    "                    for i, (p, l) in enumerate(zip(torch.unbind(y_pred, dim=1), torch.unbind(y_true, dim=0)))])\n",
    "    \n",
    "loss_function = DeepDiceCELogitInstLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c58ce42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6450)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxp = torch.randint(0,4,size=(6,3, 128, 128, 128))\n",
    "#pred = [torch.randn(6,3,8,8,6), torch.randn(6,3,8,8,6), torch.randn(6,3,8,8,6)]\n",
    "pred = torch.stack([torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128), torch.randn(6, 3, 128, 128, 128)], dim=1)\n",
    "loss_function(pred, xxp.float())\n",
    "#loss_function(pred, xxp.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244b8264",
   "metadata": {},
   "source": [
    "#### A function to create ***cache_dir*** to save transformed outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6c66433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAndcreate_cachedir(cache_dir):\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    else:\n",
    "        #print(\"Pass\")\n",
    "        shutil.rmtree(cache_dir)\n",
    "        os.makedirs(cache_dir)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b1132",
   "metadata": {},
   "source": [
    "### Pytorch training loop\n",
    "\n",
    "Following functionalities are added\n",
    "\n",
    "- Implemeting learning rate finder\n",
    "- Defining Ranger21 optimizer (learning rate scheduler attached to it)\n",
    "- Implementing mixed precision (AMP)\n",
    "- Saving the model weights based on the performance on validation data\n",
    "- Executing 5 fold cross validation (CV) training/validation pipeline, saving a few best model's weights at each fold\n",
    "- Defining train_dataset/train_loader, and val_dataset/val_loader at each fold\n",
    "- Defining a CNN based classification model (DenseNet, EfficientNet, etc.) at each fold to make sure that all accumulated gradients get vanished\n",
    "\n",
    "The key variables which are used here\n",
    "- ***val_cache_dir:*** The path where the transformed ouputs of validaion files will be cached/saved\n",
    "- ***train_cache_dir:*** The path where the transformed ouputs of training files will be cached/saved\n",
    "- ***max_epochs:*** Total number of epochs\n",
    "- ***save_dir:*** The path to save the checkpoints/weights of the model\n",
    "- ***file_prefix:*** The text file where loss/accuracy is recoded like\n",
    "\n",
    "```\n",
    "current fold: 0 current epoch: 1, acc_metric: 0.4579 accuracy: 0.5085, f1score: 0.5085 epoch 1 average training loss: 0.7250 average validation loss: 0.7128 \n",
    "current fold: 0 current epoch: 2, acc_metric: 0.4876 accuracy: 0.5424, f1score: 0.5424 epoch 2 average training loss: 0.6961 average validation loss: 0.6940 \n",
    "current fold: 0 current epoch: 3, acc_metric: 0.4870 accuracy: 0.4915, f1score: 0.4915 epoch 3 average training loss: 0.6862 average validation loss: 0.6965 \n",
    "current fold: 0 current epoch: 4, acc_metric: 0.4882 accuracy: 0.5593, f1score: 0.5593 epoch 4 average training loss: 0.6715 average validation loss: 0.6885 \n",
    "current fold: 0 current epoch: 5, acc_metric: 0.5927 accuracy: 0.5593, f1score: 0.5593 epoch 5 average training loss: 0.6555 average validation loss: 0.6826 \n",
    "```\n",
    "\n",
    "- ***val_interval*** Epoch interval to investivate the model's performance on validation data. If the current performance is better than in previous epochs, the model's weights will be saved\n",
    "- ***key_metric_n_saved*** The number of model checkpoints we want to save. It it is set as 5, top 5 checkpoints/weights will be saved in 5 different ***.pth*** files  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "998ae7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#***Executed pipeline***\\\n",
    "#<img src=\"assets/ProposedIDHClass.png\" align=\"left\" width=\"1024\" height=\"1800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58632e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_files, train_files_IDH_label, val_files, val_files_IDH_label, batch_size = 2, epochs = 10, find_lr=False, cfold = 0):\n",
    "    \n",
    "\n",
    "    #     model = DenseNet201(spatial_dims=2, in_channels=3,\n",
    "    #                        out_channels=num_classes, pretrained=True).to(device)\n",
    "\n",
    "    # create spatial 3D\n",
    "    #model = MultiDenseNet(spatial_dims=3, in_channelsList=(4, 1, 1, 1, 1), out_channels=2, block_config = (6, 12, 24, 16)).to(device)\n",
    "    #model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=4, out_channels=1).to(device)\n",
    "    #model = monai.networks.nets.DenseNet264(spatial_dims=3, in_channels=4, out_channels=1, init_features=64, growth_rate=32, block_config=(6, 12, 64, 48)).to(device)\n",
    "    #patch_size=(64, 80, 64)\n",
    "    num_classes = 2\n",
    "    \n",
    "    model = DynUNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=2,\n",
    "        out_channels=num_classes,\n",
    "        kernel_size=kernels,\n",
    "        strides=strides,\n",
    "        upsample_kernel_size=strides[1:],\n",
    "        norm_name=\"batch\",\n",
    "        filters = filters,\n",
    "        #dropout=0.2,\n",
    "        deep_supervision=True,\n",
    "        res_block=True,\n",
    "        deep_supr_num=2,\n",
    "    ).to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    auc_metric = ROCAUCMetric()\n",
    "    \n",
    "\n",
    "    #train_files, train_files_IDH_label, val_files, val_files_IDH_label = train_files[:48], train_files_IDH_label[:48], val_files[:16], val_files_IDH_label[:16]\n",
    "\n",
    "    \"\"\"\n",
    "    Block for using Monai's caching mechanishm for faster training\n",
    "    \"\"\"\n",
    "    \n",
    "    file_prefixfold = file_prefix  ##or file_prefix f\"{file_prefix}_fold{cfold}\" if saving cv file\n",
    "    data_rpath = '/home/mmiv-ml/data'\n",
    "    train_cache_dir = os.path.join(data_rpath,f'cachingDataset/{file_prefixfold}/train')    \n",
    "    val_cache_dir = os.path.join(data_rpath,f'cachingDataset/{file_prefixfold}/val')\n",
    "    \n",
    "    is_done_train = removeAndcreate_cachedir(train_cache_dir)\n",
    "    is_done_val = removeAndcreate_cachedir(val_cache_dir)\n",
    "    \n",
    "\n",
    "    n_train_cache_n_trans = len(train_transforms) #15\n",
    "    n_val_cache_n_trans = len(val_transforms)\n",
    "    \n",
    "     # create a training data loader\n",
    "\n",
    "    train_dataset = monai.data.CacheNTransDataset(data=train_files, transform=train_transforms,\\\n",
    "                                               cache_n_trans = n_train_cache_n_trans, cache_dir = train_cache_dir)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #train_dataset = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "    \n",
    "#     #train_folds['fold0_IDH_label']\n",
    "#     uval, ucnt = np.unique(train_files_IDH_label, return_counts=True)\n",
    "#     weight = 1./(ucnt/ucnt.min())\n",
    "#     #weight = 1./ucnt\n",
    "#     #weight = np.array([0.55, 0.45])\n",
    "#     sample_weights = np.array([weight[int(t)] for t in train_files_IDH_label])\n",
    "#     sample_weights = torch.from_numpy(sample_weights)\n",
    "#     sampler = WeightedRandomSampler(sample_weights, num_samples= len(sample_weights), replacement=True)\n",
    "\n",
    "\n",
    "    #train_dataset = Dataset(data=train_files, transform=train_transforms)\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    #train_dataset = CacheDataset(data=train_files, transform=train_transforms, cache_rate = 1.0, num_workers=8)\n",
    "    #train_loader = ThreadDataLoader(train_dataset, num_workers=0, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #shiffle = False, sampler = sampler, shuffle=True doesnot work with patch, num_workers=2\n",
    "    train_loader = monai.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=list_data_collate) \n",
    "    \n",
    "\n",
    "    \n",
    "    # create a validation data loader\n",
    "    \n",
    "    #val_dataset = monai.data.Dataset(data=val_files, transform=val_transforms)\n",
    "    #val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    #val_dataset = CacheDataset(data=val_files, transform=val_transforms, cache_rate = 1.0, num_workers=5)\n",
    "    \n",
    "    val_dataset = monai.data.CacheNTransDataset(data=val_files, transform=val_transforms,\\\n",
    "                                           cache_n_trans = n_val_cache_n_trans, cache_dir = val_cache_dir)\n",
    "    #val_loader = ThreadDataLoader(val_dataset, num_workers=0, batch_size=1)\n",
    "    val_loader = monai.data.DataLoader(val_dataset, batch_size=1, shuffle=False) #num_workers=2, pin_memory=True\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for ibatch in train_loader:\n",
    "#         ibatch_IDH = ibatch['IDH_label']\n",
    "#         print(torch.eq(ibatch_IDH, 0).sum(), torch.eq(ibatch_IDH, 1).sum())\n",
    "#         print('#'*50)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    just initialising some basic steps\n",
    "    \"\"\"\n",
    "    \n",
    "    max_epochs = epochs\n",
    "    find_lr=False\n",
    "    \n",
    "    ### Calling the loss function ***CrossEntropyPlusMSELoss**,and optimizer   \n",
    "    #loss_function = nn.CrossEntropyLoss()\n",
    "    #loss_function=nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=1e-03, weight_decay=1e-07)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-05, weight_decay = 1e-4)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=1e-03, momentum= 0.99, nesterov=True)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), 1e-03, weight_decay=1e-04)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    \n",
    "    max_lr_init = 1e-04\n",
    "    \"\"\"\n",
    "     ###################### Block for LR finder from pytorch ignite ########################\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if find_lr:\n",
    "\n",
    "        def prepare_batch(batch, device=None, non_blocking=False):\n",
    "            return _prepare_batch((batch['image'], batch['label'].long()), device, non_blocking)\n",
    "\n",
    "        #trainer = create_supervised_trainer(model, optimizer, loss_function, device, non_blocking=False, prepare_batch=prepare_batch)\n",
    "        def train_step(engine, batch):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            x, y = batch['image'].to(device), batch['label'].to(device)  #non_blocking=True\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_pred = model(x)\n",
    "                loss4lr = loss_function(y_pred, y)\n",
    "                \n",
    "            scaler.scale(loss4lr).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            return loss4lr.item()\n",
    "\n",
    "        trainer = Engine(train_step)\n",
    "\n",
    "        ProgressBar(persist=True).attach(trainer, output_transform=lambda x: {\"batch loss\": x})\n",
    "        lr_finder = FastaiLRFinder()\n",
    "        to_save={'model': model, 'optimizer': optimizer}\n",
    "        num_iter = 100  #2*len(train_loader)\n",
    "        run_epochs = int(np.ceil(num_iter/len(train_loader)))\n",
    "        with lr_finder.attach(trainer, to_save, end_lr=1., num_iter=num_iter, diverge_th=1.5) as trainer_with_lr_finder:    ####diverge_th=1.5\n",
    "\n",
    "            trainer_with_lr_finder.run(train_loader, max_epochs=run_epochs)  #max_epochs=run_epochs or 5\n",
    "\n",
    "        ax = lr_finder.plot()\n",
    "        plt.show()\n",
    "        \n",
    "        max_lr = lr_finder.lr_suggestion() if lr_finder.lr_suggestion()<1e-04 else max_lr_init\n",
    "        #max_lr = lr_finder.lr_suggestion() ##max_lr/10 i guess not needed, ignite does itself\n",
    "        print(f'Suggested learning rate by LR finder for this fold: {lr_finder.lr_suggestion()}')\n",
    "        \n",
    "    else:\n",
    "        max_lr = max_lr_init\n",
    "        \n",
    "    #max_lr_slice = 1e01*max_lr if max_lr<5e-03 else 1e-02\n",
    "    #max_lr_slice = 1e-01*max_lr if max_lr<1e-05 else max_lr\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    ### defining learning rate scheduler\n",
    "    \n",
    "    \"\"\"\n",
    "    #steps_per_epoch=len(train_loader)\n",
    "    #optimizer.param_groups[0]['lr'] = max_lr #*1e-01\n",
    "    #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr_slice, steps_per_epoch=len(train_loader), epochs=max_epochs)\n",
    "    #scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: (1 - epoch / max_epochs) ** 0.9)\n",
    "    \n",
    "    #max_lr = 1e-3   \n",
    "    optimizer = Ranger21(model.parameters(), lr = max_lr, num_epochs = epochs, num_batches_per_epoch = len(train_loader))\n",
    "    \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "     ###################### Block for native pytorch training loop and  ########################\n",
    "    \"\"\"\n",
    "\n",
    "    key_metric_n_saved = 2   ### Usually I keep it 5\n",
    "    save_last = False \n",
    "    dispformat_specs = '.4f'\n",
    "\n",
    "        \n",
    "#     file_prefix = 'ConvEffNet_Brats21_5CV'\n",
    "#     savedirname = 'ConvEffNet_Brats21'\n",
    "#     save_dir = os.path.join('/raid/brats2021/pthBraTS2021Radiogenomics', savedirname)\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "\n",
    "    logsfile_path = f\"{save_dir}/Logs_{file_prefix}.txt\"\n",
    "\n",
    "\n",
    "    epoch_num = max_epochs #  max_epochs\n",
    "    val_interval = 1\n",
    "    valstep = 0\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = list()\n",
    "    metric_values = list()\n",
    "\n",
    "\n",
    "    numsiters = len(train_files) // train_loader.batch_size\n",
    "\n",
    "    #first_batch = monai.utils.misc.first(train_loader)\n",
    "        \n",
    "    \n",
    "    #post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)  ### num_classes=num_classes\n",
    "    #post_label = AsDiscrete(to_onehot=num_classes) ###num_classes=num_classes\n",
    "    #dice_metric = monai.metrics.DiceMetric(include_background=False, reduction='mean', get_not_nans=False)\n",
    "    \n",
    "    \n",
    "    dice_metric = monai.metrics.DiceMetric(include_background=True, reduction='mean', get_not_nans=False)\n",
    "    dice_metric_batch = monai.metrics.DiceMetric(include_background=True, reduction='mean_batch', get_not_nans=False)\n",
    "    \n",
    "    HD_metric = HausdorffDistanceMetric(include_background=True, percentile = 95., reduction='mean', get_not_nans=False)\n",
    "    HD_metric_batch = HausdorffDistanceMetric(include_background=True, percentile = 95., reduction='mean_batch', get_not_nans=False)\n",
    "    \n",
    "    post_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])  \n",
    "    \n",
    "    def one_hot_permute(x):\n",
    "        return F.one_hot(x.squeeze(dim=0).long(), num_classes=num_classes).permute(3, 0, 1, 2)\n",
    "    \n",
    "    def get_binarize_tensor(x, dim=1):\n",
    "        x_chlist = torch.unbind(x, dim = dim)\n",
    "        bin_x = torch.zeros_like(x_chlist[0])\n",
    "        for x_i in x_chlist:\n",
    "            bin_x = torch.logical_or(x_i, bin_x)\n",
    "        return bin_x.unsqueeze(dim=dim).to(torch.float32)\n",
    "    \n",
    "    def get_segclass(x, dim = 1):\n",
    "        xdvc = x.device\n",
    "        x_chlist = torch.unbind(x, dim = dim)\n",
    "        xclassNoList = []\n",
    "        xvalueList = []\n",
    "        for x_i in x_chlist:\n",
    "            \n",
    "            xv, xc = torch.unique(x_i, return_counts  = True)\n",
    "\n",
    "            if xc.shape[0]==1:\n",
    "                xclassNoList.append(xc[0].item())\n",
    "                xvalueList.append(xv[0].item())\n",
    "            elif xc.shape[0]==2:\n",
    "                    if torch.any(torch.eq(xv, 1)):\n",
    "                        xclassNoList.append(xc[1].item())\n",
    "                        xvalueList.append(xv[1].item())\n",
    "                    else:\n",
    "                        print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "            else:\n",
    "                print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "        \n",
    "        if torch.any(torch.eq(torch.tensor(xvalueList), 1)):  \n",
    "            xclass = torch.argmax(torch.tensor(xclassNoList).to(xdvc))\n",
    "        else:\n",
    "            '''If all uniques class values are 0, we are assigning nan values as a class'''\n",
    "            xclass = torch.tensor(float('NaN')).to(xdvc)\n",
    "            \n",
    "        return xclass \n",
    "            \n",
    "#             xv, xc = torch.unique(x_i, return_counts  = True)\n",
    "            \n",
    "#             if torch.any(torch.eq(xv, 1)):\n",
    "#                 xclassNoList.append(xc[1].item())\n",
    "#             else:\n",
    "#                 xclassNoList.append(0)        \n",
    "#         xclass = torch.argmax(torch.tensor(xclassNoList).to(xdvc))          \n",
    "       \n",
    "                \n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{epoch_num}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.\n",
    "        stepiter = 0\n",
    "        for batch_data in train_loader:\n",
    "            \n",
    "            stepiter += 1\n",
    "            inputs, labels, IDH_labels= (\n",
    "                batch_data['image'].to(device),\n",
    "                batch_data['label'].to(device),\n",
    "                batch_data['IDH_label'].to(device),\n",
    "            )\n",
    "            \n",
    "          \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "\n",
    "                # compute output\n",
    "                outputs  = model(inputs)\n",
    "                loss = loss_function(outputs, labels) \n",
    "\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"{stepiter}/{numsiters}, train_loss: {loss.item():.4f}\")\n",
    "\n",
    "            #scheduler.step() \n",
    "            \n",
    "        epoch_loss /= stepiter\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        \n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "                y = torch.tensor([], dtype=torch.long, device=device)\n",
    "                val_losses = torch.tensor([], dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "                for val_data in val_loader:\n",
    "\n",
    "                    val_inputs, val_labels, val_IDH_labels = (\n",
    "                        val_data['image'].to(device),\n",
    "                        val_data['label'].to(device),\n",
    "                        val_data['IDH_label'].to(device),\n",
    "                    )\n",
    "                \n",
    "                    roi_size = patch_size #(32, 32, 32)\n",
    "                    sw_batch_size = 8\n",
    "                    val_overlap = 0.5\n",
    "                    mode=\"gaussian\"\n",
    "                            \n",
    "                    \n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        \n",
    "                        val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device) \n",
    "                        #val_outputs = model(val_inputs)\n",
    "                        val_ce_loss = loss_function(val_outputs.unsqueeze(dim=1), val_labels)\n",
    "\n",
    "                    val_losses = torch.cat([val_losses, val_ce_loss.view(1)], dim = 0)\n",
    "                    val_outputs = torch.stack([post_pred(i) for i in torch.unbind(val_outputs, dim = 0)], dim = 0)\n",
    "                    \n",
    "                    \n",
    "                    #val_labels2hot = torch.stack([one_hot_permute(i) for i in torch.unbind(val_labels, dim = 0)], dim = 0)\n",
    "\n",
    "                    \n",
    "                    val_labels_bin = get_binarize_tensor(val_labels, dim=1)\n",
    "                    val_outputs_bin = get_binarize_tensor(val_outputs, dim=1)\n",
    "                    \n",
    "                    dice_metric(y_pred=val_outputs_bin, y=val_labels_bin)\n",
    "                                        \n",
    "                    klcc = KeepLargestConnectedComponent(applied_labels = [0, 1], is_onehot = True)  ##is_onehot=True or None by default\n",
    "                    #val_labels = klcc(val_labels.squeeze(dim=0)).unsqueeze(dim=0)\n",
    "                    val_labels = torch.stack([klcc(i) for i in torch.unbind(val_labels, dim = 0)], dim = 0)\n",
    "                    \n",
    "                    val_label4mSeg_C = get_segclass(val_outputs)\n",
    "                    #val_surv_labels = val_surv_labels.squeeze(dim=1)  ###Squeezing from B, 1 to B if needed\n",
    "                    y_pred = torch.cat([y_pred, val_label4mSeg_C.view(1)], dim=0)\n",
    "                    y = torch.cat([y, val_IDH_labels], dim=0)\n",
    "\n",
    "                mdice_value = dice_metric.aggregate()\n",
    "                dice_metric.reset()\n",
    "                \n",
    "                y_pred, y = y_pred.cpu(), y.cpu()\n",
    "                \n",
    "                if torch.all(torch.isnan(y_pred))==True:\n",
    "                    \n",
    "                    auc_result, accscore, f1score = np.nan, np.nan, np.nan\n",
    "                    #print('acc_metric#', np.nan, ', auc#', np.nan, ', f1#', np.nan, '\\n')\n",
    "                \n",
    "                else:\n",
    "\n",
    "                    num_nanvalues = torch.isnan(y_pred).sum().item()\n",
    "                    not_nanmask = torch.logical_not(torch.isnan(y_pred))\n",
    "                    y = y[not_nanmask]\n",
    "                    y_pred = y_pred[not_nanmask]\n",
    "                    \n",
    "                    \n",
    "                    acc_value = torch.eq(y_pred, y)\n",
    "                    acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "\n",
    "                    '''auc metric'''\n",
    "                    auc_metric(y_pred, y)\n",
    "                    auc_result = auc_metric.aggregate()\n",
    "                    auc_metric.reset()\n",
    "                    \n",
    "                    '''balanced accuracy and f1 score'''\n",
    "                    accscore = balanced_accuracy_score(y, y_pred)\n",
    "                    f1score = f1_score(y, y_pred, average='micro')\n",
    "                    #print('acc_metric#', acc_metric, ', auc#', auc_result, ', f1#', f1score, '\\n')\n",
    "\n",
    "                del y, y_pred\n",
    "            \n",
    "            \n",
    "                epoch_val_losses=torch.mean(val_losses).detach().cpu().item()\n",
    "                #metric = auc_result\n",
    "                mdice_value = mdice_value.item()\n",
    "                #metric = mdice_value\n",
    "                metric = (mdice_value+auc_result)/2\n",
    "                metric= -1.0 if np.isnan(metric) else metric\n",
    "                metric_values.append(metric) ######List of over number of epochs\n",
    "                printstring = \"Best AUC\"\n",
    "                \n",
    "\n",
    "                with open(logsfile_path, 'a') as file:\n",
    "                    file.write(\n",
    "                        f\"current fold: {cfold} current epoch: {epoch + 1} dice_score: {mdice_value:^{dispformat_specs}} acc_metric: {auc_result:^{dispformat_specs}}\" \n",
    "                        f\" accuracy: {accscore:^{dispformat_specs}}, f1score: {f1score:^{dispformat_specs}}\"\n",
    "                        f\" epoch {epoch + 1} average training loss: {epoch_loss:^{dispformat_specs}} average validation loss: {epoch_val_losses:^{dispformat_specs}} \\n\"\n",
    "\n",
    "                    )\n",
    "\n",
    "                if valstep < key_metric_n_saved:\n",
    "                    torch.save(model.state_dict(), os.path.join(save_dir, f\"{file_prefix}_Fold{cfold}_{metric:^{dispformat_specs}}_epoch{epoch + 1}.pth\"))\n",
    "                    print(\n",
    "                        f\"current fold: {cfold} current epoch: {epoch + 1} dice_score: {mdice_value:^{dispformat_specs}} acc_metric: {auc_result:^{dispformat_specs}}\" \n",
    "                        f\" accuracy: {accscore:^{dispformat_specs}}, f1score: {f1score:^{dispformat_specs}}\"\n",
    "                        f\" epoch {epoch + 1} average training loss: {epoch_loss:^{dispformat_specs}} average validation loss: {epoch_val_losses:^{dispformat_specs}}\"\n",
    "                        \n",
    "                    )\n",
    "\n",
    "                else:\n",
    "\n",
    "                    #sortmetric_values = sorted(metric_values[:-1], reverse=True)  ###Higher loss needs to be deleted, so sorting is reversed\n",
    "                    sortmetric_values = sorted(metric_values[:-1], reverse=False)  \n",
    "\n",
    "                    if metric>=sortmetric_values[-key_metric_n_saved]:\n",
    "                        savegood_metric = metric\n",
    "                        good_metric_epoch = epoch + 1\n",
    "\n",
    "                        #if os.path.exists(f\"{save_dir}/{file_prefix}_{sortmetric_values[-key_metric_n_saved]:.4f}.pth\"):\n",
    "                        #    os.remove(f\"{save_dir}/{file_prefix}_{sortmetric_values[-key_metric_n_saved]:.4f}.pth\")\n",
    "                        #else:\n",
    "                        #    print(\"The file does not exist\")\n",
    "\n",
    "                        glblist = glob.glob(f\"{save_dir}/{file_prefix}_Fold{cfold}_{sortmetric_values[-key_metric_n_saved]:^{dispformat_specs}}_*\")\n",
    "\n",
    "                        if not glblist:\n",
    "                            print(\"The file does not exist\")\n",
    "                        else:\n",
    "                            os.remove(glblist[0])\n",
    "\n",
    "\n",
    "                        torch.save(model.state_dict(), os.path.join(save_dir, f\"{file_prefix}_Fold{cfold}_{metric:^{dispformat_specs}}_epoch{epoch + 1}.pth\"))\n",
    "                        print(\"saved new best metric model\")\n",
    "                        print(\n",
    "                            f\"current fold: {cfold} current epoch: {epoch + 1} validation loss: {epoch_val_losses:^{dispformat_specs}}\"\n",
    "                            f\" dice_score: {mdice_value:^{dispformat_specs}} acc_metric: {auc_result:^{dispformat_specs}}\"\n",
    "                            f\" accuracy: {accscore:^{dispformat_specs}}, f1score: {f1score:^{dispformat_specs}}\"\n",
    "                            f\"\\n saved {printstring}: {savegood_metric:^{dispformat_specs}} at epoch: {good_metric_epoch}\"\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        f\"current fold: {cfold} current epoch: {epoch + 1} validation loss: {epoch_val_losses:^{dispformat_specs}}\"\n",
    "                        f\" dice_score: {mdice_value:^{dispformat_specs}} acc_metric: {auc_result:^{dispformat_specs}}\"\n",
    "                        f\" accuracy: {accscore:^{dispformat_specs}}, f1score: {f1score:^{dispformat_specs}}\"\n",
    "\n",
    "                        #pass\n",
    "\n",
    "                valstep += 1\n",
    "        ####Saving last epoch\n",
    "        if epoch==epoch_num-1:\n",
    "            if save_last:\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, f\"{file_prefix}_Fold{cfold}_{metric:^{dispformat_specs}}_last_epoch{epoch + 1}.pth\"))\n",
    "            #break\n",
    "            \n",
    "    # Free up GPU memory after training\n",
    "    model = None\n",
    "    train_loader, val_loader = None, None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42234d75",
   "metadata": {},
   "source": [
    "### Loop to execute n_splits=3 fold cross validation\n",
    "if the model is trained and the checkpoints are saved already, just setting the start_training flag as false, to run remaining part of the programs of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd422ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0f9b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 Bacth Investigation, minimum batch size 2\n",
      "Ranger21 optimizer ready with following settings:\n",
      "\n",
      "Core optimizer = AdamW\n",
      "Learning rate of 0.0001\n",
      "\n",
      "Important - num_epochs of training = ** 500 epochs **\n",
      "please confirm this is correct or warmup and warmdown will be off\n",
      "\n",
      "Warm-up: linear warmup, over 2000 iterations\n",
      "\n",
      "Lookahead active, merging every 5 steps, with blend factor of 0.5\n",
      "Norm Loss active, factor = 0.0001\n",
      "Stable weight decay of 0.0001\n",
      "Gradient Centralization = On\n",
      "\n",
      "Adaptive Gradient Clipping = True\n",
      "\tclipping value of 0.01\n",
      "\tsteps for clipping = 0.001\n",
      "\n",
      "Warm-down: Linear warmdown, starting at 72.0%, iteration 5760 of 8000\n",
      "warm down will decay until 3e-05 lr\n",
      "----------\n",
      "epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params size saved\n",
      "total param groups = 1\n",
      "total params in groups = 79\n",
      "1/15, train_loss: 2.7223\n",
      "2/15, train_loss: 2.6818\n",
      "3/15, train_loss: 2.3777\n",
      "4/15, train_loss: 2.4640\n",
      "5/15, train_loss: 2.6853\n",
      "6/15, train_loss: 2.6954\n",
      "7/15, train_loss: 2.7039\n",
      "8/15, train_loss: 2.2877\n",
      "9/15, train_loss: 2.3632\n",
      "10/15, train_loss: 2.3579\n",
      "11/15, train_loss: 2.3577\n",
      "12/15, train_loss: 2.3596\n",
      "13/15, train_loss: 2.3822\n",
      "14/15, train_loss: 2.4039\n",
      "15/15, train_loss: 2.3669\n",
      "16/15, train_loss: 2.2293\n",
      "epoch 1 average loss: 2.4649\n",
      "current fold: 0 current epoch: 1 dice_score: 0.2839 acc_metric: 0.5062 accuracy: 0.5062, f1score: 0.3525 epoch 1 average training loss: 2.4649 average validation loss: 1.5547\n",
      "----------\n",
      "epoch 2/500\n",
      "1/15, train_loss: 2.7204\n",
      "2/15, train_loss: 2.6797\n",
      "3/15, train_loss: 2.3756\n",
      "4/15, train_loss: 2.4604\n",
      "5/15, train_loss: 2.6831\n",
      "6/15, train_loss: 2.6924\n",
      "7/15, train_loss: 2.7005\n",
      "8/15, train_loss: 2.2844\n",
      "9/15, train_loss: 2.3606\n",
      "10/15, train_loss: 2.3549\n",
      "11/15, train_loss: 2.3548\n",
      "12/15, train_loss: 2.3545\n",
      "13/15, train_loss: 2.3771\n",
      "14/15, train_loss: 2.3977\n",
      "15/15, train_loss: 2.3635\n",
      "16/15, train_loss: 2.2272\n",
      "epoch 2 average loss: 2.4617\n",
      "current fold: 0 current epoch: 2 dice_score: 0.3122 acc_metric: 0.5062 accuracy: 0.5062, f1score: 0.3525 epoch 2 average training loss: 2.4617 average validation loss: 1.5781\n",
      "----------\n",
      "epoch 3/500\n",
      "1/15, train_loss: 2.7154\n",
      "2/15, train_loss: 2.6749\n",
      "3/15, train_loss: 2.3711\n",
      "4/15, train_loss: 2.4553\n",
      "5/15, train_loss: 2.6774\n",
      "6/15, train_loss: 2.6864\n",
      "7/15, train_loss: 2.6939\n",
      "8/15, train_loss: 2.2781\n",
      "9/15, train_loss: 2.3570\n",
      "10/15, train_loss: 2.3483\n",
      "11/15, train_loss: 2.3491\n",
      "12/15, train_loss: 2.3454\n",
      "13/15, train_loss: 2.3681\n",
      "14/15, train_loss: 2.3905\n",
      "15/15, train_loss: 2.3561\n",
      "16/15, train_loss: 2.2239\n",
      "epoch 3 average loss: 2.4557\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 3 validation loss: 1.5830 dice_score: 0.3177 acc_metric: 0.5062 accuracy: 0.5062, f1score: 0.3525\n",
      " saved Best AUC: 0.4120 at epoch: 3\n",
      "----------\n",
      "epoch 4/500\n",
      "1/15, train_loss: 2.7072\n",
      "2/15, train_loss: 2.6672\n",
      "3/15, train_loss: 2.3660\n",
      "4/15, train_loss: 2.4450\n",
      "5/15, train_loss: 2.6686\n",
      "6/15, train_loss: 2.6771\n",
      "7/15, train_loss: 2.6841\n",
      "8/15, train_loss: 2.2718\n",
      "9/15, train_loss: 2.3507\n",
      "10/15, train_loss: 2.3387\n",
      "11/15, train_loss: 2.3411\n",
      "12/15, train_loss: 2.3325\n",
      "13/15, train_loss: 2.3594\n",
      "14/15, train_loss: 2.3776\n",
      "15/15, train_loss: 2.3454\n",
      "16/15, train_loss: 2.2193\n",
      "epoch 4 average loss: 2.4470\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 4 validation loss: 1.5805 dice_score: 0.3206 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4103 at epoch: 4\n",
      "----------\n",
      "epoch 5/500\n",
      "1/15, train_loss: 2.6961\n",
      "2/15, train_loss: 2.6598\n",
      "3/15, train_loss: 2.3572\n",
      "4/15, train_loss: 2.4312\n",
      "5/15, train_loss: 2.6568\n",
      "6/15, train_loss: 2.6650\n",
      "7/15, train_loss: 2.6754\n",
      "8/15, train_loss: 2.2609\n",
      "9/15, train_loss: 2.3418\n",
      "10/15, train_loss: 2.3261\n",
      "11/15, train_loss: 2.3306\n",
      "12/15, train_loss: 2.3207\n",
      "13/15, train_loss: 2.3444\n",
      "14/15, train_loss: 2.3604\n",
      "15/15, train_loss: 2.3321\n",
      "16/15, train_loss: 2.2137\n",
      "epoch 5 average loss: 2.4358\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 5 validation loss: 1.5773 dice_score: 0.3224 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4112 at epoch: 5\n",
      "----------\n",
      "epoch 6/500\n",
      "1/15, train_loss: 2.6859\n",
      "2/15, train_loss: 2.6474\n",
      "3/15, train_loss: 2.3462\n",
      "4/15, train_loss: 2.4140\n",
      "5/15, train_loss: 2.6422\n",
      "6/15, train_loss: 2.6547\n",
      "7/15, train_loss: 2.6605\n",
      "8/15, train_loss: 2.2470\n",
      "9/15, train_loss: 2.3317\n",
      "10/15, train_loss: 2.3109\n",
      "11/15, train_loss: 2.3216\n",
      "12/15, train_loss: 2.3016\n",
      "13/15, train_loss: 2.3260\n",
      "14/15, train_loss: 2.3406\n",
      "15/15, train_loss: 2.3157\n",
      "16/15, train_loss: 2.2086\n",
      "epoch 6 average loss: 2.4222\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 6 validation loss: 1.5714 dice_score: 0.3257 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4128 at epoch: 6\n",
      "----------\n",
      "epoch 7/500\n",
      "1/15, train_loss: 2.6701\n",
      "2/15, train_loss: 2.6329\n",
      "3/15, train_loss: 2.3331\n",
      "4/15, train_loss: 2.3935\n",
      "5/15, train_loss: 2.6303\n",
      "6/15, train_loss: 2.6377\n",
      "7/15, train_loss: 2.6428\n",
      "8/15, train_loss: 2.2312\n",
      "9/15, train_loss: 2.3187\n",
      "10/15, train_loss: 2.2980\n",
      "11/15, train_loss: 2.3076\n",
      "12/15, train_loss: 2.2792\n",
      "13/15, train_loss: 2.3049\n",
      "14/15, train_loss: 2.3166\n",
      "15/15, train_loss: 2.3028\n",
      "16/15, train_loss: 2.2013\n",
      "epoch 7 average loss: 2.4063\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 7 validation loss: 1.5643 dice_score: 0.3295 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4148 at epoch: 7\n",
      "----------\n",
      "epoch 8/500\n",
      "1/15, train_loss: 2.6517\n",
      "2/15, train_loss: 2.6161\n",
      "3/15, train_loss: 2.3182\n",
      "4/15, train_loss: 2.3769\n",
      "5/15, train_loss: 2.6113\n",
      "6/15, train_loss: 2.6184\n",
      "7/15, train_loss: 2.6228\n",
      "8/15, train_loss: 2.2128\n",
      "9/15, train_loss: 2.3080\n",
      "10/15, train_loss: 2.2783\n",
      "11/15, train_loss: 2.2914\n",
      "12/15, train_loss: 2.2542\n",
      "13/15, train_loss: 2.2807\n",
      "14/15, train_loss: 2.2979\n",
      "15/15, train_loss: 2.2827\n",
      "16/15, train_loss: 2.1938\n",
      "epoch 8 average loss: 2.3885\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 8 validation loss: 1.5561 dice_score: 0.3341 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4170 at epoch: 8\n",
      "----------\n",
      "epoch 9/500\n",
      "1/15, train_loss: 2.6310\n",
      "2/15, train_loss: 2.5973\n",
      "3/15, train_loss: 2.3060\n",
      "4/15, train_loss: 2.3516\n",
      "5/15, train_loss: 2.5904\n",
      "6/15, train_loss: 2.5972\n",
      "7/15, train_loss: 2.6008\n",
      "8/15, train_loss: 2.1990\n",
      "9/15, train_loss: 2.2938\n",
      "10/15, train_loss: 2.2573\n",
      "11/15, train_loss: 2.2747\n",
      "12/15, train_loss: 2.2271\n",
      "13/15, train_loss: 2.2623\n",
      "14/15, train_loss: 2.2705\n",
      "15/15, train_loss: 2.2600\n",
      "16/15, train_loss: 2.1853\n",
      "epoch 9 average loss: 2.3690\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 9 validation loss: 1.5471 dice_score: 0.3393 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4196 at epoch: 9\n",
      "----------\n",
      "epoch 10/500\n",
      "1/15, train_loss: 2.6089\n",
      "2/15, train_loss: 2.5826\n",
      "3/15, train_loss: 2.2882\n",
      "4/15, train_loss: 2.3241\n",
      "5/15, train_loss: 2.5678\n",
      "6/15, train_loss: 2.5740\n",
      "7/15, train_loss: 2.5845\n",
      "8/15, train_loss: 2.1783\n",
      "9/15, train_loss: 2.2770\n",
      "10/15, train_loss: 2.2342\n",
      "11/15, train_loss: 2.2564\n",
      "12/15, train_loss: 2.2061\n",
      "13/15, train_loss: 2.2349\n",
      "14/15, train_loss: 2.2398\n",
      "15/15, train_loss: 2.2365\n",
      "16/15, train_loss: 2.1770\n",
      "epoch 10 average loss: 2.3482\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 10 validation loss: 1.5397 dice_score: 0.3431 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4215 at epoch: 10\n",
      "----------\n",
      "epoch 11/500\n",
      "1/15, train_loss: 2.5914\n",
      "2/15, train_loss: 2.5611\n",
      "3/15, train_loss: 2.2693\n",
      "4/15, train_loss: 2.2950\n",
      "5/15, train_loss: 2.5442\n",
      "6/15, train_loss: 2.5573\n",
      "7/15, train_loss: 2.5595\n",
      "8/15, train_loss: 2.1556\n",
      "9/15, train_loss: 2.2608\n",
      "10/15, train_loss: 2.2103\n",
      "11/15, train_loss: 2.2423\n",
      "12/15, train_loss: 2.1760\n",
      "13/15, train_loss: 2.2053\n",
      "14/15, train_loss: 2.2089\n",
      "15/15, train_loss: 2.2109\n",
      "16/15, train_loss: 2.1699\n",
      "epoch 11 average loss: 2.3261\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 11 validation loss: 1.5288 dice_score: 0.3493 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4247 at epoch: 11\n",
      "----------\n",
      "epoch 12/500\n",
      "1/15, train_loss: 2.5672\n",
      "2/15, train_loss: 2.5388\n",
      "3/15, train_loss: 2.2495\n",
      "4/15, train_loss: 2.2644\n",
      "5/15, train_loss: 2.5268\n",
      "6/15, train_loss: 2.5319\n",
      "7/15, train_loss: 2.5332\n",
      "8/15, train_loss: 2.1328\n",
      "9/15, train_loss: 2.2418\n",
      "10/15, train_loss: 2.1915\n",
      "11/15, train_loss: 2.2229\n",
      "12/15, train_loss: 2.1445\n",
      "13/15, train_loss: 2.1750\n",
      "14/15, train_loss: 2.1753\n",
      "15/15, train_loss: 2.1930\n",
      "16/15, train_loss: 2.1601\n",
      "epoch 12 average loss: 2.3031\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 12 validation loss: 1.5168 dice_score: 0.3563 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4281 at epoch: 12\n",
      "----------\n",
      "epoch 13/500\n",
      "1/15, train_loss: 2.5422\n",
      "2/15, train_loss: 2.5158\n",
      "3/15, train_loss: 2.2290\n",
      "4/15, train_loss: 2.2415\n",
      "5/15, train_loss: 2.5011\n",
      "6/15, train_loss: 2.5053\n",
      "7/15, train_loss: 2.5059\n",
      "8/15, train_loss: 2.1079\n",
      "9/15, train_loss: 2.2267\n",
      "10/15, train_loss: 2.1654\n",
      "11/15, train_loss: 2.2023\n",
      "12/15, train_loss: 2.1126\n",
      "13/15, train_loss: 2.1429\n",
      "14/15, train_loss: 2.1510\n",
      "15/15, train_loss: 2.1668\n",
      "16/15, train_loss: 2.1512\n",
      "epoch 13 average loss: 2.2792\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 13 validation loss: 1.5037 dice_score: 0.3637 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4318 at epoch: 13\n",
      "----------\n",
      "epoch 14/500\n",
      "1/15, train_loss: 2.5161\n",
      "2/15, train_loss: 2.4914\n",
      "3/15, train_loss: 2.2132\n",
      "4/15, train_loss: 2.2090\n",
      "5/15, train_loss: 2.4750\n",
      "6/15, train_loss: 2.4786\n",
      "7/15, train_loss: 2.4781\n",
      "8/15, train_loss: 2.0905\n",
      "9/15, train_loss: 2.2086\n",
      "10/15, train_loss: 2.1391\n",
      "11/15, train_loss: 2.1822\n",
      "12/15, train_loss: 2.0802\n",
      "13/15, train_loss: 2.1200\n",
      "14/15, train_loss: 2.1171\n",
      "15/15, train_loss: 2.1384\n",
      "16/15, train_loss: 2.1401\n",
      "epoch 14 average loss: 2.2548\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 14 validation loss: 1.4897 dice_score: 0.3719 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4360 at epoch: 14\n",
      "----------\n",
      "epoch 15/500\n",
      "1/15, train_loss: 2.4898\n",
      "2/15, train_loss: 2.4731\n",
      "3/15, train_loss: 2.1912\n",
      "4/15, train_loss: 2.1756\n",
      "5/15, train_loss: 2.4484\n",
      "6/15, train_loss: 2.4508\n",
      "7/15, train_loss: 2.4591\n",
      "8/15, train_loss: 2.0657\n",
      "9/15, train_loss: 2.1873\n",
      "10/15, train_loss: 2.1118\n",
      "11/15, train_loss: 2.1611\n",
      "12/15, train_loss: 2.0564\n",
      "13/15, train_loss: 2.0873\n",
      "14/15, train_loss: 2.0809\n",
      "15/15, train_loss: 2.1106\n",
      "16/15, train_loss: 2.1296\n",
      "epoch 15 average loss: 2.2299\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 15 validation loss: 1.4788 dice_score: 0.3776 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4388 at epoch: 15\n",
      "----------\n",
      "epoch 16/500\n",
      "1/15, train_loss: 2.4697\n",
      "2/15, train_loss: 2.4476\n",
      "3/15, train_loss: 2.1690\n",
      "4/15, train_loss: 2.1425\n",
      "5/15, train_loss: 2.4218\n",
      "6/15, train_loss: 2.4317\n",
      "7/15, train_loss: 2.4303\n",
      "8/15, train_loss: 2.0391\n",
      "9/15, train_loss: 2.1676\n",
      "10/15, train_loss: 2.0845\n",
      "11/15, train_loss: 2.1449\n",
      "12/15, train_loss: 2.0233\n",
      "13/15, train_loss: 2.0528\n",
      "14/15, train_loss: 2.0463\n",
      "15/15, train_loss: 2.0809\n",
      "16/15, train_loss: 2.1196\n",
      "epoch 16 average loss: 2.2045\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 16 validation loss: 1.4631 dice_score: 0.3870 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4435 at epoch: 16\n",
      "----------\n",
      "epoch 17/500\n",
      "1/15, train_loss: 2.4426\n",
      "2/15, train_loss: 2.4220\n",
      "3/15, train_loss: 2.1464\n",
      "4/15, train_loss: 2.1085\n",
      "5/15, train_loss: 2.4030\n",
      "6/15, train_loss: 2.4036\n",
      "7/15, train_loss: 2.4013\n",
      "8/15, train_loss: 2.0137\n",
      "9/15, train_loss: 2.1440\n",
      "10/15, train_loss: 2.0630\n",
      "11/15, train_loss: 2.1230\n",
      "12/15, train_loss: 1.9896\n",
      "13/15, train_loss: 2.0190\n",
      "14/15, train_loss: 2.0094\n",
      "15/15, train_loss: 2.0610\n",
      "16/15, train_loss: 2.1055\n",
      "epoch 17 average loss: 2.1785\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 17 validation loss: 1.4465 dice_score: 0.3974 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4487 at epoch: 17\n",
      "----------\n",
      "epoch 18/500\n",
      "1/15, train_loss: 2.4158\n",
      "2/15, train_loss: 2.3965\n",
      "3/15, train_loss: 2.1237\n",
      "4/15, train_loss: 2.0842\n",
      "5/15, train_loss: 2.3758\n",
      "6/15, train_loss: 2.3750\n",
      "7/15, train_loss: 2.3724\n",
      "8/15, train_loss: 1.9861\n",
      "9/15, train_loss: 2.1253\n",
      "10/15, train_loss: 2.0339\n",
      "11/15, train_loss: 2.0998\n",
      "12/15, train_loss: 1.9566\n",
      "13/15, train_loss: 1.9838\n",
      "14/15, train_loss: 1.9838\n",
      "15/15, train_loss: 2.0320\n",
      "16/15, train_loss: 2.0929\n",
      "epoch 18 average loss: 2.1524\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 18 validation loss: 1.4290 dice_score: 0.4084 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4542 at epoch: 18\n",
      "----------\n",
      "epoch 19/500\n",
      "1/15, train_loss: 2.3882\n",
      "2/15, train_loss: 2.3703\n",
      "3/15, train_loss: 2.1066\n",
      "4/15, train_loss: 2.0505\n",
      "5/15, train_loss: 2.3495\n",
      "6/15, train_loss: 2.3476\n",
      "7/15, train_loss: 2.3442\n",
      "8/15, train_loss: 1.9675\n",
      "9/15, train_loss: 2.1043\n",
      "10/15, train_loss: 2.0061\n",
      "11/15, train_loss: 2.0780\n",
      "12/15, train_loss: 1.9240\n",
      "13/15, train_loss: 1.9596\n",
      "14/15, train_loss: 1.9491\n",
      "15/15, train_loss: 2.0014\n",
      "16/15, train_loss: 2.0771\n",
      "epoch 19 average loss: 2.1265\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 19 validation loss: 1.4109 dice_score: 0.4205 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4603 at epoch: 19\n",
      "----------\n",
      "epoch 20/500\n",
      "1/15, train_loss: 2.3616\n",
      "2/15, train_loss: 2.3509\n",
      "3/15, train_loss: 2.0835\n",
      "4/15, train_loss: 2.0167\n",
      "5/15, train_loss: 2.3231\n",
      "6/15, train_loss: 2.3197\n",
      "7/15, train_loss: 2.3255\n",
      "8/15, train_loss: 1.9422\n",
      "9/15, train_loss: 2.0797\n",
      "10/15, train_loss: 1.9779\n",
      "11/15, train_loss: 2.0559\n",
      "12/15, train_loss: 1.9005\n",
      "13/15, train_loss: 1.9258\n",
      "14/15, train_loss: 1.9129\n",
      "15/15, train_loss: 1.9730\n",
      "16/15, train_loss: 2.0630\n",
      "epoch 20 average loss: 2.1007\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 20 validation loss: 1.3973 dice_score: 0.4289 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4645 at epoch: 20\n",
      "----------\n",
      "epoch 21/500\n",
      "1/15, train_loss: 2.3415\n",
      "2/15, train_loss: 2.3250\n",
      "3/15, train_loss: 2.0607\n",
      "4/15, train_loss: 1.9852\n",
      "5/15, train_loss: 2.2981\n",
      "6/15, train_loss: 2.3018\n",
      "7/15, train_loss: 2.2977\n",
      "8/15, train_loss: 1.9154\n",
      "9/15, train_loss: 2.0593\n",
      "10/15, train_loss: 1.9517\n",
      "11/15, train_loss: 2.0391\n",
      "12/15, train_loss: 1.8697\n",
      "13/15, train_loss: 1.8918\n",
      "14/15, train_loss: 1.8806\n",
      "15/15, train_loss: 1.9443\n",
      "16/15, train_loss: 2.0505\n",
      "epoch 21 average loss: 2.0758\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 21 validation loss: 1.3783 dice_score: 0.4427 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4713 at epoch: 21\n",
      "----------\n",
      "epoch 22/500\n",
      "1/15, train_loss: 2.3161\n",
      "2/15, train_loss: 2.3002\n",
      "3/15, train_loss: 2.0385\n",
      "4/15, train_loss: 1.9534\n",
      "5/15, train_loss: 2.2811\n",
      "6/15, train_loss: 2.2759\n",
      "7/15, train_loss: 2.2707\n",
      "8/15, train_loss: 1.8918\n",
      "9/15, train_loss: 2.0352\n",
      "10/15, train_loss: 1.9307\n",
      "11/15, train_loss: 2.0181\n",
      "12/15, train_loss: 1.8396\n",
      "13/15, train_loss: 1.8602\n",
      "14/15, train_loss: 1.8469\n",
      "15/15, train_loss: 1.9268\n",
      "16/15, train_loss: 2.0340\n",
      "epoch 22 average loss: 2.0512\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 22 validation loss: 1.3594 dice_score: 0.4574 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4787 at epoch: 22\n",
      "----------\n",
      "epoch 23/500\n",
      "1/15, train_loss: 2.2923\n",
      "2/15, train_loss: 2.2771\n",
      "3/15, train_loss: 2.0169\n",
      "4/15, train_loss: 1.9317\n",
      "5/15, train_loss: 2.2573\n",
      "6/15, train_loss: 2.2508\n",
      "7/15, train_loss: 2.2450\n",
      "8/15, train_loss: 1.8663\n",
      "9/15, train_loss: 2.0169\n",
      "10/15, train_loss: 1.9052\n",
      "11/15, train_loss: 1.9974\n",
      "12/15, train_loss: 1.8126\n",
      "13/15, train_loss: 1.8291\n",
      "14/15, train_loss: 1.8249\n",
      "15/15, train_loss: 1.9018\n",
      "16/15, train_loss: 2.0213\n",
      "epoch 23 average loss: 2.0279\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 23 validation loss: 1.3404 dice_score: 0.4725 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4863 at epoch: 23\n",
      "----------\n",
      "epoch 24/500\n",
      "1/15, train_loss: 2.2692\n",
      "2/15, train_loss: 2.2542\n",
      "3/15, train_loss: 2.0009\n",
      "4/15, train_loss: 1.9033\n",
      "5/15, train_loss: 2.2354\n",
      "6/15, train_loss: 2.2282\n",
      "7/15, train_loss: 2.2209\n",
      "8/15, train_loss: 1.8500\n",
      "9/15, train_loss: 1.9995\n",
      "10/15, train_loss: 1.8826\n",
      "11/15, train_loss: 1.9798\n",
      "12/15, train_loss: 1.7871\n",
      "13/15, train_loss: 1.8089\n",
      "14/15, train_loss: 1.7968\n",
      "15/15, train_loss: 1.8759\n",
      "16/15, train_loss: 2.0065\n",
      "epoch 24 average loss: 2.0062\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 24 validation loss: 1.3221 dice_score: 0.4883 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4941 at epoch: 24\n",
      "----------\n",
      "epoch 25/500\n",
      "1/15, train_loss: 2.2476\n",
      "2/15, train_loss: 2.2373\n",
      "3/15, train_loss: 1.9804\n",
      "4/15, train_loss: 1.8755\n",
      "5/15, train_loss: 2.2141\n",
      "6/15, train_loss: 2.2057\n",
      "7/15, train_loss: 2.2059\n",
      "8/15, train_loss: 1.8301\n",
      "9/15, train_loss: 1.9782\n",
      "10/15, train_loss: 1.8609\n",
      "11/15, train_loss: 1.9629\n",
      "12/15, train_loss: 1.7694\n",
      "13/15, train_loss: 1.7815\n",
      "14/15, train_loss: 1.7676\n",
      "15/15, train_loss: 1.8532\n",
      "16/15, train_loss: 1.9953\n",
      "epoch 25 average loss: 1.9853\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 25 validation loss: 1.3092 dice_score: 0.4989 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.4995 at epoch: 25\n",
      "----------\n",
      "epoch 26/500\n",
      "1/15, train_loss: 2.2309\n",
      "2/15, train_loss: 2.2162\n",
      "3/15, train_loss: 1.9610\n",
      "4/15, train_loss: 1.8512\n",
      "5/15, train_loss: 2.1948\n",
      "6/15, train_loss: 2.1925\n",
      "7/15, train_loss: 2.1836\n",
      "8/15, train_loss: 1.8084\n",
      "9/15, train_loss: 1.9634\n",
      "10/15, train_loss: 1.8419\n",
      "11/15, train_loss: 1.9497\n",
      "12/15, train_loss: 1.7474\n",
      "13/15, train_loss: 1.7547\n",
      "14/15, train_loss: 1.7438\n",
      "15/15, train_loss: 1.8302\n",
      "16/15, train_loss: 1.9849\n",
      "epoch 26 average loss: 1.9659\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 26 validation loss: 1.2922 dice_score: 0.5145 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5073 at epoch: 26\n",
      "----------\n",
      "epoch 27/500\n",
      "1/15, train_loss: 2.2115\n",
      "2/15, train_loss: 2.1967\n",
      "3/15, train_loss: 1.9426\n",
      "4/15, train_loss: 1.8264\n",
      "5/15, train_loss: 2.1821\n",
      "6/15, train_loss: 2.1727\n",
      "7/15, train_loss: 2.1625\n",
      "8/15, train_loss: 1.7914\n",
      "9/15, train_loss: 1.9437\n",
      "10/15, train_loss: 1.8250\n",
      "11/15, train_loss: 1.9347\n",
      "12/15, train_loss: 1.7259\n",
      "13/15, train_loss: 1.7307\n",
      "14/15, train_loss: 1.7184\n",
      "15/15, train_loss: 1.8175\n",
      "16/15, train_loss: 1.9732\n",
      "epoch 27 average loss: 1.9472\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 27 validation loss: 1.2766 dice_score: 0.5305 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5152 at epoch: 27\n",
      "----------\n",
      "epoch 28/500\n",
      "1/15, train_loss: 2.1946\n",
      "2/15, train_loss: 2.1797\n",
      "3/15, train_loss: 1.9251\n",
      "4/15, train_loss: 1.8103\n",
      "5/15, train_loss: 2.1645\n",
      "6/15, train_loss: 2.1543\n",
      "7/15, train_loss: 2.1434\n",
      "8/15, train_loss: 1.7718\n",
      "9/15, train_loss: 1.9286\n",
      "10/15, train_loss: 1.8063\n",
      "11/15, train_loss: 1.9200\n",
      "12/15, train_loss: 1.7080\n",
      "13/15, train_loss: 1.7076\n",
      "14/15, train_loss: 1.7027\n",
      "15/15, train_loss: 1.7984\n",
      "16/15, train_loss: 1.9671\n",
      "epoch 28 average loss: 1.9301\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 28 validation loss: 1.2618 dice_score: 0.5451 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5226 at epoch: 28\n",
      "----------\n",
      "epoch 29/500\n",
      "1/15, train_loss: 2.1785\n",
      "2/15, train_loss: 2.1633\n",
      "3/15, train_loss: 1.9121\n",
      "4/15, train_loss: 1.7899\n",
      "5/15, train_loss: 2.1489\n",
      "6/15, train_loss: 2.1387\n",
      "7/15, train_loss: 2.1259\n",
      "8/15, train_loss: 1.7596\n",
      "9/15, train_loss: 1.9179\n",
      "10/15, train_loss: 1.7908\n",
      "11/15, train_loss: 1.9091\n",
      "12/15, train_loss: 1.6912\n",
      "13/15, train_loss: 1.6934\n",
      "14/15, train_loss: 1.6836\n",
      "15/15, train_loss: 1.7780\n",
      "16/15, train_loss: 1.9584\n",
      "epoch 29 average loss: 1.9150\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 29 validation loss: 1.2491 dice_score: 0.5588 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5294 at epoch: 29\n",
      "----------\n",
      "epoch 30/500\n",
      "1/15, train_loss: 2.1644\n",
      "2/15, train_loss: 2.1506\n",
      "3/15, train_loss: 1.8960\n",
      "4/15, train_loss: 1.7695\n",
      "5/15, train_loss: 2.1339\n",
      "6/15, train_loss: 2.1229\n",
      "7/15, train_loss: 2.1159\n",
      "8/15, train_loss: 1.7470\n",
      "9/15, train_loss: 1.9015\n",
      "10/15, train_loss: 1.7756\n",
      "11/15, train_loss: 1.8984\n",
      "12/15, train_loss: 1.6793\n",
      "13/15, train_loss: 1.6738\n",
      "14/15, train_loss: 1.6623\n",
      "15/15, train_loss: 1.7608\n",
      "16/15, train_loss: 1.9538\n",
      "epoch 30 average loss: 1.9004\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 30 validation loss: 1.2412 dice_score: 0.5669 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5334 at epoch: 30\n",
      "----------\n",
      "epoch 31/500\n",
      "1/15, train_loss: 2.1523\n",
      "2/15, train_loss: 2.1360\n",
      "3/15, train_loss: 1.8809\n",
      "4/15, train_loss: 1.7534\n",
      "5/15, train_loss: 2.1208\n",
      "6/15, train_loss: 2.1147\n",
      "7/15, train_loss: 2.1005\n",
      "8/15, train_loss: 1.7307\n",
      "9/15, train_loss: 1.8936\n",
      "10/15, train_loss: 1.7633\n",
      "11/15, train_loss: 1.8890\n",
      "12/15, train_loss: 1.6657\n",
      "13/15, train_loss: 1.6543\n",
      "14/15, train_loss: 1.6469\n",
      "15/15, train_loss: 1.7431\n",
      "16/15, train_loss: 1.9462\n",
      "epoch 31 average loss: 1.8870\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 31 validation loss: 1.2304 dice_score: 0.5781 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5391 at epoch: 31\n",
      "----------\n",
      "epoch 32/500\n",
      "1/15, train_loss: 2.1402\n",
      "2/15, train_loss: 2.1231\n",
      "3/15, train_loss: 1.8666\n",
      "4/15, train_loss: 1.7352\n",
      "5/15, train_loss: 2.1124\n",
      "6/15, train_loss: 2.1014\n",
      "7/15, train_loss: 2.0858\n",
      "8/15, train_loss: 1.7200\n",
      "9/15, train_loss: 1.8783\n",
      "10/15, train_loss: 1.7494\n",
      "11/15, train_loss: 1.8798\n",
      "12/15, train_loss: 1.6512\n",
      "13/15, train_loss: 1.6369\n",
      "14/15, train_loss: 1.6279\n",
      "15/15, train_loss: 1.7347\n",
      "16/15, train_loss: 1.9392\n",
      "epoch 32 average loss: 1.8739\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 32 validation loss: 1.2214 dice_score: 0.5888 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5444 at epoch: 32\n",
      "----------\n",
      "epoch 33/500\n",
      "1/15, train_loss: 2.1304\n",
      "2/15, train_loss: 2.1123\n",
      "3/15, train_loss: 1.8526\n",
      "4/15, train_loss: 1.7237\n",
      "5/15, train_loss: 2.1003\n",
      "6/15, train_loss: 2.0888\n",
      "7/15, train_loss: 2.0731\n",
      "8/15, train_loss: 1.7043\n",
      "9/15, train_loss: 1.8659\n",
      "10/15, train_loss: 1.7357\n",
      "11/15, train_loss: 1.8700\n",
      "12/15, train_loss: 1.6406\n",
      "13/15, train_loss: 1.6200\n",
      "14/15, train_loss: 1.6167\n",
      "15/15, train_loss: 1.7202\n",
      "16/15, train_loss: 1.9375\n",
      "epoch 33 average loss: 1.8620\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 33 validation loss: 1.2131 dice_score: 0.5971 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5486 at epoch: 33\n",
      "----------\n",
      "epoch 34/500\n",
      "1/15, train_loss: 2.1213\n",
      "2/15, train_loss: 2.1021\n",
      "3/15, train_loss: 1.8418\n",
      "4/15, train_loss: 1.7097\n",
      "5/15, train_loss: 2.0900\n",
      "6/15, train_loss: 2.0790\n",
      "7/15, train_loss: 2.0614\n",
      "8/15, train_loss: 1.6943\n",
      "9/15, train_loss: 1.8612\n",
      "10/15, train_loss: 1.7254\n",
      "11/15, train_loss: 1.8641\n",
      "12/15, train_loss: 1.6295\n",
      "13/15, train_loss: 1.6096\n",
      "14/15, train_loss: 1.6034\n",
      "15/15, train_loss: 1.7034\n",
      "16/15, train_loss: 1.9312\n",
      "epoch 34 average loss: 1.8517\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 34 validation loss: 1.2061 dice_score: 0.6049 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5525 at epoch: 34\n",
      "----------\n",
      "epoch 35/500\n",
      "1/15, train_loss: 2.1129\n",
      "2/15, train_loss: 2.0923\n",
      "3/15, train_loss: 1.8286\n",
      "4/15, train_loss: 1.6941\n",
      "5/15, train_loss: 2.0794\n",
      "6/15, train_loss: 2.0677\n",
      "7/15, train_loss: 2.0552\n",
      "8/15, train_loss: 1.6861\n",
      "9/15, train_loss: 1.8471\n",
      "10/15, train_loss: 1.7141\n",
      "11/15, train_loss: 1.8572\n",
      "12/15, train_loss: 1.6209\n",
      "13/15, train_loss: 1.5944\n",
      "14/15, train_loss: 1.5855\n",
      "15/15, train_loss: 1.6893\n",
      "16/15, train_loss: 1.9278\n",
      "epoch 35 average loss: 1.8408\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 35 validation loss: 1.2020 dice_score: 0.6085 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5542 at epoch: 35\n",
      "----------\n",
      "epoch 36/500\n",
      "1/15, train_loss: 2.1037\n",
      "2/15, train_loss: 2.0819\n",
      "3/15, train_loss: 1.8155\n",
      "4/15, train_loss: 1.6833\n",
      "5/15, train_loss: 2.0700\n",
      "6/15, train_loss: 2.0628\n",
      "7/15, train_loss: 2.0445\n",
      "8/15, train_loss: 1.6714\n",
      "9/15, train_loss: 1.8434\n",
      "10/15, train_loss: 1.7060\n",
      "11/15, train_loss: 1.8495\n",
      "12/15, train_loss: 1.6119\n",
      "13/15, train_loss: 1.5780\n",
      "14/15, train_loss: 1.5740\n",
      "15/15, train_loss: 1.6737\n",
      "16/15, train_loss: 1.9177\n",
      "epoch 36 average loss: 1.8304\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 36 validation loss: 1.1956 dice_score: 0.6147 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5573 at epoch: 36\n",
      "----------\n",
      "epoch 37/500\n",
      "1/15, train_loss: 2.0963\n",
      "2/15, train_loss: 2.0729\n",
      "3/15, train_loss: 1.8029\n",
      "4/15, train_loss: 1.6679\n",
      "5/15, train_loss: 2.0642\n",
      "6/15, train_loss: 2.0532\n",
      "7/15, train_loss: 2.0337\n",
      "8/15, train_loss: 1.6636\n",
      "9/15, train_loss: 1.8285\n",
      "10/15, train_loss: 1.6925\n",
      "11/15, train_loss: 1.8428\n",
      "12/15, train_loss: 1.5999\n",
      "13/15, train_loss: 1.5628\n",
      "14/15, train_loss: 1.5562\n",
      "15/15, train_loss: 1.6670\n",
      "16/15, train_loss: 1.9091\n",
      "epoch 37 average loss: 1.8196\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 37 validation loss: 1.1901 dice_score: 0.6211 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5606 at epoch: 37\n",
      "----------\n",
      "epoch 38/500\n",
      "1/15, train_loss: 2.0908\n",
      "2/15, train_loss: 2.0657\n",
      "3/15, train_loss: 1.7892\n",
      "4/15, train_loss: 1.6584\n",
      "5/15, train_loss: 2.0549\n",
      "6/15, train_loss: 2.0432\n",
      "7/15, train_loss: 2.0244\n",
      "8/15, train_loss: 1.6473\n",
      "9/15, train_loss: 1.8153\n",
      "10/15, train_loss: 1.6803\n",
      "11/15, train_loss: 1.8340\n",
      "12/15, train_loss: 1.5922\n",
      "13/15, train_loss: 1.5471\n",
      "14/15, train_loss: 1.5461\n",
      "15/15, train_loss: 1.6533\n",
      "16/15, train_loss: 1.9048\n",
      "epoch 38 average loss: 1.8092\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 38 validation loss: 1.1848 dice_score: 0.6261 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5630 at epoch: 38\n",
      "----------\n",
      "epoch 39/500\n",
      "1/15, train_loss: 2.0858\n",
      "2/15, train_loss: 2.0585\n",
      "3/15, train_loss: 1.7780\n",
      "4/15, train_loss: 1.6477\n",
      "5/15, train_loss: 2.0470\n",
      "6/15, train_loss: 2.0364\n",
      "7/15, train_loss: 2.0154\n",
      "8/15, train_loss: 1.6361\n",
      "9/15, train_loss: 1.8132\n",
      "10/15, train_loss: 1.6721\n",
      "11/15, train_loss: 1.8300\n",
      "12/15, train_loss: 1.5816\n",
      "13/15, train_loss: 1.5374\n",
      "14/15, train_loss: 1.5339\n",
      "15/15, train_loss: 1.6356\n",
      "16/15, train_loss: 1.8935\n",
      "epoch 39 average loss: 1.8001\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 39 validation loss: 1.1801 dice_score: 0.6312 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5656 at epoch: 39\n",
      "----------\n",
      "epoch 40/500\n",
      "1/15, train_loss: 2.0805\n",
      "2/15, train_loss: 2.0494\n",
      "3/15, train_loss: 1.7641\n",
      "4/15, train_loss: 1.6327\n",
      "5/15, train_loss: 2.0380\n",
      "6/15, train_loss: 2.0266\n",
      "7/15, train_loss: 2.0111\n",
      "8/15, train_loss: 1.6303\n",
      "9/15, train_loss: 1.7957\n",
      "10/15, train_loss: 1.6612\n",
      "11/15, train_loss: 1.8231\n",
      "12/15, train_loss: 1.5723\n",
      "13/15, train_loss: 1.5224\n",
      "14/15, train_loss: 1.5140\n",
      "15/15, train_loss: 1.6204\n",
      "16/15, train_loss: 1.8824\n",
      "epoch 40 average loss: 1.7890\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 40 validation loss: 1.1773 dice_score: 0.6335 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5667 at epoch: 40\n",
      "----------\n",
      "epoch 41/500\n",
      "1/15, train_loss: 2.0718\n",
      "2/15, train_loss: 2.0404\n",
      "3/15, train_loss: 1.7484\n",
      "4/15, train_loss: 1.6240\n",
      "5/15, train_loss: 2.0298\n",
      "6/15, train_loss: 2.0231\n",
      "7/15, train_loss: 2.0022\n",
      "8/15, train_loss: 1.6114\n",
      "9/15, train_loss: 1.7935\n",
      "10/15, train_loss: 1.6545\n",
      "11/15, train_loss: 1.8143\n",
      "12/15, train_loss: 1.5628\n",
      "13/15, train_loss: 1.5041\n",
      "14/15, train_loss: 1.5022\n",
      "15/15, train_loss: 1.6018\n",
      "16/15, train_loss: 1.8600\n",
      "epoch 41 average loss: 1.7778\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 41 validation loss: 1.1727 dice_score: 0.6379 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5689 at epoch: 41\n",
      "----------\n",
      "epoch 42/500\n",
      "1/15, train_loss: 2.0669\n",
      "2/15, train_loss: 2.0326\n",
      "3/15, train_loss: 1.7326\n",
      "4/15, train_loss: 1.6062\n",
      "5/15, train_loss: 2.0245\n",
      "6/15, train_loss: 2.0149\n",
      "7/15, train_loss: 1.9922\n",
      "8/15, train_loss: 1.6032\n",
      "9/15, train_loss: 1.7723\n",
      "10/15, train_loss: 1.6383\n",
      "11/15, train_loss: 1.8077\n",
      "12/15, train_loss: 1.5453\n",
      "13/15, train_loss: 1.4867\n",
      "14/15, train_loss: 1.4788\n",
      "15/15, train_loss: 1.5926\n",
      "16/15, train_loss: 1.8364\n",
      "epoch 42 average loss: 1.7644\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 42 validation loss: 1.1683 dice_score: 0.6429 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5714 at epoch: 42\n",
      "----------\n",
      "epoch 43/500\n",
      "1/15, train_loss: 2.0641\n",
      "2/15, train_loss: 2.0266\n",
      "3/15, train_loss: 1.7122\n",
      "4/15, train_loss: 1.5946\n",
      "5/15, train_loss: 2.0159\n",
      "6/15, train_loss: 2.0043\n",
      "7/15, train_loss: 1.9840\n",
      "8/15, train_loss: 1.5795\n",
      "9/15, train_loss: 1.7524\n",
      "10/15, train_loss: 1.6232\n",
      "11/15, train_loss: 1.7949\n",
      "12/15, train_loss: 1.5322\n",
      "13/15, train_loss: 1.4669\n",
      "14/15, train_loss: 1.4653\n",
      "15/15, train_loss: 1.5737\n",
      "16/15, train_loss: 1.8116\n",
      "epoch 43 average loss: 1.7501\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 43 validation loss: 1.1644 dice_score: 0.6465 acc_metric: 0.5000 accuracy: 0.5000, f1score: 0.3443\n",
      " saved Best AUC: 0.5732 at epoch: 43\n",
      "----------\n",
      "epoch 44/500\n",
      "1/15, train_loss: 2.0624\n",
      "2/15, train_loss: 2.0201\n",
      "3/15, train_loss: 1.6946\n",
      "4/15, train_loss: 1.5832\n",
      "5/15, train_loss: 2.0078\n",
      "6/15, train_loss: 1.9983\n",
      "7/15, train_loss: 1.9754\n",
      "8/15, train_loss: 1.5591\n",
      "9/15, train_loss: 1.7509\n",
      "10/15, train_loss: 1.6132\n",
      "11/15, train_loss: 1.7901\n",
      "12/15, train_loss: 1.5067\n",
      "13/15, train_loss: 1.4537\n",
      "14/15, train_loss: 1.4492\n",
      "15/15, train_loss: 1.5463\n",
      "16/15, train_loss: 1.7773\n",
      "epoch 44 average loss: 1.7368\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 44 validation loss: 1.1605 dice_score: 0.6504 acc_metric: 0.5062 accuracy: 0.5062, f1score: 0.3525\n",
      " saved Best AUC: 0.5783 at epoch: 44\n",
      "----------\n",
      "epoch 45/500\n",
      "1/15, train_loss: 2.0592\n",
      "2/15, train_loss: 2.0091\n",
      "3/15, train_loss: 1.6714\n",
      "4/15, train_loss: 1.5587\n",
      "5/15, train_loss: 1.9991\n",
      "6/15, train_loss: 1.9859\n",
      "7/15, train_loss: 1.9709\n",
      "8/15, train_loss: 1.5541\n",
      "9/15, train_loss: 1.7170\n",
      "10/15, train_loss: 1.5968\n",
      "11/15, train_loss: 1.7762\n",
      "12/15, train_loss: 1.4832\n",
      "13/15, train_loss: 1.4323\n",
      "14/15, train_loss: 1.4159\n",
      "15/15, train_loss: 1.5220\n",
      "16/15, train_loss: 1.7303\n",
      "epoch 45 average loss: 1.7176\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 45 validation loss: 1.1581 dice_score: 0.6518 acc_metric: 0.5062 accuracy: 0.5062, f1score: 0.3525\n",
      " saved Best AUC: 0.5790 at epoch: 45\n",
      "----------\n",
      "epoch 46/500\n",
      "1/15, train_loss: 2.0485\n",
      "2/15, train_loss: 1.9986\n",
      "3/15, train_loss: 1.6412\n",
      "4/15, train_loss: 1.5464\n",
      "5/15, train_loss: 1.9892\n",
      "6/15, train_loss: 1.9814\n",
      "7/15, train_loss: 1.9626\n",
      "8/15, train_loss: 1.5144\n",
      "9/15, train_loss: 1.7163\n",
      "10/15, train_loss: 1.5844\n",
      "11/15, train_loss: 1.7583\n",
      "12/15, train_loss: 1.4553\n",
      "13/15, train_loss: 1.4022\n",
      "14/15, train_loss: 1.3996\n",
      "15/15, train_loss: 1.4881\n",
      "16/15, train_loss: 1.6746\n",
      "epoch 46 average loss: 1.6976\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 46 validation loss: 1.1550 dice_score: 0.6549 acc_metric: 0.5125 accuracy: 0.5125, f1score: 0.3607\n",
      " saved Best AUC: 0.5837 at epoch: 46\n",
      "----------\n",
      "epoch 47/500\n",
      "1/15, train_loss: 2.0482\n",
      "2/15, train_loss: 1.9888\n",
      "3/15, train_loss: 1.6108\n",
      "4/15, train_loss: 1.5124\n",
      "5/15, train_loss: 1.9827\n",
      "6/15, train_loss: 1.9746\n",
      "7/15, train_loss: 1.9516\n",
      "8/15, train_loss: 1.5030\n",
      "9/15, train_loss: 1.6678\n",
      "10/15, train_loss: 1.5525\n",
      "11/15, train_loss: 1.7446\n",
      "12/15, train_loss: 1.4025\n",
      "13/15, train_loss: 1.3729\n",
      "14/15, train_loss: 1.3548\n",
      "15/15, train_loss: 1.4677\n",
      "16/15, train_loss: 1.6291\n",
      "epoch 47 average loss: 1.6727\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 47 validation loss: 1.1510 dice_score: 0.6586 acc_metric: 0.5125 accuracy: 0.5125, f1score: 0.3607\n",
      " saved Best AUC: 0.5855 at epoch: 47\n",
      "----------\n",
      "epoch 48/500\n",
      "1/15, train_loss: 2.0483\n",
      "2/15, train_loss: 1.9846\n",
      "3/15, train_loss: 1.5703\n",
      "4/15, train_loss: 1.4886\n",
      "5/15, train_loss: 1.9791\n",
      "6/15, train_loss: 1.9593\n",
      "7/15, train_loss: 1.9450\n",
      "8/15, train_loss: 1.4545\n",
      "9/15, train_loss: 1.6239\n",
      "10/15, train_loss: 1.5164\n",
      "11/15, train_loss: 1.7068\n",
      "12/15, train_loss: 1.3733\n",
      "13/15, train_loss: 1.3359\n",
      "14/15, train_loss: 1.3313\n",
      "15/15, train_loss: 1.4430\n",
      "16/15, train_loss: 1.5617\n",
      "epoch 48 average loss: 1.6451\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 48 validation loss: 1.1498 dice_score: 0.6606 acc_metric: 0.5125 accuracy: 0.5125, f1score: 0.3607\n",
      " saved Best AUC: 0.5865 at epoch: 48\n",
      "----------\n",
      "epoch 49/500\n",
      "1/15, train_loss: 2.0567\n",
      "2/15, train_loss: 1.9778\n",
      "3/15, train_loss: 1.5387\n",
      "4/15, train_loss: 1.4806\n",
      "5/15, train_loss: 1.9659\n",
      "6/15, train_loss: 1.9619\n",
      "7/15, train_loss: 1.9348\n",
      "8/15, train_loss: 1.4074\n",
      "9/15, train_loss: 1.6417\n",
      "10/15, train_loss: 1.4862\n",
      "11/15, train_loss: 1.7005\n",
      "12/15, train_loss: 1.3132\n",
      "13/15, train_loss: 1.3110\n",
      "14/15, train_loss: 1.3067\n",
      "15/15, train_loss: 1.3954\n",
      "16/15, train_loss: 1.5406\n",
      "epoch 49 average loss: 1.6262\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 49 validation loss: 1.1466 dice_score: 0.6630 acc_metric: 0.5125 accuracy: 0.5125, f1score: 0.3607\n",
      " saved Best AUC: 0.5877 at epoch: 49\n",
      "----------\n",
      "epoch 50/500\n",
      "1/15, train_loss: 2.0552\n",
      "2/15, train_loss: 1.9645\n",
      "3/15, train_loss: 1.5110\n",
      "4/15, train_loss: 1.4371\n",
      "5/15, train_loss: 1.9714\n",
      "6/15, train_loss: 1.9471\n",
      "7/15, train_loss: 1.9289\n",
      "8/15, train_loss: 1.4417\n",
      "9/15, train_loss: 1.5450\n",
      "10/15, train_loss: 1.4442\n",
      "11/15, train_loss: 1.6514\n",
      "12/15, train_loss: 1.2755\n",
      "13/15, train_loss: 1.3027\n",
      "14/15, train_loss: 1.2585\n",
      "15/15, train_loss: 1.3778\n",
      "16/15, train_loss: 1.4616\n",
      "epoch 50 average loss: 1.5983\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 50 validation loss: 1.1447 dice_score: 0.6631 acc_metric: 0.5125 accuracy: 0.5125, f1score: 0.3607\n",
      " saved Best AUC: 0.5878 at epoch: 50\n",
      "----------\n",
      "epoch 51/500\n",
      "1/15, train_loss: 2.0373\n",
      "2/15, train_loss: 1.9713\n",
      "3/15, train_loss: 1.4665\n",
      "4/15, train_loss: 1.4339\n",
      "5/15, train_loss: 1.9504\n",
      "6/15, train_loss: 1.9409\n",
      "7/15, train_loss: 1.9417\n",
      "8/15, train_loss: 1.3319\n",
      "9/15, train_loss: 1.5798\n",
      "10/15, train_loss: 1.4195\n",
      "11/15, train_loss: 1.6225\n",
      "12/15, train_loss: 1.2751\n",
      "13/15, train_loss: 1.2446\n",
      "14/15, train_loss: 1.2385\n",
      "15/15, train_loss: 1.3250\n",
      "16/15, train_loss: 1.4093\n",
      "epoch 51 average loss: 1.5743\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 51 validation loss: 1.1425 dice_score: 0.6655 acc_metric: 0.5125 accuracy: 0.5125, f1score: 0.3607\n",
      " saved Best AUC: 0.5890 at epoch: 51\n",
      "----------\n",
      "epoch 52/500\n",
      "1/15, train_loss: 2.0605\n",
      "2/15, train_loss: 1.9576\n",
      "3/15, train_loss: 1.4616\n",
      "4/15, train_loss: 1.3916\n",
      "5/15, train_loss: 1.9413\n",
      "6/15, train_loss: 1.9831\n",
      "7/15, train_loss: 1.9094\n",
      "8/15, train_loss: 1.3834\n",
      "9/15, train_loss: 1.4603\n",
      "10/15, train_loss: 1.3857\n",
      "11/15, train_loss: 1.6574\n",
      "12/15, train_loss: 1.1996\n",
      "13/15, train_loss: 1.2588\n",
      "14/15, train_loss: 1.1916\n",
      "15/15, train_loss: 1.3000\n",
      "16/15, train_loss: 1.4314\n",
      "epoch 52 average loss: 1.5608\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 52 validation loss: 1.1408 dice_score: 0.6670 acc_metric: 0.5125 accuracy: 0.5125, f1score: 0.3607\n",
      " saved Best AUC: 0.5898 at epoch: 52\n",
      "----------\n",
      "epoch 53/500\n",
      "1/15, train_loss: 2.0291\n",
      "2/15, train_loss: 1.9714\n",
      "3/15, train_loss: 1.4022\n",
      "4/15, train_loss: 1.3602\n",
      "5/15, train_loss: 1.9996\n",
      "6/15, train_loss: 1.9265\n",
      "7/15, train_loss: 1.9571\n",
      "8/15, train_loss: 1.2470\n",
      "9/15, train_loss: 1.4117\n",
      "10/15, train_loss: 1.3654\n",
      "11/15, train_loss: 1.5781\n",
      "12/15, train_loss: 1.1886\n",
      "13/15, train_loss: 1.1902\n",
      "14/15, train_loss: 1.1647\n",
      "15/15, train_loss: 1.3103\n",
      "16/15, train_loss: 1.3150\n",
      "epoch 53 average loss: 1.5261\n",
      "----------\n",
      "epoch 54/500\n",
      "1/15, train_loss: 2.0348\n",
      "2/15, train_loss: 1.9541\n",
      "3/15, train_loss: 1.3733\n",
      "4/15, train_loss: 1.3596\n",
      "5/15, train_loss: 1.9269\n",
      "6/15, train_loss: 1.9604\n",
      "7/15, train_loss: 1.8975\n",
      "8/15, train_loss: 1.2081\n",
      "9/15, train_loss: 1.4470\n",
      "10/15, train_loss: 1.3311\n",
      "11/15, train_loss: 1.6287\n",
      "12/15, train_loss: 1.1227\n",
      "13/15, train_loss: 1.1604\n",
      "14/15, train_loss: 1.1504\n",
      "15/15, train_loss: 1.2271\n",
      "16/15, train_loss: 1.3075\n",
      "epoch 54 average loss: 1.5056\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 54 validation loss: 1.1402 dice_score: 0.6697 acc_metric: 0.5125 accuracy: 0.5125, f1score: 0.3607\n",
      " saved Best AUC: 0.5911 at epoch: 54\n",
      "----------\n",
      "epoch 55/500\n",
      "1/15, train_loss: 2.0298\n",
      "2/15, train_loss: 1.9313\n",
      "3/15, train_loss: 1.3953\n",
      "4/15, train_loss: 1.3023\n",
      "5/15, train_loss: 2.0202\n",
      "6/15, train_loss: 1.9130\n",
      "7/15, train_loss: 1.8956\n",
      "8/15, train_loss: 1.2173\n",
      "9/15, train_loss: 1.3264\n",
      "10/15, train_loss: 1.3189\n",
      "11/15, train_loss: 1.5314\n",
      "12/15, train_loss: 1.0843\n",
      "13/15, train_loss: 1.1914\n",
      "14/15, train_loss: 1.0995\n",
      "15/15, train_loss: 1.2156\n",
      "16/15, train_loss: 1.2199\n",
      "epoch 55 average loss: 1.4808\n",
      "----------\n",
      "epoch 56/500\n",
      "1/15, train_loss: 2.0036\n",
      "2/15, train_loss: 1.9620\n",
      "3/15, train_loss: 1.3174\n",
      "4/15, train_loss: 1.2960\n",
      "5/15, train_loss: 1.9181\n",
      "6/15, train_loss: 1.9081\n",
      "7/15, train_loss: 1.9683\n",
      "8/15, train_loss: 1.1284\n",
      "9/15, train_loss: 1.3358\n",
      "10/15, train_loss: 1.2731\n",
      "11/15, train_loss: 1.4827\n",
      "12/15, train_loss: 1.0708\n",
      "13/15, train_loss: 1.0946\n",
      "14/15, train_loss: 1.0845\n",
      "15/15, train_loss: 1.1552\n",
      "16/15, train_loss: 1.1796\n",
      "epoch 56 average loss: 1.4486\n",
      "----------\n",
      "epoch 57/500\n",
      "1/15, train_loss: 2.0257\n",
      "2/15, train_loss: 1.9253\n",
      "3/15, train_loss: 1.3417\n",
      "4/15, train_loss: 1.2485\n",
      "5/15, train_loss: 1.9135\n",
      "6/15, train_loss: 1.9525\n",
      "7/15, train_loss: 1.8825\n",
      "8/15, train_loss: 1.1063\n",
      "9/15, train_loss: 1.2511\n",
      "10/15, train_loss: 1.2332\n",
      "11/15, train_loss: 1.5496\n",
      "12/15, train_loss: 1.0098\n",
      "13/15, train_loss: 1.0881\n",
      "14/15, train_loss: 1.0374\n",
      "15/15, train_loss: 1.1321\n",
      "16/15, train_loss: 1.1463\n",
      "epoch 57 average loss: 1.4277\n",
      "----------\n",
      "epoch 58/500\n",
      "1/15, train_loss: 2.0085\n",
      "2/15, train_loss: 1.9346\n",
      "3/15, train_loss: 1.2775\n",
      "4/15, train_loss: 1.2242\n",
      "5/15, train_loss: 1.9998\n",
      "6/15, train_loss: 1.9026\n",
      "7/15, train_loss: 1.9459\n",
      "8/15, train_loss: 1.0518\n",
      "9/15, train_loss: 1.2369\n",
      "10/15, train_loss: 1.2495\n",
      "11/15, train_loss: 1.4257\n",
      "12/15, train_loss: 1.0716\n",
      "13/15, train_loss: 1.0453\n",
      "14/15, train_loss: 1.0330\n",
      "15/15, train_loss: 1.1415\n",
      "16/15, train_loss: 1.0878\n",
      "epoch 58 average loss: 1.4148\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 58 validation loss: 1.1600 dice_score: 0.6727 acc_metric: 0.5268 accuracy: 0.5268, f1score: 0.4016\n",
      " saved Best AUC: 0.5997 at epoch: 58\n",
      "----------\n",
      "epoch 59/500\n",
      "1/15, train_loss: 2.0775\n",
      "2/15, train_loss: 1.9309\n",
      "3/15, train_loss: 1.2587\n",
      "4/15, train_loss: 1.3405\n",
      "5/15, train_loss: 1.9492\n",
      "6/15, train_loss: 2.0385\n",
      "7/15, train_loss: 1.9280\n",
      "8/15, train_loss: 1.0321\n",
      "9/15, train_loss: 1.3597\n",
      "10/15, train_loss: 1.1937\n",
      "11/15, train_loss: 1.5204\n",
      "12/15, train_loss: 0.9762\n",
      "13/15, train_loss: 1.0234\n",
      "14/15, train_loss: 1.2582\n",
      "15/15, train_loss: 1.0888\n",
      "16/15, train_loss: 1.0653\n",
      "epoch 59 average loss: 1.4401\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 59 validation loss: 1.1436 dice_score: 0.6788 acc_metric: 0.5598 accuracy: 0.5598, f1score: 0.4672\n",
      " saved Best AUC: 0.6193 at epoch: 59\n",
      "----------\n",
      "epoch 60/500\n",
      "1/15, train_loss: 2.0308\n",
      "2/15, train_loss: 1.9177\n",
      "3/15, train_loss: 1.3352\n",
      "4/15, train_loss: 1.2027\n",
      "5/15, train_loss: 2.0214\n",
      "6/15, train_loss: 1.8986\n",
      "7/15, train_loss: 1.9271\n",
      "8/15, train_loss: 1.1237\n",
      "9/15, train_loss: 1.2352\n",
      "10/15, train_loss: 1.3692\n",
      "11/15, train_loss: 1.4444\n",
      "12/15, train_loss: 0.9433\n",
      "13/15, train_loss: 1.0464\n",
      "14/15, train_loss: 0.9833\n",
      "15/15, train_loss: 1.1328\n",
      "16/15, train_loss: 1.0204\n",
      "epoch 60 average loss: 1.4145\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 60 validation loss: 1.1425 dice_score: 0.6797 acc_metric: 0.5592 accuracy: 0.5592, f1score: 0.4590\n",
      " saved Best AUC: 0.6194 at epoch: 60\n",
      "----------\n",
      "epoch 61/500\n",
      "1/15, train_loss: 2.0030\n",
      "2/15, train_loss: 2.0178\n",
      "3/15, train_loss: 1.2215\n",
      "4/15, train_loss: 1.3666\n",
      "5/15, train_loss: 1.9247\n",
      "6/15, train_loss: 1.8914\n",
      "7/15, train_loss: 1.9800\n",
      "8/15, train_loss: 0.9979\n",
      "9/15, train_loss: 1.3553\n",
      "10/15, train_loss: 1.1621\n",
      "11/15, train_loss: 1.3850\n",
      "12/15, train_loss: 1.0685\n",
      "13/15, train_loss: 0.9691\n",
      "14/15, train_loss: 1.1750\n",
      "15/15, train_loss: 1.0360\n",
      "16/15, train_loss: 1.0012\n",
      "epoch 61 average loss: 1.4097\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 61 validation loss: 1.1604 dice_score: 0.6757 acc_metric: 0.5708 accuracy: 0.5708, f1score: 0.5410\n",
      " saved Best AUC: 0.6232 at epoch: 61\n",
      "----------\n",
      "epoch 62/500\n",
      "1/15, train_loss: 2.0661\n",
      "2/15, train_loss: 1.9074\n",
      "3/15, train_loss: 1.2638\n",
      "4/15, train_loss: 1.1812\n",
      "5/15, train_loss: 1.9052\n",
      "6/15, train_loss: 2.0547\n",
      "7/15, train_loss: 1.8830\n",
      "8/15, train_loss: 1.0900\n",
      "9/15, train_loss: 1.1537\n",
      "10/15, train_loss: 1.1266\n",
      "11/15, train_loss: 1.4658\n",
      "12/15, train_loss: 0.8943\n",
      "13/15, train_loss: 0.9676\n",
      "14/15, train_loss: 0.9463\n",
      "15/15, train_loss: 1.0154\n",
      "16/15, train_loss: 0.9846\n",
      "epoch 62 average loss: 1.3691\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 62 validation loss: 1.1359 dice_score: 0.6879 acc_metric: 0.5985 accuracy: 0.5985, f1score: 0.5328\n",
      " saved Best AUC: 0.6432 at epoch: 62\n",
      "----------\n",
      "epoch 63/500\n",
      "1/15, train_loss: 1.9800\n",
      "2/15, train_loss: 2.0316\n",
      "3/15, train_loss: 1.1718\n",
      "4/15, train_loss: 1.1517\n",
      "5/15, train_loss: 1.9853\n",
      "6/15, train_loss: 1.8682\n",
      "7/15, train_loss: 1.9613\n",
      "8/15, train_loss: 0.9458\n",
      "9/15, train_loss: 1.1209\n",
      "10/15, train_loss: 1.2538\n",
      "11/15, train_loss: 1.3426\n",
      "12/15, train_loss: 0.9512\n",
      "13/15, train_loss: 0.9213\n",
      "14/15, train_loss: 0.9193\n",
      "15/15, train_loss: 1.0457\n",
      "16/15, train_loss: 0.9509\n",
      "epoch 63 average loss: 1.3501\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 63 validation loss: 1.1515 dice_score: 0.6883 acc_metric: 0.5896 accuracy: 0.5896, f1score: 0.5656\n",
      " saved Best AUC: 0.6390 at epoch: 63\n",
      "----------\n",
      "epoch 64/500\n",
      "1/15, train_loss: 2.0478\n",
      "2/15, train_loss: 1.9034\n",
      "3/15, train_loss: 1.1401\n",
      "4/15, train_loss: 1.3062\n",
      "5/15, train_loss: 1.8861\n",
      "6/15, train_loss: 2.0125\n",
      "7/15, train_loss: 1.8600\n",
      "8/15, train_loss: 0.9273\n",
      "9/15, train_loss: 1.1958\n",
      "10/15, train_loss: 1.0869\n",
      "11/15, train_loss: 1.3846\n",
      "12/15, train_loss: 0.8461\n",
      "13/15, train_loss: 0.9051\n",
      "14/15, train_loss: 0.9798\n",
      "15/15, train_loss: 0.9690\n",
      "16/15, train_loss: 0.9477\n",
      "epoch 64 average loss: 1.3374\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 64 validation loss: 1.1342 dice_score: 0.6942 acc_metric: 0.5940 accuracy: 0.5940, f1score: 0.5492\n",
      " saved Best AUC: 0.6441 at epoch: 64\n",
      "----------\n",
      "epoch 65/500\n",
      "1/15, train_loss: 1.9751\n",
      "2/15, train_loss: 1.8859\n",
      "3/15, train_loss: 1.1889\n",
      "4/15, train_loss: 1.1147\n",
      "5/15, train_loss: 1.9868\n",
      "6/15, train_loss: 1.8776\n",
      "7/15, train_loss: 1.8649\n",
      "8/15, train_loss: 1.0239\n",
      "9/15, train_loss: 1.0605\n",
      "10/15, train_loss: 1.1667\n",
      "11/15, train_loss: 1.3233\n",
      "12/15, train_loss: 0.8309\n",
      "13/15, train_loss: 0.9461\n",
      "14/15, train_loss: 0.8889\n",
      "15/15, train_loss: 1.0156\n",
      "16/15, train_loss: 0.9089\n",
      "epoch 65 average loss: 1.3162\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 65 validation loss: 1.1294 dice_score: 0.6964 acc_metric: 0.6155 accuracy: 0.6155, f1score: 0.5328\n",
      " saved Best AUC: 0.6559 at epoch: 65\n",
      "----------\n",
      "epoch 66/500\n",
      "1/15, train_loss: 1.9486\n",
      "2/15, train_loss: 1.9955\n",
      "3/15, train_loss: 1.1050\n",
      "4/15, train_loss: 1.1722\n",
      "5/15, train_loss: 1.8955\n",
      "6/15, train_loss: 1.8932\n",
      "7/15, train_loss: 2.0222\n",
      "8/15, train_loss: 0.9053\n",
      "9/15, train_loss: 1.1285\n",
      "10/15, train_loss: 1.0788\n",
      "11/15, train_loss: 1.2994\n",
      "12/15, train_loss: 0.8929\n",
      "13/15, train_loss: 0.8717\n",
      "14/15, train_loss: 0.9206\n",
      "15/15, train_loss: 0.9305\n",
      "16/15, train_loss: 0.8924\n",
      "epoch 66 average loss: 1.3095\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 66 validation loss: 1.1333 dice_score: 0.7013 acc_metric: 0.6110 accuracy: 0.6110, f1score: 0.5492\n",
      " saved Best AUC: 0.6562 at epoch: 66\n",
      "----------\n",
      "epoch 67/500\n",
      "1/15, train_loss: 2.0574\n",
      "2/15, train_loss: 1.9264\n",
      "3/15, train_loss: 1.1689\n",
      "4/15, train_loss: 1.1028\n",
      "5/15, train_loss: 1.8745\n",
      "6/15, train_loss: 2.0179\n",
      "7/15, train_loss: 1.8376\n",
      "8/15, train_loss: 0.9894\n",
      "9/15, train_loss: 1.0011\n",
      "10/15, train_loss: 1.0452\n",
      "11/15, train_loss: 1.4408\n",
      "12/15, train_loss: 0.7928\n",
      "13/15, train_loss: 0.9498\n",
      "14/15, train_loss: 0.8460\n",
      "15/15, train_loss: 0.9162\n",
      "16/15, train_loss: 0.9261\n",
      "epoch 67 average loss: 1.3058\n",
      "----------\n",
      "epoch 68/500\n",
      "1/15, train_loss: 1.9135\n",
      "2/15, train_loss: 1.9863\n",
      "3/15, train_loss: 1.0652\n",
      "4/15, train_loss: 1.0637\n",
      "5/15, train_loss: 2.1093\n",
      "6/15, train_loss: 1.8629\n",
      "7/15, train_loss: 2.0642\n",
      "8/15, train_loss: 0.8556\n",
      "9/15, train_loss: 0.9785\n",
      "10/15, train_loss: 1.0858\n",
      "11/15, train_loss: 1.2810\n",
      "12/15, train_loss: 0.8104\n",
      "13/15, train_loss: 0.8385\n",
      "14/15, train_loss: 0.8282\n",
      "15/15, train_loss: 1.0215\n",
      "16/15, train_loss: 0.8439\n",
      "epoch 68 average loss: 1.2880\n",
      "----------\n",
      "epoch 69/500\n",
      "1/15, train_loss: 2.0425\n",
      "2/15, train_loss: 1.9092\n",
      "3/15, train_loss: 1.0431\n",
      "4/15, train_loss: 1.1021\n",
      "5/15, train_loss: 1.8659\n",
      "6/15, train_loss: 1.9708\n",
      "7/15, train_loss: 1.8332\n",
      "8/15, train_loss: 0.8312\n",
      "9/15, train_loss: 1.0276\n",
      "10/15, train_loss: 1.0003\n",
      "11/15, train_loss: 1.4080\n",
      "12/15, train_loss: 0.7531\n",
      "13/15, train_loss: 0.8217\n",
      "14/15, train_loss: 0.8511\n",
      "15/15, train_loss: 0.8775\n",
      "16/15, train_loss: 0.8753\n",
      "epoch 69 average loss: 1.2633\n",
      "----------\n",
      "epoch 70/500\n",
      "1/15, train_loss: 1.9410\n",
      "2/15, train_loss: 1.8790\n",
      "3/15, train_loss: 1.1139\n",
      "4/15, train_loss: 1.0322\n",
      "5/15, train_loss: 2.0428\n",
      "6/15, train_loss: 1.8611\n",
      "7/15, train_loss: 1.8550\n",
      "8/15, train_loss: 0.8779\n",
      "9/15, train_loss: 0.9406\n",
      "10/15, train_loss: 1.0756\n",
      "11/15, train_loss: 1.2477\n",
      "12/15, train_loss: 0.7416\n",
      "13/15, train_loss: 0.8863\n",
      "14/15, train_loss: 0.8041\n",
      "15/15, train_loss: 0.9326\n",
      "16/15, train_loss: 0.8019\n",
      "epoch 70 average loss: 1.2521\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 70 validation loss: 1.1235 dice_score: 0.7101 acc_metric: 0.6173 accuracy: 0.6173, f1score: 0.5574\n",
      " saved Best AUC: 0.6637 at epoch: 70\n",
      "----------\n",
      "epoch 71/500\n",
      "1/15, train_loss: 1.9448\n",
      "2/15, train_loss: 2.0177\n",
      "3/15, train_loss: 1.0342\n",
      "4/15, train_loss: 1.1025\n",
      "5/15, train_loss: 1.9142\n",
      "6/15, train_loss: 1.8659\n",
      "7/15, train_loss: 1.9938\n",
      "8/15, train_loss: 0.7947\n",
      "9/15, train_loss: 1.0909\n",
      "10/15, train_loss: 0.9655\n",
      "11/15, train_loss: 1.2541\n",
      "12/15, train_loss: 0.8336\n",
      "13/15, train_loss: 0.7997\n",
      "14/15, train_loss: 0.9032\n",
      "15/15, train_loss: 0.8549\n",
      "16/15, train_loss: 0.7827\n",
      "epoch 71 average loss: 1.2595\n",
      "----------\n",
      "epoch 72/500\n",
      "1/15, train_loss: 2.0560\n",
      "2/15, train_loss: 1.8640\n",
      "3/15, train_loss: 1.1209\n",
      "4/15, train_loss: 1.0074\n",
      "5/15, train_loss: 1.9021\n",
      "6/15, train_loss: 2.0597\n",
      "7/15, train_loss: 1.8749\n",
      "8/15, train_loss: 0.8588\n",
      "9/15, train_loss: 0.9437\n",
      "10/15, train_loss: 0.9411\n",
      "11/15, train_loss: 1.4023\n",
      "12/15, train_loss: 0.7073\n",
      "13/15, train_loss: 0.8256\n",
      "14/15, train_loss: 0.7708\n",
      "15/15, train_loss: 0.8381\n",
      "16/15, train_loss: 0.7746\n",
      "epoch 72 average loss: 1.2467\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 72 validation loss: 1.1165 dice_score: 0.7180 acc_metric: 0.6027 accuracy: 0.6027, f1score: 0.5902\n",
      " saved Best AUC: 0.6604 at epoch: 72\n",
      "----------\n",
      "epoch 73/500\n",
      "1/15, train_loss: 1.9465\n",
      "2/15, train_loss: 2.0574\n",
      "3/15, train_loss: 1.0135\n",
      "4/15, train_loss: 0.9920\n",
      "5/15, train_loss: 1.9987\n",
      "6/15, train_loss: 1.8400\n",
      "7/15, train_loss: 1.9668\n",
      "8/15, train_loss: 0.7633\n",
      "9/15, train_loss: 0.9056\n",
      "10/15, train_loss: 1.0854\n",
      "11/15, train_loss: 1.2247\n",
      "12/15, train_loss: 0.7808\n",
      "13/15, train_loss: 0.7623\n",
      "14/15, train_loss: 0.7560\n",
      "15/15, train_loss: 0.8806\n",
      "16/15, train_loss: 0.7519\n",
      "epoch 73 average loss: 1.2328\n",
      "----------\n",
      "epoch 74/500\n",
      "1/15, train_loss: 2.0361\n",
      "2/15, train_loss: 1.8824\n",
      "3/15, train_loss: 0.9801\n",
      "4/15, train_loss: 1.1551\n",
      "5/15, train_loss: 1.8661\n",
      "6/15, train_loss: 2.0707\n",
      "7/15, train_loss: 1.8437\n",
      "8/15, train_loss: 0.7506\n",
      "9/15, train_loss: 0.9820\n",
      "10/15, train_loss: 0.9150\n",
      "11/15, train_loss: 1.3108\n",
      "12/15, train_loss: 0.6802\n",
      "13/15, train_loss: 0.7487\n",
      "14/15, train_loss: 0.8561\n",
      "15/15, train_loss: 0.8021\n",
      "16/15, train_loss: 0.7674\n",
      "epoch 74 average loss: 1.2280\n",
      "----------\n",
      "epoch 75/500\n",
      "1/15, train_loss: 1.9363\n",
      "2/15, train_loss: 1.8698\n",
      "3/15, train_loss: 1.0540\n",
      "4/15, train_loss: 0.9762\n",
      "5/15, train_loss: 1.9882\n",
      "6/15, train_loss: 1.8609\n",
      "7/15, train_loss: 1.8508\n",
      "8/15, train_loss: 0.8620\n",
      "9/15, train_loss: 0.8615\n",
      "10/15, train_loss: 1.0121\n",
      "11/15, train_loss: 1.1993\n",
      "12/15, train_loss: 0.6720\n",
      "13/15, train_loss: 0.8129\n",
      "14/15, train_loss: 0.7388\n",
      "15/15, train_loss: 0.8893\n",
      "16/15, train_loss: 0.7321\n",
      "epoch 75 average loss: 1.2073\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 75 validation loss: 1.1066 dice_score: 0.7256 acc_metric: 0.6202 accuracy: 0.6202, f1score: 0.5984\n",
      " saved Best AUC: 0.6729 at epoch: 75\n",
      "----------\n",
      "epoch 76/500\n",
      "1/15, train_loss: 1.9131\n",
      "2/15, train_loss: 2.0390\n",
      "3/15, train_loss: 0.9527\n",
      "4/15, train_loss: 1.0863\n",
      "5/15, train_loss: 1.8632\n",
      "6/15, train_loss: 1.8660\n",
      "7/15, train_loss: 2.0274\n",
      "8/15, train_loss: 0.7382\n",
      "9/15, train_loss: 0.9465\n",
      "10/15, train_loss: 0.9211\n",
      "11/15, train_loss: 1.1566\n",
      "12/15, train_loss: 0.7452\n",
      "13/15, train_loss: 0.7301\n",
      "14/15, train_loss: 0.7829\n",
      "15/15, train_loss: 0.7766\n",
      "16/15, train_loss: 0.7227\n",
      "epoch 76 average loss: 1.2042\n",
      "----------\n",
      "epoch 77/500\n",
      "1/15, train_loss: 2.0907\n",
      "2/15, train_loss: 1.8959\n",
      "3/15, train_loss: 1.0148\n",
      "4/15, train_loss: 0.9722\n",
      "5/15, train_loss: 1.8480\n",
      "6/15, train_loss: 1.9928\n",
      "7/15, train_loss: 1.8147\n",
      "8/15, train_loss: 0.8154\n",
      "9/15, train_loss: 0.8247\n",
      "10/15, train_loss: 0.8899\n",
      "11/15, train_loss: 1.3018\n",
      "12/15, train_loss: 0.6459\n",
      "13/15, train_loss: 0.7890\n",
      "14/15, train_loss: 0.7076\n",
      "15/15, train_loss: 0.7657\n",
      "16/15, train_loss: 0.7555\n",
      "epoch 77 average loss: 1.1953\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 77 validation loss: 1.1020 dice_score: 0.7318 acc_metric: 0.6146 accuracy: 0.6146, f1score: 0.5984\n",
      " saved Best AUC: 0.6732 at epoch: 77\n",
      "----------\n",
      "epoch 78/500\n",
      "1/15, train_loss: 1.9064\n",
      "2/15, train_loss: 1.9675\n",
      "3/15, train_loss: 0.9195\n",
      "4/15, train_loss: 0.9365\n",
      "5/15, train_loss: 2.0805\n",
      "6/15, train_loss: 1.8274\n",
      "7/15, train_loss: 2.0710\n",
      "8/15, train_loss: 0.7034\n",
      "9/15, train_loss: 0.8097\n",
      "10/15, train_loss: 0.9337\n",
      "11/15, train_loss: 1.1392\n",
      "12/15, train_loss: 0.6613\n",
      "13/15, train_loss: 0.7106\n",
      "14/15, train_loss: 0.6940\n",
      "15/15, train_loss: 0.8253\n",
      "16/15, train_loss: 0.6870\n",
      "epoch 78 average loss: 1.1796\n",
      "----------\n",
      "epoch 79/500\n",
      "1/15, train_loss: 2.0487\n",
      "2/15, train_loss: 1.8722\n",
      "3/15, train_loss: 0.9020\n",
      "4/15, train_loss: 0.9710\n",
      "5/15, train_loss: 1.8538\n",
      "6/15, train_loss: 1.9101\n",
      "7/15, train_loss: 1.8193\n",
      "8/15, train_loss: 0.6892\n",
      "9/15, train_loss: 0.8544\n",
      "10/15, train_loss: 0.8697\n",
      "11/15, train_loss: 1.2201\n",
      "12/15, train_loss: 0.6245\n",
      "13/15, train_loss: 0.6981\n",
      "14/15, train_loss: 0.7336\n",
      "15/15, train_loss: 0.7468\n",
      "16/15, train_loss: 0.7138\n",
      "epoch 79 average loss: 1.1580\n",
      "----------\n",
      "epoch 80/500\n",
      "1/15, train_loss: 1.9756\n",
      "2/15, train_loss: 1.8558\n",
      "3/15, train_loss: 0.9778\n",
      "4/15, train_loss: 0.9169\n",
      "5/15, train_loss: 2.0595\n",
      "6/15, train_loss: 1.8411\n",
      "7/15, train_loss: 1.8532\n",
      "8/15, train_loss: 0.7599\n",
      "9/15, train_loss: 0.8066\n",
      "10/15, train_loss: 1.0237\n",
      "11/15, train_loss: 1.1581\n",
      "12/15, train_loss: 0.6147\n",
      "13/15, train_loss: 0.7467\n",
      "14/15, train_loss: 0.6719\n",
      "15/15, train_loss: 0.8206\n",
      "16/15, train_loss: 0.6551\n",
      "epoch 80 average loss: 1.1711\n",
      "----------\n",
      "epoch 81/500\n",
      "1/15, train_loss: 1.9531\n",
      "2/15, train_loss: 2.0313\n",
      "3/15, train_loss: 0.9037\n",
      "4/15, train_loss: 1.0337\n",
      "5/15, train_loss: 1.9008\n",
      "6/15, train_loss: 1.8309\n",
      "7/15, train_loss: 1.9965\n",
      "8/15, train_loss: 0.6654\n",
      "9/15, train_loss: 1.0071\n",
      "10/15, train_loss: 0.8487\n",
      "11/15, train_loss: 1.1474\n",
      "12/15, train_loss: 0.7057\n",
      "13/15, train_loss: 0.6795\n",
      "14/15, train_loss: 0.7645\n",
      "15/15, train_loss: 0.7290\n",
      "16/15, train_loss: 0.6451\n",
      "epoch 81 average loss: 1.1777\n",
      "----------\n",
      "epoch 82/500\n",
      "1/15, train_loss: 2.0932\n",
      "2/15, train_loss: 1.8383\n",
      "3/15, train_loss: 0.9915\n",
      "4/15, train_loss: 0.8966\n",
      "5/15, train_loss: 1.8743\n",
      "6/15, train_loss: 2.0576\n",
      "7/15, train_loss: 1.8344\n",
      "8/15, train_loss: 0.7661\n",
      "9/15, train_loss: 0.7908\n",
      "10/15, train_loss: 0.8286\n",
      "11/15, train_loss: 1.3155\n",
      "12/15, train_loss: 0.5965\n",
      "13/15, train_loss: 0.7124\n",
      "14/15, train_loss: 0.6549\n",
      "15/15, train_loss: 0.7143\n",
      "16/15, train_loss: 0.6738\n",
      "epoch 82 average loss: 1.1649\n",
      "----------\n",
      "epoch 83/500\n",
      "1/15, train_loss: 1.9170\n",
      "2/15, train_loss: 2.0591\n",
      "3/15, train_loss: 0.8712\n",
      "4/15, train_loss: 0.8842\n",
      "5/15, train_loss: 2.0039\n",
      "6/15, train_loss: 1.8245\n",
      "7/15, train_loss: 1.9784\n",
      "8/15, train_loss: 0.6508\n",
      "9/15, train_loss: 0.7630\n",
      "10/15, train_loss: 0.9449\n",
      "11/15, train_loss: 1.1054\n",
      "12/15, train_loss: 0.6609\n",
      "13/15, train_loss: 0.6657\n",
      "14/15, train_loss: 0.6447\n",
      "15/15, train_loss: 0.8313\n",
      "16/15, train_loss: 0.6272\n",
      "epoch 83 average loss: 1.1520\n",
      "----------\n",
      "epoch 84/500\n",
      "1/15, train_loss: 2.0707\n",
      "2/15, train_loss: 1.8445\n",
      "3/15, train_loss: 0.8528\n",
      "4/15, train_loss: 1.0036\n",
      "5/15, train_loss: 1.8624\n",
      "6/15, train_loss: 2.0300\n",
      "7/15, train_loss: 1.8145\n",
      "8/15, train_loss: 0.6382\n",
      "9/15, train_loss: 0.9066\n",
      "10/15, train_loss: 0.8121\n",
      "11/15, train_loss: 1.3287\n",
      "12/15, train_loss: 0.5826\n",
      "13/15, train_loss: 0.6543\n",
      "14/15, train_loss: 0.7148\n",
      "15/15, train_loss: 0.6953\n",
      "16/15, train_loss: 0.6995\n",
      "epoch 84 average loss: 1.1569\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 84 validation loss: 1.0792 dice_score: 0.7485 acc_metric: 0.6188 accuracy: 0.6188, f1score: 0.6557\n",
      " saved Best AUC: 0.6836 at epoch: 84\n",
      "----------\n",
      "epoch 85/500\n",
      "1/15, train_loss: 1.8802\n",
      "2/15, train_loss: 1.8288\n",
      "3/15, train_loss: 0.9345\n",
      "4/15, train_loss: 0.8629\n",
      "5/15, train_loss: 2.0652\n",
      "6/15, train_loss: 1.8182\n",
      "7/15, train_loss: 1.8062\n",
      "8/15, train_loss: 0.7322\n",
      "9/15, train_loss: 0.7363\n",
      "10/15, train_loss: 0.9031\n",
      "11/15, train_loss: 1.0995\n",
      "12/15, train_loss: 0.5716\n",
      "13/15, train_loss: 0.7228\n",
      "14/15, train_loss: 0.6245\n",
      "15/15, train_loss: 0.7700\n",
      "16/15, train_loss: 0.6099\n",
      "epoch 85 average loss: 1.1229\n",
      "----------\n",
      "epoch 86/500\n",
      "1/15, train_loss: 1.8592\n",
      "2/15, train_loss: 1.9635\n",
      "3/15, train_loss: 0.8306\n",
      "4/15, train_loss: 0.9132\n",
      "5/15, train_loss: 1.8437\n",
      "6/15, train_loss: 1.8043\n",
      "7/15, train_loss: 2.0353\n",
      "8/15, train_loss: 0.6183\n",
      "9/15, train_loss: 0.8025\n",
      "10/15, train_loss: 0.8103\n",
      "11/15, train_loss: 1.0769\n",
      "12/15, train_loss: 0.6015\n",
      "13/15, train_loss: 0.6359\n",
      "14/15, train_loss: 0.6569\n",
      "15/15, train_loss: 0.6747\n",
      "16/15, train_loss: 0.6017\n",
      "epoch 86 average loss: 1.1080\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 86 validation loss: 1.1201 dice_score: 0.7428 acc_metric: 0.6223 accuracy: 0.6223, f1score: 0.5492\n",
      " saved Best AUC: 0.6826 at epoch: 86\n",
      "----------\n",
      "epoch 87/500\n",
      "1/15, train_loss: 2.0291\n",
      "2/15, train_loss: 1.8218\n",
      "3/15, train_loss: 0.8900\n",
      "4/15, train_loss: 0.8452\n",
      "5/15, train_loss: 1.8430\n",
      "6/15, train_loss: 1.9423\n",
      "7/15, train_loss: 1.8102\n",
      "8/15, train_loss: 0.6672\n",
      "9/15, train_loss: 0.7114\n",
      "10/15, train_loss: 0.7844\n",
      "11/15, train_loss: 1.2363\n",
      "12/15, train_loss: 0.5598\n",
      "13/15, train_loss: 0.6835\n",
      "14/15, train_loss: 0.6121\n",
      "15/15, train_loss: 0.6638\n",
      "16/15, train_loss: 0.6321\n",
      "epoch 87 average loss: 1.1083\n",
      "----------\n",
      "epoch 88/500\n",
      "1/15, train_loss: 1.8952\n",
      "2/15, train_loss: 2.0227\n",
      "3/15, train_loss: 0.8219\n",
      "4/15, train_loss: 0.8323\n",
      "5/15, train_loss: 2.0214\n",
      "6/15, train_loss: 1.8238\n",
      "7/15, train_loss: 1.9629\n",
      "8/15, train_loss: 0.5997\n",
      "9/15, train_loss: 0.7035\n",
      "10/15, train_loss: 0.9413\n",
      "11/15, train_loss: 1.0835\n",
      "12/15, train_loss: 0.6735\n",
      "13/15, train_loss: 0.6162\n",
      "14/15, train_loss: 0.5996\n",
      "15/15, train_loss: 0.7330\n",
      "16/15, train_loss: 0.5754\n",
      "epoch 88 average loss: 1.1191\n",
      "----------\n",
      "epoch 89/500\n",
      "1/15, train_loss: 2.1084\n",
      "2/15, train_loss: 1.8456\n",
      "3/15, train_loss: 0.8118\n",
      "4/15, train_loss: 0.9938\n",
      "5/15, train_loss: 1.8612\n",
      "6/15, train_loss: 2.0356\n",
      "7/15, train_loss: 1.8207\n",
      "8/15, train_loss: 0.5878\n",
      "9/15, train_loss: 0.8463\n",
      "10/15, train_loss: 0.7612\n",
      "11/15, train_loss: 1.2525\n",
      "12/15, train_loss: 0.5448\n",
      "13/15, train_loss: 0.6037\n",
      "14/15, train_loss: 0.6827\n",
      "15/15, train_loss: 0.6448\n",
      "16/15, train_loss: 0.5986\n",
      "epoch 89 average loss: 1.1250\n",
      "----------\n",
      "epoch 90/500\n",
      "1/15, train_loss: 1.8883\n",
      "2/15, train_loss: 1.8247\n",
      "3/15, train_loss: 0.8922\n",
      "4/15, train_loss: 0.8115\n",
      "5/15, train_loss: 1.9820\n",
      "6/15, train_loss: 1.8046\n",
      "7/15, train_loss: 1.7976\n",
      "8/15, train_loss: 0.6812\n",
      "9/15, train_loss: 0.6855\n",
      "10/15, train_loss: 0.8945\n",
      "11/15, train_loss: 1.0741\n",
      "12/15, train_loss: 0.5307\n",
      "13/15, train_loss: 0.6391\n",
      "14/15, train_loss: 0.5799\n",
      "15/15, train_loss: 0.7070\n",
      "16/15, train_loss: 0.5589\n",
      "epoch 90 average loss: 1.0845\n",
      "----------\n",
      "epoch 91/500\n",
      "1/15, train_loss: 1.8406\n",
      "2/15, train_loss: 2.0405\n",
      "3/15, train_loss: 0.7840\n",
      "4/15, train_loss: 0.9629\n",
      "5/15, train_loss: 1.8172\n",
      "6/15, train_loss: 1.8068\n",
      "7/15, train_loss: 1.9008\n",
      "8/15, train_loss: 0.5753\n",
      "9/15, train_loss: 0.7617\n",
      "10/15, train_loss: 0.7564\n",
      "11/15, train_loss: 1.0157\n",
      "12/15, train_loss: 0.5748\n",
      "13/15, train_loss: 0.5902\n",
      "14/15, train_loss: 0.6101\n",
      "15/15, train_loss: 0.6329\n",
      "16/15, train_loss: 0.5538\n",
      "epoch 91 average loss: 1.0765\n",
      "----------\n",
      "epoch 92/500\n",
      "1/15, train_loss: 1.9918\n",
      "2/15, train_loss: 1.8292\n",
      "3/15, train_loss: 0.8319\n",
      "4/15, train_loss: 0.8070\n",
      "5/15, train_loss: 1.8142\n",
      "6/15, train_loss: 1.9436\n",
      "7/15, train_loss: 1.7774\n",
      "8/15, train_loss: 0.6173\n",
      "9/15, train_loss: 0.6649\n",
      "10/15, train_loss: 0.7357\n",
      "11/15, train_loss: 1.1874\n",
      "12/15, train_loss: 0.5160\n",
      "13/15, train_loss: 0.6447\n",
      "14/15, train_loss: 0.5653\n",
      "15/15, train_loss: 0.6232\n",
      "16/15, train_loss: 0.5830\n",
      "epoch 92 average loss: 1.0708\n",
      "----------\n",
      "epoch 93/500\n",
      "1/15, train_loss: 1.8314\n",
      "2/15, train_loss: 1.9870\n",
      "3/15, train_loss: 0.7574\n",
      "4/15, train_loss: 0.7844\n",
      "5/15, train_loss: 2.0247\n",
      "6/15, train_loss: 1.7861\n",
      "7/15, train_loss: 1.9761\n",
      "8/15, train_loss: 0.5544\n",
      "9/15, train_loss: 0.6500\n",
      "10/15, train_loss: 0.7875\n",
      "11/15, train_loss: 1.0090\n",
      "12/15, train_loss: 0.5502\n",
      "13/15, train_loss: 0.5840\n",
      "14/15, train_loss: 0.5576\n",
      "15/15, train_loss: 0.6948\n",
      "16/15, train_loss: 0.5395\n",
      "epoch 93 average loss: 1.0671\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 93 validation loss: 1.0812 dice_score: 0.7634 acc_metric: 0.6146 accuracy: 0.6146, f1score: 0.5984\n",
      " saved Best AUC: 0.6890 at epoch: 93\n",
      "----------\n",
      "epoch 94/500\n",
      "1/15, train_loss: 1.9985\n",
      "2/15, train_loss: 1.8428\n",
      "3/15, train_loss: 0.7395\n",
      "4/15, train_loss: 0.8489\n",
      "5/15, train_loss: 1.7937\n",
      "6/15, train_loss: 1.9258\n",
      "7/15, train_loss: 1.7614\n",
      "8/15, train_loss: 0.5422\n",
      "9/15, train_loss: 0.7016\n",
      "10/15, train_loss: 0.7195\n",
      "11/15, train_loss: 1.1564\n",
      "12/15, train_loss: 0.5025\n",
      "13/15, train_loss: 0.5699\n",
      "14/15, train_loss: 0.5914\n",
      "15/15, train_loss: 0.6073\n",
      "16/15, train_loss: 0.5753\n",
      "epoch 94 average loss: 1.0548\n",
      "----------\n",
      "epoch 95/500\n",
      "1/15, train_loss: 1.8789\n",
      "2/15, train_loss: 1.8132\n",
      "3/15, train_loss: 0.8023\n",
      "4/15, train_loss: 0.7656\n",
      "5/15, train_loss: 1.9878\n",
      "6/15, train_loss: 1.8019\n",
      "7/15, train_loss: 1.7762\n",
      "8/15, train_loss: 0.6076\n",
      "9/15, train_loss: 0.6350\n",
      "10/15, train_loss: 0.8578\n",
      "11/15, train_loss: 1.0301\n",
      "12/15, train_loss: 0.4964\n",
      "13/15, train_loss: 0.6167\n",
      "14/15, train_loss: 0.5453\n",
      "15/15, train_loss: 0.6603\n",
      "16/15, train_loss: 0.5184\n",
      "epoch 95 average loss: 1.0496\n",
      "----------\n",
      "epoch 96/500\n",
      "1/15, train_loss: 1.8546\n",
      "2/15, train_loss: 2.0477\n",
      "3/15, train_loss: 0.7329\n",
      "4/15, train_loss: 0.8950\n",
      "5/15, train_loss: 1.8162\n",
      "6/15, train_loss: 1.7865\n",
      "7/15, train_loss: 1.9496\n",
      "8/15, train_loss: 0.5296\n",
      "9/15, train_loss: 0.7376\n",
      "10/15, train_loss: 0.7161\n",
      "11/15, train_loss: 0.9937\n",
      "12/15, train_loss: 0.5909\n",
      "13/15, train_loss: 0.5566\n",
      "14/15, train_loss: 0.6052\n",
      "15/15, train_loss: 0.5885\n",
      "16/15, train_loss: 0.5095\n",
      "epoch 96 average loss: 1.0569\n",
      "----------\n",
      "epoch 97/500\n",
      "1/15, train_loss: 2.0602\n",
      "2/15, train_loss: 1.7975\n",
      "3/15, train_loss: 0.7882\n",
      "4/15, train_loss: 0.7662\n",
      "5/15, train_loss: 1.7911\n",
      "6/15, train_loss: 1.9825\n",
      "7/15, train_loss: 1.7437\n",
      "8/15, train_loss: 0.5763\n",
      "9/15, train_loss: 0.6192\n",
      "10/15, train_loss: 0.6874\n",
      "11/15, train_loss: 1.1286\n",
      "12/15, train_loss: 0.4865\n",
      "13/15, train_loss: 0.5721\n",
      "14/15, train_loss: 0.5305\n",
      "15/15, train_loss: 0.5747\n",
      "16/15, train_loss: 0.5177\n",
      "epoch 97 average loss: 1.0389\n",
      "----------\n",
      "epoch 98/500\n",
      "1/15, train_loss: 1.8399\n",
      "2/15, train_loss: 1.9443\n",
      "3/15, train_loss: 0.7191\n",
      "4/15, train_loss: 0.7358\n",
      "5/15, train_loss: 1.9337\n",
      "6/15, train_loss: 1.7618\n",
      "7/15, train_loss: 1.8586\n",
      "8/15, train_loss: 0.5158\n",
      "9/15, train_loss: 0.6031\n",
      "10/15, train_loss: 0.7824\n",
      "11/15, train_loss: 0.9673\n",
      "12/15, train_loss: 0.5357\n",
      "13/15, train_loss: 0.5416\n",
      "14/15, train_loss: 0.5224\n",
      "15/15, train_loss: 0.6202\n",
      "16/15, train_loss: 0.4942\n",
      "epoch 98 average loss: 1.0235\n",
      "----------\n",
      "epoch 99/500\n",
      "1/15, train_loss: 2.0399\n",
      "2/15, train_loss: 1.7770\n",
      "3/15, train_loss: 0.7025\n",
      "4/15, train_loss: 0.8284\n",
      "5/15, train_loss: 1.7987\n",
      "6/15, train_loss: 1.9052\n",
      "7/15, train_loss: 1.7543\n",
      "8/15, train_loss: 0.5089\n",
      "9/15, train_loss: 0.6653\n",
      "10/15, train_loss: 0.6854\n",
      "11/15, train_loss: 1.1293\n",
      "12/15, train_loss: 0.4753\n",
      "13/15, train_loss: 0.5346\n",
      "14/15, train_loss: 0.5793\n",
      "15/15, train_loss: 0.5689\n",
      "16/15, train_loss: 0.5374\n",
      "epoch 99 average loss: 1.0307\n",
      "----------\n",
      "epoch 100/500\n",
      "1/15, train_loss: 1.8247\n",
      "2/15, train_loss: 1.7649\n",
      "3/15, train_loss: 0.7888\n",
      "4/15, train_loss: 0.7285\n",
      "5/15, train_loss: 2.0152\n",
      "6/15, train_loss: 1.7581\n",
      "7/15, train_loss: 1.7472\n",
      "8/15, train_loss: 0.6416\n",
      "9/15, train_loss: 0.6005\n",
      "10/15, train_loss: 0.7974\n",
      "11/15, train_loss: 0.9959\n",
      "12/15, train_loss: 0.4678\n",
      "13/15, train_loss: 0.5961\n",
      "14/15, train_loss: 0.5156\n",
      "15/15, train_loss: 0.6411\n",
      "16/15, train_loss: 0.4859\n",
      "epoch 100 average loss: 1.0231\n",
      "----------\n",
      "epoch 101/500\n",
      "1/15, train_loss: 1.7796\n",
      "2/15, train_loss: 1.9220\n",
      "3/15, train_loss: 0.6933\n",
      "4/15, train_loss: 0.8721\n",
      "5/15, train_loss: 1.7599\n",
      "6/15, train_loss: 1.7493\n",
      "7/15, train_loss: 1.8907\n",
      "8/15, train_loss: 0.5020\n",
      "9/15, train_loss: 0.7102\n",
      "10/15, train_loss: 0.6686\n",
      "11/15, train_loss: 0.9421\n",
      "12/15, train_loss: 0.5336\n",
      "13/15, train_loss: 0.5280\n",
      "14/15, train_loss: 0.5613\n",
      "15/15, train_loss: 0.5603\n",
      "16/15, train_loss: 0.4794\n",
      "epoch 101 average loss: 1.0095\n",
      "----------\n",
      "epoch 102/500\n",
      "1/15, train_loss: 1.9877\n",
      "2/15, train_loss: 1.7524\n",
      "3/15, train_loss: 0.7500\n",
      "4/15, train_loss: 0.7150\n",
      "5/15, train_loss: 1.7460\n",
      "6/15, train_loss: 1.9442\n",
      "7/15, train_loss: 1.7212\n",
      "8/15, train_loss: 0.5721\n",
      "9/15, train_loss: 0.5906\n",
      "10/15, train_loss: 0.6452\n",
      "11/15, train_loss: 1.1826\n",
      "12/15, train_loss: 0.4633\n",
      "13/15, train_loss: 0.5801\n",
      "14/15, train_loss: 0.5031\n",
      "15/15, train_loss: 0.5488\n",
      "16/15, train_loss: 0.5239\n",
      "epoch 102 average loss: 1.0141\n",
      "----------\n",
      "epoch 103/500\n",
      "1/15, train_loss: 1.7817\n",
      "2/15, train_loss: 1.9732\n",
      "3/15, train_loss: 0.6755\n",
      "4/15, train_loss: 0.6985\n",
      "5/15, train_loss: 1.8888\n",
      "6/15, train_loss: 1.7409\n",
      "7/15, train_loss: 1.8616\n",
      "8/15, train_loss: 0.4889\n",
      "9/15, train_loss: 0.5706\n",
      "10/15, train_loss: 0.7693\n",
      "11/15, train_loss: 0.9399\n",
      "12/15, train_loss: 0.5403\n",
      "13/15, train_loss: 0.5133\n",
      "14/15, train_loss: 0.4961\n",
      "15/15, train_loss: 0.6084\n",
      "16/15, train_loss: 0.4710\n",
      "epoch 103 average loss: 1.0011\n",
      "----------\n",
      "epoch 104/500\n",
      "1/15, train_loss: 1.9473\n",
      "2/15, train_loss: 1.7801\n",
      "3/15, train_loss: 0.6618\n",
      "4/15, train_loss: 0.8265\n",
      "5/15, train_loss: 1.7353\n",
      "6/15, train_loss: 1.9195\n",
      "7/15, train_loss: 1.7152\n",
      "8/15, train_loss: 0.4767\n",
      "9/15, train_loss: 0.6221\n",
      "10/15, train_loss: 0.6371\n",
      "11/15, train_loss: 1.0797\n",
      "12/15, train_loss: 0.4592\n",
      "13/15, train_loss: 0.5011\n",
      "14/15, train_loss: 0.5469\n",
      "15/15, train_loss: 0.5283\n",
      "16/15, train_loss: 0.4849\n",
      "epoch 104 average loss: 0.9951\n",
      "----------\n",
      "epoch 105/500\n",
      "1/15, train_loss: 1.7847\n",
      "2/15, train_loss: 1.7328\n",
      "3/15, train_loss: 0.7259\n",
      "4/15, train_loss: 0.6898\n",
      "5/15, train_loss: 1.8625\n",
      "6/15, train_loss: 1.7338\n",
      "7/15, train_loss: 1.7027\n",
      "8/15, train_loss: 0.5111\n",
      "9/15, train_loss: 0.5589\n",
      "10/15, train_loss: 0.7166\n",
      "11/15, train_loss: 0.9112\n",
      "12/15, train_loss: 0.4426\n",
      "13/15, train_loss: 0.5565\n",
      "14/15, train_loss: 0.4863\n",
      "15/15, train_loss: 0.5827\n",
      "16/15, train_loss: 0.4586\n",
      "epoch 105 average loss: 0.9660\n",
      "----------\n",
      "epoch 106/500\n",
      "1/15, train_loss: 1.7635\n",
      "2/15, train_loss: 1.9289\n",
      "3/15, train_loss: 0.6533\n",
      "4/15, train_loss: 0.7706\n",
      "5/15, train_loss: 1.7238\n",
      "6/15, train_loss: 1.7249\n",
      "7/15, train_loss: 1.8511\n",
      "8/15, train_loss: 0.4681\n",
      "9/15, train_loss: 0.6124\n",
      "10/15, train_loss: 0.6365\n",
      "11/15, train_loss: 0.9089\n",
      "12/15, train_loss: 0.4825\n",
      "13/15, train_loss: 0.5011\n",
      "14/15, train_loss: 0.5252\n",
      "15/15, train_loss: 0.5302\n",
      "16/15, train_loss: 0.4553\n",
      "epoch 106 average loss: 0.9710\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 106 validation loss: 1.0608 dice_score: 0.7729 acc_metric: 0.6176 accuracy: 0.6176, f1score: 0.6393\n",
      " saved Best AUC: 0.6952 at epoch: 106\n",
      "----------\n",
      "epoch 107/500\n",
      "1/15, train_loss: 2.0252\n",
      "2/15, train_loss: 1.7557\n",
      "3/15, train_loss: 0.7174\n",
      "4/15, train_loss: 0.6875\n",
      "5/15, train_loss: 1.7151\n",
      "6/15, train_loss: 1.9094\n",
      "7/15, train_loss: 1.6925\n",
      "8/15, train_loss: 0.5628\n",
      "9/15, train_loss: 0.5536\n",
      "10/15, train_loss: 0.6125\n",
      "11/15, train_loss: 1.1354\n",
      "12/15, train_loss: 0.4352\n",
      "13/15, train_loss: 0.5630\n",
      "14/15, train_loss: 0.4822\n",
      "15/15, train_loss: 0.5228\n",
      "16/15, train_loss: 0.5080\n",
      "epoch 107 average loss: 0.9924\n",
      "----------\n",
      "epoch 108/500\n",
      "1/15, train_loss: 1.7522\n",
      "2/15, train_loss: 1.9577\n",
      "3/15, train_loss: 0.6389\n",
      "4/15, train_loss: 0.6675\n",
      "5/15, train_loss: 1.9168\n",
      "6/15, train_loss: 1.7190\n",
      "7/15, train_loss: 1.8830\n",
      "8/15, train_loss: 0.4637\n",
      "9/15, train_loss: 0.5408\n",
      "10/15, train_loss: 0.6985\n",
      "11/15, train_loss: 0.8936\n",
      "12/15, train_loss: 0.4760\n",
      "13/15, train_loss: 0.5007\n",
      "14/15, train_loss: 0.4742\n",
      "15/15, train_loss: 0.6161\n",
      "16/15, train_loss: 0.4475\n",
      "epoch 108 average loss: 0.9779\n",
      "----------\n",
      "epoch 109/500\n",
      "1/15, train_loss: 1.9678\n",
      "2/15, train_loss: 1.7517\n",
      "3/15, train_loss: 0.6337\n",
      "4/15, train_loss: 0.7686\n",
      "5/15, train_loss: 1.6966\n",
      "6/15, train_loss: 1.9146\n",
      "7/15, train_loss: 1.6940\n",
      "8/15, train_loss: 0.4540\n",
      "9/15, train_loss: 0.6596\n",
      "10/15, train_loss: 0.6107\n",
      "11/15, train_loss: 1.1124\n",
      "12/15, train_loss: 0.4349\n",
      "13/15, train_loss: 0.4902\n",
      "14/15, train_loss: 0.5277\n",
      "15/15, train_loss: 0.5194\n",
      "16/15, train_loss: 0.4781\n",
      "epoch 109 average loss: 0.9821\n",
      "----------\n",
      "epoch 110/500\n",
      "1/15, train_loss: 1.7425\n",
      "2/15, train_loss: 1.7080\n",
      "3/15, train_loss: 0.6955\n",
      "4/15, train_loss: 0.6522\n",
      "5/15, train_loss: 1.7910\n",
      "6/15, train_loss: 1.7134\n",
      "7/15, train_loss: 1.6682\n",
      "8/15, train_loss: 0.5074\n",
      "9/15, train_loss: 0.5420\n",
      "10/15, train_loss: 0.6984\n",
      "11/15, train_loss: 0.9309\n",
      "12/15, train_loss: 0.4274\n",
      "13/15, train_loss: 0.5661\n",
      "14/15, train_loss: 0.4670\n",
      "15/15, train_loss: 0.5871\n",
      "16/15, train_loss: 0.4375\n",
      "epoch 110 average loss: 0.9459\n",
      "----------\n",
      "epoch 111/500\n",
      "1/15, train_loss: 1.7152\n",
      "2/15, train_loss: 1.9555\n",
      "3/15, train_loss: 0.6214\n",
      "4/15, train_loss: 0.7517\n",
      "5/15, train_loss: 1.6916\n",
      "6/15, train_loss: 1.6923\n",
      "7/15, train_loss: 1.7724\n",
      "8/15, train_loss: 0.4511\n",
      "9/15, train_loss: 0.6172\n",
      "10/15, train_loss: 0.6084\n",
      "11/15, train_loss: 0.8834\n",
      "12/15, train_loss: 0.4730\n",
      "13/15, train_loss: 0.4819\n",
      "14/15, train_loss: 0.5045\n",
      "15/15, train_loss: 0.5110\n",
      "16/15, train_loss: 0.4310\n",
      "epoch 111 average loss: 0.9476\n",
      "----------\n",
      "epoch 112/500\n",
      "1/15, train_loss: 1.8611\n",
      "2/15, train_loss: 1.6950\n",
      "3/15, train_loss: 0.6858\n",
      "4/15, train_loss: 0.6410\n",
      "5/15, train_loss: 1.6842\n",
      "6/15, train_loss: 1.8228\n",
      "7/15, train_loss: 1.6899\n",
      "8/15, train_loss: 0.4769\n",
      "9/15, train_loss: 0.5297\n",
      "10/15, train_loss: 0.5891\n",
      "11/15, train_loss: 1.0912\n",
      "12/15, train_loss: 0.4141\n",
      "13/15, train_loss: 0.5162\n",
      "14/15, train_loss: 0.4569\n",
      "15/15, train_loss: 0.4962\n",
      "16/15, train_loss: 0.4579\n",
      "epoch 112 average loss: 0.9442\n",
      "----------\n",
      "epoch 113/500\n",
      "1/15, train_loss: 1.7391\n",
      "2/15, train_loss: 1.9208\n",
      "3/15, train_loss: 0.6082\n",
      "4/15, train_loss: 0.6258\n",
      "5/15, train_loss: 1.8548\n",
      "6/15, train_loss: 1.6841\n",
      "7/15, train_loss: 1.9000\n",
      "8/15, train_loss: 0.4382\n",
      "9/15, train_loss: 0.5142\n",
      "10/15, train_loss: 0.6798\n",
      "11/15, train_loss: 0.8742\n",
      "12/15, train_loss: 0.4642\n",
      "13/15, train_loss: 0.4754\n",
      "14/15, train_loss: 0.4522\n",
      "15/15, train_loss: 0.5773\n",
      "16/15, train_loss: 0.4270\n",
      "epoch 113 average loss: 0.9522\n",
      "----------\n",
      "epoch 114/500\n",
      "1/15, train_loss: 1.9879\n",
      "2/15, train_loss: 1.7224\n",
      "3/15, train_loss: 0.6020\n",
      "4/15, train_loss: 0.7147\n",
      "5/15, train_loss: 1.6714\n",
      "6/15, train_loss: 1.8306\n",
      "7/15, train_loss: 1.6630\n",
      "8/15, train_loss: 0.4324\n",
      "9/15, train_loss: 0.6092\n",
      "10/15, train_loss: 0.5802\n",
      "11/15, train_loss: 1.0656\n",
      "12/15, train_loss: 0.4095\n",
      "13/15, train_loss: 0.4677\n",
      "14/15, train_loss: 0.5219\n",
      "15/15, train_loss: 0.4990\n",
      "16/15, train_loss: 0.4998\n",
      "epoch 114 average loss: 0.9548\n",
      "----------\n",
      "epoch 115/500\n",
      "1/15, train_loss: 1.7206\n",
      "2/15, train_loss: 1.6856\n",
      "3/15, train_loss: 0.6806\n",
      "4/15, train_loss: 0.6245\n",
      "5/15, train_loss: 1.8166\n",
      "6/15, train_loss: 1.6764\n",
      "7/15, train_loss: 1.6550\n",
      "8/15, train_loss: 0.5223\n",
      "9/15, train_loss: 0.5105\n",
      "10/15, train_loss: 0.7239\n",
      "11/15, train_loss: 0.8809\n",
      "12/15, train_loss: 0.4041\n",
      "13/15, train_loss: 0.5229\n",
      "14/15, train_loss: 0.4480\n",
      "15/15, train_loss: 0.5719\n",
      "16/15, train_loss: 0.4159\n",
      "epoch 115 average loss: 0.9287\n",
      "----------\n",
      "epoch 116/500\n",
      "1/15, train_loss: 1.7139\n",
      "2/15, train_loss: 1.8787\n",
      "3/15, train_loss: 0.6030\n",
      "4/15, train_loss: 0.7371\n",
      "5/15, train_loss: 1.6729\n",
      "6/15, train_loss: 1.6531\n",
      "7/15, train_loss: 1.8492\n",
      "8/15, train_loss: 0.4235\n",
      "9/15, train_loss: 0.6268\n",
      "10/15, train_loss: 0.5718\n",
      "11/15, train_loss: 0.8377\n",
      "12/15, train_loss: 0.4454\n",
      "13/15, train_loss: 0.4593\n",
      "14/15, train_loss: 0.4946\n",
      "15/15, train_loss: 0.4866\n",
      "16/15, train_loss: 0.4084\n",
      "epoch 116 average loss: 0.9289\n",
      "----------\n",
      "epoch 117/500\n",
      "1/15, train_loss: 1.8984\n",
      "2/15, train_loss: 1.6761\n",
      "3/15, train_loss: 0.6597\n",
      "4/15, train_loss: 0.6225\n",
      "5/15, train_loss: 1.6563\n",
      "6/15, train_loss: 1.8760\n",
      "7/15, train_loss: 1.6353\n",
      "8/15, train_loss: 0.4706\n",
      "9/15, train_loss: 0.5035\n",
      "10/15, train_loss: 0.5542\n",
      "11/15, train_loss: 0.9665\n",
      "12/15, train_loss: 0.3982\n",
      "13/15, train_loss: 0.4862\n",
      "14/15, train_loss: 0.4409\n",
      "15/15, train_loss: 0.4693\n",
      "16/15, train_loss: 0.4281\n",
      "epoch 117 average loss: 0.9214\n",
      "----------\n",
      "epoch 118/500\n",
      "1/15, train_loss: 1.7136\n",
      "2/15, train_loss: 1.8210\n",
      "3/15, train_loss: 0.5895\n",
      "4/15, train_loss: 0.6051\n",
      "5/15, train_loss: 1.7648\n",
      "6/15, train_loss: 1.6642\n",
      "7/15, train_loss: 1.7385\n",
      "8/15, train_loss: 0.4178\n",
      "9/15, train_loss: 0.4925\n",
      "10/15, train_loss: 0.6777\n",
      "11/15, train_loss: 0.8201\n",
      "12/15, train_loss: 0.4604\n",
      "13/15, train_loss: 0.4522\n",
      "14/15, train_loss: 0.4361\n",
      "15/15, train_loss: 0.5254\n",
      "16/15, train_loss: 0.4027\n",
      "epoch 118 average loss: 0.9113\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 118 validation loss: 1.0509 dice_score: 0.7808 acc_metric: 0.6357 accuracy: 0.6357, f1score: 0.6557\n",
      " saved Best AUC: 0.7083 at epoch: 118\n",
      "----------\n",
      "epoch 119/500\n",
      "1/15, train_loss: 1.8712\n",
      "2/15, train_loss: 1.6924\n",
      "3/15, train_loss: 0.5771\n",
      "4/15, train_loss: 0.7087\n",
      "5/15, train_loss: 1.6473\n",
      "6/15, train_loss: 1.8060\n",
      "7/15, train_loss: 1.6364\n",
      "8/15, train_loss: 0.4139\n",
      "9/15, train_loss: 0.5488\n",
      "10/15, train_loss: 0.5635\n",
      "11/15, train_loss: 0.9821\n",
      "12/15, train_loss: 0.3917\n",
      "13/15, train_loss: 0.4444\n",
      "14/15, train_loss: 0.4874\n",
      "15/15, train_loss: 0.4656\n",
      "16/15, train_loss: 0.4575\n",
      "epoch 119 average loss: 0.9184\n",
      "----------\n",
      "epoch 120/500\n",
      "1/15, train_loss: 1.6905\n",
      "2/15, train_loss: 1.6595\n",
      "3/15, train_loss: 0.6593\n",
      "4/15, train_loss: 0.6035\n",
      "5/15, train_loss: 1.8380\n",
      "6/15, train_loss: 1.6524\n",
      "7/15, train_loss: 1.6249\n",
      "8/15, train_loss: 0.4883\n",
      "9/15, train_loss: 0.4894\n",
      "10/15, train_loss: 0.6414\n",
      "11/15, train_loss: 0.8364\n",
      "12/15, train_loss: 0.3870\n",
      "13/15, train_loss: 0.5051\n",
      "14/15, train_loss: 0.4304\n",
      "15/15, train_loss: 0.5287\n",
      "16/15, train_loss: 0.3966\n",
      "epoch 120 average loss: 0.9020\n",
      "----------\n",
      "epoch 121/500\n",
      "1/15, train_loss: 1.6590\n",
      "2/15, train_loss: 1.8472\n",
      "3/15, train_loss: 0.5722\n",
      "4/15, train_loss: 0.6984\n",
      "5/15, train_loss: 1.6349\n",
      "6/15, train_loss: 1.6438\n",
      "7/15, train_loss: 1.7852\n",
      "8/15, train_loss: 0.4089\n",
      "9/15, train_loss: 0.5945\n",
      "10/15, train_loss: 0.5444\n",
      "11/15, train_loss: 0.8026\n",
      "12/15, train_loss: 0.4211\n",
      "13/15, train_loss: 0.4462\n",
      "14/15, train_loss: 0.4656\n",
      "15/15, train_loss: 0.4658\n",
      "16/15, train_loss: 0.3902\n",
      "epoch 121 average loss: 0.8987\n",
      "----------\n",
      "epoch 122/500\n",
      "1/15, train_loss: 1.8416\n",
      "2/15, train_loss: 1.6443\n",
      "3/15, train_loss: 0.6268\n",
      "4/15, train_loss: 0.5955\n",
      "5/15, train_loss: 1.6262\n",
      "6/15, train_loss: 1.8775\n",
      "7/15, train_loss: 1.6190\n",
      "8/15, train_loss: 0.4607\n",
      "9/15, train_loss: 0.4906\n",
      "10/15, train_loss: 0.5280\n",
      "11/15, train_loss: 1.0968\n",
      "12/15, train_loss: 0.3806\n",
      "13/15, train_loss: 0.5162\n",
      "14/15, train_loss: 0.4213\n",
      "15/15, train_loss: 0.4561\n",
      "16/15, train_loss: 0.4194\n",
      "epoch 122 average loss: 0.9125\n",
      "----------\n",
      "epoch 123/500\n",
      "1/15, train_loss: 1.6591\n",
      "2/15, train_loss: 1.8663\n",
      "3/15, train_loss: 0.5594\n",
      "4/15, train_loss: 0.5806\n",
      "5/15, train_loss: 1.7352\n",
      "6/15, train_loss: 1.6290\n",
      "7/15, train_loss: 1.7204\n",
      "8/15, train_loss: 0.4005\n",
      "9/15, train_loss: 0.4758\n",
      "10/15, train_loss: 0.6379\n",
      "11/15, train_loss: 0.7827\n",
      "12/15, train_loss: 0.4433\n",
      "13/15, train_loss: 0.4316\n",
      "14/15, train_loss: 0.4177\n",
      "15/15, train_loss: 0.5270\n",
      "16/15, train_loss: 0.3857\n",
      "epoch 123 average loss: 0.8908\n",
      "----------\n",
      "epoch 124/500\n",
      "1/15, train_loss: 1.7642\n",
      "2/15, train_loss: 1.6622\n",
      "3/15, train_loss: 0.5489\n",
      "4/15, train_loss: 0.6735\n",
      "5/15, train_loss: 1.6167\n",
      "6/15, train_loss: 1.8003\n",
      "7/15, train_loss: 1.6346\n",
      "8/15, train_loss: 0.3934\n",
      "9/15, train_loss: 0.5304\n",
      "10/15, train_loss: 0.5320\n",
      "11/15, train_loss: 0.9709\n",
      "12/15, train_loss: 0.3775\n",
      "13/15, train_loss: 0.4253\n",
      "14/15, train_loss: 0.4519\n",
      "15/15, train_loss: 0.4481\n",
      "16/15, train_loss: 0.4110\n",
      "epoch 124 average loss: 0.8901\n",
      "----------\n",
      "epoch 125/500\n",
      "1/15, train_loss: 1.6714\n",
      "2/15, train_loss: 1.6214\n",
      "3/15, train_loss: 0.6166\n",
      "4/15, train_loss: 0.5764\n",
      "5/15, train_loss: 1.7068\n",
      "6/15, train_loss: 1.6334\n",
      "7/15, train_loss: 1.6092\n",
      "8/15, train_loss: 0.4337\n",
      "9/15, train_loss: 0.4771\n",
      "10/15, train_loss: 0.6046\n",
      "11/15, train_loss: 0.7964\n",
      "12/15, train_loss: 0.3695\n",
      "13/15, train_loss: 0.5192\n",
      "14/15, train_loss: 0.4105\n",
      "15/15, train_loss: 0.5558\n",
      "16/15, train_loss: 0.3797\n",
      "epoch 125 average loss: 0.8739\n",
      "----------\n",
      "epoch 126/500\n",
      "\n",
      "** Ranger21 update = Warmup complete - lr set to 0.0001\n",
      "\n",
      "1/15, train_loss: 1.6347\n",
      "2/15, train_loss: 1.8274\n",
      "3/15, train_loss: 0.5500\n",
      "4/15, train_loss: 0.6485\n",
      "5/15, train_loss: 1.6016\n",
      "6/15, train_loss: 1.6056\n",
      "7/15, train_loss: 1.7213\n",
      "8/15, train_loss: 0.3888\n",
      "9/15, train_loss: 0.5577\n",
      "10/15, train_loss: 0.5171\n",
      "11/15, train_loss: 0.7528\n",
      "12/15, train_loss: 0.4074\n",
      "13/15, train_loss: 0.4267\n",
      "14/15, train_loss: 0.4425\n",
      "15/15, train_loss: 0.4536\n",
      "16/15, train_loss: 0.3766\n",
      "epoch 126 average loss: 0.8695\n",
      "----------\n",
      "epoch 127/500\n",
      "1/15, train_loss: 1.7827\n",
      "2/15, train_loss: 1.6157\n",
      "3/15, train_loss: 0.6065\n",
      "4/15, train_loss: 0.5742\n",
      "5/15, train_loss: 1.5929\n",
      "6/15, train_loss: 1.7437\n",
      "7/15, train_loss: 1.5781\n",
      "8/15, train_loss: 0.4425\n",
      "9/15, train_loss: 0.4747\n",
      "10/15, train_loss: 0.5012\n",
      "11/15, train_loss: 0.9629\n",
      "12/15, train_loss: 0.3614\n",
      "13/15, train_loss: 0.4780\n",
      "14/15, train_loss: 0.4057\n",
      "15/15, train_loss: 0.4383\n",
      "16/15, train_loss: 0.4183\n",
      "epoch 127 average loss: 0.8736\n",
      "----------\n",
      "epoch 128/500\n",
      "1/15, train_loss: 1.6465\n",
      "2/15, train_loss: 1.7687\n",
      "3/15, train_loss: 0.5421\n",
      "4/15, train_loss: 0.5603\n",
      "5/15, train_loss: 1.7137\n",
      "6/15, train_loss: 1.6002\n",
      "7/15, train_loss: 1.7261\n",
      "8/15, train_loss: 0.3853\n",
      "9/15, train_loss: 0.4591\n",
      "10/15, train_loss: 0.6063\n",
      "11/15, train_loss: 0.7478\n",
      "12/15, train_loss: 0.4009\n",
      "13/15, train_loss: 0.4177\n",
      "14/15, train_loss: 0.4015\n",
      "15/15, train_loss: 0.4963\n",
      "16/15, train_loss: 0.3693\n",
      "epoch 128 average loss: 0.8651\n",
      "----------\n",
      "epoch 129/500\n",
      "1/15, train_loss: 1.8909\n",
      "2/15, train_loss: 1.6231\n",
      "3/15, train_loss: 0.5301\n",
      "4/15, train_loss: 0.6741\n",
      "5/15, train_loss: 1.5919\n",
      "6/15, train_loss: 1.7283\n",
      "7/15, train_loss: 1.5742\n",
      "8/15, train_loss: 0.3783\n",
      "9/15, train_loss: 0.5217\n",
      "10/15, train_loss: 0.4947\n",
      "11/15, train_loss: 0.8759\n",
      "12/15, train_loss: 0.3560\n",
      "13/15, train_loss: 0.4068\n",
      "14/15, train_loss: 0.4554\n",
      "15/15, train_loss: 0.4272\n",
      "16/15, train_loss: 0.3944\n",
      "epoch 129 average loss: 0.8702\n",
      "----------\n",
      "epoch 130/500\n",
      "1/15, train_loss: 1.6322\n",
      "2/15, train_loss: 1.5825\n",
      "3/15, train_loss: 0.6002\n",
      "4/15, train_loss: 0.5588\n",
      "5/15, train_loss: 1.6980\n",
      "6/15, train_loss: 1.5898\n",
      "7/15, train_loss: 1.5628\n",
      "8/15, train_loss: 0.4315\n",
      "9/15, train_loss: 0.4494\n",
      "10/15, train_loss: 0.6079\n",
      "11/15, train_loss: 0.7246\n",
      "12/15, train_loss: 0.3504\n",
      "13/15, train_loss: 0.4614\n",
      "14/15, train_loss: 0.3978\n",
      "15/15, train_loss: 0.4730\n",
      "16/15, train_loss: 0.3618\n",
      "epoch 130 average loss: 0.8426\n",
      "----------\n",
      "epoch 131/500\n",
      "1/15, train_loss: 1.5911\n",
      "2/15, train_loss: 1.7590\n",
      "3/15, train_loss: 0.5252\n",
      "4/15, train_loss: 0.6649\n",
      "5/15, train_loss: 1.5818\n",
      "6/15, train_loss: 1.5742\n",
      "7/15, train_loss: 1.7066\n",
      "8/15, train_loss: 0.3735\n",
      "9/15, train_loss: 0.4958\n",
      "10/15, train_loss: 0.4943\n",
      "11/15, train_loss: 0.7107\n",
      "12/15, train_loss: 0.4111\n",
      "13/15, train_loss: 0.4007\n",
      "14/15, train_loss: 0.4377\n",
      "15/15, train_loss: 0.4205\n",
      "16/15, train_loss: 0.3585\n",
      "epoch 131 average loss: 0.8441\n",
      "----------\n",
      "epoch 132/500\n",
      "1/15, train_loss: 1.7736\n",
      "2/15, train_loss: 1.5765\n",
      "3/15, train_loss: 0.5904\n",
      "4/15, train_loss: 0.5486\n",
      "5/15, train_loss: 1.5727\n",
      "6/15, train_loss: 1.6967\n",
      "7/15, train_loss: 1.5532\n",
      "8/15, train_loss: 0.4293\n",
      "9/15, train_loss: 0.4404\n",
      "10/15, train_loss: 0.4815\n",
      "11/15, train_loss: 0.8729\n",
      "12/15, train_loss: 0.3465\n",
      "13/15, train_loss: 0.4288\n",
      "14/15, train_loss: 0.3951\n",
      "15/15, train_loss: 0.4156\n",
      "16/15, train_loss: 0.4008\n",
      "epoch 132 average loss: 0.8452\n",
      "----------\n",
      "epoch 133/500\n",
      "1/15, train_loss: 1.5813\n",
      "2/15, train_loss: 1.6823\n",
      "3/15, train_loss: 0.5212\n",
      "4/15, train_loss: 0.5353\n",
      "5/15, train_loss: 1.6979\n",
      "6/15, train_loss: 1.5604\n",
      "7/15, train_loss: 1.6881\n",
      "8/15, train_loss: 0.3691\n",
      "9/15, train_loss: 0.4400\n",
      "10/15, train_loss: 0.5706\n",
      "11/15, train_loss: 0.7096\n",
      "12/15, train_loss: 0.3834\n",
      "13/15, train_loss: 0.4105\n",
      "14/15, train_loss: 0.3889\n",
      "15/15, train_loss: 0.4991\n",
      "16/15, train_loss: 0.3612\n",
      "epoch 133 average loss: 0.8374\n",
      "----------\n",
      "epoch 134/500\n",
      "1/15, train_loss: 1.7354\n",
      "2/15, train_loss: 1.5745\n",
      "3/15, train_loss: 0.5149\n",
      "4/15, train_loss: 0.5951\n",
      "5/15, train_loss: 1.5604\n",
      "6/15, train_loss: 1.6857\n",
      "7/15, train_loss: 1.5554\n",
      "8/15, train_loss: 0.3672\n",
      "9/15, train_loss: 0.5462\n",
      "10/15, train_loss: 0.4822\n",
      "11/15, train_loss: 0.9377\n",
      "12/15, train_loss: 0.3472\n",
      "13/15, train_loss: 0.4034\n",
      "14/15, train_loss: 0.4355\n",
      "15/15, train_loss: 0.4192\n",
      "16/15, train_loss: 0.4036\n",
      "epoch 134 average loss: 0.8477\n",
      "----------\n",
      "epoch 135/500\n",
      "1/15, train_loss: 1.5879\n",
      "2/15, train_loss: 1.5675\n",
      "3/15, train_loss: 0.5795\n",
      "4/15, train_loss: 0.5344\n",
      "5/15, train_loss: 1.6520\n",
      "6/15, train_loss: 1.5769\n",
      "7/15, train_loss: 1.5427\n",
      "8/15, train_loss: 0.4540\n",
      "9/15, train_loss: 0.4362\n",
      "10/15, train_loss: 0.6012\n",
      "11/15, train_loss: 0.7489\n",
      "12/15, train_loss: 0.3440\n",
      "13/15, train_loss: 0.4651\n",
      "14/15, train_loss: 0.3853\n",
      "15/15, train_loss: 0.4925\n",
      "16/15, train_loss: 0.3532\n",
      "epoch 135 average loss: 0.8326\n",
      "----------\n",
      "epoch 136/500\n",
      "1/15, train_loss: 1.5583\n",
      "2/15, train_loss: 1.9076\n",
      "3/15, train_loss: 0.5027\n",
      "4/15, train_loss: 0.6821\n",
      "5/15, train_loss: 1.5471\n",
      "6/15, train_loss: 1.5618\n",
      "7/15, train_loss: 1.6319\n",
      "8/15, train_loss: 0.3638\n",
      "9/15, train_loss: 0.5200\n",
      "10/15, train_loss: 0.4762\n",
      "11/15, train_loss: 0.6893\n",
      "12/15, train_loss: 0.3925\n",
      "13/15, train_loss: 0.3924\n",
      "14/15, train_loss: 0.4207\n",
      "15/15, train_loss: 0.4147\n",
      "16/15, train_loss: 0.3477\n",
      "epoch 136 average loss: 0.8380\n",
      "----------\n",
      "epoch 137/500\n",
      "1/15, train_loss: 1.6990\n",
      "2/15, train_loss: 1.5492\n",
      "3/15, train_loss: 0.5530\n",
      "4/15, train_loss: 0.5285\n",
      "5/15, train_loss: 1.5436\n",
      "6/15, train_loss: 1.6861\n",
      "7/15, train_loss: 1.5372\n",
      "8/15, train_loss: 0.3959\n",
      "9/15, train_loss: 0.4390\n",
      "10/15, train_loss: 0.4633\n",
      "11/15, train_loss: 0.9523\n",
      "12/15, train_loss: 0.3394\n",
      "13/15, train_loss: 0.4348\n",
      "14/15, train_loss: 0.3827\n",
      "15/15, train_loss: 0.4068\n",
      "16/15, train_loss: 0.3797\n",
      "epoch 137 average loss: 0.8307\n",
      "----------\n",
      "epoch 138/500\n",
      "1/15, train_loss: 1.5571\n",
      "2/15, train_loss: 1.7508\n",
      "3/15, train_loss: 0.4993\n",
      "4/15, train_loss: 0.5191\n",
      "5/15, train_loss: 1.6263\n",
      "6/15, train_loss: 1.5493\n",
      "7/15, train_loss: 1.6571\n",
      "8/15, train_loss: 0.3586\n",
      "9/15, train_loss: 0.4275\n",
      "10/15, train_loss: 0.5400\n",
      "11/15, train_loss: 0.6783\n",
      "12/15, train_loss: 0.3858\n",
      "13/15, train_loss: 0.3929\n",
      "14/15, train_loss: 0.3791\n",
      "15/15, train_loss: 0.4871\n",
      "16/15, train_loss: 0.3429\n",
      "epoch 138 average loss: 0.8219\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 138 validation loss: 1.0482 dice_score: 0.7722 acc_metric: 0.6214 accuracy: 0.6214, f1score: 0.6148\n",
      " saved Best AUC: 0.6968 at epoch: 138\n",
      "----------\n",
      "epoch 139/500\n",
      "1/15, train_loss: 1.6764\n",
      "2/15, train_loss: 1.5542\n",
      "3/15, train_loss: 0.4963\n",
      "4/15, train_loss: 0.5969\n",
      "5/15, train_loss: 1.5353\n",
      "6/15, train_loss: 1.6429\n",
      "7/15, train_loss: 1.5201\n",
      "8/15, train_loss: 0.3521\n",
      "9/15, train_loss: 0.5116\n",
      "10/15, train_loss: 0.4551\n",
      "11/15, train_loss: 0.8731\n",
      "12/15, train_loss: 0.3312\n",
      "13/15, train_loss: 0.3840\n",
      "14/15, train_loss: 0.4206\n",
      "15/15, train_loss: 0.4012\n",
      "16/15, train_loss: 0.3833\n",
      "epoch 139 average loss: 0.8209\n",
      "----------\n",
      "epoch 140/500\n",
      "1/15, train_loss: 1.5492\n",
      "2/15, train_loss: 1.5225\n",
      "3/15, train_loss: 0.5521\n",
      "4/15, train_loss: 0.5128\n",
      "5/15, train_loss: 1.6236\n",
      "6/15, train_loss: 1.5317\n",
      "7/15, train_loss: 1.5104\n",
      "8/15, train_loss: 0.3995\n",
      "9/15, train_loss: 0.4209\n",
      "10/15, train_loss: 0.5185\n",
      "11/15, train_loss: 0.6861\n",
      "12/15, train_loss: 0.3241\n",
      "13/15, train_loss: 0.4429\n",
      "14/15, train_loss: 0.3717\n",
      "15/15, train_loss: 0.4536\n",
      "16/15, train_loss: 0.3376\n",
      "epoch 140 average loss: 0.7973\n",
      "----------\n",
      "epoch 141/500\n",
      "1/15, train_loss: 1.5327\n",
      "2/15, train_loss: 1.6410\n",
      "3/15, train_loss: 0.4870\n",
      "4/15, train_loss: 0.5732\n",
      "5/15, train_loss: 1.5284\n",
      "6/15, train_loss: 1.5174\n",
      "7/15, train_loss: 1.6329\n",
      "8/15, train_loss: 0.3473\n",
      "9/15, train_loss: 0.4754\n",
      "10/15, train_loss: 0.4465\n",
      "11/15, train_loss: 0.6399\n",
      "12/15, train_loss: 0.3543\n",
      "13/15, train_loss: 0.3792\n",
      "14/15, train_loss: 0.3992\n",
      "15/15, train_loss: 0.3997\n",
      "16/15, train_loss: 0.3354\n",
      "epoch 141 average loss: 0.7931\n",
      "----------\n",
      "epoch 142/500\n",
      "1/15, train_loss: 1.7291\n",
      "2/15, train_loss: 1.5192\n",
      "3/15, train_loss: 0.5328\n",
      "4/15, train_loss: 0.5142\n",
      "5/15, train_loss: 1.5190\n",
      "6/15, train_loss: 1.6520\n",
      "7/15, train_loss: 1.5007\n",
      "8/15, train_loss: 0.3875\n",
      "9/15, train_loss: 0.4109\n",
      "10/15, train_loss: 0.4389\n",
      "11/15, train_loss: 0.8129\n",
      "12/15, train_loss: 0.3211\n",
      "13/15, train_loss: 0.4142\n",
      "14/15, train_loss: 0.3686\n",
      "15/15, train_loss: 0.3895\n",
      "16/15, train_loss: 0.3691\n",
      "epoch 142 average loss: 0.8050\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 142 validation loss: 0.9986 dice_score: 0.7951 acc_metric: 0.5991 accuracy: 0.5991, f1score: 0.6967\n",
      " saved Best AUC: 0.6971 at epoch: 142\n",
      "----------\n",
      "epoch 143/500\n",
      "1/15, train_loss: 1.5357\n",
      "2/15, train_loss: 1.6257\n",
      "3/15, train_loss: 0.4855\n",
      "4/15, train_loss: 0.5017\n",
      "5/15, train_loss: 1.6033\n",
      "6/15, train_loss: 1.5080\n",
      "7/15, train_loss: 1.6009\n",
      "8/15, train_loss: 0.3470\n",
      "9/15, train_loss: 0.4090\n",
      "10/15, train_loss: 0.5401\n",
      "11/15, train_loss: 0.6329\n",
      "12/15, train_loss: 0.3775\n",
      "13/15, train_loss: 0.3756\n",
      "14/15, train_loss: 0.3669\n",
      "15/15, train_loss: 0.4579\n",
      "16/15, train_loss: 0.3343\n",
      "epoch 143 average loss: 0.7939\n",
      "----------\n",
      "epoch 144/500\n",
      "1/15, train_loss: 1.6214\n",
      "2/15, train_loss: 1.5158\n",
      "3/15, train_loss: 0.4784\n",
      "4/15, train_loss: 0.5748\n",
      "5/15, train_loss: 1.5111\n",
      "6/15, train_loss: 1.5933\n",
      "7/15, train_loss: 1.5011\n",
      "8/15, train_loss: 0.3422\n",
      "9/15, train_loss: 0.4824\n",
      "10/15, train_loss: 0.4399\n",
      "11/15, train_loss: 0.7757\n",
      "12/15, train_loss: 0.3205\n",
      "13/15, train_loss: 0.3716\n",
      "14/15, train_loss: 0.4024\n",
      "15/15, train_loss: 0.3884\n",
      "16/15, train_loss: 0.3689\n",
      "epoch 144 average loss: 0.7930\n",
      "----------\n",
      "epoch 145/500\n",
      "1/15, train_loss: 1.5227\n",
      "2/15, train_loss: 1.4978\n",
      "3/15, train_loss: 0.5457\n",
      "4/15, train_loss: 0.4954\n",
      "5/15, train_loss: 1.5980\n",
      "6/15, train_loss: 1.5038\n",
      "7/15, train_loss: 1.4909\n",
      "8/15, train_loss: 0.3940\n",
      "9/15, train_loss: 0.4108\n",
      "10/15, train_loss: 0.4855\n",
      "11/15, train_loss: 0.6462\n",
      "12/15, train_loss: 0.3150\n",
      "13/15, train_loss: 0.4545\n",
      "14/15, train_loss: 0.3579\n",
      "15/15, train_loss: 0.4787\n",
      "16/15, train_loss: 0.3251\n",
      "epoch 145 average loss: 0.7826\n",
      "----------\n",
      "epoch 146/500\n",
      "1/15, train_loss: 1.5004\n",
      "2/15, train_loss: 1.6928\n",
      "3/15, train_loss: 0.4705\n",
      "4/15, train_loss: 0.5692\n",
      "5/15, train_loss: 1.4932\n",
      "6/15, train_loss: 1.4964\n",
      "7/15, train_loss: 1.5814\n",
      "8/15, train_loss: 0.3359\n",
      "9/15, train_loss: 0.4745\n",
      "10/15, train_loss: 0.4274\n",
      "11/15, train_loss: 0.6185\n",
      "12/15, train_loss: 0.3393\n",
      "13/15, train_loss: 0.3759\n",
      "14/15, train_loss: 0.3786\n",
      "15/15, train_loss: 0.3928\n",
      "16/15, train_loss: 0.3225\n",
      "epoch 146 average loss: 0.7793\n",
      "----------\n",
      "epoch 147/500\n",
      "1/15, train_loss: 1.6045\n",
      "2/15, train_loss: 1.4961\n",
      "3/15, train_loss: 0.5244\n",
      "4/15, train_loss: 0.4887\n",
      "5/15, train_loss: 1.4908\n",
      "6/15, train_loss: 1.6374\n",
      "7/15, train_loss: 1.4807\n",
      "8/15, train_loss: 0.3735\n",
      "9/15, train_loss: 0.4011\n",
      "10/15, train_loss: 0.4182\n",
      "11/15, train_loss: 0.8564\n",
      "12/15, train_loss: 0.3087\n",
      "13/15, train_loss: 0.4253\n",
      "14/15, train_loss: 0.3540\n",
      "15/15, train_loss: 0.3781\n",
      "16/15, train_loss: 0.3491\n",
      "epoch 147 average loss: 0.7867\n",
      "----------\n",
      "epoch 148/500\n",
      "1/15, train_loss: 1.5023\n",
      "2/15, train_loss: 1.6883\n",
      "3/15, train_loss: 0.4638\n",
      "4/15, train_loss: 0.4809\n",
      "5/15, train_loss: 1.5571\n",
      "6/15, train_loss: 1.4954\n",
      "7/15, train_loss: 1.5638\n",
      "8/15, train_loss: 0.3321\n",
      "9/15, train_loss: 0.3980\n",
      "10/15, train_loss: 0.5050\n",
      "11/15, train_loss: 0.6126\n",
      "12/15, train_loss: 0.3512\n",
      "13/15, train_loss: 0.3622\n",
      "14/15, train_loss: 0.3515\n",
      "15/15, train_loss: 0.4227\n",
      "16/15, train_loss: 0.3215\n",
      "epoch 148 average loss: 0.7755\n",
      "----------\n",
      "epoch 149/500\n",
      "1/15, train_loss: 1.6094\n",
      "2/15, train_loss: 1.5004\n",
      "3/15, train_loss: 0.4584\n",
      "4/15, train_loss: 0.5461\n",
      "5/15, train_loss: 1.4884\n",
      "6/15, train_loss: 1.5567\n",
      "7/15, train_loss: 1.4715\n",
      "8/15, train_loss: 0.3284\n",
      "9/15, train_loss: 0.4504\n",
      "10/15, train_loss: 0.4185\n",
      "11/15, train_loss: 0.7421\n",
      "12/15, train_loss: 0.3097\n",
      "13/15, train_loss: 0.3552\n",
      "14/15, train_loss: 0.3968\n",
      "15/15, train_loss: 0.3699\n",
      "16/15, train_loss: 0.3526\n",
      "epoch 149 average loss: 0.7722\n",
      "----------\n",
      "epoch 150/500\n",
      "1/15, train_loss: 1.5038\n",
      "2/15, train_loss: 1.4714\n",
      "3/15, train_loss: 0.5238\n",
      "4/15, train_loss: 0.4784\n",
      "5/15, train_loss: 1.5596\n",
      "6/15, train_loss: 1.4800\n",
      "7/15, train_loss: 1.4662\n",
      "8/15, train_loss: 0.3813\n",
      "9/15, train_loss: 0.3926\n",
      "10/15, train_loss: 0.4888\n",
      "11/15, train_loss: 0.6130\n",
      "12/15, train_loss: 0.3040\n",
      "13/15, train_loss: 0.3950\n",
      "14/15, train_loss: 0.3500\n",
      "15/15, train_loss: 0.4138\n",
      "16/15, train_loss: 0.3180\n",
      "epoch 150 average loss: 0.7587\n",
      "----------\n",
      "epoch 151/500\n",
      "1/15, train_loss: 1.4842\n",
      "2/15, train_loss: 1.5752\n",
      "3/15, train_loss: 0.4592\n",
      "4/15, train_loss: 0.5274\n",
      "5/15, train_loss: 1.4784\n",
      "6/15, train_loss: 1.4650\n",
      "7/15, train_loss: 1.5774\n",
      "8/15, train_loss: 0.3245\n",
      "9/15, train_loss: 0.4601\n",
      "10/15, train_loss: 0.4121\n",
      "11/15, train_loss: 0.5970\n",
      "12/15, train_loss: 0.3338\n",
      "13/15, train_loss: 0.3539\n",
      "14/15, train_loss: 0.3800\n",
      "15/15, train_loss: 0.3684\n",
      "16/15, train_loss: 0.3170\n",
      "epoch 151 average loss: 0.7571\n",
      "----------\n",
      "epoch 152/500\n",
      "1/15, train_loss: 1.5932\n",
      "2/15, train_loss: 1.4581\n",
      "3/15, train_loss: 0.4967\n",
      "4/15, train_loss: 0.4755\n",
      "5/15, train_loss: 1.4671\n",
      "6/15, train_loss: 1.5434\n",
      "7/15, train_loss: 1.4475\n",
      "8/15, train_loss: 0.3678\n",
      "9/15, train_loss: 0.3888\n",
      "10/15, train_loss: 0.3998\n",
      "11/15, train_loss: 0.7428\n",
      "12/15, train_loss: 0.2972\n",
      "13/15, train_loss: 0.3883\n",
      "14/15, train_loss: 0.3454\n",
      "15/15, train_loss: 0.3646\n",
      "16/15, train_loss: 0.3535\n",
      "epoch 152 average loss: 0.7581\n",
      "----------\n",
      "epoch 153/500\n",
      "1/15, train_loss: 1.4709\n",
      "2/15, train_loss: 1.5220\n",
      "3/15, train_loss: 0.4528\n",
      "4/15, train_loss: 0.4630\n",
      "5/15, train_loss: 1.5397\n",
      "6/15, train_loss: 1.4531\n",
      "7/15, train_loss: 1.5276\n",
      "8/15, train_loss: 0.3218\n",
      "9/15, train_loss: 0.3799\n",
      "10/15, train_loss: 0.4742\n",
      "11/15, train_loss: 0.5866\n",
      "12/15, train_loss: 0.3284\n",
      "13/15, train_loss: 0.3534\n",
      "14/15, train_loss: 0.3417\n",
      "15/15, train_loss: 0.4202\n",
      "16/15, train_loss: 0.3136\n",
      "epoch 153 average loss: 0.7468\n",
      "----------\n",
      "epoch 154/500\n",
      "1/15, train_loss: 1.6121\n",
      "2/15, train_loss: 1.4559\n",
      "3/15, train_loss: 0.4440\n",
      "4/15, train_loss: 0.5186\n",
      "5/15, train_loss: 1.4677\n",
      "6/15, train_loss: 1.5269\n",
      "7/15, train_loss: 1.4482\n",
      "8/15, train_loss: 0.3171\n",
      "9/15, train_loss: 0.4356\n",
      "10/15, train_loss: 0.3988\n",
      "11/15, train_loss: 0.6969\n",
      "12/15, train_loss: 0.2949\n",
      "13/15, train_loss: 0.3472\n",
      "14/15, train_loss: 0.3821\n",
      "15/15, train_loss: 0.3599\n",
      "16/15, train_loss: 0.3409\n",
      "epoch 154 average loss: 0.7529\n",
      "----------\n",
      "epoch 155/500\n",
      "1/15, train_loss: 1.4787\n",
      "2/15, train_loss: 1.4399\n",
      "3/15, train_loss: 0.4950\n",
      "4/15, train_loss: 0.4592\n",
      "5/15, train_loss: 1.5390\n",
      "6/15, train_loss: 1.4495\n",
      "7/15, train_loss: 1.4376\n",
      "8/15, train_loss: 0.3621\n",
      "9/15, train_loss: 0.3800\n",
      "10/15, train_loss: 0.4603\n",
      "11/15, train_loss: 0.5660\n",
      "12/15, train_loss: 0.2930\n",
      "13/15, train_loss: 0.3895\n",
      "14/15, train_loss: 0.3389\n",
      "15/15, train_loss: 0.4063\n",
      "16/15, train_loss: 0.3061\n",
      "epoch 155 average loss: 0.7376\n",
      "----------\n",
      "epoch 156/500\n",
      "1/15, train_loss: 1.4566\n",
      "2/15, train_loss: 1.5234\n",
      "3/15, train_loss: 0.4392\n",
      "4/15, train_loss: 0.5358\n",
      "5/15, train_loss: 1.4605\n",
      "6/15, train_loss: 1.4430\n",
      "7/15, train_loss: 1.5151\n",
      "8/15, train_loss: 0.3152\n",
      "9/15, train_loss: 0.4355\n",
      "10/15, train_loss: 0.3955\n",
      "11/15, train_loss: 0.5552\n",
      "12/15, train_loss: 0.3347\n",
      "13/15, train_loss: 0.3435\n",
      "14/15, train_loss: 0.3747\n",
      "15/15, train_loss: 0.3542\n",
      "16/15, train_loss: 0.3030\n",
      "epoch 156 average loss: 0.7366\n",
      "----------\n",
      "epoch 157/500\n",
      "1/15, train_loss: 1.5343\n",
      "2/15, train_loss: 1.4440\n",
      "3/15, train_loss: 0.4889\n",
      "4/15, train_loss: 0.4677\n",
      "5/15, train_loss: 1.4525\n",
      "6/15, train_loss: 1.5602\n",
      "7/15, train_loss: 1.4311\n",
      "8/15, train_loss: 0.3553\n",
      "9/15, train_loss: 0.3742\n",
      "10/15, train_loss: 0.3881\n",
      "11/15, train_loss: 0.7689\n",
      "12/15, train_loss: 0.2931\n",
      "13/15, train_loss: 0.3781\n",
      "14/15, train_loss: 0.3363\n",
      "15/15, train_loss: 0.3479\n",
      "16/15, train_loss: 0.3255\n",
      "epoch 157 average loss: 0.7466\n",
      "----------\n",
      "epoch 158/500\n",
      "1/15, train_loss: 1.4471\n",
      "2/15, train_loss: 1.6632\n",
      "3/15, train_loss: 0.4286\n",
      "4/15, train_loss: 0.4535\n",
      "5/15, train_loss: 1.5131\n",
      "6/15, train_loss: 1.4472\n",
      "7/15, train_loss: 1.4957\n",
      "8/15, train_loss: 0.3157\n",
      "9/15, train_loss: 0.3658\n",
      "10/15, train_loss: 0.4465\n",
      "11/15, train_loss: 0.5479\n",
      "12/15, train_loss: 0.3239\n",
      "13/15, train_loss: 0.3449\n",
      "14/15, train_loss: 0.3310\n",
      "15/15, train_loss: 0.4077\n",
      "16/15, train_loss: 0.3015\n",
      "epoch 158 average loss: 0.7396\n",
      "----------\n",
      "epoch 159/500\n",
      "1/15, train_loss: 1.5190\n",
      "2/15, train_loss: 1.4490\n",
      "3/15, train_loss: 0.4290\n",
      "4/15, train_loss: 0.5039\n",
      "5/15, train_loss: 1.4395\n",
      "6/15, train_loss: 1.5295\n",
      "7/15, train_loss: 1.4206\n",
      "8/15, train_loss: 0.3079\n",
      "9/15, train_loss: 0.4268\n",
      "10/15, train_loss: 0.3842\n",
      "11/15, train_loss: 0.7323\n",
      "12/15, train_loss: 0.2850\n",
      "13/15, train_loss: 0.3401\n",
      "14/15, train_loss: 0.3542\n",
      "15/15, train_loss: 0.3537\n",
      "16/15, train_loss: 0.3252\n",
      "epoch 159 average loss: 0.7375\n",
      "----------\n",
      "epoch 160/500\n",
      "1/15, train_loss: 1.4481\n",
      "2/15, train_loss: 1.4198\n",
      "3/15, train_loss: 0.4802\n",
      "4/15, train_loss: 0.4426\n",
      "5/15, train_loss: 1.4780\n",
      "6/15, train_loss: 1.4320\n",
      "7/15, train_loss: 1.4167\n",
      "8/15, train_loss: 0.3496\n",
      "9/15, train_loss: 0.3694\n",
      "10/15, train_loss: 0.4296\n",
      "11/15, train_loss: 0.5697\n",
      "12/15, train_loss: 0.2818\n",
      "13/15, train_loss: 0.4041\n",
      "14/15, train_loss: 0.3276\n",
      "15/15, train_loss: 0.4031\n",
      "16/15, train_loss: 0.2965\n",
      "epoch 160 average loss: 0.7218\n",
      "----------\n",
      "epoch 161/500\n",
      "1/15, train_loss: 1.4328\n",
      "2/15, train_loss: 1.4830\n",
      "3/15, train_loss: 0.4271\n",
      "4/15, train_loss: 0.4819\n",
      "5/15, train_loss: 1.4361\n",
      "6/15, train_loss: 1.4213\n",
      "7/15, train_loss: 1.4871\n",
      "8/15, train_loss: 0.3046\n",
      "9/15, train_loss: 0.4207\n",
      "10/15, train_loss: 0.3784\n",
      "11/15, train_loss: 0.5460\n",
      "12/15, train_loss: 0.3076\n",
      "13/15, train_loss: 0.3353\n",
      "14/15, train_loss: 0.3607\n",
      "15/15, train_loss: 0.3494\n",
      "16/15, train_loss: 0.2953\n",
      "epoch 161 average loss: 0.7167\n",
      "----------\n",
      "epoch 162/500\n",
      "1/15, train_loss: 1.5164\n",
      "2/15, train_loss: 1.4135\n",
      "3/15, train_loss: 0.4692\n",
      "4/15, train_loss: 0.4411\n",
      "5/15, train_loss: 1.4278\n",
      "6/15, train_loss: 1.4859\n",
      "7/15, train_loss: 1.4029\n",
      "8/15, train_loss: 0.3483\n",
      "9/15, train_loss: 0.3616\n",
      "10/15, train_loss: 0.3694\n",
      "11/15, train_loss: 0.6195\n",
      "12/15, train_loss: 0.2768\n",
      "13/15, train_loss: 0.3580\n",
      "14/15, train_loss: 0.3264\n",
      "15/15, train_loss: 0.3387\n",
      "16/15, train_loss: 0.3292\n",
      "epoch 162 average loss: 0.7178\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 162 validation loss: 1.0038 dice_score: 0.7992 acc_metric: 0.5985 accuracy: 0.5985, f1score: 0.6885\n",
      " saved Best AUC: 0.6989 at epoch: 162\n",
      "----------\n",
      "epoch 163/500\n",
      "1/15, train_loss: 1.4231\n",
      "2/15, train_loss: 1.4602\n",
      "3/15, train_loss: 0.4218\n",
      "4/15, train_loss: 0.4309\n",
      "5/15, train_loss: 1.4965\n",
      "6/15, train_loss: 1.4090\n",
      "7/15, train_loss: 1.4721\n",
      "8/15, train_loss: 0.3011\n",
      "9/15, train_loss: 0.3538\n",
      "10/15, train_loss: 0.4396\n",
      "11/15, train_loss: 0.5235\n",
      "12/15, train_loss: 0.3217\n",
      "13/15, train_loss: 0.3271\n",
      "14/15, train_loss: 0.3226\n",
      "15/15, train_loss: 0.3823\n",
      "16/15, train_loss: 0.2938\n",
      "epoch 163 average loss: 0.7112\n",
      "----------\n",
      "epoch 164/500\n",
      "1/15, train_loss: 1.5062\n",
      "2/15, train_loss: 1.3989\n",
      "3/15, train_loss: 0.4141\n",
      "4/15, train_loss: 0.4859\n",
      "5/15, train_loss: 1.4243\n",
      "6/15, train_loss: 1.4593\n",
      "7/15, train_loss: 1.3988\n",
      "8/15, train_loss: 0.2986\n",
      "9/15, train_loss: 0.4051\n",
      "10/15, train_loss: 0.3639\n",
      "11/15, train_loss: 0.6025\n",
      "12/15, train_loss: 0.2737\n",
      "13/15, train_loss: 0.3245\n",
      "14/15, train_loss: 0.3584\n",
      "15/15, train_loss: 0.3342\n",
      "16/15, train_loss: 0.3241\n",
      "epoch 164 average loss: 0.7108\n",
      "----------\n",
      "epoch 165/500\n",
      "1/15, train_loss: 1.4180\n",
      "2/15, train_loss: 1.3913\n",
      "3/15, train_loss: 0.4654\n",
      "4/15, train_loss: 0.4268\n",
      "5/15, train_loss: 1.4907\n",
      "6/15, train_loss: 1.3987\n",
      "7/15, train_loss: 1.3916\n",
      "8/15, train_loss: 0.3352\n",
      "9/15, train_loss: 0.3531\n",
      "10/15, train_loss: 0.3899\n",
      "11/15, train_loss: 0.5156\n",
      "12/15, train_loss: 0.2714\n",
      "13/15, train_loss: 0.3637\n",
      "14/15, train_loss: 0.3191\n",
      "15/15, train_loss: 0.3684\n",
      "16/15, train_loss: 0.2877\n",
      "epoch 165 average loss: 0.6992\n",
      "----------\n",
      "epoch 166/500\n",
      "1/15, train_loss: 1.4084\n",
      "2/15, train_loss: 1.4686\n",
      "3/15, train_loss: 0.4105\n",
      "4/15, train_loss: 0.4744\n",
      "5/15, train_loss: 1.4182\n",
      "6/15, train_loss: 1.3922\n",
      "7/15, train_loss: 1.4551\n",
      "8/15, train_loss: 0.2962\n",
      "9/15, train_loss: 0.3991\n",
      "10/15, train_loss: 0.3609\n",
      "11/15, train_loss: 0.5058\n",
      "12/15, train_loss: 0.2943\n",
      "13/15, train_loss: 0.3310\n",
      "14/15, train_loss: 0.3396\n",
      "15/15, train_loss: 0.3456\n",
      "16/15, train_loss: 0.2854\n",
      "epoch 166 average loss: 0.6991\n",
      "----------\n",
      "epoch 167/500\n",
      "1/15, train_loss: 1.4672\n",
      "2/15, train_loss: 1.3892\n",
      "3/15, train_loss: 0.4506\n",
      "4/15, train_loss: 0.4314\n",
      "5/15, train_loss: 1.4178\n",
      "6/15, train_loss: 1.4395\n",
      "7/15, train_loss: 1.3900\n",
      "8/15, train_loss: 0.3282\n",
      "9/15, train_loss: 0.3566\n",
      "10/15, train_loss: 0.3581\n",
      "11/15, train_loss: 0.6786\n",
      "12/15, train_loss: 0.2710\n",
      "13/15, train_loss: 0.3843\n",
      "14/15, train_loss: 0.3143\n",
      "15/15, train_loss: 0.3336\n",
      "16/15, train_loss: 0.3059\n",
      "epoch 167 average loss: 0.7073\n",
      "----------\n",
      "epoch 168/500\n",
      "1/15, train_loss: 1.4086\n",
      "2/15, train_loss: 1.4607\n",
      "3/15, train_loss: 0.4100\n",
      "4/15, train_loss: 0.4209\n",
      "5/15, train_loss: 1.4747\n",
      "6/15, train_loss: 1.3943\n",
      "7/15, train_loss: 1.4430\n",
      "8/15, train_loss: 0.2943\n",
      "9/15, train_loss: 0.3501\n",
      "10/15, train_loss: 0.4233\n",
      "11/15, train_loss: 0.5067\n",
      "12/15, train_loss: 0.3136\n",
      "13/15, train_loss: 0.3218\n",
      "14/15, train_loss: 0.3118\n",
      "15/15, train_loss: 0.3794\n",
      "16/15, train_loss: 0.2825\n",
      "epoch 168 average loss: 0.6997\n",
      "----------\n",
      "epoch 169/500\n",
      "1/15, train_loss: 1.4878\n",
      "2/15, train_loss: 1.3933\n",
      "3/15, train_loss: 0.4028\n",
      "4/15, train_loss: 0.4804\n",
      "5/15, train_loss: 1.4065\n",
      "6/15, train_loss: 1.4537\n",
      "7/15, train_loss: 1.3818\n",
      "8/15, train_loss: 0.2889\n",
      "9/15, train_loss: 0.3971\n",
      "10/15, train_loss: 0.3546\n",
      "11/15, train_loss: 0.6434\n",
      "12/15, train_loss: 0.2659\n",
      "13/15, train_loss: 0.3169\n",
      "14/15, train_loss: 0.3382\n",
      "15/15, train_loss: 0.3260\n",
      "16/15, train_loss: 0.3025\n",
      "epoch 169 average loss: 0.7025\n",
      "----------\n",
      "epoch 170/500\n",
      "1/15, train_loss: 1.4079\n",
      "2/15, train_loss: 1.3755\n",
      "3/15, train_loss: 0.4538\n",
      "4/15, train_loss: 0.4133\n",
      "5/15, train_loss: 1.4628\n",
      "6/15, train_loss: 1.3865\n",
      "7/15, train_loss: 1.3789\n",
      "8/15, train_loss: 0.3197\n",
      "9/15, train_loss: 0.3471\n",
      "10/15, train_loss: 0.3943\n",
      "11/15, train_loss: 0.5061\n",
      "12/15, train_loss: 0.2644\n",
      "13/15, train_loss: 0.3547\n",
      "14/15, train_loss: 0.3087\n",
      "15/15, train_loss: 0.3566\n",
      "16/15, train_loss: 0.2782\n",
      "epoch 170 average loss: 0.6880\n",
      "----------\n",
      "epoch 171/500\n",
      "1/15, train_loss: 1.3912\n",
      "2/15, train_loss: 1.4594\n",
      "3/15, train_loss: 0.4012\n",
      "4/15, train_loss: 0.4453\n",
      "5/15, train_loss: 1.3955\n",
      "6/15, train_loss: 1.3763\n",
      "7/15, train_loss: 1.4433\n",
      "8/15, train_loss: 0.2851\n",
      "9/15, train_loss: 0.3964\n",
      "10/15, train_loss: 0.3520\n",
      "11/15, train_loss: 0.4964\n",
      "12/15, train_loss: 0.2857\n",
      "13/15, train_loss: 0.3187\n",
      "14/15, train_loss: 0.3230\n",
      "15/15, train_loss: 0.3342\n",
      "16/15, train_loss: 0.2791\n",
      "epoch 171 average loss: 0.6864\n",
      "----------\n",
      "epoch 172/500\n",
      "1/15, train_loss: 1.4528\n",
      "2/15, train_loss: 1.3667\n",
      "3/15, train_loss: 0.4323\n",
      "4/15, train_loss: 0.4117\n",
      "5/15, train_loss: 1.3878\n",
      "6/15, train_loss: 1.4180\n",
      "7/15, train_loss: 1.3629\n",
      "8/15, train_loss: 0.3135\n",
      "9/15, train_loss: 0.3455\n",
      "10/15, train_loss: 0.3446\n",
      "11/15, train_loss: 0.5951\n",
      "12/15, train_loss: 0.2601\n",
      "13/15, train_loss: 0.3604\n",
      "14/15, train_loss: 0.3066\n",
      "15/15, train_loss: 0.3246\n",
      "16/15, train_loss: 0.2999\n",
      "epoch 172 average loss: 0.6864\n",
      "----------\n",
      "epoch 173/500\n",
      "1/15, train_loss: 1.3843\n",
      "2/15, train_loss: 1.3964\n",
      "3/15, train_loss: 0.3989\n",
      "4/15, train_loss: 0.4033\n",
      "5/15, train_loss: 1.4257\n",
      "6/15, train_loss: 1.3687\n",
      "7/15, train_loss: 1.4004\n",
      "8/15, train_loss: 0.2840\n",
      "9/15, train_loss: 0.3397\n",
      "10/15, train_loss: 0.3938\n",
      "11/15, train_loss: 0.4875\n",
      "12/15, train_loss: 0.2917\n",
      "13/15, train_loss: 0.3157\n",
      "14/15, train_loss: 0.3043\n",
      "15/15, train_loss: 0.3823\n",
      "16/15, train_loss: 0.2737\n",
      "epoch 173 average loss: 0.6781\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 173 validation loss: 1.0312 dice_score: 0.7778 acc_metric: 0.6232 accuracy: 0.6232, f1score: 0.6393\n",
      " saved Best AUC: 0.7005 at epoch: 173\n",
      "----------\n",
      "epoch 174/500\n",
      "1/15, train_loss: 1.4585\n",
      "2/15, train_loss: 1.3607\n",
      "3/15, train_loss: 0.3929\n",
      "4/15, train_loss: 0.4501\n",
      "5/15, train_loss: 1.3904\n",
      "6/15, train_loss: 1.4147\n",
      "7/15, train_loss: 1.3616\n",
      "8/15, train_loss: 0.2803\n",
      "9/15, train_loss: 0.3904\n",
      "10/15, train_loss: 0.3417\n",
      "11/15, train_loss: 0.5421\n",
      "12/15, train_loss: 0.2575\n",
      "13/15, train_loss: 0.3063\n",
      "14/15, train_loss: 0.3358\n",
      "15/15, train_loss: 0.3162\n",
      "16/15, train_loss: 0.2919\n",
      "epoch 174 average loss: 0.6807\n",
      "----------\n",
      "epoch 175/500\n",
      "1/15, train_loss: 1.3912\n",
      "2/15, train_loss: 1.3534\n",
      "3/15, train_loss: 0.4411\n",
      "4/15, train_loss: 0.4005\n",
      "5/15, train_loss: 1.4431\n",
      "6/15, train_loss: 1.3651\n",
      "7/15, train_loss: 1.3508\n",
      "8/15, train_loss: 0.3202\n",
      "9/15, train_loss: 0.3364\n",
      "10/15, train_loss: 0.3898\n",
      "11/15, train_loss: 0.4764\n",
      "12/15, train_loss: 0.2556\n",
      "13/15, train_loss: 0.3300\n",
      "14/15, train_loss: 0.3012\n",
      "15/15, train_loss: 0.3421\n",
      "16/15, train_loss: 0.2717\n",
      "epoch 175 average loss: 0.6730\n",
      "----------\n",
      "epoch 176/500\n",
      "1/15, train_loss: 1.3718\n",
      "2/15, train_loss: 1.4381\n",
      "3/15, train_loss: 0.3891\n",
      "4/15, train_loss: 0.4672\n",
      "5/15, train_loss: 1.3795\n",
      "6/15, train_loss: 1.3569\n",
      "7/15, train_loss: 1.4024\n",
      "8/15, train_loss: 0.2791\n",
      "9/15, train_loss: 0.3719\n",
      "10/15, train_loss: 0.3386\n",
      "11/15, train_loss: 0.4623\n",
      "12/15, train_loss: 0.2902\n",
      "13/15, train_loss: 0.3043\n",
      "14/15, train_loss: 0.3271\n",
      "15/15, train_loss: 0.3125\n",
      "16/15, train_loss: 0.2692\n",
      "epoch 176 average loss: 0.6725\n",
      "----------\n",
      "epoch 177/500\n",
      "1/15, train_loss: 1.4562\n",
      "2/15, train_loss: 1.3482\n",
      "3/15, train_loss: 0.4282\n",
      "4/15, train_loss: 0.4043\n",
      "5/15, train_loss: 1.3745\n",
      "6/15, train_loss: 1.4193\n",
      "7/15, train_loss: 1.3429\n",
      "8/15, train_loss: 0.3135\n",
      "9/15, train_loss: 0.3272\n",
      "10/15, train_loss: 0.3323\n",
      "11/15, train_loss: 0.5684\n",
      "12/15, train_loss: 0.2557\n",
      "13/15, train_loss: 0.3286\n",
      "14/15, train_loss: 0.2991\n",
      "15/15, train_loss: 0.3082\n",
      "16/15, train_loss: 0.2843\n",
      "epoch 177 average loss: 0.6744\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 177 validation loss: 1.0071 dice_score: 0.8004 acc_metric: 0.6024 accuracy: 0.6024, f1score: 0.6639\n",
      " saved Best AUC: 0.7014 at epoch: 177\n",
      "----------\n",
      "epoch 178/500\n",
      "1/15, train_loss: 1.3662\n",
      "2/15, train_loss: 1.4231\n",
      "3/15, train_loss: 0.3817\n",
      "4/15, train_loss: 0.3963\n",
      "5/15, train_loss: 1.4206\n",
      "6/15, train_loss: 1.3545\n",
      "7/15, train_loss: 1.3857\n",
      "8/15, train_loss: 0.2793\n",
      "9/15, train_loss: 0.3240\n",
      "10/15, train_loss: 0.3766\n",
      "11/15, train_loss: 0.4592\n",
      "12/15, train_loss: 0.2796\n",
      "13/15, train_loss: 0.3046\n",
      "14/15, train_loss: 0.2960\n",
      "15/15, train_loss: 0.3505\n",
      "16/15, train_loss: 0.2681\n",
      "epoch 178 average loss: 0.6666\n",
      "----------\n",
      "epoch 179/500\n",
      "1/15, train_loss: 1.4030\n",
      "2/15, train_loss: 1.3515\n",
      "3/15, train_loss: 0.3802\n",
      "4/15, train_loss: 0.4362\n",
      "5/15, train_loss: 1.3685\n",
      "6/15, train_loss: 1.4012\n",
      "7/15, train_loss: 1.3374\n",
      "8/15, train_loss: 0.2744\n",
      "9/15, train_loss: 0.3866\n",
      "10/15, train_loss: 0.3286\n",
      "11/15, train_loss: 0.5705\n",
      "12/15, train_loss: 0.2502\n",
      "13/15, train_loss: 0.3004\n",
      "14/15, train_loss: 0.3195\n",
      "15/15, train_loss: 0.3079\n",
      "16/15, train_loss: 0.2853\n",
      "epoch 179 average loss: 0.6688\n",
      "----------\n",
      "epoch 180/500\n",
      "1/15, train_loss: 1.3603\n",
      "2/15, train_loss: 1.3358\n",
      "3/15, train_loss: 0.4277\n",
      "4/15, train_loss: 0.3906\n",
      "5/15, train_loss: 1.4287\n",
      "6/15, train_loss: 1.3455\n",
      "7/15, train_loss: 1.3317\n",
      "8/15, train_loss: 0.3080\n",
      "9/15, train_loss: 0.3266\n",
      "10/15, train_loss: 0.3527\n",
      "11/15, train_loss: 0.4708\n",
      "12/15, train_loss: 0.2478\n",
      "13/15, train_loss: 0.3467\n",
      "14/15, train_loss: 0.2934\n",
      "15/15, train_loss: 0.3454\n",
      "16/15, train_loss: 0.2649\n",
      "epoch 180 average loss: 0.6610\n",
      "----------\n",
      "epoch 181/500\n",
      "1/15, train_loss: 1.3498\n",
      "2/15, train_loss: 1.3877\n",
      "3/15, train_loss: 0.3774\n",
      "4/15, train_loss: 0.4328\n",
      "5/15, train_loss: 1.3621\n",
      "6/15, train_loss: 1.3417\n",
      "7/15, train_loss: 1.3873\n",
      "8/15, train_loss: 0.2712\n",
      "9/15, train_loss: 0.3676\n",
      "10/15, train_loss: 0.3245\n",
      "11/15, train_loss: 0.4532\n",
      "12/15, train_loss: 0.2749\n",
      "13/15, train_loss: 0.2972\n",
      "14/15, train_loss: 0.3209\n",
      "15/15, train_loss: 0.3065\n",
      "16/15, train_loss: 0.2629\n",
      "epoch 181 average loss: 0.6574\n",
      "----------\n",
      "epoch 182/500\n",
      "1/15, train_loss: 1.4292\n",
      "2/15, train_loss: 1.3291\n",
      "3/15, train_loss: 0.4216\n",
      "4/15, train_loss: 0.3877\n",
      "5/15, train_loss: 1.3563\n",
      "6/15, train_loss: 1.4091\n",
      "7/15, train_loss: 1.3228\n",
      "8/15, train_loss: 0.3066\n",
      "9/15, train_loss: 0.3179\n",
      "10/15, train_loss: 0.3197\n",
      "11/15, train_loss: 0.5340\n",
      "12/15, train_loss: 0.2458\n",
      "13/15, train_loss: 0.3159\n",
      "14/15, train_loss: 0.2918\n",
      "15/15, train_loss: 0.3005\n",
      "16/15, train_loss: 0.2883\n",
      "epoch 182 average loss: 0.6610\n",
      "----------\n",
      "epoch 183/500\n",
      "1/15, train_loss: 1.3489\n",
      "2/15, train_loss: 1.3789\n",
      "3/15, train_loss: 0.3753\n",
      "4/15, train_loss: 0.3813\n",
      "5/15, train_loss: 1.3981\n",
      "6/15, train_loss: 1.3344\n",
      "7/15, train_loss: 1.3664\n",
      "8/15, train_loss: 0.2699\n",
      "9/15, train_loss: 0.3175\n",
      "10/15, train_loss: 0.3729\n",
      "11/15, train_loss: 0.4448\n",
      "12/15, train_loss: 0.2766\n",
      "13/15, train_loss: 0.2936\n",
      "14/15, train_loss: 0.2878\n",
      "15/15, train_loss: 0.3369\n",
      "16/15, train_loss: 0.2597\n",
      "epoch 183 average loss: 0.6527\n",
      "----------\n",
      "epoch 184/500\n",
      "1/15, train_loss: 1.4033\n",
      "2/15, train_loss: 1.3208\n",
      "3/15, train_loss: 0.3705\n",
      "4/15, train_loss: 0.4286\n",
      "5/15, train_loss: 1.3513\n",
      "6/15, train_loss: 1.3587\n",
      "7/15, train_loss: 1.3188\n",
      "8/15, train_loss: 0.2653\n",
      "9/15, train_loss: 0.3665\n",
      "10/15, train_loss: 0.3178\n",
      "11/15, train_loss: 0.5268\n",
      "12/15, train_loss: 0.2436\n",
      "13/15, train_loss: 0.2904\n",
      "14/15, train_loss: 0.3185\n",
      "15/15, train_loss: 0.2978\n",
      "16/15, train_loss: 0.2754\n",
      "epoch 184 average loss: 0.6534\n",
      "----------\n",
      "epoch 185/500\n",
      "1/15, train_loss: 1.3421\n",
      "2/15, train_loss: 1.3160\n",
      "3/15, train_loss: 0.4111\n",
      "4/15, train_loss: 0.3779\n",
      "5/15, train_loss: 1.3894\n",
      "6/15, train_loss: 1.3258\n",
      "7/15, train_loss: 1.3116\n",
      "8/15, train_loss: 0.2967\n",
      "9/15, train_loss: 0.3147\n",
      "10/15, train_loss: 0.3525\n",
      "11/15, train_loss: 0.4354\n",
      "12/15, train_loss: 0.2427\n",
      "13/15, train_loss: 0.3341\n",
      "14/15, train_loss: 0.2856\n",
      "15/15, train_loss: 0.3366\n",
      "16/15, train_loss: 0.2566\n",
      "epoch 185 average loss: 0.6455\n",
      "----------\n",
      "epoch 186/500\n",
      "1/15, train_loss: 1.3283\n",
      "2/15, train_loss: 1.3696\n",
      "3/15, train_loss: 0.3704\n",
      "4/15, train_loss: 0.4106\n",
      "5/15, train_loss: 1.3417\n",
      "6/15, train_loss: 1.3184\n",
      "7/15, train_loss: 1.3545\n",
      "8/15, train_loss: 0.2649\n",
      "9/15, train_loss: 0.3539\n",
      "10/15, train_loss: 0.3141\n",
      "11/15, train_loss: 0.4303\n",
      "12/15, train_loss: 0.2575\n",
      "13/15, train_loss: 0.2951\n",
      "14/15, train_loss: 0.2980\n",
      "15/15, train_loss: 0.3026\n",
      "16/15, train_loss: 0.2562\n",
      "epoch 186 average loss: 0.6416\n",
      "----------\n",
      "epoch 187/500\n",
      "1/15, train_loss: 1.3823\n",
      "2/15, train_loss: 1.3106\n",
      "3/15, train_loss: 0.4074\n",
      "4/15, train_loss: 0.3798\n",
      "5/15, train_loss: 1.3339\n",
      "6/15, train_loss: 1.3654\n",
      "7/15, train_loss: 1.3044\n",
      "8/15, train_loss: 0.2878\n",
      "9/15, train_loss: 0.3205\n",
      "10/15, train_loss: 0.3090\n",
      "11/15, train_loss: 0.5303\n",
      "12/15, train_loss: 0.2394\n",
      "13/15, train_loss: 0.3305\n",
      "14/15, train_loss: 0.2832\n",
      "15/15, train_loss: 0.2967\n",
      "16/15, train_loss: 0.2738\n",
      "epoch 187 average loss: 0.6472\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 187 validation loss: 1.0226 dice_score: 0.7910 acc_metric: 0.6527 accuracy: 0.6527, f1score: 0.6557\n",
      " saved Best AUC: 0.7218 at epoch: 187\n",
      "----------\n",
      "epoch 188/500\n",
      "1/15, train_loss: 1.3302\n",
      "2/15, train_loss: 1.3436\n",
      "3/15, train_loss: 0.3694\n",
      "4/15, train_loss: 0.3722\n",
      "5/15, train_loss: 1.3751\n",
      "6/15, train_loss: 1.3127\n",
      "7/15, train_loss: 1.3453\n",
      "8/15, train_loss: 0.2601\n",
      "9/15, train_loss: 0.3178\n",
      "10/15, train_loss: 0.3453\n",
      "11/15, train_loss: 0.4449\n",
      "12/15, train_loss: 0.2645\n",
      "13/15, train_loss: 0.2985\n",
      "14/15, train_loss: 0.2803\n",
      "15/15, train_loss: 0.3410\n",
      "16/15, train_loss: 0.2526\n",
      "epoch 188 average loss: 0.6408\n",
      "----------\n",
      "epoch 189/500\n",
      "1/15, train_loss: 1.3876\n",
      "2/15, train_loss: 1.3027\n",
      "3/15, train_loss: 0.3655\n",
      "4/15, train_loss: 0.4092\n",
      "5/15, train_loss: 1.3320\n",
      "6/15, train_loss: 1.3409\n",
      "7/15, train_loss: 1.3023\n",
      "8/15, train_loss: 0.2586\n",
      "9/15, train_loss: 0.3654\n",
      "10/15, train_loss: 0.3055\n",
      "11/15, train_loss: 0.5002\n",
      "12/15, train_loss: 0.2366\n",
      "13/15, train_loss: 0.2875\n",
      "14/15, train_loss: 0.3119\n",
      "15/15, train_loss: 0.2910\n",
      "16/15, train_loss: 0.2770\n",
      "epoch 189 average loss: 0.6421\n",
      "----------\n",
      "epoch 190/500\n",
      "1/15, train_loss: 1.3328\n",
      "2/15, train_loss: 1.2968\n",
      "3/15, train_loss: 0.4257\n",
      "4/15, train_loss: 0.3665\n",
      "5/15, train_loss: 1.3853\n",
      "6/15, train_loss: 1.3061\n",
      "7/15, train_loss: 1.2956\n",
      "8/15, train_loss: 0.2938\n",
      "9/15, train_loss: 0.3085\n",
      "10/15, train_loss: 0.3381\n",
      "11/15, train_loss: 0.4389\n",
      "12/15, train_loss: 0.2340\n",
      "13/15, train_loss: 0.3167\n",
      "14/15, train_loss: 0.2775\n",
      "15/15, train_loss: 0.3141\n",
      "16/15, train_loss: 0.2487\n",
      "epoch 190 average loss: 0.6362\n",
      "----------\n",
      "epoch 191/500\n",
      "1/15, train_loss: 1.3178\n",
      "2/15, train_loss: 1.3371\n",
      "3/15, train_loss: 0.3598\n",
      "4/15, train_loss: 0.4082\n",
      "5/15, train_loss: 1.3262\n",
      "6/15, train_loss: 1.3015\n",
      "7/15, train_loss: 1.3515\n",
      "8/15, train_loss: 0.2576\n",
      "9/15, train_loss: 0.3347\n",
      "10/15, train_loss: 0.3022\n",
      "11/15, train_loss: 0.4220\n",
      "12/15, train_loss: 0.2613\n",
      "13/15, train_loss: 0.2784\n",
      "14/15, train_loss: 0.2999\n",
      "15/15, train_loss: 0.2835\n",
      "16/15, train_loss: 0.2490\n",
      "epoch 191 average loss: 0.6307\n",
      "----------\n",
      "epoch 192/500\n",
      "1/15, train_loss: 1.4095\n",
      "2/15, train_loss: 1.2943\n",
      "3/15, train_loss: 0.3865\n",
      "4/15, train_loss: 0.3666\n",
      "5/15, train_loss: 1.3159\n",
      "6/15, train_loss: 1.3536\n",
      "7/15, train_loss: 1.2853\n",
      "8/15, train_loss: 0.2813\n",
      "9/15, train_loss: 0.3000\n",
      "10/15, train_loss: 0.2981\n",
      "11/15, train_loss: 0.5328\n",
      "12/15, train_loss: 0.2317\n",
      "13/15, train_loss: 0.2979\n",
      "14/15, train_loss: 0.2758\n",
      "15/15, train_loss: 0.2833\n",
      "16/15, train_loss: 0.2670\n",
      "epoch 192 average loss: 0.6362\n",
      "----------\n",
      "epoch 193/500\n",
      "1/15, train_loss: 1.3067\n",
      "2/15, train_loss: 1.3368\n",
      "3/15, train_loss: 0.3580\n",
      "4/15, train_loss: 0.3609\n",
      "5/15, train_loss: 1.3487\n",
      "6/15, train_loss: 1.2963\n",
      "7/15, train_loss: 1.3093\n",
      "8/15, train_loss: 0.2542\n",
      "9/15, train_loss: 0.2991\n",
      "10/15, train_loss: 0.3346\n",
      "11/15, train_loss: 0.4162\n",
      "12/15, train_loss: 0.2606\n",
      "13/15, train_loss: 0.2820\n",
      "14/15, train_loss: 0.2733\n",
      "15/15, train_loss: 0.3290\n",
      "16/15, train_loss: 0.2470\n",
      "epoch 193 average loss: 0.6258\n",
      "----------\n",
      "epoch 194/500\n",
      "1/15, train_loss: 1.3469\n",
      "2/15, train_loss: 1.2890\n",
      "3/15, train_loss: 0.3546\n",
      "4/15, train_loss: 0.4057\n",
      "5/15, train_loss: 1.3156\n",
      "6/15, train_loss: 1.3271\n",
      "7/15, train_loss: 1.2854\n",
      "8/15, train_loss: 0.2521\n",
      "9/15, train_loss: 0.3475\n",
      "10/15, train_loss: 0.2957\n",
      "11/15, train_loss: 0.4672\n",
      "12/15, train_loss: 0.2295\n",
      "13/15, train_loss: 0.2781\n",
      "14/15, train_loss: 0.2995\n",
      "15/15, train_loss: 0.2837\n",
      "16/15, train_loss: 0.2692\n",
      "epoch 194 average loss: 0.6279\n",
      "----------\n",
      "epoch 195/500\n",
      "1/15, train_loss: 1.3034\n",
      "2/15, train_loss: 1.2848\n",
      "3/15, train_loss: 0.3992\n",
      "4/15, train_loss: 0.3577\n",
      "5/15, train_loss: 1.3676\n",
      "6/15, train_loss: 1.2935\n",
      "7/15, train_loss: 1.2795\n",
      "8/15, train_loss: 0.2857\n",
      "9/15, train_loss: 0.2993\n",
      "10/15, train_loss: 0.3242\n",
      "11/15, train_loss: 0.4212\n",
      "12/15, train_loss: 0.2281\n",
      "13/15, train_loss: 0.3204\n",
      "14/15, train_loss: 0.2700\n",
      "15/15, train_loss: 0.3205\n",
      "16/15, train_loss: 0.2439\n",
      "epoch 195 average loss: 0.6249\n",
      "----------\n",
      "epoch 196/500\n",
      "1/15, train_loss: 1.2927\n",
      "2/15, train_loss: 1.3575\n",
      "3/15, train_loss: 0.3511\n",
      "4/15, train_loss: 0.3953\n",
      "5/15, train_loss: 1.3066\n",
      "6/15, train_loss: 1.2881\n",
      "7/15, train_loss: 1.3201\n",
      "8/15, train_loss: 0.2493\n",
      "9/15, train_loss: 0.3266\n",
      "10/15, train_loss: 0.2958\n",
      "11/15, train_loss: 0.4028\n",
      "12/15, train_loss: 0.2523\n",
      "13/15, train_loss: 0.2761\n",
      "14/15, train_loss: 0.2926\n",
      "15/15, train_loss: 0.2818\n",
      "16/15, train_loss: 0.2419\n",
      "epoch 196 average loss: 0.6207\n",
      "----------\n",
      "epoch 197/500\n",
      "1/15, train_loss: 1.3350\n",
      "2/15, train_loss: 1.2810\n",
      "3/15, train_loss: 0.3760\n",
      "4/15, train_loss: 0.3570\n",
      "5/15, train_loss: 1.2991\n",
      "6/15, train_loss: 1.3494\n",
      "7/15, train_loss: 1.2703\n",
      "8/15, train_loss: 0.2763\n",
      "9/15, train_loss: 0.2945\n",
      "10/15, train_loss: 0.2927\n",
      "11/15, train_loss: 0.5230\n",
      "12/15, train_loss: 0.2264\n",
      "13/15, train_loss: 0.3020\n",
      "14/15, train_loss: 0.2682\n",
      "15/15, train_loss: 0.2781\n",
      "16/15, train_loss: 0.2580\n",
      "epoch 197 average loss: 0.6242\n",
      "----------\n",
      "epoch 198/500\n",
      "1/15, train_loss: 1.2883\n",
      "2/15, train_loss: 1.3254\n",
      "3/15, train_loss: 0.3455\n",
      "4/15, train_loss: 0.3528\n",
      "5/15, train_loss: 1.3204\n",
      "6/15, train_loss: 1.2870\n",
      "7/15, train_loss: 1.2978\n",
      "8/15, train_loss: 0.2491\n",
      "9/15, train_loss: 0.2905\n",
      "10/15, train_loss: 0.3187\n",
      "11/15, train_loss: 0.4062\n",
      "12/15, train_loss: 0.2470\n",
      "13/15, train_loss: 0.2765\n",
      "14/15, train_loss: 0.2662\n",
      "15/15, train_loss: 0.3234\n",
      "16/15, train_loss: 0.2395\n",
      "epoch 198 average loss: 0.6147\n",
      "----------\n",
      "epoch 199/500\n",
      "1/15, train_loss: 1.3126\n",
      "2/15, train_loss: 1.2845\n",
      "3/15, train_loss: 0.3427\n",
      "4/15, train_loss: 0.4001\n",
      "5/15, train_loss: 1.2914\n",
      "6/15, train_loss: 1.3195\n",
      "7/15, train_loss: 1.2631\n",
      "8/15, train_loss: 0.2452\n",
      "9/15, train_loss: 0.3367\n",
      "10/15, train_loss: 0.2868\n",
      "11/15, train_loss: 0.4757\n",
      "12/15, train_loss: 0.2228\n",
      "13/15, train_loss: 0.2699\n",
      "14/15, train_loss: 0.2835\n",
      "15/15, train_loss: 0.2754\n",
      "16/15, train_loss: 0.2545\n",
      "epoch 199 average loss: 0.6165\n",
      "saved new best metric model\n",
      "current fold: 0 current epoch: 199 validation loss: 1.0329 dice_score: 0.7892 acc_metric: 0.6384 accuracy: 0.6384, f1score: 0.6148\n",
      " saved Best AUC: 0.7138 at epoch: 199\n",
      "----------\n",
      "epoch 200/500\n",
      "1/15, train_loss: 1.2831\n",
      "2/15, train_loss: 1.2710\n",
      "3/15, train_loss: 0.3850\n",
      "4/15, train_loss: 0.3496\n",
      "5/15, train_loss: 1.3247\n",
      "6/15, train_loss: 1.2797\n",
      "7/15, train_loss: 1.2582\n",
      "8/15, train_loss: 0.2749\n",
      "9/15, train_loss: 0.2908\n",
      "10/15, train_loss: 0.3092\n",
      "11/15, train_loss: 0.4149\n",
      "12/15, train_loss: 0.2202\n",
      "13/15, train_loss: 0.3058\n",
      "14/15, train_loss: 0.2634\n",
      "15/15, train_loss: 0.3005\n",
      "16/15, train_loss: 0.2378\n",
      "epoch 200 average loss: 0.6105\n",
      "----------\n",
      "epoch 201/500\n",
      "1/15, train_loss: 1.2780\n",
      "2/15, train_loss: 1.3082\n",
      "3/15, train_loss: 0.3403\n",
      "4/15, train_loss: 0.3891\n",
      "5/15, train_loss: 1.2889\n",
      "6/15, train_loss: 1.2725\n",
      "7/15, train_loss: 1.2919\n",
      "8/15, train_loss: 0.2440\n",
      "9/15, train_loss: 0.3238\n",
      "10/15, train_loss: 0.2843\n",
      "11/15, train_loss: 0.3959\n",
      "12/15, train_loss: 0.2429\n",
      "13/15, train_loss: 0.2650\n",
      "14/15, train_loss: 0.2834\n",
      "15/15, train_loss: 0.2712\n",
      "16/15, train_loss: 0.2375\n",
      "epoch 201 average loss: 0.6073\n",
      "----------\n",
      "epoch 202/500\n",
      "1/15, train_loss: 1.3299\n",
      "2/15, train_loss: 1.2648\n",
      "3/15, train_loss: 0.3670\n",
      "4/15, train_loss: 0.3465\n",
      "5/15, train_loss: 1.2829\n",
      "6/15, train_loss: 1.3098\n",
      "7/15, train_loss: 1.2505\n",
      "8/15, train_loss: 0.2632\n",
      "9/15, train_loss: 0.2846\n",
      "10/15, train_loss: 0.2811\n",
      "11/15, train_loss: 0.4465\n",
      "12/15, train_loss: 0.2208\n",
      "13/15, train_loss: 0.2782\n",
      "14/15, train_loss: 0.2627\n",
      "15/15, train_loss: 0.2689\n",
      "16/15, train_loss: 0.2549\n",
      "epoch 202 average loss: 0.6070\n",
      "----------\n",
      "epoch 203/500\n",
      "1/15, train_loss: 1.2729\n",
      "2/15, train_loss: 1.2803\n",
      "3/15, train_loss: 0.3390\n",
      "4/15, train_loss: 0.3427\n",
      "5/15, train_loss: 1.3150\n",
      "6/15, train_loss: 1.2652\n",
      "7/15, train_loss: 1.2712\n",
      "8/15, train_loss: 0.2436\n",
      "9/15, train_loss: 0.2837\n",
      "10/15, train_loss: 0.3228\n",
      "11/15, train_loss: 0.3895\n",
      "12/15, train_loss: 0.2481\n",
      "13/15, train_loss: 0.2675\n",
      "14/15, train_loss: 0.2598\n",
      "15/15, train_loss: 0.3074\n",
      "16/15, train_loss: 0.2355\n",
      "epoch 203 average loss: 0.6028\n",
      "----------\n",
      "epoch 204/500\n",
      "1/15, train_loss: 1.3024\n",
      "2/15, train_loss: 1.2575\n",
      "3/15, train_loss: 0.3361\n",
      "4/15, train_loss: 0.3878\n",
      "5/15, train_loss: 1.2816\n",
      "6/15, train_loss: 1.2853\n",
      "7/15, train_loss: 1.2486\n",
      "8/15, train_loss: 0.2398\n",
      "9/15, train_loss: 0.3279\n",
      "10/15, train_loss: 0.2811\n",
      "11/15, train_loss: 0.4289\n",
      "12/15, train_loss: 0.2211\n",
      "13/15, train_loss: 0.2622\n",
      "14/15, train_loss: 0.2898\n",
      "15/15, train_loss: 0.2687\n",
      "16/15, train_loss: 0.2535\n",
      "epoch 204 average loss: 0.6045\n",
      "----------\n",
      "epoch 205/500\n",
      "1/15, train_loss: 1.2654\n",
      "2/15, train_loss: 1.2571\n",
      "3/15, train_loss: 0.3722\n",
      "4/15, train_loss: 0.3410\n",
      "5/15, train_loss: 1.3216\n",
      "6/15, train_loss: 1.2624\n",
      "7/15, train_loss: 1.2443\n",
      "8/15, train_loss: 0.2704\n",
      "9/15, train_loss: 0.2841\n",
      "10/15, train_loss: 0.3067\n",
      "11/15, train_loss: 0.3811\n",
      "12/15, train_loss: 0.2169\n",
      "13/15, train_loss: 0.2942\n",
      "14/15, train_loss: 0.2581\n",
      "15/15, train_loss: 0.2954\n",
      "16/15, train_loss: 0.2318\n",
      "epoch 205 average loss: 0.6002\n",
      "----------\n",
      "epoch 206/500\n",
      "1/15, train_loss: 1.2575\n",
      "2/15, train_loss: 1.3057\n",
      "3/15, train_loss: 0.3320\n",
      "4/15, train_loss: 0.3733\n",
      "5/15, train_loss: 1.2754\n",
      "6/15, train_loss: 1.2580\n",
      "7/15, train_loss: 1.2922\n",
      "8/15, train_loss: 0.2387\n",
      "9/15, train_loss: 0.3208\n",
      "10/15, train_loss: 0.2769\n",
      "11/15, train_loss: 0.3784\n",
      "12/15, train_loss: 0.2280\n",
      "13/15, train_loss: 0.2617\n",
      "14/15, train_loss: 0.2731\n",
      "15/15, train_loss: 0.2662\n",
      "16/15, train_loss: 0.2298\n",
      "epoch 206 average loss: 0.5980\n",
      "----------\n",
      "epoch 207/500\n",
      "1/15, train_loss: 1.3268\n",
      "2/15, train_loss: 1.2547\n",
      "3/15, train_loss: 0.3645\n",
      "4/15, train_loss: 0.3402\n",
      "5/15, train_loss: 1.2681\n",
      "6/15, train_loss: 1.3207\n",
      "7/15, train_loss: 1.2374\n",
      "8/15, train_loss: 0.2614\n",
      "9/15, train_loss: 0.2783\n",
      "10/15, train_loss: 0.2735\n",
      "11/15, train_loss: 0.4654\n",
      "12/15, train_loss: 0.2145\n",
      "13/15, train_loss: 0.2847\n",
      "14/15, train_loss: 0.2551\n",
      "15/15, train_loss: 0.2631\n",
      "16/15, train_loss: 0.2409\n",
      "epoch 207 average loss: 0.6031\n",
      "----------\n",
      "epoch 208/500\n",
      "1/15, train_loss: 1.2559\n",
      "2/15, train_loss: 1.2868\n",
      "3/15, train_loss: 0.3279\n",
      "4/15, train_loss: 0.3317\n",
      "5/15, train_loss: 1.3071\n",
      "6/15, train_loss: 1.2502\n",
      "7/15, train_loss: 1.2754\n",
      "8/15, train_loss: 0.2338\n",
      "9/15, train_loss: 0.2771\n",
      "10/15, train_loss: 0.2926\n",
      "11/15, train_loss: 0.3835\n",
      "12/15, train_loss: 0.2350\n",
      "13/15, train_loss: 0.2622\n",
      "14/15, train_loss: 0.2532\n",
      "15/15, train_loss: 0.3026\n",
      "16/15, train_loss: 0.2288\n",
      "epoch 208 average loss: 0.5940\n",
      "----------\n",
      "epoch 209/500\n",
      "1/15, train_loss: 1.2905\n",
      "2/15, train_loss: 1.2468\n",
      "3/15, train_loss: 0.3232\n",
      "4/15, train_loss: 0.3572\n",
      "5/15, train_loss: 1.2643\n",
      "6/15, train_loss: 1.2706\n",
      "7/15, train_loss: 1.2339\n",
      "8/15, train_loss: 0.2333\n",
      "9/15, train_loss: 0.3151\n",
      "10/15, train_loss: 0.2709\n",
      "11/15, train_loss: 0.4205\n",
      "12/15, train_loss: 0.2123\n",
      "13/15, train_loss: 0.2545\n",
      "14/15, train_loss: 0.2811\n",
      "15/15, train_loss: 0.2604\n",
      "16/15, train_loss: 0.2426\n",
      "epoch 209 average loss: 0.5923\n",
      "----------\n",
      "epoch 210/500\n",
      "1/15, train_loss: 1.2581\n",
      "2/15, train_loss: 1.2413\n",
      "3/15, train_loss: 0.3533\n",
      "4/15, train_loss: 0.3275\n",
      "5/15, train_loss: 1.2875\n",
      "6/15, train_loss: 1.2484\n",
      "7/15, train_loss: 1.2304\n",
      "8/15, train_loss: 0.2718\n",
      "9/15, train_loss: 0.2763\n",
      "10/15, train_loss: 0.3027\n",
      "11/15, train_loss: 0.3764\n",
      "12/15, train_loss: 0.2103\n",
      "13/15, train_loss: 0.2758\n",
      "14/15, train_loss: 0.2509\n",
      "15/15, train_loss: 0.2839\n",
      "16/15, train_loss: 0.2257\n",
      "epoch 210 average loss: 0.5888\n",
      "----------\n",
      "epoch 211/500\n",
      "1/15, train_loss: 1.2525\n",
      "2/15, train_loss: 1.2660\n",
      "3/15, train_loss: 0.3255\n",
      "4/15, train_loss: 0.3523\n",
      "5/15, train_loss: 1.2630\n",
      "6/15, train_loss: 1.2409\n",
      "7/15, train_loss: 1.2760\n",
      "8/15, train_loss: 0.2311\n",
      "9/15, train_loss: 0.3131\n",
      "10/15, train_loss: 0.2682\n",
      "11/15, train_loss: 0.3675\n",
      "12/15, train_loss: 0.2319\n",
      "13/15, train_loss: 0.2573\n",
      "14/15, train_loss: 0.2748\n",
      "15/15, train_loss: 0.2615\n",
      "16/15, train_loss: 0.2238\n",
      "epoch 211 average loss: 0.5878\n",
      "----------\n",
      "epoch 212/500\n",
      "1/15, train_loss: 1.2969\n",
      "2/15, train_loss: 1.2385\n",
      "3/15, train_loss: 0.3547\n",
      "4/15, train_loss: 0.3295\n",
      "5/15, train_loss: 1.2540\n",
      "6/15, train_loss: 1.2924\n",
      "7/15, train_loss: 1.2237\n",
      "8/15, train_loss: 0.2656\n",
      "9/15, train_loss: 0.2734\n",
      "10/15, train_loss: 0.2670\n",
      "11/15, train_loss: 0.4345\n",
      "12/15, train_loss: 0.2109\n",
      "13/15, train_loss: 0.2803\n",
      "14/15, train_loss: 0.2501\n",
      "15/15, train_loss: 0.2581\n",
      "16/15, train_loss: 0.2344\n",
      "epoch 212 average loss: 0.5915\n",
      "----------\n",
      "epoch 213/500\n",
      "1/15, train_loss: 1.2433\n",
      "2/15, train_loss: 1.2738\n",
      "3/15, train_loss: 0.3221\n",
      "4/15, train_loss: 0.3255\n",
      "5/15, train_loss: 1.2708\n",
      "6/15, train_loss: 1.2456\n",
      "7/15, train_loss: 1.2388\n",
      "8/15, train_loss: 0.2327\n",
      "9/15, train_loss: 0.2701\n",
      "10/15, train_loss: 0.3058\n",
      "11/15, train_loss: 0.3658\n",
      "12/15, train_loss: 0.2397\n",
      "13/15, train_loss: 0.2575\n",
      "14/15, train_loss: 0.2480\n",
      "15/15, train_loss: 0.2997\n",
      "16/15, train_loss: 0.2233\n",
      "epoch 213 average loss: 0.5851\n",
      "----------\n",
      "epoch 214/500\n",
      "1/15, train_loss: 1.2652\n",
      "2/15, train_loss: 1.2437\n",
      "3/15, train_loss: 0.3172\n",
      "4/15, train_loss: 0.3629\n",
      "5/15, train_loss: 1.2480\n",
      "6/15, train_loss: 1.2799\n",
      "7/15, train_loss: 1.2197\n",
      "8/15, train_loss: 0.2318\n",
      "9/15, train_loss: 0.3058\n",
      "10/15, train_loss: 0.2667\n",
      "11/15, train_loss: 0.4436\n",
      "12/15, train_loss: 0.2087\n",
      "13/15, train_loss: 0.2527\n",
      "14/15, train_loss: 0.2698\n",
      "15/15, train_loss: 0.2564\n",
      "16/15, train_loss: 0.2398\n",
      "epoch 214 average loss: 0.5882\n",
      "----------\n",
      "epoch 215/500\n",
      "1/15, train_loss: 1.2384\n",
      "2/15, train_loss: 1.2339\n",
      "3/15, train_loss: 0.3526\n",
      "4/15, train_loss: 0.3246\n",
      "5/15, train_loss: 1.2783\n",
      "6/15, train_loss: 1.2384\n",
      "7/15, train_loss: 1.2152\n",
      "8/15, train_loss: 0.2604\n",
      "9/15, train_loss: 0.2679\n",
      "10/15, train_loss: 0.2852\n",
      "11/15, train_loss: 0.3711\n",
      "12/15, train_loss: 0.2059\n",
      "13/15, train_loss: 0.3000\n",
      "14/15, train_loss: 0.2445\n",
      "15/15, train_loss: 0.2864\n",
      "16/15, train_loss: 0.2204\n",
      "epoch 215 average loss: 0.5827\n",
      "----------\n",
      "epoch 216/500\n",
      "1/15, train_loss: 1.2335\n",
      "2/15, train_loss: 1.2669\n",
      "3/15, train_loss: 0.3175\n",
      "4/15, train_loss: 0.3514\n",
      "5/15, train_loss: 1.2452\n",
      "6/15, train_loss: 1.2288\n",
      "7/15, train_loss: 1.2477\n",
      "8/15, train_loss: 0.2263\n",
      "9/15, train_loss: 0.3039\n",
      "10/15, train_loss: 0.2608\n",
      "11/15, train_loss: 0.3644\n",
      "12/15, train_loss: 0.2202\n",
      "13/15, train_loss: 0.2506\n",
      "14/15, train_loss: 0.2625\n",
      "15/15, train_loss: 0.2561\n",
      "16/15, train_loss: 0.2200\n",
      "epoch 216 average loss: 0.5785\n",
      "----------\n",
      "epoch 217/500\n",
      "1/15, train_loss: 1.2658\n",
      "2/15, train_loss: 1.2276\n",
      "3/15, train_loss: 0.3437\n",
      "4/15, train_loss: 0.3205\n",
      "5/15, train_loss: 1.2404\n",
      "6/15, train_loss: 1.2624\n",
      "7/15, train_loss: 1.2114\n",
      "8/15, train_loss: 0.2527\n",
      "9/15, train_loss: 0.2667\n",
      "10/15, train_loss: 0.2594\n",
      "11/15, train_loss: 0.4229\n",
      "12/15, train_loss: 0.2031\n",
      "13/15, train_loss: 0.2730\n",
      "14/15, train_loss: 0.2442\n",
      "15/15, train_loss: 0.2511\n",
      "16/15, train_loss: 0.2418\n",
      "epoch 217 average loss: 0.5804\n",
      "----------\n",
      "epoch 218/500\n",
      "1/15, train_loss: 1.2320\n",
      "2/15, train_loss: 1.2563\n",
      "3/15, train_loss: 0.3140\n",
      "4/15, train_loss: 0.3164\n",
      "5/15, train_loss: 1.2755\n",
      "6/15, train_loss: 1.2258\n",
      "7/15, train_loss: 1.2358\n",
      "8/15, train_loss: 0.2245\n",
      "9/15, train_loss: 0.2645\n",
      "10/15, train_loss: 0.3044\n",
      "11/15, train_loss: 0.3606\n",
      "12/15, train_loss: 0.2323\n",
      "13/15, train_loss: 0.2473\n",
      "14/15, train_loss: 0.2420\n",
      "15/15, train_loss: 0.2875\n",
      "16/15, train_loss: 0.2177\n",
      "epoch 218 average loss: 0.5773\n",
      "----------\n",
      "epoch 219/500\n",
      "1/15, train_loss: 1.2778\n",
      "2/15, train_loss: 1.2223\n",
      "3/15, train_loss: 0.3098\n",
      "4/15, train_loss: 0.3552\n",
      "5/15, train_loss: 1.2382\n",
      "6/15, train_loss: 1.2512\n",
      "7/15, train_loss: 1.2084\n",
      "8/15, train_loss: 0.2216\n",
      "9/15, train_loss: 0.2994\n",
      "10/15, train_loss: 0.2565\n",
      "11/15, train_loss: 0.3924\n",
      "12/15, train_loss: 0.2012\n",
      "13/15, train_loss: 0.2424\n",
      "14/15, train_loss: 0.2696\n",
      "15/15, train_loss: 0.2469\n",
      "16/15, train_loss: 0.2374\n",
      "epoch 219 average loss: 0.5769\n",
      "----------\n",
      "epoch 220/500\n",
      "1/15, train_loss: 1.2312\n",
      "2/15, train_loss: 1.2184\n",
      "3/15, train_loss: 0.3397\n",
      "4/15, train_loss: 0.3121\n",
      "5/15, train_loss: 1.2757\n",
      "6/15, train_loss: 1.2213\n",
      "7/15, train_loss: 1.2036\n",
      "8/15, train_loss: 0.2496\n",
      "9/15, train_loss: 0.2628\n",
      "10/15, train_loss: 0.2780\n",
      "11/15, train_loss: 0.3561\n",
      "12/15, train_loss: 0.2004\n",
      "13/15, train_loss: 0.2643\n",
      "14/15, train_loss: 0.2389\n",
      "15/15, train_loss: 0.2691\n",
      "16/15, train_loss: 0.2152\n",
      "epoch 220 average loss: 0.5710\n",
      "----------\n",
      "epoch 221/500\n",
      "1/15, train_loss: 1.2232\n",
      "2/15, train_loss: 1.2454\n",
      "3/15, train_loss: 0.3071\n",
      "4/15, train_loss: 0.3334\n",
      "5/15, train_loss: 1.2328\n",
      "6/15, train_loss: 1.2203\n",
      "7/15, train_loss: 1.2325\n",
      "8/15, train_loss: 0.2210\n",
      "9/15, train_loss: 0.2939\n",
      "10/15, train_loss: 0.2543\n",
      "11/15, train_loss: 0.3433\n",
      "12/15, train_loss: 0.2152\n",
      "13/15, train_loss: 0.2454\n",
      "14/15, train_loss: 0.2515\n",
      "15/15, train_loss: 0.2510\n",
      "16/15, train_loss: 0.2142\n",
      "epoch 221 average loss: 0.5678\n",
      "----------\n",
      "epoch 222/500\n",
      "1/15, train_loss: 1.2451\n",
      "2/15, train_loss: 1.2198\n",
      "3/15, train_loss: 0.3247\n",
      "4/15, train_loss: 0.3162\n",
      "5/15, train_loss: 1.2260\n",
      "6/15, train_loss: 1.2658\n",
      "7/15, train_loss: 1.2003\n",
      "8/15, train_loss: 0.2465\n",
      "9/15, train_loss: 0.2583\n",
      "10/15, train_loss: 0.2536\n",
      "11/15, train_loss: 0.4045\n",
      "12/15, train_loss: 0.2007\n",
      "13/15, train_loss: 0.2763\n",
      "14/15, train_loss: 0.2381\n",
      "15/15, train_loss: 0.2474\n",
      "16/15, train_loss: 0.2294\n",
      "epoch 222 average loss: 0.5720\n",
      "----------\n",
      "epoch 223/500\n",
      "1/15, train_loss: 1.2188\n",
      "2/15, train_loss: 1.2435\n",
      "3/15, train_loss: 0.3049\n",
      "4/15, train_loss: 0.3084\n",
      "5/15, train_loss: 1.2505\n",
      "6/15, train_loss: 1.2140\n",
      "7/15, train_loss: 1.2294\n",
      "8/15, train_loss: 0.2185\n",
      "9/15, train_loss: 0.2572\n",
      "10/15, train_loss: 0.2801\n",
      "11/15, train_loss: 0.3435\n",
      "12/15, train_loss: 0.2257\n",
      "13/15, train_loss: 0.2489\n",
      "14/15, train_loss: 0.2351\n",
      "15/15, train_loss: 0.2875\n",
      "16/15, train_loss: 0.2128\n",
      "epoch 223 average loss: 0.5674\n",
      "----------\n",
      "epoch 224/500\n",
      "1/15, train_loss: 1.2441\n",
      "2/15, train_loss: 1.2146\n",
      "3/15, train_loss: 0.3007\n",
      "4/15, train_loss: 0.3438\n",
      "5/15, train_loss: 1.2218\n",
      "6/15, train_loss: 1.2505\n",
      "7/15, train_loss: 1.1953\n",
      "8/15, train_loss: 0.2162\n",
      "9/15, train_loss: 0.2877\n",
      "10/15, train_loss: 0.2525\n",
      "11/15, train_loss: 0.3845\n",
      "12/15, train_loss: 0.1976\n",
      "13/15, train_loss: 0.2400\n",
      "14/15, train_loss: 0.2570\n",
      "15/15, train_loss: 0.2431\n",
      "16/15, train_loss: 0.2245\n",
      "epoch 224 average loss: 0.5671\n",
      "----------\n",
      "epoch 225/500\n",
      "1/15, train_loss: 1.2161\n",
      "2/15, train_loss: 1.2078\n",
      "3/15, train_loss: 0.3288\n",
      "4/15, train_loss: 0.3031\n",
      "5/15, train_loss: 1.2457\n",
      "6/15, train_loss: 1.2090\n",
      "7/15, train_loss: 1.1908\n",
      "8/15, train_loss: 0.2391\n",
      "9/15, train_loss: 0.2531\n",
      "10/15, train_loss: 0.2719\n",
      "11/15, train_loss: 0.3434\n",
      "12/15, train_loss: 0.1953\n",
      "13/15, train_loss: 0.2709\n",
      "14/15, train_loss: 0.2331\n",
      "15/15, train_loss: 0.2599\n",
      "16/15, train_loss: 0.2104\n",
      "epoch 225 average loss: 0.5612\n",
      "----------\n",
      "epoch 226/500\n",
      "1/15, train_loss: 1.2109\n",
      "2/15, train_loss: 1.2300\n",
      "3/15, train_loss: 0.3018\n",
      "4/15, train_loss: 0.3203\n",
      "5/15, train_loss: 1.2201\n",
      "6/15, train_loss: 1.2025\n",
      "7/15, train_loss: 1.2143\n",
      "8/15, train_loss: 0.2151\n",
      "9/15, train_loss: 0.2723\n",
      "10/15, train_loss: 0.2495\n",
      "11/15, train_loss: 0.3427\n",
      "12/15, train_loss: 0.2113\n",
      "13/15, train_loss: 0.2385\n",
      "14/15, train_loss: 0.2468\n",
      "15/15, train_loss: 0.2427\n",
      "16/15, train_loss: 0.2098\n",
      "epoch 226 average loss: 0.5580\n",
      "----------\n",
      "epoch 227/500\n",
      "1/15, train_loss: 1.2323\n",
      "2/15, train_loss: 1.2050\n",
      "3/15, train_loss: 0.3274\n",
      "4/15, train_loss: 0.3038\n",
      "5/15, train_loss: 1.2150\n",
      "6/15, train_loss: 1.2252\n",
      "7/15, train_loss: 1.1864\n",
      "8/15, train_loss: 0.2392\n",
      "9/15, train_loss: 0.2543\n",
      "10/15, train_loss: 0.2474\n",
      "11/15, train_loss: 0.4037\n",
      "12/15, train_loss: 0.1940\n",
      "13/15, train_loss: 0.2594\n",
      "14/15, train_loss: 0.2320\n",
      "15/15, train_loss: 0.2389\n",
      "16/15, train_loss: 0.2279\n",
      "epoch 227 average loss: 0.5620\n",
      "----------\n",
      "epoch 228/500\n",
      "1/15, train_loss: 1.2071\n",
      "2/15, train_loss: 1.2178\n",
      "3/15, train_loss: 0.2996\n",
      "4/15, train_loss: 0.3005\n",
      "5/15, train_loss: 1.2465\n",
      "6/15, train_loss: 1.2026\n",
      "7/15, train_loss: 1.2108\n",
      "8/15, train_loss: 0.2134\n",
      "9/15, train_loss: 0.2504\n",
      "10/15, train_loss: 0.2757\n",
      "11/15, train_loss: 0.3415\n",
      "12/15, train_loss: 0.2124\n",
      "13/15, train_loss: 0.2361\n",
      "14/15, train_loss: 0.2300\n",
      "15/15, train_loss: 0.2636\n",
      "16/15, train_loss: 0.2090\n",
      "epoch 228 average loss: 0.5573\n",
      "----------\n",
      "epoch 229/500\n",
      "1/15, train_loss: 1.2648\n",
      "2/15, train_loss: 1.2054\n",
      "3/15, train_loss: 0.2951\n",
      "4/15, train_loss: 0.3274\n",
      "5/15, train_loss: 1.2147\n",
      "6/15, train_loss: 1.2383\n",
      "7/15, train_loss: 1.1913\n",
      "8/15, train_loss: 0.2120\n",
      "9/15, train_loss: 0.2837\n",
      "10/15, train_loss: 0.2448\n",
      "11/15, train_loss: 0.3829\n",
      "12/15, train_loss: 0.1922\n",
      "13/15, train_loss: 0.2337\n",
      "14/15, train_loss: 0.2470\n",
      "15/15, train_loss: 0.2379\n",
      "16/15, train_loss: 0.2282\n",
      "epoch 229 average loss: 0.5625\n",
      "----------\n",
      "epoch 230/500\n",
      "1/15, train_loss: 1.2078\n",
      "2/15, train_loss: 1.2035\n",
      "3/15, train_loss: 0.3300\n",
      "4/15, train_loss: 0.3003\n",
      "5/15, train_loss: 1.2433\n",
      "6/15, train_loss: 1.2045\n",
      "7/15, train_loss: 1.1844\n",
      "8/15, train_loss: 0.2451\n",
      "9/15, train_loss: 0.2491\n",
      "10/15, train_loss: 0.2631\n",
      "11/15, train_loss: 0.3380\n",
      "12/15, train_loss: 0.1920\n",
      "13/15, train_loss: 0.2674\n",
      "14/15, train_loss: 0.2279\n",
      "15/15, train_loss: 0.2695\n",
      "16/15, train_loss: 0.2066\n",
      "epoch 230 average loss: 0.5583\n",
      "----------\n",
      "epoch 231/500\n",
      "1/15, train_loss: 1.1999\n",
      "2/15, train_loss: 1.2450\n",
      "3/15, train_loss: 0.2950\n",
      "4/15, train_loss: 0.3308\n",
      "5/15, train_loss: 1.2091\n",
      "6/15, train_loss: 1.1943\n",
      "7/15, train_loss: 1.2187\n",
      "8/15, train_loss: 0.2105\n",
      "9/15, train_loss: 0.2930\n",
      "10/15, train_loss: 0.2428\n",
      "11/15, train_loss: 0.3276\n",
      "12/15, train_loss: 0.2107\n",
      "13/15, train_loss: 0.2349\n",
      "14/15, train_loss: 0.2404\n",
      "15/15, train_loss: 0.2408\n",
      "16/15, train_loss: 0.2039\n",
      "epoch 231 average loss: 0.5561\n",
      "----------\n",
      "epoch 232/500\n",
      "1/15, train_loss: 1.2255\n",
      "2/15, train_loss: 1.1972\n",
      "3/15, train_loss: 0.3201\n",
      "4/15, train_loss: 0.2972\n",
      "5/15, train_loss: 1.2030\n",
      "6/15, train_loss: 1.2468\n",
      "7/15, train_loss: 1.1781\n",
      "8/15, train_loss: 0.2393\n",
      "9/15, train_loss: 0.2455\n",
      "10/15, train_loss: 0.2394\n",
      "11/15, train_loss: 0.3778\n",
      "12/15, train_loss: 0.1883\n",
      "13/15, train_loss: 0.2549\n",
      "14/15, train_loss: 0.2254\n",
      "15/15, train_loss: 0.2313\n",
      "16/15, train_loss: 0.2149\n",
      "epoch 232 average loss: 0.5553\n",
      "----------\n",
      "epoch 233/500\n",
      "1/15, train_loss: 1.1964\n",
      "2/15, train_loss: 1.2127\n",
      "3/15, train_loss: 0.2905\n",
      "4/15, train_loss: 0.2913\n",
      "5/15, train_loss: 1.2184\n",
      "6/15, train_loss: 1.1914\n",
      "7/15, train_loss: 1.1926\n",
      "8/15, train_loss: 0.2086\n",
      "9/15, train_loss: 0.2417\n",
      "10/15, train_loss: 0.2538\n",
      "11/15, train_loss: 0.3264\n",
      "12/15, train_loss: 0.1979\n",
      "13/15, train_loss: 0.2322\n",
      "14/15, train_loss: 0.2244\n",
      "15/15, train_loss: 0.2552\n",
      "16/15, train_loss: 0.2060\n",
      "epoch 233 average loss: 0.5462\n",
      "----------\n",
      "epoch 234/500\n",
      "1/15, train_loss: 1.2136\n",
      "2/15, train_loss: 1.1937\n",
      "3/15, train_loss: 0.2868\n",
      "4/15, train_loss: 0.3218\n",
      "5/15, train_loss: 1.1998\n",
      "6/15, train_loss: 1.2278\n",
      "7/15, train_loss: 1.1748\n",
      "8/15, train_loss: 0.2070\n",
      "9/15, train_loss: 0.2696\n",
      "10/15, train_loss: 0.2386\n",
      "11/15, train_loss: 0.3616\n",
      "12/15, train_loss: 0.1870\n",
      "13/15, train_loss: 0.2294\n",
      "14/15, train_loss: 0.2473\n",
      "15/15, train_loss: 0.2329\n",
      "16/15, train_loss: 0.2201\n",
      "epoch 234 average loss: 0.5508\n",
      "----------\n",
      "epoch 235/500\n",
      "1/15, train_loss: 1.1962\n",
      "2/15, train_loss: 1.1885\n",
      "3/15, train_loss: 0.3158\n",
      "4/15, train_loss: 0.2883\n",
      "5/15, train_loss: 1.2250\n",
      "6/15, train_loss: 1.1874\n",
      "7/15, train_loss: 1.1728\n",
      "8/15, train_loss: 0.2288\n",
      "9/15, train_loss: 0.2440\n",
      "10/15, train_loss: 0.2613\n",
      "11/15, train_loss: 0.3375\n",
      "12/15, train_loss: 0.1858\n",
      "13/15, train_loss: 0.2537\n",
      "14/15, train_loss: 0.2224\n",
      "15/15, train_loss: 0.2567\n",
      "16/15, train_loss: 0.2020\n",
      "epoch 235 average loss: 0.5479\n",
      "----------\n",
      "epoch 236/500\n",
      "1/15, train_loss: 1.1915\n",
      "2/15, train_loss: 1.2145\n",
      "3/15, train_loss: 0.2859\n",
      "4/15, train_loss: 0.3170\n",
      "5/15, train_loss: 1.1989\n",
      "6/15, train_loss: 1.1841\n",
      "7/15, train_loss: 1.2052\n",
      "8/15, train_loss: 0.2046\n",
      "9/15, train_loss: 0.2703\n",
      "10/15, train_loss: 0.2374\n",
      "11/15, train_loss: 0.3261\n",
      "12/15, train_loss: 0.2084\n",
      "13/15, train_loss: 0.2259\n",
      "14/15, train_loss: 0.2402\n",
      "15/15, train_loss: 0.2284\n",
      "16/15, train_loss: 0.2017\n",
      "epoch 236 average loss: 0.5463\n",
      "----------\n",
      "epoch 237/500\n",
      "1/15, train_loss: 1.2286\n",
      "2/15, train_loss: 1.1894\n",
      "3/15, train_loss: 0.3088\n",
      "4/15, train_loss: 0.2918\n",
      "5/15, train_loss: 1.1918\n",
      "6/15, train_loss: 1.2123\n",
      "7/15, train_loss: 1.1680\n",
      "8/15, train_loss: 0.2224\n",
      "9/15, train_loss: 0.2390\n",
      "10/15, train_loss: 0.2354\n",
      "11/15, train_loss: 0.3795\n",
      "12/15, train_loss: 0.1856\n",
      "13/15, train_loss: 0.2353\n",
      "14/15, train_loss: 0.2223\n",
      "15/15, train_loss: 0.2268\n",
      "16/15, train_loss: 0.2150\n",
      "epoch 237 average loss: 0.5470\n",
      "----------\n",
      "epoch 238/500\n",
      "1/15, train_loss: 1.1868\n",
      "2/15, train_loss: 1.2013\n",
      "3/15, train_loss: 0.2840\n",
      "4/15, train_loss: 0.2854\n",
      "5/15, train_loss: 1.2179\n",
      "6/15, train_loss: 1.1791\n",
      "7/15, train_loss: 1.1881\n",
      "8/15, train_loss: 0.2030\n",
      "9/15, train_loss: 0.2371\n",
      "10/15, train_loss: 0.2595\n",
      "11/15, train_loss: 0.3184\n",
      "12/15, train_loss: 0.2051\n",
      "13/15, train_loss: 0.2257\n",
      "14/15, train_loss: 0.2192\n",
      "15/15, train_loss: 0.2578\n",
      "16/15, train_loss: 0.1985\n",
      "epoch 238 average loss: 0.5417\n",
      "----------\n",
      "epoch 239/500\n",
      "1/15, train_loss: 1.2133\n",
      "2/15, train_loss: 1.1824\n",
      "3/15, train_loss: 0.2818\n",
      "4/15, train_loss: 0.3118\n",
      "5/15, train_loss: 1.1898\n",
      "6/15, train_loss: 1.2063\n",
      "7/15, train_loss: 1.1674\n",
      "8/15, train_loss: 0.2016\n",
      "9/15, train_loss: 0.2740\n",
      "10/15, train_loss: 0.2330\n",
      "11/15, train_loss: 0.3690\n",
      "12/15, train_loss: 0.1841\n",
      "13/15, train_loss: 0.2222\n",
      "14/15, train_loss: 0.2359\n",
      "15/15, train_loss: 0.2267\n",
      "16/15, train_loss: 0.2120\n",
      "epoch 239 average loss: 0.5444\n",
      "----------\n",
      "epoch 240/500\n",
      "1/15, train_loss: 1.1828\n",
      "2/15, train_loss: 1.1796\n",
      "3/15, train_loss: 0.3143\n",
      "4/15, train_loss: 0.2825\n",
      "5/15, train_loss: 1.2174\n",
      "6/15, train_loss: 1.1785\n",
      "7/15, train_loss: 1.1630\n",
      "8/15, train_loss: 0.2277\n",
      "9/15, train_loss: 0.2353\n",
      "10/15, train_loss: 0.2476\n",
      "11/15, train_loss: 0.3170\n",
      "12/15, train_loss: 0.1809\n",
      "13/15, train_loss: 0.2559\n",
      "14/15, train_loss: 0.2165\n",
      "15/15, train_loss: 0.2443\n",
      "16/15, train_loss: 0.1967\n",
      "epoch 240 average loss: 0.5400\n",
      "----------\n",
      "epoch 241/500\n",
      "1/15, train_loss: 1.1780\n",
      "2/15, train_loss: 1.2130\n",
      "3/15, train_loss: 0.2769\n",
      "4/15, train_loss: 0.3029\n",
      "5/15, train_loss: 1.1845\n",
      "6/15, train_loss: 1.1731\n",
      "7/15, train_loss: 1.1793\n",
      "8/15, train_loss: 0.1994\n",
      "9/15, train_loss: 0.2537\n",
      "10/15, train_loss: 0.2325\n",
      "11/15, train_loss: 0.3096\n",
      "12/15, train_loss: 0.1921\n",
      "13/15, train_loss: 0.2226\n",
      "14/15, train_loss: 0.2278\n",
      "15/15, train_loss: 0.2284\n",
      "16/15, train_loss: 0.1953\n",
      "epoch 241 average loss: 0.5356\n",
      "----------\n",
      "epoch 242/500\n",
      "1/15, train_loss: 1.1986\n",
      "2/15, train_loss: 1.1792\n",
      "3/15, train_loss: 0.2972\n",
      "4/15, train_loss: 0.2847\n",
      "5/15, train_loss: 1.1818\n",
      "6/15, train_loss: 1.2202\n",
      "7/15, train_loss: 1.1579\n",
      "8/15, train_loss: 0.2192\n",
      "9/15, train_loss: 0.2364\n",
      "10/15, train_loss: 0.2292\n",
      "11/15, train_loss: 0.3970\n",
      "12/15, train_loss: 0.1797\n",
      "13/15, train_loss: 0.2404\n",
      "14/15, train_loss: 0.2150\n",
      "15/15, train_loss: 0.2235\n",
      "16/15, train_loss: 0.2062\n",
      "epoch 242 average loss: 0.5416\n",
      "----------\n",
      "epoch 243/500\n",
      "1/15, train_loss: 1.1774\n",
      "2/15, train_loss: 1.1976\n",
      "3/15, train_loss: 0.2777\n",
      "4/15, train_loss: 0.2795\n",
      "5/15, train_loss: 1.2072\n",
      "6/15, train_loss: 1.1706\n",
      "7/15, train_loss: 1.1771\n",
      "8/15, train_loss: 0.1989\n",
      "9/15, train_loss: 0.2343\n",
      "10/15, train_loss: 0.2597\n",
      "11/15, train_loss: 0.3142\n",
      "12/15, train_loss: 0.2014\n",
      "13/15, train_loss: 0.2243\n",
      "14/15, train_loss: 0.2138\n",
      "15/15, train_loss: 0.2546\n",
      "16/15, train_loss: 0.1952\n",
      "epoch 243 average loss: 0.5365\n",
      "----------\n",
      "epoch 244/500\n",
      "1/15, train_loss: 1.1950\n",
      "2/15, train_loss: 1.1750\n",
      "3/15, train_loss: 0.2745\n",
      "4/15, train_loss: 0.3161\n",
      "5/15, train_loss: 1.1801\n",
      "6/15, train_loss: 1.1969\n",
      "7/15, train_loss: 1.1574\n",
      "8/15, train_loss: 0.1974\n",
      "9/15, train_loss: 0.2611\n",
      "10/15, train_loss: 0.2291\n",
      "11/15, train_loss: 0.3444\n",
      "12/15, train_loss: 0.1797\n",
      "13/15, train_loss: 0.2199\n",
      "14/15, train_loss: 0.2369\n",
      "15/15, train_loss: 0.2233\n",
      "16/15, train_loss: 0.2069\n",
      "epoch 244 average loss: 0.5371\n",
      "----------\n",
      "epoch 245/500\n",
      "1/15, train_loss: 1.1741\n",
      "2/15, train_loss: 1.1706\n",
      "3/15, train_loss: 0.3056\n",
      "4/15, train_loss: 0.2784\n",
      "5/15, train_loss: 1.2001\n",
      "6/15, train_loss: 1.1684\n",
      "7/15, train_loss: 1.1536\n",
      "8/15, train_loss: 0.2205\n",
      "9/15, train_loss: 0.2324\n",
      "10/15, train_loss: 0.2454\n",
      "11/15, train_loss: 0.3172\n",
      "12/15, train_loss: 0.1780\n",
      "13/15, train_loss: 0.2525\n",
      "14/15, train_loss: 0.2120\n",
      "15/15, train_loss: 0.2515\n",
      "16/15, train_loss: 0.1920\n",
      "epoch 245 average loss: 0.5345\n",
      "----------\n",
      "epoch 246/500\n",
      "1/15, train_loss: 1.1703\n",
      "2/15, train_loss: 1.1997\n",
      "3/15, train_loss: 0.2725\n",
      "4/15, train_loss: 0.3150\n",
      "5/15, train_loss: 1.1744\n",
      "6/15, train_loss: 1.1651\n",
      "7/15, train_loss: 1.1777\n",
      "8/15, train_loss: 0.1975\n",
      "9/15, train_loss: 0.2570\n",
      "10/15, train_loss: 0.2250\n",
      "11/15, train_loss: 0.3034\n",
      "12/15, train_loss: 0.1946\n",
      "13/15, train_loss: 0.2172\n",
      "14/15, train_loss: 0.2249\n",
      "15/15, train_loss: 0.2202\n",
      "16/15, train_loss: 0.1926\n",
      "epoch 246 average loss: 0.5317\n",
      "----------\n",
      "epoch 247/500\n",
      "1/15, train_loss: 1.1833\n",
      "2/15, train_loss: 1.1693\n",
      "3/15, train_loss: 0.2856\n",
      "4/15, train_loss: 0.2786\n",
      "5/15, train_loss: 1.1706\n",
      "6/15, train_loss: 1.1919\n",
      "7/15, train_loss: 1.1501\n",
      "8/15, train_loss: 0.2160\n",
      "9/15, train_loss: 0.2322\n",
      "10/15, train_loss: 0.2234\n",
      "11/15, train_loss: 0.3375\n",
      "12/15, train_loss: 0.1775\n",
      "13/15, train_loss: 0.2399\n",
      "14/15, train_loss: 0.2116\n",
      "15/15, train_loss: 0.2185\n",
      "16/15, train_loss: 0.2090\n",
      "epoch 247 average loss: 0.5309\n",
      "----------\n",
      "epoch 248/500\n",
      "1/15, train_loss: 1.1685\n",
      "2/15, train_loss: 1.1785\n",
      "3/15, train_loss: 0.2754\n",
      "4/15, train_loss: 0.2720\n",
      "5/15, train_loss: 1.1874\n",
      "6/15, train_loss: 1.1607\n",
      "7/15, train_loss: 1.1680\n",
      "8/15, train_loss: 0.1942\n",
      "9/15, train_loss: 0.2318\n",
      "10/15, train_loss: 0.2484\n",
      "11/15, train_loss: 0.3033\n",
      "12/15, train_loss: 0.1968\n",
      "13/15, train_loss: 0.2212\n",
      "14/15, train_loss: 0.2094\n",
      "15/15, train_loss: 0.2549\n",
      "16/15, train_loss: 0.1903\n",
      "epoch 248 average loss: 0.5288\n",
      "----------\n",
      "epoch 249/500\n",
      "1/15, train_loss: 1.1979\n",
      "2/15, train_loss: 1.1638\n",
      "3/15, train_loss: 0.2726\n",
      "4/15, train_loss: 0.3017\n",
      "5/15, train_loss: 1.1707\n",
      "6/15, train_loss: 1.1833\n",
      "7/15, train_loss: 1.1501\n",
      "8/15, train_loss: 0.1919\n",
      "9/15, train_loss: 0.2725\n",
      "10/15, train_loss: 0.2217\n",
      "11/15, train_loss: 0.3363\n",
      "12/15, train_loss: 0.1759\n",
      "13/15, train_loss: 0.2137\n",
      "14/15, train_loss: 0.2335\n",
      "15/15, train_loss: 0.2164\n",
      "16/15, train_loss: 0.2070\n",
      "epoch 249 average loss: 0.5318\n",
      "----------\n",
      "epoch 250/500\n",
      "1/15, train_loss: 1.1660\n",
      "2/15, train_loss: 1.1618\n",
      "3/15, train_loss: 0.3052\n",
      "4/15, train_loss: 0.2698\n",
      "5/15, train_loss: 1.2000\n",
      "6/15, train_loss: 1.1583\n",
      "7/15, train_loss: 1.1455\n",
      "8/15, train_loss: 0.2186\n",
      "9/15, train_loss: 0.2265\n",
      "10/15, train_loss: 0.2416\n",
      "11/15, train_loss: 0.3014\n",
      "12/15, train_loss: 0.1735\n",
      "13/15, train_loss: 0.2323\n",
      "14/15, train_loss: 0.2081\n",
      "15/15, train_loss: 0.2314\n",
      "16/15, train_loss: 0.1889\n",
      "epoch 250 average loss: 0.5268\n",
      "----------\n",
      "epoch 251/500\n",
      "1/15, train_loss: 1.1622\n",
      "2/15, train_loss: 1.1818\n",
      "3/15, train_loss: 0.2705\n",
      "4/15, train_loss: 0.2888\n",
      "5/15, train_loss: 1.1689\n",
      "6/15, train_loss: 1.1532\n",
      "7/15, train_loss: 1.1689\n",
      "8/15, train_loss: 0.1903\n",
      "9/15, train_loss: 0.2459\n",
      "10/15, train_loss: 0.2197\n",
      "11/15, train_loss: 0.2969\n",
      "12/15, train_loss: 0.1906\n",
      "13/15, train_loss: 0.2142\n",
      "14/15, train_loss: 0.2233\n",
      "15/15, train_loss: 0.2160\n",
      "16/15, train_loss: 0.1883\n",
      "epoch 251 average loss: 0.5237\n",
      "----------\n",
      "epoch 252/500\n",
      "1/15, train_loss: 1.1787\n",
      "2/15, train_loss: 1.1596\n",
      "3/15, train_loss: 0.2861\n",
      "4/15, train_loss: 0.2688\n",
      "5/15, train_loss: 1.1660\n",
      "6/15, train_loss: 1.1800\n",
      "7/15, train_loss: 1.1445\n",
      "8/15, train_loss: 0.2137\n",
      "9/15, train_loss: 0.2253\n",
      "10/15, train_loss: 0.2176\n",
      "11/15, train_loss: 0.3395\n",
      "12/15, train_loss: 0.1733\n",
      "13/15, train_loss: 0.2394\n",
      "14/15, train_loss: 0.2075\n",
      "15/15, train_loss: 0.2140\n",
      "16/15, train_loss: 0.2067\n",
      "epoch 252 average loss: 0.5263\n",
      "----------\n",
      "epoch 253/500\n",
      "1/15, train_loss: 1.1598\n",
      "2/15, train_loss: 1.1773\n",
      "3/15, train_loss: 0.2652\n",
      "4/15, train_loss: 0.2664\n",
      "5/15, train_loss: 1.1882\n",
      "6/15, train_loss: 1.1540\n",
      "7/15, train_loss: 1.1629\n",
      "8/15, train_loss: 0.1901\n",
      "9/15, train_loss: 0.2225\n",
      "10/15, train_loss: 0.2327\n",
      "11/15, train_loss: 0.2936\n",
      "12/15, train_loss: 0.1883\n",
      "13/15, train_loss: 0.2148\n",
      "14/15, train_loss: 0.2049\n",
      "15/15, train_loss: 0.2452\n",
      "16/15, train_loss: 0.1865\n",
      "epoch 253 average loss: 0.5220\n",
      "----------\n",
      "epoch 254/500\n",
      "1/15, train_loss: 1.1824\n",
      "2/15, train_loss: 1.1588\n",
      "3/15, train_loss: 0.2623\n",
      "4/15, train_loss: 0.2930\n",
      "5/15, train_loss: 1.1636\n",
      "6/15, train_loss: 1.1713\n",
      "7/15, train_loss: 1.1443\n",
      "8/15, train_loss: 0.1879\n",
      "9/15, train_loss: 0.2568\n",
      "10/15, train_loss: 0.2161\n",
      "11/15, train_loss: 0.3220\n",
      "12/15, train_loss: 0.1716\n",
      "13/15, train_loss: 0.2111\n",
      "14/15, train_loss: 0.2208\n",
      "15/15, train_loss: 0.2149\n",
      "16/15, train_loss: 0.1989\n",
      "epoch 254 average loss: 0.5235\n",
      "----------\n",
      "epoch 255/500\n",
      "1/15, train_loss: 1.1586\n",
      "2/15, train_loss: 1.1542\n",
      "3/15, train_loss: 0.2855\n",
      "4/15, train_loss: 0.2635\n",
      "5/15, train_loss: 1.1839\n",
      "6/15, train_loss: 1.1481\n",
      "7/15, train_loss: 1.1410\n",
      "8/15, train_loss: 0.2082\n",
      "9/15, train_loss: 0.2238\n",
      "10/15, train_loss: 0.2323\n",
      "11/15, train_loss: 0.3041\n",
      "12/15, train_loss: 0.1697\n",
      "13/15, train_loss: 0.2448\n",
      "14/15, train_loss: 0.2030\n",
      "15/15, train_loss: 0.2370\n",
      "16/15, train_loss: 0.1843\n",
      "epoch 255 average loss: 0.5214\n",
      "----------\n",
      "epoch 256/500\n",
      "1/15, train_loss: 1.1554\n",
      "2/15, train_loss: 1.1696\n",
      "3/15, train_loss: 0.2615\n",
      "4/15, train_loss: 0.2800\n",
      "5/15, train_loss: 1.1619\n",
      "6/15, train_loss: 1.1463\n",
      "7/15, train_loss: 1.1623\n",
      "8/15, train_loss: 0.1869\n",
      "9/15, train_loss: 0.2420\n",
      "10/15, train_loss: 0.2147\n",
      "11/15, train_loss: 0.2867\n",
      "12/15, train_loss: 0.1850\n",
      "13/15, train_loss: 0.2074\n",
      "14/15, train_loss: 0.2232\n",
      "15/15, train_loss: 0.2095\n",
      "16/15, train_loss: 0.1830\n",
      "epoch 256 average loss: 0.5172\n",
      "----------\n",
      "epoch 257/500\n",
      "1/15, train_loss: 1.1735\n",
      "2/15, train_loss: 1.1540\n",
      "3/15, train_loss: 0.2772\n",
      "4/15, train_loss: 0.2632\n",
      "5/15, train_loss: 1.1565\n",
      "6/15, train_loss: 1.1899\n",
      "7/15, train_loss: 1.1360\n",
      "8/15, train_loss: 0.2160\n",
      "9/15, train_loss: 0.2178\n",
      "10/15, train_loss: 0.2127\n",
      "11/15, train_loss: 0.3520\n",
      "12/15, train_loss: 0.1678\n",
      "13/15, train_loss: 0.2213\n",
      "14/15, train_loss: 0.2019\n",
      "15/15, train_loss: 0.2076\n",
      "16/15, train_loss: 0.1948\n",
      "epoch 257 average loss: 0.5214\n",
      "----------\n",
      "epoch 258/500\n",
      "1/15, train_loss: 1.1521\n",
      "2/15, train_loss: 1.1797\n",
      "3/15, train_loss: 0.2588\n",
      "4/15, train_loss: 0.2588\n",
      "5/15, train_loss: 1.1708\n",
      "6/15, train_loss: 1.1444\n",
      "7/15, train_loss: 1.1508\n",
      "8/15, train_loss: 0.1858\n",
      "9/15, train_loss: 0.2161\n",
      "10/15, train_loss: 0.2289\n",
      "11/15, train_loss: 0.2885\n",
      "12/15, train_loss: 0.1850\n",
      "13/15, train_loss: 0.2096\n",
      "14/15, train_loss: 0.2012\n",
      "15/15, train_loss: 0.2338\n",
      "16/15, train_loss: 0.1840\n",
      "epoch 258 average loss: 0.5155\n",
      "----------\n",
      "epoch 259/500\n",
      "1/15, train_loss: 1.1613\n",
      "2/15, train_loss: 1.1527\n",
      "3/15, train_loss: 0.2558\n",
      "4/15, train_loss: 0.2860\n",
      "5/15, train_loss: 1.1530\n",
      "6/15, train_loss: 1.1583\n",
      "7/15, train_loss: 1.1336\n",
      "8/15, train_loss: 0.1833\n",
      "9/15, train_loss: 0.2462\n",
      "10/15, train_loss: 0.2119\n",
      "11/15, train_loss: 0.3162\n",
      "12/15, train_loss: 0.1676\n",
      "13/15, train_loss: 0.2035\n",
      "14/15, train_loss: 0.2188\n",
      "15/15, train_loss: 0.2056\n",
      "16/15, train_loss: 0.1966\n",
      "epoch 259 average loss: 0.5156\n",
      "----------\n",
      "epoch 260/500\n",
      "1/15, train_loss: 1.1487\n",
      "2/15, train_loss: 1.1460\n",
      "3/15, train_loss: 0.2768\n",
      "4/15, train_loss: 0.2578\n",
      "5/15, train_loss: 1.1658\n",
      "6/15, train_loss: 1.1417\n",
      "7/15, train_loss: 1.1308\n",
      "8/15, train_loss: 0.2043\n",
      "9/15, train_loss: 0.2152\n",
      "10/15, train_loss: 0.2250\n",
      "11/15, train_loss: 0.2872\n",
      "12/15, train_loss: 0.1659\n",
      "13/15, train_loss: 0.2241\n",
      "14/15, train_loss: 0.1990\n",
      "15/15, train_loss: 0.2248\n",
      "16/15, train_loss: 0.1815\n",
      "epoch 260 average loss: 0.5122\n",
      "----------\n",
      "epoch 261/500\n",
      "1/15, train_loss: 1.1457\n",
      "2/15, train_loss: 1.1654\n",
      "3/15, train_loss: 0.2552\n",
      "4/15, train_loss: 0.2758\n",
      "5/15, train_loss: 1.1508\n",
      "6/15, train_loss: 1.1373\n",
      "7/15, train_loss: 1.1516\n",
      "8/15, train_loss: 0.1824\n",
      "9/15, train_loss: 0.2326\n",
      "10/15, train_loss: 0.2098\n",
      "11/15, train_loss: 0.2789\n",
      "12/15, train_loss: 0.1773\n",
      "13/15, train_loss: 0.2067\n",
      "14/15, train_loss: 0.2093\n",
      "15/15, train_loss: 0.2103\n",
      "16/15, train_loss: 0.1791\n",
      "epoch 261 average loss: 0.5105\n",
      "----------\n",
      "epoch 262/500\n",
      "1/15, train_loss: 1.1637\n",
      "2/15, train_loss: 1.1451\n",
      "3/15, train_loss: 0.2718\n",
      "4/15, train_loss: 0.2573\n",
      "5/15, train_loss: 1.1490\n",
      "6/15, train_loss: 1.1606\n",
      "7/15, train_loss: 1.1285\n",
      "8/15, train_loss: 0.2015\n",
      "9/15, train_loss: 0.2157\n",
      "10/15, train_loss: 0.2077\n",
      "11/15, train_loss: 0.3180\n",
      "12/15, train_loss: 0.1651\n",
      "13/15, train_loss: 0.2227\n",
      "14/15, train_loss: 0.1981\n",
      "15/15, train_loss: 0.2037\n",
      "16/15, train_loss: 0.1893\n",
      "epoch 262 average loss: 0.5124\n",
      "----------\n",
      "epoch 263/500\n",
      "1/15, train_loss: 1.1439\n",
      "2/15, train_loss: 1.1582\n",
      "3/15, train_loss: 0.2554\n",
      "4/15, train_loss: 0.2541\n",
      "5/15, train_loss: 1.1690\n",
      "6/15, train_loss: 1.1367\n",
      "7/15, train_loss: 1.1433\n",
      "8/15, train_loss: 0.1810\n",
      "9/15, train_loss: 0.2123\n",
      "10/15, train_loss: 0.2403\n",
      "11/15, train_loss: 0.2814\n",
      "12/15, train_loss: 0.1856\n",
      "13/15, train_loss: 0.2020\n",
      "14/15, train_loss: 0.1963\n",
      "15/15, train_loss: 0.2252\n",
      "16/15, train_loss: 0.1799\n",
      "epoch 263 average loss: 0.5103\n",
      "----------\n",
      "epoch 264/500\n",
      "1/15, train_loss: 1.1657\n",
      "2/15, train_loss: 1.1424\n",
      "3/15, train_loss: 0.2521\n",
      "4/15, train_loss: 0.2842\n",
      "5/15, train_loss: 1.1484\n",
      "6/15, train_loss: 1.1604\n",
      "7/15, train_loss: 1.1273\n",
      "8/15, train_loss: 0.1797\n",
      "9/15, train_loss: 0.2358\n",
      "10/15, train_loss: 0.2084\n",
      "11/15, train_loss: 0.3062\n",
      "12/15, train_loss: 0.1645\n",
      "13/15, train_loss: 0.1995\n",
      "14/15, train_loss: 0.2241\n",
      "15/15, train_loss: 0.2036\n",
      "16/15, train_loss: 0.1956\n",
      "epoch 264 average loss: 0.5124\n",
      "----------\n",
      "epoch 265/500\n",
      "1/15, train_loss: 1.1420\n",
      "2/15, train_loss: 1.1391\n",
      "3/15, train_loss: 0.2775\n",
      "4/15, train_loss: 0.2508\n",
      "5/15, train_loss: 1.1676\n",
      "6/15, train_loss: 1.1344\n",
      "7/15, train_loss: 1.1248\n",
      "8/15, train_loss: 0.2004\n",
      "9/15, train_loss: 0.2120\n",
      "10/15, train_loss: 0.2239\n",
      "11/15, train_loss: 0.2846\n",
      "12/15, train_loss: 0.1628\n",
      "13/15, train_loss: 0.2266\n",
      "14/15, train_loss: 0.1938\n",
      "15/15, train_loss: 0.2298\n",
      "16/15, train_loss: 0.1773\n",
      "epoch 265 average loss: 0.5092\n",
      "----------\n",
      "epoch 266/500\n",
      "1/15, train_loss: 1.1390\n",
      "2/15, train_loss: 1.1553\n",
      "3/15, train_loss: 0.2491\n",
      "4/15, train_loss: 0.2704\n",
      "5/15, train_loss: 1.1439\n",
      "6/15, train_loss: 1.1325\n",
      "7/15, train_loss: 1.1482\n",
      "8/15, train_loss: 0.1794\n",
      "9/15, train_loss: 0.2318\n",
      "10/15, train_loss: 0.2056\n",
      "11/15, train_loss: 0.2704\n",
      "12/15, train_loss: 0.1773\n",
      "13/15, train_loss: 0.1985\n",
      "14/15, train_loss: 0.2053\n",
      "15/15, train_loss: 0.2019\n",
      "16/15, train_loss: 0.1756\n",
      "epoch 266 average loss: 0.5053\n",
      "----------\n",
      "epoch 267/500\n",
      "1/15, train_loss: 1.1557\n",
      "2/15, train_loss: 1.1421\n",
      "3/15, train_loss: 0.2662\n",
      "4/15, train_loss: 0.2551\n",
      "5/15, train_loss: 1.1405\n",
      "6/15, train_loss: 1.1653\n",
      "7/15, train_loss: 1.1225\n",
      "8/15, train_loss: 0.2010\n",
      "9/15, train_loss: 0.2076\n",
      "10/15, train_loss: 0.2036\n",
      "11/15, train_loss: 0.3227\n",
      "12/15, train_loss: 0.1629\n",
      "13/15, train_loss: 0.2164\n",
      "14/15, train_loss: 0.1938\n",
      "15/15, train_loss: 0.2011\n",
      "16/15, train_loss: 0.1876\n",
      "epoch 267 average loss: 0.5090\n",
      "----------\n",
      "epoch 268/500\n",
      "1/15, train_loss: 1.1374\n",
      "2/15, train_loss: 1.1531\n",
      "3/15, train_loss: 0.2492\n",
      "4/15, train_loss: 0.2486\n",
      "5/15, train_loss: 1.1546\n",
      "6/15, train_loss: 1.1271\n",
      "7/15, train_loss: 1.1366\n",
      "8/15, train_loss: 0.1765\n",
      "9/15, train_loss: 0.2075\n",
      "10/15, train_loss: 0.2174\n",
      "11/15, train_loss: 0.2681\n",
      "12/15, train_loss: 0.1774\n",
      "13/15, train_loss: 0.2020\n",
      "14/15, train_loss: 0.1914\n",
      "15/15, train_loss: 0.2349\n",
      "16/15, train_loss: 0.1760\n",
      "epoch 268 average loss: 0.5036\n",
      "----------\n",
      "epoch 269/500\n",
      "1/15, train_loss: 1.1479\n",
      "2/15, train_loss: 1.1374\n",
      "3/15, train_loss: 0.2457\n",
      "4/15, train_loss: 0.2719\n",
      "5/15, train_loss: 1.1383\n",
      "6/15, train_loss: 1.1428\n",
      "7/15, train_loss: 1.1211\n",
      "8/15, train_loss: 0.1764\n",
      "9/15, train_loss: 0.2321\n",
      "10/15, train_loss: 0.2030\n",
      "11/15, train_loss: 0.2905\n",
      "12/15, train_loss: 0.1615\n",
      "13/15, train_loss: 0.1968\n",
      "14/15, train_loss: 0.2102\n",
      "15/15, train_loss: 0.1990\n",
      "16/15, train_loss: 0.1909\n",
      "epoch 269 average loss: 0.5041\n",
      "----------\n",
      "epoch 270/500\n",
      "1/15, train_loss: 1.1356\n",
      "2/15, train_loss: 1.1343\n",
      "3/15, train_loss: 0.2704\n",
      "4/15, train_loss: 0.2499\n",
      "5/15, train_loss: 1.1543\n",
      "6/15, train_loss: 1.1316\n",
      "7/15, train_loss: 1.1179\n",
      "8/15, train_loss: 0.2032\n",
      "9/15, train_loss: 0.2115\n",
      "10/15, train_loss: 0.2152\n",
      "11/15, train_loss: 0.2697\n",
      "12/15, train_loss: 0.1593\n",
      "13/15, train_loss: 0.2170\n",
      "14/15, train_loss: 0.1908\n",
      "15/15, train_loss: 0.2092\n",
      "16/15, train_loss: 0.1739\n",
      "epoch 270 average loss: 0.5027\n",
      "----------\n",
      "epoch 271/500\n",
      "1/15, train_loss: 1.1326\n",
      "2/15, train_loss: 1.1551\n",
      "3/15, train_loss: 0.2501\n",
      "4/15, train_loss: 0.2680\n",
      "5/15, train_loss: 1.1382\n",
      "6/15, train_loss: 1.1252\n",
      "7/15, train_loss: 1.1396\n",
      "8/15, train_loss: 0.1740\n",
      "9/15, train_loss: 0.2349\n",
      "10/15, train_loss: 0.1993\n",
      "11/15, train_loss: 0.2711\n",
      "12/15, train_loss: 0.1724\n",
      "13/15, train_loss: 0.1958\n",
      "14/15, train_loss: 0.2039\n",
      "15/15, train_loss: 0.1976\n",
      "16/15, train_loss: 0.1731\n",
      "epoch 271 average loss: 0.5019\n",
      "----------\n",
      "epoch 272/500\n",
      "1/15, train_loss: 1.1490\n",
      "2/15, train_loss: 1.1327\n",
      "3/15, train_loss: 0.2677\n",
      "4/15, train_loss: 0.2458\n",
      "5/15, train_loss: 1.1347\n",
      "6/15, train_loss: 1.1576\n",
      "7/15, train_loss: 1.1150\n",
      "8/15, train_loss: 0.1939\n",
      "9/15, train_loss: 0.2059\n",
      "10/15, train_loss: 0.1986\n",
      "11/15, train_loss: 0.3186\n",
      "12/15, train_loss: 0.1577\n",
      "13/15, train_loss: 0.2077\n",
      "14/15, train_loss: 0.1893\n",
      "15/15, train_loss: 0.1934\n",
      "16/15, train_loss: 0.1850\n",
      "epoch 272 average loss: 0.5033\n",
      "----------\n",
      "epoch 273/500\n",
      "1/15, train_loss: 1.1309\n",
      "2/15, train_loss: 1.1440\n",
      "3/15, train_loss: 0.2432\n",
      "4/15, train_loss: 0.2437\n",
      "5/15, train_loss: 1.1506\n",
      "6/15, train_loss: 1.1255\n",
      "7/15, train_loss: 1.1272\n",
      "8/15, train_loss: 0.1740\n",
      "9/15, train_loss: 0.2012\n",
      "10/15, train_loss: 0.2134\n",
      "11/15, train_loss: 0.2652\n",
      "12/15, train_loss: 0.1734\n",
      "13/15, train_loss: 0.1946\n",
      "14/15, train_loss: 0.1879\n",
      "15/15, train_loss: 0.2182\n",
      "16/15, train_loss: 0.1717\n",
      "epoch 273 average loss: 0.4978\n",
      "----------\n",
      "epoch 274/500\n",
      "1/15, train_loss: 1.1420\n",
      "2/15, train_loss: 1.1315\n",
      "3/15, train_loss: 0.2388\n",
      "4/15, train_loss: 0.2674\n",
      "5/15, train_loss: 1.1327\n",
      "6/15, train_loss: 1.1411\n",
      "7/15, train_loss: 1.1148\n",
      "8/15, train_loss: 0.1721\n",
      "9/15, train_loss: 0.2254\n",
      "10/15, train_loss: 0.1980\n",
      "11/15, train_loss: 0.2850\n",
      "12/15, train_loss: 0.1574\n",
      "13/15, train_loss: 0.1913\n",
      "14/15, train_loss: 0.2083\n",
      "15/15, train_loss: 0.1936\n",
      "16/15, train_loss: 0.1836\n",
      "epoch 274 average loss: 0.4989\n",
      "----------\n",
      "epoch 275/500\n",
      "1/15, train_loss: 1.1290\n",
      "2/15, train_loss: 1.1264\n",
      "3/15, train_loss: 0.2604\n",
      "4/15, train_loss: 0.2411\n",
      "5/15, train_loss: 1.1519\n",
      "6/15, train_loss: 1.1209\n",
      "7/15, train_loss: 1.1131\n",
      "8/15, train_loss: 0.1920\n",
      "9/15, train_loss: 0.2011\n",
      "10/15, train_loss: 0.2164\n",
      "11/15, train_loss: 0.2661\n",
      "12/15, train_loss: 0.1563\n",
      "13/15, train_loss: 0.2136\n",
      "14/15, train_loss: 0.1858\n",
      "15/15, train_loss: 0.2130\n",
      "16/15, train_loss: 0.1694\n",
      "epoch 275 average loss: 0.4973\n",
      "----------\n",
      "epoch 276/500\n",
      "1/15, train_loss: 1.1271\n",
      "2/15, train_loss: 1.1428\n",
      "3/15, train_loss: 0.2398\n",
      "4/15, train_loss: 0.2585\n",
      "5/15, train_loss: 1.1330\n",
      "6/15, train_loss: 1.1180\n",
      "7/15, train_loss: 1.1387\n",
      "8/15, train_loss: 0.1722\n",
      "9/15, train_loss: 0.2233\n",
      "10/15, train_loss: 0.1968\n",
      "11/15, train_loss: 0.2560\n",
      "12/15, train_loss: 0.1736\n",
      "13/15, train_loss: 0.1926\n",
      "14/15, train_loss: 0.2020\n",
      "15/15, train_loss: 0.1948\n",
      "16/15, train_loss: 0.1691\n",
      "epoch 276 average loss: 0.4961\n",
      "----------\n",
      "epoch 277/500\n",
      "1/15, train_loss: 1.1422\n",
      "2/15, train_loss: 1.1269\n",
      "3/15, train_loss: 0.2553\n",
      "4/15, train_loss: 0.2434\n",
      "5/15, train_loss: 1.1279\n",
      "6/15, train_loss: 1.1446\n",
      "7/15, train_loss: 1.1101\n",
      "8/15, train_loss: 0.1959\n",
      "9/15, train_loss: 0.2000\n",
      "10/15, train_loss: 0.1961\n",
      "11/15, train_loss: 0.2801\n",
      "12/15, train_loss: 0.1566\n",
      "13/15, train_loss: 0.2123\n",
      "14/15, train_loss: 0.1869\n",
      "15/15, train_loss: 0.1919\n",
      "16/15, train_loss: 0.1797\n",
      "epoch 277 average loss: 0.4969\n",
      "----------\n",
      "epoch 278/500\n",
      "1/15, train_loss: 1.1255\n",
      "2/15, train_loss: 1.1333\n",
      "3/15, train_loss: 0.2395\n",
      "4/15, train_loss: 0.2379\n",
      "5/15, train_loss: 1.1398\n",
      "6/15, train_loss: 1.1164\n",
      "7/15, train_loss: 1.1212\n",
      "8/15, train_loss: 0.1695\n",
      "9/15, train_loss: 0.2006\n",
      "10/15, train_loss: 0.2141\n",
      "11/15, train_loss: 0.2595\n",
      "12/15, train_loss: 0.1683\n",
      "13/15, train_loss: 0.1932\n",
      "14/15, train_loss: 0.1841\n",
      "15/15, train_loss: 0.2170\n",
      "16/15, train_loss: 0.1686\n",
      "epoch 278 average loss: 0.4930\n",
      "----------\n",
      "epoch 279/500\n",
      "1/15, train_loss: 1.1353\n",
      "2/15, train_loss: 1.1231\n",
      "3/15, train_loss: 0.2367\n",
      "4/15, train_loss: 0.2608\n",
      "5/15, train_loss: 1.1275\n",
      "6/15, train_loss: 1.1309\n",
      "7/15, train_loss: 1.1093\n",
      "8/15, train_loss: 0.1680\n",
      "9/15, train_loss: 0.2371\n",
      "10/15, train_loss: 0.1939\n",
      "11/15, train_loss: 0.2902\n",
      "12/15, train_loss: 0.1552\n",
      "13/15, train_loss: 0.1878\n",
      "14/15, train_loss: 0.1994\n",
      "15/15, train_loss: 0.1906\n",
      "16/15, train_loss: 0.1828\n",
      "epoch 279 average loss: 0.4955\n",
      "----------\n",
      "epoch 280/500\n",
      "1/15, train_loss: 1.1228\n",
      "2/15, train_loss: 1.1211\n",
      "3/15, train_loss: 0.2558\n",
      "4/15, train_loss: 0.2393\n",
      "5/15, train_loss: 1.1429\n",
      "6/15, train_loss: 1.1157\n",
      "7/15, train_loss: 1.1066\n",
      "8/15, train_loss: 0.1855\n",
      "9/15, train_loss: 0.1994\n",
      "10/15, train_loss: 0.2061\n",
      "11/15, train_loss: 0.2582\n",
      "12/15, train_loss: 0.1528\n",
      "13/15, train_loss: 0.2183\n",
      "14/15, train_loss: 0.1828\n",
      "15/15, train_loss: 0.2133\n",
      "16/15, train_loss: 0.1671\n",
      "epoch 280 average loss: 0.4930\n",
      "----------\n",
      "epoch 281/500\n",
      "1/15, train_loss: 1.1202\n",
      "2/15, train_loss: 1.1424\n",
      "3/15, train_loss: 0.2354\n",
      "4/15, train_loss: 0.2659\n",
      "5/15, train_loss: 1.1253\n",
      "6/15, train_loss: 1.1122\n",
      "7/15, train_loss: 1.1274\n",
      "8/15, train_loss: 0.1675\n",
      "9/15, train_loss: 0.2211\n",
      "10/15, train_loss: 0.1913\n",
      "11/15, train_loss: 0.2519\n",
      "12/15, train_loss: 0.1682\n",
      "13/15, train_loss: 0.1878\n",
      "14/15, train_loss: 0.1960\n",
      "15/15, train_loss: 0.1903\n",
      "16/15, train_loss: 0.1650\n",
      "epoch 281 average loss: 0.4917\n",
      "----------\n",
      "epoch 282/500\n",
      "1/15, train_loss: 1.1369\n",
      "2/15, train_loss: 1.1222\n",
      "3/15, train_loss: 0.2542\n",
      "4/15, train_loss: 0.2402\n",
      "5/15, train_loss: 1.1241\n",
      "6/15, train_loss: 1.1496\n",
      "7/15, train_loss: 1.1051\n",
      "8/15, train_loss: 0.1924\n",
      "9/15, train_loss: 0.1984\n",
      "10/15, train_loss: 0.1900\n",
      "11/15, train_loss: 0.2870\n",
      "12/15, train_loss: 0.1527\n",
      "13/15, train_loss: 0.1957\n",
      "14/15, train_loss: 0.1817\n",
      "15/15, train_loss: 0.1866\n",
      "16/15, train_loss: 0.1760\n",
      "epoch 282 average loss: 0.4933\n",
      "----------\n",
      "epoch 283/500\n",
      "1/15, train_loss: 1.1205\n",
      "2/15, train_loss: 1.1394\n",
      "3/15, train_loss: 0.2359\n",
      "4/15, train_loss: 0.2347\n",
      "5/15, train_loss: 1.1406\n",
      "6/15, train_loss: 1.1126\n",
      "7/15, train_loss: 1.1178\n",
      "8/15, train_loss: 0.1676\n",
      "9/15, train_loss: 0.1959\n",
      "10/15, train_loss: 0.2053\n",
      "11/15, train_loss: 0.2560\n",
      "12/15, train_loss: 0.1674\n",
      "13/15, train_loss: 0.1891\n",
      "14/15, train_loss: 0.1799\n",
      "15/15, train_loss: 0.2117\n",
      "16/15, train_loss: 0.1663\n",
      "epoch 283 average loss: 0.4900\n",
      "----------\n",
      "epoch 284/500\n",
      "1/15, train_loss: 1.1324\n",
      "2/15, train_loss: 1.1192\n",
      "3/15, train_loss: 0.2315\n",
      "4/15, train_loss: 0.2674\n",
      "5/15, train_loss: 1.1207\n",
      "6/15, train_loss: 1.1340\n",
      "7/15, train_loss: 1.1024\n",
      "8/15, train_loss: 0.1652\n",
      "9/15, train_loss: 0.2218\n",
      "10/15, train_loss: 0.1891\n",
      "11/15, train_loss: 0.2794\n",
      "12/15, train_loss: 0.1500\n",
      "13/15, train_loss: 0.1851\n",
      "14/15, train_loss: 0.2024\n",
      "15/15, train_loss: 0.1864\n",
      "16/15, train_loss: 0.1807\n",
      "epoch 284 average loss: 0.4917\n",
      "----------\n",
      "epoch 285/500\n",
      "1/15, train_loss: 1.1174\n",
      "2/15, train_loss: 1.1150\n",
      "3/15, train_loss: 0.2531\n",
      "4/15, train_loss: 0.2329\n",
      "5/15, train_loss: 1.1347\n",
      "6/15, train_loss: 1.1110\n",
      "7/15, train_loss: 1.1005\n",
      "8/15, train_loss: 0.1826\n",
      "9/15, train_loss: 0.1957\n",
      "10/15, train_loss: 0.1980\n",
      "11/15, train_loss: 0.2547\n",
      "12/15, train_loss: 0.1497\n",
      "13/15, train_loss: 0.1997\n",
      "14/15, train_loss: 0.1786\n",
      "15/15, train_loss: 0.1975\n",
      "16/15, train_loss: 0.1633\n",
      "epoch 285 average loss: 0.4865\n",
      "----------\n",
      "epoch 286/500\n",
      "1/15, train_loss: 1.1162\n",
      "2/15, train_loss: 1.1281\n",
      "3/15, train_loss: 0.2317\n",
      "4/15, train_loss: 0.2458\n",
      "5/15, train_loss: 1.1214\n",
      "6/15, train_loss: 1.1070\n",
      "7/15, train_loss: 1.1247\n",
      "8/15, train_loss: 0.1641\n",
      "9/15, train_loss: 0.2087\n",
      "10/15, train_loss: 0.1883\n",
      "11/15, train_loss: 0.2431\n",
      "12/15, train_loss: 0.1577\n",
      "13/15, train_loss: 0.1857\n",
      "14/15, train_loss: 0.1856\n",
      "15/15, train_loss: 0.1893\n",
      "16/15, train_loss: 0.1631\n",
      "epoch 286 average loss: 0.4850\n",
      "----------\n",
      "epoch 287/500\n",
      "1/15, train_loss: 1.1321\n",
      "2/15, train_loss: 1.1153\n",
      "3/15, train_loss: 0.2459\n",
      "4/15, train_loss: 0.2321\n",
      "5/15, train_loss: 1.1188\n",
      "6/15, train_loss: 1.1290\n",
      "7/15, train_loss: 1.1004\n",
      "8/15, train_loss: 0.1793\n",
      "9/15, train_loss: 0.1932\n",
      "10/15, train_loss: 0.1854\n",
      "11/15, train_loss: 0.2734\n",
      "12/15, train_loss: 0.1485\n",
      "13/15, train_loss: 0.2089\n",
      "14/15, train_loss: 0.1777\n",
      "15/15, train_loss: 0.1850\n",
      "16/15, train_loss: 0.1799\n",
      "epoch 287 average loss: 0.4878\n",
      "----------\n",
      "epoch 288/500\n",
      "1/15, train_loss: 1.1133\n",
      "2/15, train_loss: 1.1349\n",
      "3/15, train_loss: 0.2281\n",
      "4/15, train_loss: 0.2288\n",
      "5/15, train_loss: 1.1397\n",
      "6/15, train_loss: 1.1068\n",
      "7/15, train_loss: 1.1198\n",
      "8/15, train_loss: 0.1631\n",
      "9/15, train_loss: 0.1893\n",
      "10/15, train_loss: 0.2020\n",
      "11/15, train_loss: 0.2415\n",
      "12/15, train_loss: 0.1657\n",
      "13/15, train_loss: 0.1843\n",
      "14/15, train_loss: 0.1764\n",
      "15/15, train_loss: 0.2104\n",
      "16/15, train_loss: 0.1619\n",
      "epoch 288 average loss: 0.4854\n",
      "----------\n",
      "epoch 289/500\n",
      "1/15, train_loss: 1.1287\n",
      "2/15, train_loss: 1.1145\n",
      "3/15, train_loss: 0.2254\n",
      "4/15, train_loss: 0.2527\n",
      "5/15, train_loss: 1.1164\n",
      "6/15, train_loss: 1.1270\n",
      "7/15, train_loss: 1.0987\n",
      "8/15, train_loss: 0.1608\n",
      "9/15, train_loss: 0.2154\n",
      "10/15, train_loss: 0.1853\n",
      "11/15, train_loss: 0.2664\n",
      "12/15, train_loss: 0.1480\n",
      "13/15, train_loss: 0.1796\n",
      "14/15, train_loss: 0.1946\n",
      "15/15, train_loss: 0.1820\n",
      "16/15, train_loss: 0.1723\n",
      "epoch 289 average loss: 0.4855\n",
      "----------\n",
      "epoch 290/500\n",
      "1/15, train_loss: 1.1109\n",
      "2/15, train_loss: 1.1123\n",
      "3/15, train_loss: 0.2461\n",
      "4/15, train_loss: 0.2292\n",
      "5/15, train_loss: 1.1340\n",
      "6/15, train_loss: 1.1079\n",
      "7/15, train_loss: 1.0946\n",
      "8/15, train_loss: 0.1768\n",
      "9/15, train_loss: 0.1900\n",
      "10/15, train_loss: 0.2022\n",
      "11/15, train_loss: 0.2426\n",
      "12/15, train_loss: 0.1475\n",
      "13/15, train_loss: 0.1974\n",
      "14/15, train_loss: 0.1759\n",
      "15/15, train_loss: 0.1963\n",
      "16/15, train_loss: 0.1615\n",
      "epoch 290 average loss: 0.4828\n",
      "----------\n",
      "epoch 291/500\n",
      "1/15, train_loss: 1.1100\n",
      "2/15, train_loss: 1.1296\n",
      "3/15, train_loss: 0.2256\n",
      "4/15, train_loss: 0.2471\n",
      "5/15, train_loss: 1.1176\n",
      "6/15, train_loss: 1.1028\n",
      "7/15, train_loss: 1.1166\n",
      "8/15, train_loss: 0.1595\n",
      "9/15, train_loss: 0.2092\n",
      "10/15, train_loss: 0.1841\n",
      "11/15, train_loss: 0.2424\n",
      "12/15, train_loss: 0.1685\n",
      "13/15, train_loss: 0.1819\n",
      "14/15, train_loss: 0.1916\n",
      "15/15, train_loss: 0.1835\n",
      "16/15, train_loss: 0.1596\n",
      "epoch 291 average loss: 0.4831\n",
      "----------\n",
      "epoch 292/500\n",
      "1/15, train_loss: 1.1303\n",
      "2/15, train_loss: 1.1085\n",
      "3/15, train_loss: 0.2500\n",
      "4/15, train_loss: 0.2272\n",
      "5/15, train_loss: 1.1141\n",
      "6/15, train_loss: 1.1217\n",
      "7/15, train_loss: 1.0950\n",
      "8/15, train_loss: 0.1797\n",
      "9/15, train_loss: 0.1926\n",
      "10/15, train_loss: 0.1837\n",
      "11/15, train_loss: 0.2745\n",
      "12/15, train_loss: 0.1469\n",
      "13/15, train_loss: 0.1981\n",
      "14/15, train_loss: 0.1754\n",
      "15/15, train_loss: 0.1809\n",
      "16/15, train_loss: 0.1769\n",
      "epoch 292 average loss: 0.4847\n",
      "----------\n",
      "epoch 293/500\n",
      "1/15, train_loss: 1.1082\n",
      "2/15, train_loss: 1.1233\n",
      "3/15, train_loss: 0.2244\n",
      "4/15, train_loss: 0.2259\n",
      "5/15, train_loss: 1.1267\n",
      "6/15, train_loss: 1.1028\n",
      "7/15, train_loss: 1.1096\n",
      "8/15, train_loss: 0.1607\n",
      "9/15, train_loss: 0.1870\n",
      "10/15, train_loss: 0.2005\n",
      "11/15, train_loss: 0.2384\n",
      "12/15, train_loss: 0.1628\n",
      "13/15, train_loss: 0.1792\n",
      "14/15, train_loss: 0.1727\n",
      "15/15, train_loss: 0.2070\n",
      "16/15, train_loss: 0.1582\n",
      "epoch 293 average loss: 0.4805\n",
      "----------\n",
      "epoch 294/500\n",
      "1/15, train_loss: 1.1198\n",
      "2/15, train_loss: 1.1123\n",
      "3/15, train_loss: 0.2208\n",
      "4/15, train_loss: 0.2603\n",
      "5/15, train_loss: 1.1102\n",
      "6/15, train_loss: 1.1298\n",
      "7/15, train_loss: 1.0920\n",
      "8/15, train_loss: 0.1590\n",
      "9/15, train_loss: 0.2183\n",
      "10/15, train_loss: 0.1818\n",
      "11/15, train_loss: 0.2599\n",
      "12/15, train_loss: 0.1456\n",
      "13/15, train_loss: 0.1771\n",
      "14/15, train_loss: 0.1837\n",
      "15/15, train_loss: 0.1799\n",
      "16/15, train_loss: 0.1695\n",
      "epoch 294 average loss: 0.4825\n",
      "----------\n",
      "epoch 295/500\n",
      "1/15, train_loss: 1.1067\n",
      "2/15, train_loss: 1.1066\n",
      "3/15, train_loss: 0.2443\n",
      "4/15, train_loss: 0.2240\n",
      "5/15, train_loss: 1.1271\n",
      "6/15, train_loss: 1.1027\n",
      "7/15, train_loss: 1.0898\n",
      "8/15, train_loss: 0.1779\n",
      "9/15, train_loss: 0.1860\n",
      "10/15, train_loss: 0.1928\n",
      "11/15, train_loss: 0.2396\n",
      "12/15, train_loss: 0.1449\n",
      "13/15, train_loss: 0.2051\n",
      "14/15, train_loss: 0.1725\n",
      "15/15, train_loss: 0.2015\n",
      "16/15, train_loss: 0.1600\n",
      "epoch 295 average loss: 0.4801\n",
      "----------\n",
      "epoch 296/500\n",
      "1/15, train_loss: 1.1041\n",
      "2/15, train_loss: 1.1204\n",
      "3/15, train_loss: 0.2218\n",
      "4/15, train_loss: 0.2378\n",
      "5/15, train_loss: 1.1084\n",
      "6/15, train_loss: 1.0976\n",
      "7/15, train_loss: 1.1036\n",
      "8/15, train_loss: 0.1583\n",
      "9/15, train_loss: 0.2059\n",
      "10/15, train_loss: 0.1824\n",
      "11/15, train_loss: 0.2331\n",
      "12/15, train_loss: 0.1641\n",
      "13/15, train_loss: 0.1766\n",
      "14/15, train_loss: 0.1847\n",
      "15/15, train_loss: 0.1794\n",
      "16/15, train_loss: 0.1563\n",
      "epoch 296 average loss: 0.4772\n",
      "----------\n",
      "epoch 297/500\n",
      "1/15, train_loss: 1.1215\n",
      "2/15, train_loss: 1.1042\n",
      "3/15, train_loss: 0.2459\n",
      "4/15, train_loss: 0.2219\n",
      "5/15, train_loss: 1.1085\n",
      "6/15, train_loss: 1.1176\n",
      "7/15, train_loss: 1.0894\n",
      "8/15, train_loss: 0.1793\n",
      "9/15, train_loss: 0.1857\n",
      "10/15, train_loss: 0.1785\n",
      "11/15, train_loss: 0.2620\n",
      "12/15, train_loss: 0.1428\n",
      "13/15, train_loss: 0.1883\n",
      "14/15, train_loss: 0.1704\n",
      "15/15, train_loss: 0.1746\n",
      "16/15, train_loss: 0.1676\n",
      "epoch 297 average loss: 0.4786\n",
      "----------\n",
      "epoch 298/500\n",
      "1/15, train_loss: 1.1037\n",
      "2/15, train_loss: 1.1173\n",
      "3/15, train_loss: 0.2221\n",
      "4/15, train_loss: 0.2196\n",
      "5/15, train_loss: 1.1251\n",
      "6/15, train_loss: 1.0975\n",
      "7/15, train_loss: 1.1002\n",
      "8/15, train_loss: 0.1577\n",
      "9/15, train_loss: 0.1818\n",
      "10/15, train_loss: 0.1937\n",
      "11/15, train_loss: 0.2342\n",
      "12/15, train_loss: 0.1550\n",
      "13/15, train_loss: 0.1746\n",
      "14/15, train_loss: 0.1696\n",
      "15/15, train_loss: 0.1908\n",
      "16/15, train_loss: 0.1592\n",
      "epoch 298 average loss: 0.4751\n",
      "----------\n",
      "epoch 299/500\n",
      "1/15, train_loss: 1.1147\n",
      "2/15, train_loss: 1.1043\n",
      "3/15, train_loss: 0.2173\n",
      "4/15, train_loss: 0.2428\n",
      "5/15, train_loss: 1.1067\n",
      "6/15, train_loss: 1.1128\n",
      "7/15, train_loss: 1.0872\n",
      "8/15, train_loss: 0.1560\n",
      "9/15, train_loss: 0.1981\n",
      "10/15, train_loss: 0.1791\n",
      "11/15, train_loss: 0.2564\n",
      "12/15, train_loss: 0.1434\n",
      "13/15, train_loss: 0.1734\n",
      "14/15, train_loss: 0.1864\n",
      "15/15, train_loss: 0.1752\n",
      "16/15, train_loss: 0.1732\n",
      "epoch 299 average loss: 0.4767\n",
      "----------\n",
      "epoch 300/500\n",
      "1/15, train_loss: 1.1021\n",
      "2/15, train_loss: 1.1003\n",
      "3/15, train_loss: 0.2347\n",
      "4/15, train_loss: 0.2192\n",
      "5/15, train_loss: 1.1196\n",
      "6/15, train_loss: 1.0943\n",
      "7/15, train_loss: 1.0857\n",
      "8/15, train_loss: 0.1698\n",
      "9/15, train_loss: 0.1823\n",
      "10/15, train_loss: 0.1888\n",
      "11/15, train_loss: 0.2371\n",
      "12/15, train_loss: 0.1426\n",
      "13/15, train_loss: 0.1947\n",
      "14/15, train_loss: 0.1684\n",
      "15/15, train_loss: 0.1953\n",
      "16/15, train_loss: 0.1554\n",
      "epoch 300 average loss: 0.4744\n",
      "----------\n",
      "epoch 301/500\n",
      "1/15, train_loss: 1.1009\n",
      "2/15, train_loss: 1.1140\n",
      "3/15, train_loss: 0.2180\n",
      "4/15, train_loss: 0.2371\n",
      "5/15, train_loss: 1.1064\n",
      "6/15, train_loss: 1.0921\n",
      "7/15, train_loss: 1.1059\n",
      "8/15, train_loss: 0.1560\n",
      "9/15, train_loss: 0.2015\n",
      "10/15, train_loss: 0.1764\n",
      "11/15, train_loss: 0.2263\n",
      "12/15, train_loss: 0.1569\n",
      "13/15, train_loss: 0.1739\n",
      "14/15, train_loss: 0.1820\n",
      "15/15, train_loss: 0.1770\n",
      "16/15, train_loss: 0.1534\n",
      "epoch 301 average loss: 0.4736\n",
      "----------\n",
      "epoch 302/500\n",
      "1/15, train_loss: 1.1121\n",
      "2/15, train_loss: 1.1006\n",
      "3/15, train_loss: 0.2324\n",
      "4/15, train_loss: 0.2200\n",
      "5/15, train_loss: 1.1027\n",
      "6/15, train_loss: 1.1152\n",
      "7/15, train_loss: 1.0840\n",
      "8/15, train_loss: 0.1771\n",
      "9/15, train_loss: 0.1817\n",
      "10/15, train_loss: 0.1764\n",
      "11/15, train_loss: 0.2533\n",
      "12/15, train_loss: 0.1425\n",
      "13/15, train_loss: 0.1907\n",
      "14/15, train_loss: 0.1680\n",
      "15/15, train_loss: 0.1728\n",
      "16/15, train_loss: 0.1637\n",
      "epoch 302 average loss: 0.4746\n",
      "----------\n",
      "epoch 303/500\n",
      "1/15, train_loss: 1.0990\n",
      "2/15, train_loss: 1.1113\n",
      "3/15, train_loss: 0.2149\n",
      "4/15, train_loss: 0.2159\n",
      "5/15, train_loss: 1.1143\n",
      "6/15, train_loss: 1.0918\n",
      "7/15, train_loss: 1.0947\n",
      "8/15, train_loss: 0.1538\n",
      "9/15, train_loss: 0.1793\n",
      "10/15, train_loss: 0.1907\n",
      "11/15, train_loss: 0.2251\n",
      "12/15, train_loss: 0.1563\n",
      "13/15, train_loss: 0.1745\n",
      "14/15, train_loss: 0.1661\n",
      "15/15, train_loss: 0.1982\n",
      "16/15, train_loss: 0.1528\n",
      "epoch 303 average loss: 0.4712\n",
      "----------\n",
      "epoch 304/500\n",
      "1/15, train_loss: 1.1110\n",
      "2/15, train_loss: 1.0986\n",
      "3/15, train_loss: 0.2128\n",
      "4/15, train_loss: 0.2367\n",
      "5/15, train_loss: 1.1026\n",
      "6/15, train_loss: 1.1075\n",
      "7/15, train_loss: 1.0838\n",
      "8/15, train_loss: 0.1514\n",
      "9/15, train_loss: 0.2030\n",
      "10/15, train_loss: 0.1744\n",
      "11/15, train_loss: 0.2459\n",
      "12/15, train_loss: 0.1399\n",
      "13/15, train_loss: 0.1703\n",
      "14/15, train_loss: 0.1821\n",
      "15/15, train_loss: 0.1716\n",
      "16/15, train_loss: 0.1627\n",
      "epoch 304 average loss: 0.4721\n",
      "----------\n",
      "epoch 305/500\n",
      "1/15, train_loss: 1.0977\n",
      "2/15, train_loss: 1.0961\n",
      "3/15, train_loss: 0.2323\n",
      "4/15, train_loss: 0.2132\n",
      "5/15, train_loss: 1.1183\n",
      "6/15, train_loss: 1.0907\n",
      "7/15, train_loss: 1.0804\n",
      "8/15, train_loss: 0.1690\n",
      "9/15, train_loss: 0.1789\n",
      "10/15, train_loss: 0.1877\n",
      "11/15, train_loss: 0.2292\n",
      "12/15, train_loss: 0.1387\n",
      "13/15, train_loss: 0.1856\n",
      "14/15, train_loss: 0.1654\n",
      "15/15, train_loss: 0.1859\n",
      "16/15, train_loss: 0.1522\n",
      "epoch 305 average loss: 0.4701\n",
      "----------\n",
      "epoch 306/500\n",
      "1/15, train_loss: 1.0955\n",
      "2/15, train_loss: 1.1083\n",
      "3/15, train_loss: 0.2142\n",
      "4/15, train_loss: 0.2269\n",
      "5/15, train_loss: 1.1008\n",
      "6/15, train_loss: 1.0868\n",
      "7/15, train_loss: 1.0933\n",
      "8/15, train_loss: 0.1511\n",
      "9/15, train_loss: 0.1973\n",
      "10/15, train_loss: 0.1731\n",
      "11/15, train_loss: 0.2238\n",
      "12/15, train_loss: 0.1507\n",
      "13/15, train_loss: 0.1705\n",
      "14/15, train_loss: 0.1779\n",
      "15/15, train_loss: 0.1718\n",
      "16/15, train_loss: 0.1515\n",
      "epoch 306 average loss: 0.4683\n",
      "----------\n",
      "epoch 307/500\n",
      "1/15, train_loss: 1.1029\n",
      "2/15, train_loss: 1.0980\n",
      "3/15, train_loss: 0.2258\n",
      "4/15, train_loss: 0.2178\n",
      "5/15, train_loss: 1.0991\n",
      "6/15, train_loss: 1.1049\n",
      "7/15, train_loss: 1.0796\n",
      "8/15, train_loss: 0.1705\n",
      "9/15, train_loss: 0.1766\n",
      "10/15, train_loss: 0.1724\n",
      "11/15, train_loss: 0.2525\n",
      "12/15, train_loss: 0.1393\n",
      "13/15, train_loss: 0.1877\n",
      "14/15, train_loss: 0.1661\n",
      "15/15, train_loss: 0.1697\n",
      "16/15, train_loss: 0.1629\n",
      "epoch 307 average loss: 0.4704\n",
      "----------\n",
      "epoch 308/500\n",
      "1/15, train_loss: 1.0948\n",
      "2/15, train_loss: 1.1086\n",
      "3/15, train_loss: 0.2114\n",
      "4/15, train_loss: 0.2134\n",
      "5/15, train_loss: 1.1128\n",
      "6/15, train_loss: 1.0888\n",
      "7/15, train_loss: 1.0927\n",
      "8/15, train_loss: 0.1509\n",
      "9/15, train_loss: 0.1757\n",
      "10/15, train_loss: 0.1856\n",
      "11/15, train_loss: 0.2236\n",
      "12/15, train_loss: 0.1491\n",
      "13/15, train_loss: 0.1725\n",
      "14/15, train_loss: 0.1630\n",
      "15/15, train_loss: 0.1937\n",
      "16/15, train_loss: 0.1494\n",
      "epoch 308 average loss: 0.4679\n",
      "----------\n",
      "epoch 309/500\n",
      "1/15, train_loss: 1.1072\n",
      "2/15, train_loss: 1.0940\n",
      "3/15, train_loss: 0.2088\n",
      "4/15, train_loss: 0.2329\n",
      "5/15, train_loss: 1.0976\n",
      "6/15, train_loss: 1.1061\n",
      "7/15, train_loss: 1.0783\n",
      "8/15, train_loss: 0.1488\n",
      "9/15, train_loss: 0.1966\n",
      "10/15, train_loss: 0.1711\n",
      "11/15, train_loss: 0.2457\n",
      "12/15, train_loss: 0.1375\n",
      "13/15, train_loss: 0.1680\n",
      "14/15, train_loss: 0.1786\n",
      "15/15, train_loss: 0.1700\n",
      "16/15, train_loss: 0.1566\n",
      "epoch 309 average loss: 0.4686\n",
      "----------\n",
      "epoch 310/500\n",
      "1/15, train_loss: 1.0924\n",
      "2/15, train_loss: 1.0916\n",
      "3/15, train_loss: 0.2273\n",
      "4/15, train_loss: 0.2104\n",
      "5/15, train_loss: 1.1144\n",
      "6/15, train_loss: 1.0870\n",
      "7/15, train_loss: 1.0756\n",
      "8/15, train_loss: 0.1661\n",
      "9/15, train_loss: 0.1746\n",
      "10/15, train_loss: 0.1853\n",
      "11/15, train_loss: 0.2238\n",
      "12/15, train_loss: 0.1365\n",
      "13/15, train_loss: 0.1869\n",
      "14/15, train_loss: 0.1617\n",
      "15/15, train_loss: 0.1861\n",
      "16/15, train_loss: 0.1487\n",
      "epoch 310 average loss: 0.4668\n",
      "----------\n",
      "epoch 311/500\n",
      "1/15, train_loss: 1.0905\n",
      "2/15, train_loss: 1.1056\n",
      "3/15, train_loss: 0.2088\n",
      "4/15, train_loss: 0.2265\n",
      "5/15, train_loss: 1.0966\n",
      "6/15, train_loss: 1.0836\n",
      "7/15, train_loss: 1.0929\n",
      "8/15, train_loss: 0.1489\n",
      "9/15, train_loss: 0.1919\n",
      "10/15, train_loss: 0.1698\n",
      "11/15, train_loss: 0.2183\n",
      "12/15, train_loss: 0.1509\n",
      "13/15, train_loss: 0.1666\n",
      "14/15, train_loss: 0.1751\n",
      "15/15, train_loss: 0.1688\n",
      "16/15, train_loss: 0.1474\n",
      "epoch 311 average loss: 0.4651\n",
      "----------\n",
      "epoch 312/500\n",
      "1/15, train_loss: 1.1004\n",
      "2/15, train_loss: 1.0919\n",
      "3/15, train_loss: 0.2230\n",
      "4/15, train_loss: 0.2112\n",
      "5/15, train_loss: 1.0940\n",
      "6/15, train_loss: 1.1051\n",
      "7/15, train_loss: 1.0744\n",
      "8/15, train_loss: 0.1635\n",
      "9/15, train_loss: 0.1744\n",
      "10/15, train_loss: 0.1694\n",
      "11/15, train_loss: 0.2492\n",
      "12/15, train_loss: 0.1359\n",
      "13/15, train_loss: 0.1816\n",
      "14/15, train_loss: 0.1609\n",
      "15/15, train_loss: 0.1676\n",
      "16/15, train_loss: 0.1574\n",
      "epoch 312 average loss: 0.4662\n",
      "----------\n",
      "epoch 313/500\n",
      "1/15, train_loss: 1.0904\n",
      "2/15, train_loss: 1.0985\n",
      "3/15, train_loss: 0.2092\n",
      "4/15, train_loss: 0.2079\n",
      "5/15, train_loss: 1.1059\n",
      "6/15, train_loss: 1.0830\n",
      "7/15, train_loss: 1.0852\n",
      "8/15, train_loss: 0.1483\n",
      "9/15, train_loss: 0.1735\n",
      "10/15, train_loss: 0.1804\n",
      "11/15, train_loss: 0.2165\n",
      "12/15, train_loss: 0.1458\n",
      "13/15, train_loss: 0.1721\n",
      "14/15, train_loss: 0.1595\n",
      "15/15, train_loss: 0.1936\n",
      "16/15, train_loss: 0.1486\n",
      "epoch 313 average loss: 0.4636\n",
      "----------\n",
      "epoch 314/500\n",
      "1/15, train_loss: 1.0988\n",
      "2/15, train_loss: 1.0908\n",
      "3/15, train_loss: 0.2068\n",
      "4/15, train_loss: 0.2254\n",
      "5/15, train_loss: 1.0941\n",
      "6/15, train_loss: 1.0965\n",
      "7/15, train_loss: 1.0749\n",
      "8/15, train_loss: 0.1465\n",
      "9/15, train_loss: 0.1974\n",
      "10/15, train_loss: 0.1675\n",
      "11/15, train_loss: 0.2362\n",
      "12/15, train_loss: 0.1348\n",
      "13/15, train_loss: 0.1655\n",
      "14/15, train_loss: 0.1761\n",
      "15/15, train_loss: 0.1665\n",
      "16/15, train_loss: 0.1635\n",
      "epoch 314 average loss: 0.4651\n",
      "----------\n",
      "epoch 315/500\n",
      "1/15, train_loss: 1.0877\n",
      "2/15, train_loss: 1.0888\n",
      "3/15, train_loss: 0.2247\n",
      "4/15, train_loss: 0.2075\n",
      "5/15, train_loss: 1.1042\n",
      "6/15, train_loss: 1.0834\n",
      "7/15, train_loss: 1.0711\n",
      "8/15, train_loss: 0.1652\n",
      "9/15, train_loss: 0.1714\n",
      "10/15, train_loss: 0.1755\n",
      "11/15, train_loss: 0.2157\n",
      "12/15, train_loss: 0.1337\n",
      "13/15, train_loss: 0.1808\n",
      "14/15, train_loss: 0.1590\n",
      "15/15, train_loss: 0.1739\n",
      "16/15, train_loss: 0.1469\n",
      "epoch 315 average loss: 0.4618\n",
      "----------\n",
      "epoch 316/500\n",
      "1/15, train_loss: 1.0855\n",
      "2/15, train_loss: 1.1052\n",
      "3/15, train_loss: 0.2069\n",
      "4/15, train_loss: 0.2196\n",
      "5/15, train_loss: 1.0918\n",
      "6/15, train_loss: 1.0789\n",
      "7/15, train_loss: 1.0886\n",
      "8/15, train_loss: 0.1460\n",
      "9/15, train_loss: 0.1849\n",
      "10/15, train_loss: 0.1668\n",
      "11/15, train_loss: 0.2164\n",
      "12/15, train_loss: 0.1445\n",
      "13/15, train_loss: 0.1654\n",
      "14/15, train_loss: 0.1683\n",
      "15/15, train_loss: 0.1683\n",
      "16/15, train_loss: 0.1466\n",
      "epoch 316 average loss: 0.4615\n",
      "----------\n",
      "epoch 317/500\n",
      "1/15, train_loss: 1.0938\n",
      "2/15, train_loss: 1.0874\n",
      "3/15, train_loss: 0.2215\n",
      "4/15, train_loss: 0.2071\n",
      "5/15, train_loss: 1.0905\n",
      "6/15, train_loss: 1.0977\n",
      "7/15, train_loss: 1.0699\n",
      "8/15, train_loss: 0.1586\n",
      "9/15, train_loss: 0.1707\n",
      "10/15, train_loss: 0.1658\n",
      "11/15, train_loss: 0.2503\n",
      "12/15, train_loss: 0.1327\n",
      "13/15, train_loss: 0.1809\n",
      "14/15, train_loss: 0.1580\n",
      "15/15, train_loss: 0.1638\n",
      "16/15, train_loss: 0.1599\n",
      "epoch 317 average loss: 0.4630\n",
      "----------\n",
      "epoch 318/500\n",
      "1/15, train_loss: 1.0854\n",
      "2/15, train_loss: 1.0966\n",
      "3/15, train_loss: 0.2047\n",
      "4/15, train_loss: 0.2049\n",
      "5/15, train_loss: 1.1069\n",
      "6/15, train_loss: 1.0803\n",
      "7/15, train_loss: 1.0802\n",
      "8/15, train_loss: 0.1456\n",
      "9/15, train_loss: 0.1678\n",
      "10/15, train_loss: 0.1851\n",
      "11/15, train_loss: 0.2158\n",
      "12/15, train_loss: 0.1472\n",
      "13/15, train_loss: 0.1633\n",
      "14/15, train_loss: 0.1569\n",
      "15/15, train_loss: 0.1810\n",
      "16/15, train_loss: 0.1446\n",
      "epoch 318 average loss: 0.4604\n",
      "----------\n",
      "epoch 319/500\n",
      "1/15, train_loss: 1.1004\n",
      "2/15, train_loss: 1.0862\n",
      "3/15, train_loss: 0.2012\n",
      "4/15, train_loss: 0.2302\n",
      "5/15, train_loss: 1.0897\n",
      "6/15, train_loss: 1.0978\n",
      "7/15, train_loss: 1.0704\n",
      "8/15, train_loss: 0.1438\n",
      "9/15, train_loss: 0.1875\n",
      "10/15, train_loss: 0.1651\n",
      "11/15, train_loss: 0.2265\n",
      "12/15, train_loss: 0.1329\n",
      "13/15, train_loss: 0.1597\n",
      "14/15, train_loss: 0.1735\n",
      "15/15, train_loss: 0.1627\n",
      "16/15, train_loss: 0.1560\n",
      "epoch 319 average loss: 0.4615\n",
      "----------\n",
      "epoch 320/500\n",
      "1/15, train_loss: 1.0838\n",
      "2/15, train_loss: 1.0829\n",
      "3/15, train_loss: 0.2168\n",
      "4/15, train_loss: 0.2026\n",
      "5/15, train_loss: 1.1032\n",
      "6/15, train_loss: 1.0773\n",
      "7/15, train_loss: 1.0683\n",
      "8/15, train_loss: 0.1598\n",
      "9/15, train_loss: 0.1686\n",
      "10/15, train_loss: 0.1763\n",
      "11/15, train_loss: 0.2132\n",
      "12/15, train_loss: 0.1332\n",
      "13/15, train_loss: 0.1754\n",
      "14/15, train_loss: 0.1563\n",
      "15/15, train_loss: 0.1787\n",
      "16/15, train_loss: 0.1436\n",
      "epoch 320 average loss: 0.4587\n",
      "----------\n",
      "epoch 321/500\n",
      "1/15, train_loss: 1.0824\n",
      "2/15, train_loss: 1.0960\n",
      "3/15, train_loss: 0.2011\n",
      "4/15, train_loss: 0.2156\n",
      "5/15, train_loss: 1.0885\n",
      "6/15, train_loss: 1.0751\n",
      "7/15, train_loss: 1.0852\n",
      "8/15, train_loss: 0.1436\n",
      "9/15, train_loss: 0.1841\n",
      "10/15, train_loss: 0.1639\n",
      "11/15, train_loss: 0.2063\n",
      "12/15, train_loss: 0.1466\n",
      "13/15, train_loss: 0.1620\n",
      "14/15, train_loss: 0.1655\n",
      "15/15, train_loss: 0.1638\n",
      "16/15, train_loss: 0.1437\n",
      "epoch 321 average loss: 0.4577\n",
      "----------\n",
      "epoch 322/500\n",
      "1/15, train_loss: 1.0941\n",
      "2/15, train_loss: 1.0840\n",
      "3/15, train_loss: 0.2179\n",
      "4/15, train_loss: 0.2064\n",
      "5/15, train_loss: 1.0861\n",
      "6/15, train_loss: 1.0975\n",
      "7/15, train_loss: 1.0665\n",
      "8/15, train_loss: 0.1632\n",
      "9/15, train_loss: 0.1709\n",
      "10/15, train_loss: 0.1636\n",
      "11/15, train_loss: 0.2332\n",
      "12/15, train_loss: 0.1322\n",
      "13/15, train_loss: 0.1805\n",
      "14/15, train_loss: 0.1555\n",
      "15/15, train_loss: 0.1620\n",
      "16/15, train_loss: 0.1575\n",
      "epoch 322 average loss: 0.4607\n",
      "----------\n",
      "epoch 323/500\n",
      "1/15, train_loss: 1.0805\n",
      "2/15, train_loss: 1.0925\n",
      "3/15, train_loss: 0.2021\n",
      "4/15, train_loss: 0.2001\n",
      "5/15, train_loss: 1.0984\n",
      "6/15, train_loss: 1.0736\n",
      "7/15, train_loss: 1.0791\n",
      "8/15, train_loss: 0.1422\n",
      "9/15, train_loss: 0.1669\n",
      "10/15, train_loss: 0.1762\n",
      "11/15, train_loss: 0.2102\n",
      "12/15, train_loss: 0.1424\n",
      "13/15, train_loss: 0.1623\n",
      "14/15, train_loss: 0.1541\n",
      "15/15, train_loss: 0.1836\n",
      "16/15, train_loss: 0.1426\n",
      "epoch 323 average loss: 0.4567\n",
      "----------\n",
      "epoch 324/500\n",
      "1/15, train_loss: 1.0898\n",
      "2/15, train_loss: 1.0830\n",
      "3/15, train_loss: 0.1989\n",
      "4/15, train_loss: 0.2203\n",
      "5/15, train_loss: 1.0849\n",
      "6/15, train_loss: 1.0884\n",
      "7/15, train_loss: 1.0651\n",
      "8/15, train_loss: 0.1410\n",
      "9/15, train_loss: 0.1868\n",
      "10/15, train_loss: 0.1624\n",
      "11/15, train_loss: 0.2323\n",
      "12/15, train_loss: 0.1311\n",
      "13/15, train_loss: 0.1577\n",
      "14/15, train_loss: 0.1700\n",
      "15/15, train_loss: 0.1599\n",
      "16/15, train_loss: 0.1524\n",
      "epoch 324 average loss: 0.4578\n",
      "----------\n",
      "epoch 325/500\n",
      "1/15, train_loss: 1.0798\n",
      "2/15, train_loss: 1.0791\n",
      "3/15, train_loss: 0.2215\n",
      "4/15, train_loss: 0.1998\n",
      "5/15, train_loss: 1.0996\n",
      "6/15, train_loss: 1.0727\n",
      "7/15, train_loss: 1.0634\n",
      "8/15, train_loss: 0.1554\n",
      "9/15, train_loss: 0.1665\n",
      "10/15, train_loss: 0.1701\n",
      "11/15, train_loss: 0.2123\n",
      "12/15, train_loss: 0.1289\n",
      "13/15, train_loss: 0.1795\n",
      "14/15, train_loss: 0.1533\n",
      "15/15, train_loss: 0.1767\n",
      "16/15, train_loss: 0.1414\n",
      "epoch 325 average loss: 0.4562\n",
      "----------\n",
      "epoch 326/500\n",
      "1/15, train_loss: 1.0788\n",
      "2/15, train_loss: 1.0893\n",
      "3/15, train_loss: 0.1987\n",
      "4/15, train_loss: 0.2170\n",
      "5/15, train_loss: 1.0842\n",
      "6/15, train_loss: 1.0699\n",
      "7/15, train_loss: 1.0802\n",
      "8/15, train_loss: 0.1398\n",
      "9/15, train_loss: 0.1820\n",
      "10/15, train_loss: 0.1606\n",
      "11/15, train_loss: 0.2048\n",
      "12/15, train_loss: 0.1458\n",
      "13/15, train_loss: 0.1579\n",
      "14/15, train_loss: 0.1668\n",
      "15/15, train_loss: 0.1595\n",
      "16/15, train_loss: 0.1397\n",
      "epoch 326 average loss: 0.4547\n",
      "----------\n",
      "epoch 327/500\n",
      "1/15, train_loss: 1.0873\n",
      "2/15, train_loss: 1.0777\n",
      "3/15, train_loss: 0.2101\n",
      "4/15, train_loss: 0.2009\n",
      "5/15, train_loss: 1.0819\n",
      "6/15, train_loss: 1.0875\n",
      "7/15, train_loss: 1.0623\n",
      "8/15, train_loss: 0.1595\n",
      "9/15, train_loss: 0.1640\n",
      "10/15, train_loss: 0.1593\n",
      "11/15, train_loss: 0.2251\n",
      "12/15, train_loss: 0.1294\n",
      "13/15, train_loss: 0.1724\n",
      "14/15, train_loss: 0.1520\n",
      "15/15, train_loss: 0.1570\n",
      "16/15, train_loss: 0.1464\n",
      "epoch 327 average loss: 0.4545\n",
      "----------\n",
      "epoch 328/500\n",
      "1/15, train_loss: 1.0767\n",
      "2/15, train_loss: 1.0894\n",
      "3/15, train_loss: 0.1974\n",
      "4/15, train_loss: 0.1994\n",
      "5/15, train_loss: 1.0916\n",
      "6/15, train_loss: 1.0723\n",
      "7/15, train_loss: 1.0716\n",
      "8/15, train_loss: 0.1426\n",
      "9/15, train_loss: 0.1634\n",
      "10/15, train_loss: 0.1680\n",
      "11/15, train_loss: 0.2013\n",
      "12/15, train_loss: 0.1394\n",
      "13/15, train_loss: 0.1595\n",
      "14/15, train_loss: 0.1516\n",
      "15/15, train_loss: 0.1750\n",
      "16/15, train_loss: 0.1446\n",
      "epoch 328 average loss: 0.4527\n",
      "----------\n",
      "epoch 329/500\n",
      "1/15, train_loss: 1.0879\n",
      "2/15, train_loss: 1.0826\n",
      "3/15, train_loss: 0.1950\n",
      "4/15, train_loss: 0.2282\n",
      "5/15, train_loss: 1.0821\n",
      "6/15, train_loss: 1.0887\n",
      "7/15, train_loss: 1.0632\n",
      "8/15, train_loss: 0.1390\n",
      "9/15, train_loss: 0.1896\n",
      "10/15, train_loss: 0.1589\n",
      "11/15, train_loss: 0.2206\n",
      "12/15, train_loss: 0.1282\n",
      "13/15, train_loss: 0.1580\n",
      "14/15, train_loss: 0.1651\n",
      "15/15, train_loss: 0.1606\n",
      "16/15, train_loss: 0.1569\n",
      "epoch 329 average loss: 0.4565\n",
      "----------\n",
      "epoch 330/500\n",
      "1/15, train_loss: 1.0770\n",
      "2/15, train_loss: 1.0755\n",
      "3/15, train_loss: 0.2164\n",
      "4/15, train_loss: 0.1960\n",
      "5/15, train_loss: 1.0963\n",
      "6/15, train_loss: 1.0702\n",
      "7/15, train_loss: 1.0598\n",
      "8/15, train_loss: 0.1548\n",
      "9/15, train_loss: 0.1657\n",
      "10/15, train_loss: 0.1719\n",
      "11/15, train_loss: 0.2103\n",
      "12/15, train_loss: 0.1270\n",
      "13/15, train_loss: 0.1805\n",
      "14/15, train_loss: 0.1505\n",
      "15/15, train_loss: 0.1792\n",
      "16/15, train_loss: 0.1394\n",
      "epoch 330 average loss: 0.4544\n",
      "----------\n",
      "epoch 331/500\n",
      "1/15, train_loss: 1.0755\n",
      "2/15, train_loss: 1.0837\n",
      "3/15, train_loss: 0.1977\n",
      "4/15, train_loss: 0.2095\n",
      "5/15, train_loss: 1.0826\n",
      "6/15, train_loss: 1.0663\n",
      "7/15, train_loss: 1.0782\n",
      "8/15, train_loss: 0.1379\n",
      "9/15, train_loss: 0.1882\n",
      "10/15, train_loss: 0.1569\n",
      "11/15, train_loss: 0.2016\n",
      "12/15, train_loss: 0.1435\n",
      "13/15, train_loss: 0.1534\n",
      "14/15, train_loss: 0.1666\n",
      "15/15, train_loss: 0.1540\n",
      "16/15, train_loss: 0.1377\n",
      "epoch 331 average loss: 0.4521\n",
      "----------\n",
      "epoch 332/500\n",
      "1/15, train_loss: 1.0904\n",
      "2/15, train_loss: 1.0745\n",
      "3/15, train_loss: 0.2182\n",
      "4/15, train_loss: 0.1943\n",
      "5/15, train_loss: 1.0795\n",
      "6/15, train_loss: 1.0874\n",
      "7/15, train_loss: 1.0596\n",
      "8/15, train_loss: 0.1603\n",
      "9/15, train_loss: 0.1626\n",
      "10/15, train_loss: 0.1557\n",
      "11/15, train_loss: 0.2168\n",
      "12/15, train_loss: 0.1264\n",
      "13/15, train_loss: 0.1585\n",
      "14/15, train_loss: 0.1502\n",
      "15/15, train_loss: 0.1523\n",
      "16/15, train_loss: 0.1454\n",
      "epoch 332 average loss: 0.4520\n",
      "----------\n",
      "epoch 333/500\n",
      "1/15, train_loss: 1.0734\n",
      "2/15, train_loss: 1.0837\n",
      "3/15, train_loss: 0.1959\n",
      "4/15, train_loss: 0.1933\n",
      "5/15, train_loss: 1.0934\n",
      "6/15, train_loss: 1.0673\n",
      "7/15, train_loss: 1.0707\n",
      "8/15, train_loss: 0.1391\n",
      "9/15, train_loss: 0.1587\n",
      "10/15, train_loss: 0.1676\n",
      "11/15, train_loss: 0.1971\n",
      "12/15, train_loss: 0.1379\n",
      "13/15, train_loss: 0.1540\n",
      "14/15, train_loss: 0.1497\n",
      "15/15, train_loss: 0.1703\n",
      "16/15, train_loss: 0.1403\n",
      "epoch 333 average loss: 0.4495\n",
      "----------\n",
      "epoch 334/500\n",
      "1/15, train_loss: 1.0829\n",
      "2/15, train_loss: 1.0757\n",
      "3/15, train_loss: 0.1923\n",
      "4/15, train_loss: 0.2157\n",
      "5/15, train_loss: 1.0789\n",
      "6/15, train_loss: 1.0811\n",
      "7/15, train_loss: 1.0602\n",
      "8/15, train_loss: 0.1375\n",
      "9/15, train_loss: 0.1818\n",
      "10/15, train_loss: 0.1566\n",
      "11/15, train_loss: 0.2249\n",
      "12/15, train_loss: 0.1282\n",
      "13/15, train_loss: 0.1535\n",
      "14/15, train_loss: 0.1651\n",
      "15/15, train_loss: 0.1564\n",
      "16/15, train_loss: 0.1514\n",
      "epoch 334 average loss: 0.4526\n",
      "----------\n",
      "epoch 335/500\n",
      "1/15, train_loss: 1.0732\n",
      "2/15, train_loss: 1.0719\n",
      "3/15, train_loss: 0.2138\n",
      "4/15, train_loss: 0.1935\n",
      "5/15, train_loss: 1.0917\n",
      "6/15, train_loss: 1.0651\n",
      "7/15, train_loss: 1.0571\n",
      "8/15, train_loss: 0.1538\n",
      "9/15, train_loss: 0.1638\n",
      "10/15, train_loss: 0.1662\n",
      "11/15, train_loss: 0.2054\n",
      "12/15, train_loss: 0.1253\n",
      "13/15, train_loss: 0.1739\n",
      "14/15, train_loss: 0.1476\n",
      "15/15, train_loss: 0.1731\n",
      "16/15, train_loss: 0.1368\n",
      "epoch 335 average loss: 0.4508\n",
      "----------\n",
      "epoch 336/500\n",
      "1/15, train_loss: 1.0703\n",
      "2/15, train_loss: 1.0873\n",
      "3/15, train_loss: 0.1909\n",
      "4/15, train_loss: 0.2089\n",
      "5/15, train_loss: 1.0759\n",
      "6/15, train_loss: 1.0633\n",
      "7/15, train_loss: 1.0684\n",
      "8/15, train_loss: 0.1360\n",
      "9/15, train_loss: 0.1734\n",
      "10/15, train_loss: 0.1551\n",
      "11/15, train_loss: 0.1954\n",
      "12/15, train_loss: 0.1387\n",
      "13/15, train_loss: 0.1506\n",
      "14/15, train_loss: 0.1578\n",
      "15/15, train_loss: 0.1515\n",
      "16/15, train_loss: 0.1357\n",
      "epoch 336 average loss: 0.4475\n",
      "----------\n",
      "epoch 337/500\n",
      "1/15, train_loss: 1.0798\n",
      "2/15, train_loss: 1.0736\n",
      "3/15, train_loss: 0.2027\n",
      "4/15, train_loss: 0.1939\n",
      "5/15, train_loss: 1.0742\n",
      "6/15, train_loss: 1.0851\n",
      "7/15, train_loss: 1.0547\n",
      "8/15, train_loss: 0.1522\n",
      "9/15, train_loss: 0.1574\n",
      "10/15, train_loss: 0.1535\n",
      "11/15, train_loss: 0.2262\n",
      "12/15, train_loss: 0.1241\n",
      "13/15, train_loss: 0.1571\n",
      "14/15, train_loss: 0.1466\n",
      "15/15, train_loss: 0.1500\n",
      "16/15, train_loss: 0.1438\n",
      "epoch 337 average loss: 0.4484\n",
      "----------\n",
      "epoch 338/500\n",
      "1/15, train_loss: 1.0700\n",
      "2/15, train_loss: 1.0807\n",
      "3/15, train_loss: 0.1912\n",
      "4/15, train_loss: 0.1896\n",
      "5/15, train_loss: 1.0880\n",
      "6/15, train_loss: 1.0627\n",
      "7/15, train_loss: 1.0667\n",
      "8/15, train_loss: 0.1351\n",
      "9/15, train_loss: 0.1579\n",
      "10/15, train_loss: 0.1662\n",
      "11/15, train_loss: 0.2000\n",
      "12/15, train_loss: 0.1330\n",
      "13/15, train_loss: 0.1536\n",
      "14/15, train_loss: 0.1464\n",
      "15/15, train_loss: 0.1710\n",
      "16/15, train_loss: 0.1367\n",
      "epoch 338 average loss: 0.4468\n",
      "----------\n",
      "epoch 339/500\n",
      "1/15, train_loss: 1.0817\n",
      "2/15, train_loss: 1.0702\n",
      "3/15, train_loss: 0.1891\n",
      "4/15, train_loss: 0.2141\n",
      "5/15, train_loss: 1.0743\n",
      "6/15, train_loss: 1.0785\n",
      "7/15, train_loss: 1.0549\n",
      "8/15, train_loss: 0.1343\n",
      "9/15, train_loss: 0.1794\n",
      "10/15, train_loss: 0.1535\n",
      "11/15, train_loss: 0.2171\n",
      "12/15, train_loss: 0.1236\n",
      "13/15, train_loss: 0.1494\n",
      "14/15, train_loss: 0.1662\n",
      "15/15, train_loss: 0.1499\n",
      "16/15, train_loss: 0.1469\n",
      "epoch 339 average loss: 0.4489\n",
      "----------\n",
      "epoch 340/500\n",
      "1/15, train_loss: 1.0675\n",
      "2/15, train_loss: 1.0673\n",
      "3/15, train_loss: 0.2028\n",
      "4/15, train_loss: 0.1884\n",
      "5/15, train_loss: 1.0824\n",
      "6/15, train_loss: 1.0610\n",
      "7/15, train_loss: 1.0522\n",
      "8/15, train_loss: 0.1495\n",
      "9/15, train_loss: 0.1557\n",
      "10/15, train_loss: 0.1657\n",
      "11/15, train_loss: 0.1973\n",
      "12/15, train_loss: 0.1227\n",
      "13/15, train_loss: 0.1654\n",
      "14/15, train_loss: 0.1450\n",
      "15/15, train_loss: 0.1649\n",
      "16/15, train_loss: 0.1340\n",
      "epoch 340 average loss: 0.4451\n",
      "----------\n",
      "epoch 341/500\n",
      "1/15, train_loss: 1.0658\n",
      "2/15, train_loss: 1.0777\n",
      "3/15, train_loss: 0.1887\n",
      "4/15, train_loss: 0.2016\n",
      "5/15, train_loss: 1.0738\n",
      "6/15, train_loss: 1.0586\n",
      "7/15, train_loss: 1.0635\n",
      "8/15, train_loss: 0.1333\n",
      "9/15, train_loss: 0.1697\n",
      "10/15, train_loss: 0.1522\n",
      "11/15, train_loss: 0.1915\n",
      "12/15, train_loss: 0.1296\n",
      "13/15, train_loss: 0.1491\n",
      "14/15, train_loss: 0.1510\n",
      "15/15, train_loss: 0.1504\n",
      "16/15, train_loss: 0.1334\n",
      "epoch 341 average loss: 0.4431\n",
      "----------\n",
      "epoch 342/500\n",
      "1/15, train_loss: 1.0758\n",
      "2/15, train_loss: 1.0700\n",
      "3/15, train_loss: 0.2013\n",
      "4/15, train_loss: 0.1956\n",
      "5/15, train_loss: 1.0711\n",
      "6/15, train_loss: 1.0752\n",
      "7/15, train_loss: 1.0517\n",
      "8/15, train_loss: 0.1491\n",
      "9/15, train_loss: 0.1587\n",
      "10/15, train_loss: 0.1516\n",
      "11/15, train_loss: 0.2114\n",
      "12/15, train_loss: 0.1240\n",
      "13/15, train_loss: 0.1598\n",
      "14/15, train_loss: 0.1463\n",
      "15/15, train_loss: 0.1501\n",
      "16/15, train_loss: 0.1402\n",
      "epoch 342 average loss: 0.4458\n",
      "----------\n",
      "epoch 343/500\n",
      "1/15, train_loss: 1.0666\n",
      "2/15, train_loss: 1.0789\n",
      "3/15, train_loss: 0.1914\n",
      "4/15, train_loss: 0.1874\n",
      "5/15, train_loss: 1.0840\n",
      "6/15, train_loss: 1.0587\n",
      "7/15, train_loss: 1.0632\n",
      "8/15, train_loss: 0.1339\n",
      "9/15, train_loss: 0.1561\n",
      "10/15, train_loss: 0.1657\n",
      "11/15, train_loss: 0.1936\n",
      "12/15, train_loss: 0.1373\n",
      "13/15, train_loss: 0.1518\n",
      "14/15, train_loss: 0.1439\n",
      "15/15, train_loss: 0.1745\n",
      "16/15, train_loss: 0.1340\n",
      "epoch 343 average loss: 0.4451\n",
      "----------\n",
      "epoch 344/500\n",
      "1/15, train_loss: 1.0775\n",
      "2/15, train_loss: 1.0664\n",
      "3/15, train_loss: 0.1888\n",
      "4/15, train_loss: 0.2063\n",
      "5/15, train_loss: 1.0712\n",
      "6/15, train_loss: 1.0714\n",
      "7/15, train_loss: 1.0521\n",
      "8/15, train_loss: 0.1315\n",
      "9/15, train_loss: 0.1790\n",
      "10/15, train_loss: 0.1503\n",
      "11/15, train_loss: 0.2105\n",
      "12/15, train_loss: 0.1220\n",
      "13/15, train_loss: 0.1469\n",
      "14/15, train_loss: 0.1596\n",
      "15/15, train_loss: 0.1478\n",
      "16/15, train_loss: 0.1424\n",
      "epoch 344 average loss: 0.4452\n",
      "----------\n",
      "epoch 345/500\n",
      "1/15, train_loss: 1.0641\n",
      "2/15, train_loss: 1.0650\n",
      "3/15, train_loss: 0.2100\n",
      "4/15, train_loss: 0.1852\n",
      "5/15, train_loss: 1.0844\n",
      "6/15, train_loss: 1.0591\n",
      "7/15, train_loss: 1.0493\n",
      "8/15, train_loss: 0.1489\n",
      "9/15, train_loss: 0.1541\n",
      "10/15, train_loss: 0.1601\n",
      "11/15, train_loss: 0.1916\n",
      "12/15, train_loss: 0.1198\n",
      "13/15, train_loss: 0.1596\n",
      "14/15, train_loss: 0.1420\n",
      "15/15, train_loss: 0.1536\n",
      "16/15, train_loss: 0.1313\n",
      "epoch 345 average loss: 0.4424\n",
      "----------\n",
      "epoch 346/500\n",
      "1/15, train_loss: 1.0630\n",
      "2/15, train_loss: 1.0817\n",
      "3/15, train_loss: 0.1881\n",
      "4/15, train_loss: 0.2015\n",
      "5/15, train_loss: 1.0707\n",
      "6/15, train_loss: 1.0565\n",
      "7/15, train_loss: 1.0686\n",
      "8/15, train_loss: 0.1311\n",
      "9/15, train_loss: 0.1700\n",
      "10/15, train_loss: 0.1495\n",
      "11/15, train_loss: 0.1884\n",
      "12/15, train_loss: 0.1294\n",
      "13/15, train_loss: 0.1490\n",
      "14/15, train_loss: 0.1518\n",
      "15/15, train_loss: 0.1486\n",
      "16/15, train_loss: 0.1313\n",
      "epoch 346 average loss: 0.4425\n",
      "----------\n",
      "epoch 347/500\n",
      "1/15, train_loss: 1.0767\n",
      "2/15, train_loss: 1.0655\n",
      "3/15, train_loss: 0.2010\n",
      "4/15, train_loss: 0.1882\n",
      "5/15, train_loss: 1.0672\n",
      "6/15, train_loss: 1.0764\n",
      "7/15, train_loss: 1.0483\n",
      "8/15, train_loss: 0.1474\n",
      "9/15, train_loss: 0.1528\n",
      "10/15, train_loss: 0.1490\n",
      "11/15, train_loss: 0.2066\n",
      "12/15, train_loss: 0.1208\n",
      "13/15, train_loss: 0.1641\n",
      "14/15, train_loss: 0.1417\n",
      "15/15, train_loss: 0.1473\n",
      "16/15, train_loss: 0.1405\n",
      "epoch 347 average loss: 0.4433\n",
      "----------\n",
      "epoch 348/500\n",
      "1/15, train_loss: 1.0629\n"
     ]
    }
   ],
   "source": [
    "n_splits = 3\n",
    "if start_training:\n",
    "    #### Running 10 folds\n",
    "    for i in range(0, n_splits):\n",
    "        \n",
    "        #train_files_fld, train_files_fld_IDH_label, val_files_fld, val_files_fld_IDH_label  = copy.deepcopy(train_folds[f'fold{i}']), copy.deepcopy(train_folds[f'fold{i}_IDH_label']),\\\n",
    "        #copy.deepcopy(val_folds[f'fold{i}']), copy.deepcopy(val_folds[f'fold{i}_IDH_label'])\n",
    "        \n",
    "        \n",
    "              \n",
    "        train_files_fld, val_files_fld  = copy.deepcopy(BraTS20SubjectsIDHTrainDCT[f'fold{i}']), copy.deepcopy(BraTS20SubjectsIDHValDCT[f'fold{i}'])\n",
    "        train_files_fld_IDH_label, val_files_fld_IDH_label = None, None\n",
    "        batch_size=8\n",
    "        ### Need to change batch size if minimux training batch size == 1\n",
    "        print('fold', i, \"Bacth Investigation, minimum batch size\", len(train_files_fld)%batch_size)        \n",
    "        #train(train_files_fld, train_files_fld_IDH_label, val_files_fld, val_files_fld_IDH_label, batch_size=batch_size, epochs = 200, cfold = i)\n",
    "        #train(train_files_fld, train_files_fld_IDH_label, val_files_fld, val_files_fld_IDH_label, batch_size=batch_size, epochs = 500, cfold = i)\n",
    "        train(train_files_fld, train_files_fld_IDH_label, val_files_fld, val_files_fld_IDH_label, batch_size=batch_size, epochs = 500, cfold = i)\n",
    "        \n",
    "        \n",
    "    \n",
    "    start_training = False\n",
    "else:\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6de498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcaf18c4",
   "metadata": {},
   "source": [
    "### Performing inference on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "035c6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferSubjectsWithTA(data_loader,listmodels, prediction_folder=\"./\", topk=1, num_channels = 4,\\\n",
    "                orientation=\"LPS\", withoptimizer = False, softmaxEnsemble=False, save_inference = False):\n",
    "    \"\"\"\n",
    "    run inference, the output folder will be \"./output\"\n",
    "    \"\"\"        \n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    listmodels = listmodels[0:topk]\n",
    "    for x in listmodels:\n",
    "        print(f\"available model file: {x}.\")\n",
    "        \n",
    "    channel_nums =  monai.utils.misc.first(data_loader)['image'].shape[1] ##next(iter(val_loader[\"image\"])).shape[1]\n",
    "    channelNums = f\"{channel_nums} channels\"\n",
    "    keys = ('image',)\n",
    "    patch_size = (32, 32, 32)\n",
    "    \n",
    "    post_trans_sigbin = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "    post_trans_bin = Compose([AsDiscrete(threshold=0.5)])\n",
    "    post_trans_sig = Compose([Activations(sigmoid=True)])\n",
    "    \n",
    "    auc_metric = ROCAUCMetric()\n",
    "    dice_metric = monai.metrics.DiceMetric(include_background=True, reduction='mean', get_not_nans=False)\n",
    "    dice_metric_batch = monai.metrics.DiceMetric(include_background=True, reduction='mean_batch', get_not_nans=False)\n",
    "    \n",
    "    HD_metric = HausdorffDistanceMetric(include_background=True, percentile = 95., reduction='mean', get_not_nans=False)\n",
    "    HD_metric_batch = HausdorffDistanceMetric(include_background=True, percentile = 95., reduction='mean_batch', get_not_nans=False)\n",
    "    \n",
    "    post_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])  \n",
    "    \n",
    "    def one_hot_permute(x):\n",
    "        return F.one_hot(x.squeeze(dim=0).long(), num_classes=num_classes).permute(3, 0, 1, 2)\n",
    "    \n",
    "    def get_binarize_tensor(x, dim=1):\n",
    "        x_chlist = torch.unbind(x, dim = dim)\n",
    "        bin_x = torch.zeros_like(x_chlist[0])\n",
    "        for x_i in x_chlist:\n",
    "            bin_x = torch.logical_or(x_i, bin_x)\n",
    "        return bin_x.unsqueeze(dim=dim).to(torch.float32)\n",
    "        \n",
    "    \n",
    "    def get_segclass(x, dim = 1):\n",
    "        xdvc = x.device\n",
    "        x_chlist = torch.unbind(x, dim = dim)\n",
    "        xclassNoList = []\n",
    "        xvalueList = []\n",
    "        for x_i in x_chlist:\n",
    "            \n",
    "            xv, xc = torch.unique(x_i, return_counts  = True)\n",
    "\n",
    "            if xc.shape[0]==1:\n",
    "                xclassNoList.append(xc[0].item())\n",
    "                xvalueList.append(xv[0].item())\n",
    "            elif xc.shape[0]==2:\n",
    "                    if torch.any(torch.eq(xv, 1)):\n",
    "                        xclassNoList.append(xc[1].item())\n",
    "                        xvalueList.append(xv[1].item())\n",
    "                    else:\n",
    "                        print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "            else:\n",
    "                print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "        \n",
    "        if torch.any(torch.eq(torch.tensor(xvalueList), 1)):  \n",
    "            xclass = torch.argmax(torch.tensor(xclassNoList).to(xdvc))\n",
    "        else:\n",
    "            '''If all uniques class values are 0, we are assigning nan values as a class'''\n",
    "            xclass = torch.tensor(float('NaN')).to(xdvc)\n",
    "            \n",
    "        return xclass \n",
    "    \n",
    "    \n",
    "    \n",
    "    keys = (\"image\",)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "        y = torch.tensor([], dtype=torch.long, device=device)\n",
    "        Infer_idLst = list()\n",
    "    \n",
    "        for infindx, infer_data in enumerate(tqdm(data_loader)):\n",
    "\n",
    "            \n",
    "            val_inputs, val_labels, val_IDH_labels = (\n",
    "                infer_data['image'].to(device),\n",
    "                infer_data['label'].to(device),\n",
    "                infer_data['IDH_label'].to(device),\n",
    "            )\n",
    "                \n",
    "\n",
    "            n_class = 2\n",
    "            val_outputsAll = torch.zeros(val_inputs.shape[0], n_class, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]).to(device)\n",
    "            n_model = 0.\n",
    "            \n",
    "            #for mdlindx in (number+1 for number in range(topk)):\n",
    "            for mdlindx in range(topk):\n",
    "    \n",
    "                print(f'Model {mdlindx}, {listmodels[mdlindx]} is running now')\n",
    "                model = None          \n",
    "                model = DynUNet(\n",
    "                    spatial_dims=3,\n",
    "                    in_channels=4,\n",
    "                    out_channels=n_class,\n",
    "                    kernel_size=kernels,\n",
    "                    strides=strides,\n",
    "                    upsample_kernel_size=strides[1:],\n",
    "                    norm_name=\"batch\",\n",
    "                    filters = filters,\n",
    "                    deep_supervision=True,\n",
    "                    res_block=True,\n",
    "                    deep_supr_num=2,\n",
    "                ).to(device)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                if withoptimizer ==True:\n",
    "                    \n",
    "                    state_dictsAll = torch.load(listmodels[mdlindx], map_location=device)\n",
    "                    model.load_state_dict(state_dictsAll[\"model_state_dict\"])\n",
    "                    model.eval()\n",
    "                \n",
    "                else:    \n",
    "                    model.load_state_dict(torch.load(listmodels[mdlindx], map_location=device))\n",
    "                    model.eval()\n",
    "                \n",
    "                \n",
    "                n = 1.0\n",
    "                roi_size = patch_size #(32, 32, 32)\n",
    "                sw_batch_size = 8\n",
    "                val_overlap = 0.5\n",
    "                mode=\"gaussian\"\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "\n",
    "                    preds = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device)\n",
    "                    \n",
    "                flip_val_inputs = torch.flip(val_inputs, dims=(2, 3, 4))\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    \n",
    "                    mfpred = sliding_window_inference(flip_val_inputs, roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device)\n",
    "                 \n",
    "                flip_pred = torch.flip(mfpred, dims=(2, 3, 4))\n",
    "                preds  = preds + flip_pred\n",
    "                n = n + 1.0\n",
    "                    \n",
    "                \n",
    "                for _ in range(4):\n",
    "                    # test time augmentations\n",
    "                    _img = RandGaussianNoised(keys[0], prob=1.0, std=0.01)(infer_data)[keys[0]]\n",
    "\n",
    "\n",
    "                    with torch.cuda.amp.autocast():\n",
    "\n",
    "                        #val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model, sw_device = device, device = device)\n",
    "                        _img_pred = sliding_window_inference(_img.to(device), roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device)\n",
    "                        preds = preds + _img_pred\n",
    "                        n = n + 1.0\n",
    "\n",
    "                    \n",
    "                    _img_flip = torch.flip(_img, dims=(2, 3, 4)) \n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        _mf_flip_pred = sliding_window_inference(_img_flip.to(device), roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device)\n",
    "                    \n",
    "                    _img_flip_pred = torch.flip(_mf_flip_pred, dims=(2, 3, 4))\n",
    "                    preds = preds + _img_flip_pred\n",
    "                    n = n + 1.0\n",
    "                 \n",
    "                \n",
    "                preds = preds / n\n",
    "                \n",
    "                if softmaxEnsemble:\n",
    "                    preds = torch.stack([post_trans_sig(i) for i in torch.unbind(preds, dim = 0)], dim = 0)\n",
    "                val_outputsAll = val_outputsAll + preds\n",
    "                n_model = n_model+1.0\n",
    "                \n",
    "                # Free up GPU memory after training\n",
    "                model = None\n",
    "                del model\n",
    "                #train_loader, val_loader = None, None        \n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                           \n",
    "            val_outputsAll = val_outputsAll / n_model\n",
    "            \n",
    "            val_outputs = post_trans_bin(val_outputsAll) if softmaxEnsemble else post_trans_sigbin(val_outputsAll)\n",
    "            '''Sigmoid or logit'''\n",
    "            val_outputsSig = val_outputsAll if softmaxEnsemble else post_trans_sig(val_outputsAll)\n",
    "            #val_outputsSig = val_outputsAll\n",
    "            \n",
    "            \n",
    "\n",
    "            #val_labels2hot = torch.stack([one_hot_permute(i) for i in torch.unbind(val_labels, dim = 0)], dim = 0)\n",
    "\n",
    "\n",
    "            val_labels_bin = get_binarize_tensor(val_labels, dim=1)\n",
    "            val_outputs_bin = get_binarize_tensor(val_outputs, dim=1)\n",
    "\n",
    "            dice_metric(y_pred=val_outputs_bin, y=val_labels_bin)\n",
    "            HD_metric(y_pred=val_outputs_bin, y=val_labels_bin)\n",
    "\n",
    "            klcc = KeepLargestConnectedComponent(applied_labels = [0, 1], is_onehot = True)  ##is_onehot=True or None by default\n",
    "            #val_labels = klcc(val_labels.squeeze(dim=0)).unsqueeze(dim=0)\n",
    "            val_labels = torch.stack([klcc(i) for i in torch.unbind(val_labels, dim = 0)], dim = 0)\n",
    "\n",
    "            val_label4mSeg_C = get_segclass(val_outputs)\n",
    "            #val_surv_labels = val_surv_labels.squeeze(dim=1)  ###Squeezing from B, 1 to B if needed\n",
    "            y_pred = torch.cat([y_pred, val_label4mSeg_C.view(1)], dim=0)\n",
    "            y = torch.cat([y, val_IDH_labels], dim=0)\n",
    "                \n",
    "            \n",
    "            if save_inference:  \n",
    "            \n",
    "                val_outputs4save = val_outputs.squeeze(dim = 0)\n",
    "                '''Sigmoid or logit'''\n",
    "                val_outputsSig4Save = val_outputsSig.squeeze(dim = 0)\n",
    "\n",
    "\n",
    "                ##### For padding going back to original position\n",
    "\n",
    "                val_outputsPadded = np.zeros(shape = (val_outputs4save.shape[0], 240, 240, 155))\n",
    "                '''Sigmoid or logit'''\n",
    "                val_outputsSigPadded = np.zeros(shape = (val_outputsSig4Save.shape[0], 240, 240, 155))\n",
    "\n",
    "\n",
    "                infr_start_cord = infer_data[\"fg_start_coord\"][0]\n",
    "                infr_end_cord = infer_data[\"fg_end_coord\"][0]\n",
    "                start_coord = infr_start_cord[0].item(), infr_start_cord[1].item(), infr_start_cord[2].item() \n",
    "                end_coord = infr_end_cord[0].item(), infr_end_cord[1].item(), infr_end_cord[2].item()\n",
    "\n",
    "                if orientation ==\"LPS\":\n",
    "\n",
    "                    val_outputs_now_affine = infer_data['image_meta_dict']['affine'][0]\n",
    "                    val_outputsLPSData = val_outputs4save.cpu().numpy()\n",
    "                    '''Sigmoid or logit'''\n",
    "                    val_outputsSigLPSData = val_outputsSig4Save.cpu().numpy()\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print(\"Stopp the execution\")\n",
    "\n",
    "\n",
    "                xs, xe = 0,val_outputsLPSData.shape[1]\n",
    "                ys, ye = 0,val_outputsLPSData.shape[2]\n",
    "                zs, ze = 0,val_outputsLPSData.shape[3]\n",
    "\n",
    "                if((end_coord[0]-start_coord[0])<roi_size[0]):\n",
    "                    padlen = roi_size[0]-(end_coord[0]-start_coord[0])\n",
    "                    xs = padlen//2\n",
    "                    xe = roi_size[0] -(padlen - xs)\n",
    "                if((end_coord[1]-start_coord[1])<roi_size[1]):\n",
    "                    padlen = roi_size[1]-(end_coord[1]-start_coord[1])\n",
    "                    ys = padlen//2\n",
    "                    ye = roi_size[1] -(padlen - ys)\n",
    "                if((end_coord[2]-start_coord[2])<roi_size[2]):\n",
    "                    padlen = roi_size[2]-(end_coord[2]-start_coord[2])\n",
    "                    zs = padlen//2\n",
    "                    ze = roi_size[2] - (padlen - zs)\n",
    "\n",
    "                val_outputsPadded[:, start_coord[0]:end_coord[0], start_coord[1]:end_coord[1], start_coord[2]:end_coord[2]] = \\\n",
    "                val_outputsLPSData[:, xs:xe, ys:ye, zs:ze]\n",
    "\n",
    "                '''Sigmoid or logit'''\n",
    "                val_outputsSigPadded[:, start_coord[0]:end_coord[0], start_coord[1]:end_coord[1], start_coord[2]:end_coord[2]] = \\\n",
    "                val_outputsSigLPSData[:, xs:xe, ys:ye, zs:ze]\n",
    "\n",
    "\n",
    "                ''' If both classes (IDH and nonIDH) overlaps, assigning to the region to non-IDH'''\n",
    "\n",
    "                WT_noidh = torch.from_numpy(val_outputsPadded[0:1,:, :, :]).long()\n",
    "                WT_idh = torch.from_numpy(val_outputsPadded[1:2,:, :, :]).long()\n",
    "\n",
    "                WT=torch.logical_or(WT_noidh, WT_idh).long()\n",
    "\n",
    "                WT_noidhMask = torch.eq(WT_noidh,1)\n",
    "                WTidhOnly = WT.masked_fill(WT_noidhMask, 0)\n",
    "                WTidhOnly2=torch.where(WTidhOnly==1, 2, Edema)\n",
    "\n",
    "                mergedWT=WTidhOnly2 + WT_noidh\n",
    "\n",
    "\n",
    "                print('#'*20)\n",
    "                bi = 0\n",
    "                input_file_path = infer_data['image_meta_dict']['filename_or_obj'][bi]\n",
    "                subID=os.path.basename(os.path.dirname(input_file_path))\n",
    "                print('Subject ID: ', subID)\n",
    "\n",
    "\n",
    "\n",
    "                val_outputsPadded_3class=WTidhOnly2 + WT_noidh #(0=background, 1=no IDH, 2= IDH)\n",
    "                val_outputsPadded_3class = torch.where(val_outputsPadded_3class==3, 1, val_outputsPadded_3class).numpy().astype(np.uint8)\n",
    "\n",
    "                val_outputsPadded_3class = np.moveaxis(val_outputsPadded_3class, 0, -1).astype(np.float32)\n",
    "                val_outputsSigPadded = np.moveaxis(val_outputsSigPadded, 0, -1).astype(np.float32)\n",
    "\n",
    "                #input_file_path = infer_data['image_meta_dict']['filename_or_obj'][bi]\n",
    "                #subID=os.path.basename(os.path.dirname(input_file_path))\n",
    "\n",
    "                if channel_nums==4:\n",
    "                    new_file_name_sig = subID + \"_sigmoid.nii.gz\"\n",
    "                    new_file_name = subID + \".nii.gz\"\n",
    "                elif channel_nums==2:\n",
    "                    new_file_name_sig = subID + \"_sigmoid.nii.gz\"\n",
    "                    new_file_name = subID + \".nii.gz\"\n",
    "                else:\n",
    "                    print(\"The program is not saving\")\n",
    "\n",
    "\n",
    "                if not os.path.exists(prediction_folder):\n",
    "                    os.makedirs(prediction_folder)\n",
    "\n",
    "                prediction_folder_sub = os.path.join(prediction_folder, subID)\n",
    "                if not os.path.exists(prediction_folder_sub):\n",
    "                    os.makedirs(prediction_folder_sub)\n",
    "\n",
    "\n",
    "                exact_file_path = os.path.join(prediction_folder_sub, new_file_name)\n",
    "                exact_file_sig_path = os.path.join(prediction_folder_sub, new_file_name_sig)\n",
    "\n",
    "                affine = infer_data['image_meta_dict']['affine'][bi]\n",
    "                target_affine = infer_data['image_meta_dict']['original_affine'][bi]\n",
    "                #affine=val_outputs_now_affine\n",
    "                #target_affine=val_outputs_now_affine\n",
    "\n",
    "                resample=False\n",
    "                mode = 'nearest'\n",
    "                monai.data.write_nifti(data=val_outputsPadded_3class, file_name=exact_file_path, affine=affine, target_affine=target_affine, resample=resample)\n",
    "                monai.data.write_nifti(data=val_outputsSigPadded, file_name=exact_file_sig_path, affine=affine, target_affine=target_affine, resample=resample)\n",
    "\n",
    "                \n",
    "    \n",
    "                ## Free up GPU memory after training\n",
    "                #model = None\n",
    "                #del model\n",
    "                ##train_loader, val_loader = None, None        \n",
    "                #gc.collect()\n",
    "                #torch.cuda.empty_cache()\n",
    "                \n",
    "            \n",
    "            \n",
    "         \n",
    "            WTdices.append(dice_metric.aggregate().item())\n",
    "            dice_metric.reset()\n",
    "            \n",
    "            WTHD95s.append(HD_metric.aggregate().item())\n",
    "            HD_metric.reset()\n",
    "\n",
    "            input_file_path = infer_data['image_meta_dict']['filename_or_obj'][bi]\n",
    "            subID=os.path.basename(os.path.dirname(input_file_path))\n",
    "            print('Subject ID: ', subID)\n",
    "            Infer_idLst.append(subID)\n",
    "\n",
    "        y_pred, y = y_pred.cpu(), y.cpu()\n",
    "        \n",
    "        \n",
    "        \n",
    "        aDF = pd.DataFrame.from_dict({\"SubjectID\": Infer_idLst,  \"Model\": [\"DynUnet\"]* len(WTdices), \"Channels\":[channelNums]*len(WTdices), \"Tumor regions\": [\"WT\"]*len(WTdices), \"Dice score\": WTdices, \"HD95\": WTHD95s,\\\n",
    "                                     'ylabel':y.tolist(), 'ypred': y_pred.tolist()})\n",
    "        del y, y_pred\n",
    "        \n",
    "                    \n",
    "#     dfET = pd.DataFrame({\"BraTS21ID\":Infer_idLst, \"Model\": [\"DynUnet\"]* len(ETdices), \"Channels\":[channelNums]*len(ETdices), \"Tumor regions\": [\"ET\"]*len(ETdices), \"Dice score\": ETdices, \"HD95\": ETHD95s})\n",
    "#     dfTC = pd.DataFrame({\"BraTS21ID\":Infer_idLst, \"Model\": [\"DynUnet\"]* len(TCdices), \"Channels\":[channelNums]*len(TCdices), \"Tumor regions\": [\"TC\"]*len(TCdices), \"Dice score\": TCdices, \"HD95\": TCHD95s})\n",
    "#     dfWT = pd.DataFrame({\"BraTS21ID\":Infer_idLst,\"Model\": [\"DynUnet\"]* len(WTdices), \"Channels\":[channelNums]*len(WTdices), \"Tumor regions\": [\"WT\"]*len(WTdices), \"Dice score\": WTdices, \"HD95\": WTHD95s})\n",
    "#     dfWT_TC_ET = pd.DataFrame({\"BraTS21ID\":Infer_idLst,\"Model\": [\"DynUnet\"]* len(WTdices), \"Channels\":[channelNums]*len(WTdices), \"Tumor regions\": [\"WT_TC_ET_Regions\"]*len(WTdices), \"Dice score\": Alldices, \"HD95\": AllHD95s})\n",
    "    \n",
    "#     dfRegions = pd.concat([dfTC, dfWT, dfET, dfWT_TC_ET])\n",
    "#     return dfRegions\n",
    "\n",
    "\n",
    "    return aDF\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d565c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1260e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferWithTA(data_loader,listmodels, prediction_folder=\"./\", topk=1, num_channels = 4,\\\n",
    "                orientation=\"LPS\", withoptimizer = False, softmaxEnsemble=False, save_inference = False, tta = False):\n",
    "    \"\"\"\n",
    "    run inference, the output folder will be \"./output\"\n",
    "    \"\"\"        \n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    listmodels = listmodels[0:topk]\n",
    "    for x in listmodels:\n",
    "        print(f\"available model file: {x}.\")\n",
    "        \n",
    "    channel_nums =  monai.utils.misc.first(data_loader)['image'].shape[1] ##next(iter(val_loader[\"image\"])).shape[1]\n",
    "    channelNums = f\"{channel_nums} channels\"\n",
    "    keys = ('image',)\n",
    "    patch_size = (32, 32, 32)\n",
    "    \n",
    "    post_trans_sigbin = Compose([EnsureType(), Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "    post_trans_bin = Compose([EnsureType(), AsDiscrete(threshold=0.5)])\n",
    "    post_trans_sig = Compose([EnsureType(), Activations(sigmoid=True)])\n",
    "    \n",
    "    auc_metric = ROCAUCMetric()\n",
    "    dice_metric = monai.metrics.DiceMetric(include_background=True, reduction='mean', get_not_nans=False)\n",
    "    dice_metric_batch = monai.metrics.DiceMetric(include_background=True, reduction='mean_batch', get_not_nans=False)\n",
    "    \n",
    "    \n",
    "    HD_metric = HausdorffDistanceMetric(include_background=True, percentile = 95., reduction='mean', get_not_nans=False)\n",
    "    HD_metric_batch = HausdorffDistanceMetric(include_background=True, percentile = 95., reduction='mean_batch', get_not_nans=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    post_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])  \n",
    "    \n",
    "    def one_hot_permute(x):\n",
    "        return F.one_hot(x.squeeze(dim=0).long(), num_classes=num_classes).permute(3, 0, 1, 2)\n",
    "    \n",
    "    def get_binarize_tensor(x, dim=1):\n",
    "        x_chlist = torch.unbind(x, dim = dim)\n",
    "        bin_x = torch.zeros_like(x_chlist[0])\n",
    "        for x_i in x_chlist:\n",
    "            bin_x = torch.logical_or(x_i, bin_x)\n",
    "        return bin_x.unsqueeze(dim=dim).to(torch.float32)\n",
    "        \n",
    "    \n",
    "    def get_segclass(x, dim = 1):\n",
    "        xdvc = x.device\n",
    "        x_chlist = torch.unbind(x, dim = dim)\n",
    "        xclassNoList = []\n",
    "        xvalueList = []\n",
    "        for x_i in x_chlist:\n",
    "            \n",
    "            xv, xc = torch.unique(x_i, return_counts  = True)\n",
    "\n",
    "            if xc.shape[0]==1:\n",
    "                xclassNoList.append(xc[0].item())\n",
    "                xvalueList.append(xv[0].item())\n",
    "            elif xc.shape[0]==2:\n",
    "                    if torch.any(torch.eq(xv, 1)):\n",
    "                        xclassNoList.append(xc[1].item())\n",
    "                        xvalueList.append(xv[1].item())\n",
    "                    else:\n",
    "                        print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "            else:\n",
    "                print('The function only supports binarized tensor (binarized unique values, 0(n=...) and 1(n=...) only)\\n')\n",
    "        \n",
    "        if torch.any(torch.eq(torch.tensor(xvalueList), 1)):  \n",
    "            xclass = torch.argmax(torch.tensor(xclassNoList).to(xdvc))\n",
    "        else:\n",
    "            '''If all uniques class values are 0, we are assigning nan values as a class'''\n",
    "            xclass = torch.tensor(float('NaN')).to(xdvc)\n",
    "            \n",
    "        return xclass \n",
    "    \n",
    "    \n",
    "    \n",
    "    keys = (\"image\",)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "        y = torch.tensor([], dtype=torch.long, device=device)\n",
    "    \n",
    "        for infindx, infer_data in enumerate(tqdm(data_loader)):\n",
    "\n",
    "            \n",
    "            val_inputs, val_labels, val_IDH_labels = (\n",
    "                infer_data['image'].to(device),\n",
    "                infer_data['label'].to(device),\n",
    "                infer_data['IDH_label'].to(device),\n",
    "            )\n",
    "                \n",
    "\n",
    "            n_class = 2\n",
    "            val_outputsAll = torch.zeros(val_inputs.shape[0], n_class, val_inputs.shape[2], val_inputs.shape[3], val_inputs.shape[4]).to(device)\n",
    "            n_model = 0.\n",
    "            \n",
    "            #for mdlindx in (number+1 for number in range(topk)):\n",
    "            for mdlindx in range(topk):\n",
    "    \n",
    "                print(f'Model {mdlindx}, {listmodels[mdlindx]} is running now')\n",
    "                model = None          \n",
    "                model = DynUNet(\n",
    "                    spatial_dims=3,\n",
    "                    in_channels=4,\n",
    "                    out_channels=n_class,\n",
    "                    kernel_size=kernels,\n",
    "                    strides=strides,\n",
    "                    upsample_kernel_size=strides[1:],\n",
    "                    norm_name=\"batch\",\n",
    "                    filters = filters,\n",
    "                    deep_supervision=True,\n",
    "                    res_block=True,\n",
    "                    deep_supr_num=2,\n",
    "                ).to(device)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                if withoptimizer ==True:\n",
    "                    \n",
    "                    state_dictsAll = torch.load(listmodels[mdlindx], map_location=device)\n",
    "                    model.load_state_dict(state_dictsAll[\"model_state_dict\"])\n",
    "                    model.eval()\n",
    "                \n",
    "                else:    \n",
    "                    model.load_state_dict(torch.load(listmodels[mdlindx], map_location=device))\n",
    "                    model.eval()\n",
    "                \n",
    "                \n",
    "                n = 1.0\n",
    "                roi_size = patch_size #(32, 32, 32)\n",
    "                sw_batch_size = 8\n",
    "                val_overlap = 0.5\n",
    "                mode=\"gaussian\"\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "\n",
    "                    preds = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device)\n",
    "                    \n",
    "                flip_val_inputs = torch.flip(val_inputs, dims=(2, 3, 4))\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    \n",
    "                    mfpred = sliding_window_inference(flip_val_inputs, roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device)\n",
    "                 \n",
    "                flip_pred = torch.flip(mfpred, dims=(2, 3, 4))\n",
    "                preds  = preds + flip_pred\n",
    "                n = n + 1.0\n",
    "                \n",
    "                if tta:\n",
    "                    \n",
    "                    for _ in range(4):\n",
    "                        # test time augmentations\n",
    "                        _img = RandGaussianNoised(keys[0], prob=1.0, std=0.01)(infer_data)[keys[0]]\n",
    "\n",
    "\n",
    "                        with torch.cuda.amp.autocast():\n",
    "\n",
    "                            #val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model, sw_device = device, device = device)\n",
    "                            _img_pred = sliding_window_inference(_img.to(device), roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device)\n",
    "                            preds = preds + _img_pred\n",
    "                            n = n + 1.0\n",
    "\n",
    "\n",
    "                        _img_flip = torch.flip(_img, dims=(2, 3, 4)) \n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            _mf_flip_pred = sliding_window_inference(_img_flip.to(device), roi_size, sw_batch_size, model, mode = mode, overlap = val_overlap, sw_device = device, device = device)\n",
    "\n",
    "                        _img_flip_pred = torch.flip(_mf_flip_pred, dims=(2, 3, 4))\n",
    "                        preds = preds + _img_flip_pred\n",
    "                        n = n + 1.0\n",
    "                 \n",
    "                \n",
    "                preds = preds / n\n",
    "                \n",
    "                if softmaxEnsemble:\n",
    "                    preds = torch.stack([post_trans_sig(i) for i in torch.unbind(preds, dim = 0)], dim = 0)\n",
    "                val_outputsAll = val_outputsAll + preds\n",
    "                n_model = n_model+1.0\n",
    "                \n",
    "                # Free up GPU memory after training\n",
    "                model = None\n",
    "                del model\n",
    "                #train_loader, val_loader = None, None        \n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                           \n",
    "            val_outputsAll = val_outputsAll / n_model\n",
    "            \n",
    "            val_outputs = post_trans_bin(val_outputsAll) if softmaxEnsemble else post_trans_sigbin(val_outputsAll)\n",
    "            '''Sigmoid or logit'''\n",
    "            val_outputsSig = val_outputsAll if softmaxEnsemble else post_trans_sig(val_outputsAll)\n",
    "            #val_outputsSig = val_outputsAll\n",
    "            \n",
    "            \n",
    "\n",
    "            #val_labels2hot = torch.stack([one_hot_permute(i) for i in torch.unbind(val_labels, dim = 0)], dim = 0)\n",
    "\n",
    "\n",
    "            val_labels_bin = get_binarize_tensor(val_labels, dim=1)\n",
    "            val_outputs_bin = get_binarize_tensor(val_outputs, dim=1)\n",
    "\n",
    "            dice_metric(y_pred=val_outputs_bin, y=val_labels_bin)\n",
    "\n",
    "            klcc = KeepLargestConnectedComponent(applied_labels = [0, 1], is_onehot = True)  ##is_onehot=True or None by default\n",
    "            #val_labels = klcc(val_labels.squeeze(dim=0)).unsqueeze(dim=0)\n",
    "            val_labels = torch.stack([klcc(i) for i in torch.unbind(val_labels, dim = 0)], dim = 0)\n",
    "\n",
    "            val_label4mSeg_C = get_segclass(val_outputs)\n",
    "            #val_surv_labels = val_surv_labels.squeeze(dim=1)  ###Squeezing from B, 1 to B if needed\n",
    "            y_pred = torch.cat([y_pred, val_label4mSeg_C.view(1)], dim=0)\n",
    "            y = torch.cat([y, val_IDH_labels], dim=0)\n",
    "                \n",
    "                        \n",
    "            \n",
    "        mdice_value = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()\n",
    "\n",
    "        y_pred, y = y_pred.cpu(), y.cpu()\n",
    "        \n",
    "        num_nanvalues = torch.isnan(y_pred).sum().item()\n",
    "        not_nanmask = torch.logical_not(torch.isnan(y_pred))\n",
    "        y = y[not_nanmask]\n",
    "        y_pred = y_pred[not_nanmask]\n",
    "        \n",
    "        \n",
    "        acc_value = torch.eq(y_pred, y)\n",
    "        acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "\n",
    "        #y_onehot = [post_label(i) for i in torch.unbind(y, dim=0)]\n",
    "        #y_pred_act = [post_pred(i) for i in torch.unbind(y_pred, dim=0)]\n",
    "\n",
    "        auc_metric(y_pred, y)\n",
    "        auc_result = auc_metric.aggregate()\n",
    "        auc_metric.reset()\n",
    "\n",
    "        accscore = balanced_accuracy_score(y, y_pred)\n",
    "        f1score = f1_score(y, y_pred, average='micro')\n",
    "        del y, y_pred\n",
    "        \n",
    "        aDCT = {\"dice_score\": mdice_value,  'acc_metric':auc_result,  \"accuracy\": accscore,  'f1score':f1score, 'NanSubjectNos':num_nanvalues}\n",
    "        \n",
    "                    \n",
    "#     dfET = pd.DataFrame({\"BraTS21ID\":Infer_idLst, \"Model\": [\"DynUnet\"]* len(ETdices), \"Channels\":[channelNums]*len(ETdices), \"Tumor regions\": [\"ET\"]*len(ETdices), \"Dice score\": ETdices, \"HD95\": ETHD95s})\n",
    "#     dfTC = pd.DataFrame({\"BraTS21ID\":Infer_idLst, \"Model\": [\"DynUnet\"]* len(TCdices), \"Channels\":[channelNums]*len(TCdices), \"Tumor regions\": [\"TC\"]*len(TCdices), \"Dice score\": TCdices, \"HD95\": TCHD95s})\n",
    "#     dfWT = pd.DataFrame({\"BraTS21ID\":Infer_idLst,\"Model\": [\"DynUnet\"]* len(WTdices), \"Channels\":[channelNums]*len(WTdices), \"Tumor regions\": [\"WT\"]*len(WTdices), \"Dice score\": WTdices, \"HD95\": WTHD95s})\n",
    "#     dfWT_TC_ET = pd.DataFrame({\"BraTS21ID\":Infer_idLst,\"Model\": [\"DynUnet\"]* len(WTdices), \"Channels\":[channelNums]*len(WTdices), \"Tumor regions\": [\"WT_TC_ET_Regions\"]*len(WTdices), \"Dice score\": Alldices, \"HD95\": AllHD95s})\n",
    "    \n",
    "#     dfRegions = pd.concat([dfTC, dfWT, dfET, dfWT_TC_ET])\n",
    "#     return dfRegions\n",
    "\n",
    "\n",
    "    return aDCT\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d3d2c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fold0': ['/raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth',\n",
       "  '/raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth'],\n",
       " 'fold1': ['/raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth',\n",
       "  '/raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth'],\n",
       " 'fold2': ['/raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold2_0.8901_epoch122.pth',\n",
       "  ['/raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold2_0.8862_epoch132.pth']]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# listmodels = glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_1PatchEp500V2_Fold0_0.8371_epoch207.pt*') \n",
    "# listmodels.extend(glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_1PatchEp500V2_Fold1_0.8944_epoch75.pt*')) \n",
    "# listmodels.extend(glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_1PatchEp500V2_Fold2_0.8719_epoch482.pt*'))\n",
    "# print(listmodels)\n",
    "\n",
    "prediction_folder = f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_1PatchEp500V2' \n",
    "modelDCTList = {'fold0': [glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pt*')[0],\n",
    "                          glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth*')[0]],\\\n",
    "               'fold1':[glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pt*')[0],\n",
    "                        glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pt*')[0]],\\\n",
    "               'fold2':[glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold2_0.8901_epoch122.pt*')[0],\n",
    "                        glob.glob(f'{save_dir}/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold2_0.8862_epoch132.pt*')]}\n",
    "modelDCTList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef7c42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_inference = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39cfc71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available model file: /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth.\n",
      "available model file: /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n",
      "  0%|                                                                                           | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–ˆâ–                                                                                 | 1/71 [00:08<10:27,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–ˆâ–Ž                                                                                | 2/71 [00:13<07:32,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–ˆâ–ˆâ–ˆâ–Œ                                                                               | 3/71 [00:19<06:49,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                              | 4/71 [00:24<06:17,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                             | 5/71 [00:32<07:06,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                            | 6/71 [00:42<08:15,  7.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                          | 7/71 [00:47<07:21,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                         | 8/71 [00:52<06:43,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                        | 9/71 [00:57<06:12,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                      | 10/71 [01:07<07:10,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                     | 11/71 [01:14<06:58,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                    | 12/71 [01:20<06:46,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                   | 13/71 [01:25<05:58,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                 | 14/71 [01:34<06:42,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                | 15/71 [01:39<05:57,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                               | 16/71 [01:44<05:26,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                              | 17/71 [01:51<05:48,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                             | 18/71 [02:02<06:46,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                            | 19/71 [02:07<05:54,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                           | 20/71 [02:20<07:34,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                         | 21/71 [02:30<07:40,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                        | 22/71 [02:36<06:37,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                       | 23/71 [02:46<06:57,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                      | 24/71 [02:49<05:30,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                     | 25/71 [02:56<05:15,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                    | 26/71 [03:01<04:47,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                  | 27/71 [03:07<04:42,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                 | 28/71 [03:14<04:33,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                | 29/71 [03:20<04:31,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                               | 30/71 [03:26<04:11,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                              | 31/71 [03:33<04:22,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                             | 32/71 [03:43<04:47,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                            | 33/71 [03:48<04:17,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                          | 34/71 [03:55<04:15,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                         | 35/71 [04:03<04:23,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 36/71 [04:11<04:24,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                       | 37/71 [04:18<04:02,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                      | 38/71 [04:23<03:39,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 39/71 [04:26<03:01,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 40/71 [04:33<03:05,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  | 41/71 [04:41<03:19,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                 | 42/71 [04:53<03:56,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 43/71 [05:01<03:44,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 44/71 [05:04<03:00,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 45/71 [05:13<03:06,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 46/71 [05:17<02:39,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                           | 47/71 [05:23<02:27,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 48/71 [05:29<02:20,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 49/71 [05:37<02:29,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 50/71 [05:46<02:34,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 51/71 [05:50<02:06,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 52/71 [05:56<01:59,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 53/71 [06:03<01:59,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 54/71 [06:09<01:45,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 55/71 [06:15<01:40,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 56/71 [06:22<01:36,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 57/71 [06:28<01:26,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 58/71 [06:35<01:25,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 59/71 [06:41<01:17,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 60/71 [06:45<01:01,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 61/71 [06:49<00:49,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 62/71 [06:53<00:42,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 63/71 [06:59<00:42,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 64/71 [07:12<00:52,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 65/71 [07:17<00:41,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/71 [07:22<00:31,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 67/71 [07:25<00:21,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 68/71 [07:31<00:16,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 69/71 [07:39<00:12,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 70/71 [07:43<00:05,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8423_epoch197.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold0_0.8422_epoch115.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [07:50<00:00,  6.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available model file: /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth.\n",
      "available model file: /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         239.00209204]\n",
      "  0%|                                                                                           | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–ˆâ–                                                                                 | 1/72 [00:04<05:40,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–ˆâ–Ž                                                                                | 2/72 [00:15<09:29,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–ˆâ–ˆâ–ˆâ–                                                                               | 3/72 [00:22<08:59,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                              | 4/72 [00:32<09:31,  8.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                             | 5/72 [00:35<07:18,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                            | 6/72 [00:39<06:18,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                           | 7/72 [00:44<05:58,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                         | 8/72 [00:48<05:23,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                        | 9/72 [00:55<05:57,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                      | 10/72 [01:08<08:10,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                     | 11/72 [01:13<07:06,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                    | 12/72 [01:21<07:14,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                   | 13/72 [01:27<06:50,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                  | 14/72 [01:33<06:19,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                 | 15/72 [01:38<05:56,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                               | 16/72 [01:47<06:25,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                              | 17/72 [01:51<05:32,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                             | 18/72 [01:57<05:35,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                            | 19/72 [02:07<06:23,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                           | 20/72 [02:13<05:55,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                          | 21/72 [02:18<05:21,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                         | 22/72 [02:26<05:39,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                       | 23/72 [02:30<04:47,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                      | 24/72 [02:34<04:20,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                     | 25/72 [02:40<04:19,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 26/72 [02:47<04:44,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                   | 27/72 [02:56<05:13,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                  | 28/72 [03:00<04:27,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                 | 29/72 [03:10<05:11,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 30/72 [03:16<04:41,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                              | 31/72 [03:21<04:16,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                             | 32/72 [03:27<04:11,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                            | 33/72 [03:36<04:36,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                           | 34/72 [03:41<03:59,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                          | 35/72 [03:49<04:14,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 36/72 [03:57<04:18,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 37/72 [04:02<03:53,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                      | 38/72 [04:12<04:17,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 39/72 [04:15<03:25,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 40/72 [04:22<03:25,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 41/72 [04:29<03:30,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 42/72 [04:34<03:05,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8695_epoch319.pth is running now\n",
      "Model 1, /raid/brats2021/pthBraTS2020_IDHGenomics/DynUNetVariants_Brats20/DynUNet_BratsTCGA_3CV_4Chnls1PatchSWIRngr21_2nclass_MorePatchBNormEp500_Fold1_0.8692_epoch384.pth is running now\n"
     ]
    }
   ],
   "source": [
    "n_splits = 3\n",
    "aDCTResultList = list()\n",
    "if start_inference:\n",
    "    #### Running 10 folds\n",
    "    for i in range(0, n_splits):\n",
    "        \n",
    "\n",
    "        \n",
    "        infer_files_fld = copy.deepcopy(BraTS20SubjectsIDHTestDCT[f'fold{i}'])\n",
    "        \n",
    "        infer_dataset_fld = monai.data.Dataset(data=infer_files_fld, transform=val_transforms)\n",
    "       \n",
    "        infer_loader_fld = monai.data.DataLoader(infer_dataset_fld, batch_size=1, shuffle=False) #num_workers=2, pin_memory=True\n",
    "        \n",
    "        aDCTResult = inferWithTA(data_loader = infer_loader_fld, listmodels=modelDCTList[f'fold{i}'], prediction_folder=prediction_folder, topk=len(modelDCTList[f'fold{i}']), num_channels=4,\\\n",
    "                    orientation=\"LPS\", withoptimizer = False, softmaxEnsemble= True, tta = True)\n",
    "        aDCTResult['TestSplitName'] = f\"split{i}\"\n",
    "        aDCTResultList.append(aDCTResult.copy())\n",
    "        \n",
    "    \n",
    "    start_inference = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04fd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Result on the testset (3 testsest from 3 splits)')\n",
    "DFResult = pd.DataFrame.from_dict(aDCTResultList)\n",
    "display(DFResult)\n",
    "DFResult.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df6cb7",
   "metadata": {},
   "source": [
    "### Extracting validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48c45d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on the validation set (3 validationsets from 3 splits)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ValSplitName</th>\n",
       "      <th>dice_score</th>\n",
       "      <th>acc_metric</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1score</th>\n",
       "      <th>NanSubjectNos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vsplit0</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.8371</td>\n",
       "      <td>0.8451</td>\n",
       "      <td>0.8451</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vsplit1</td>\n",
       "      <td>0.8634</td>\n",
       "      <td>0.8944</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vsplit2</td>\n",
       "      <td>0.8833</td>\n",
       "      <td>0.8719</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ValSplitName  dice_score  acc_metric  accuracy  f1score  NanSubjectNos\n",
       "0      vsplit0      0.8495      0.8371    0.8451   0.8451              0\n",
       "1      vsplit1      0.8634      0.8944    0.9014   0.9014              0\n",
       "2      vsplit2      0.8833      0.8719    0.8750   0.8750              0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dice_score</th>\n",
       "      <th>acc_metric</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1score</th>\n",
       "      <th>NanSubjectNos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.865400</td>\n",
       "      <td>0.867800</td>\n",
       "      <td>0.873833</td>\n",
       "      <td>0.873833</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.016989</td>\n",
       "      <td>0.028869</td>\n",
       "      <td>0.028168</td>\n",
       "      <td>0.028168</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.849500</td>\n",
       "      <td>0.837100</td>\n",
       "      <td>0.845100</td>\n",
       "      <td>0.845100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.856450</td>\n",
       "      <td>0.854500</td>\n",
       "      <td>0.860050</td>\n",
       "      <td>0.860050</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.863400</td>\n",
       "      <td>0.871900</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.873350</td>\n",
       "      <td>0.883150</td>\n",
       "      <td>0.888200</td>\n",
       "      <td>0.888200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.883300</td>\n",
       "      <td>0.894400</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dice_score  acc_metric  accuracy   f1score  NanSubjectNos\n",
       "count    3.000000    3.000000  3.000000  3.000000            3.0\n",
       "mean     0.865400    0.867800  0.873833  0.873833            0.0\n",
       "std      0.016989    0.028869  0.028168  0.028168            0.0\n",
       "min      0.849500    0.837100  0.845100  0.845100            0.0\n",
       "25%      0.856450    0.854500  0.860050  0.860050            0.0\n",
       "50%      0.863400    0.871900  0.875000  0.875000            0.0\n",
       "75%      0.873350    0.883150  0.888200  0.888200            0.0\n",
       "max      0.883300    0.894400  0.901400  0.901400            0.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current fold: 0 current epoch: 207 dice_score: 0.8495 acc_metric: 0.8371 accuracy: 0.8451, f1score: 0.8451 epoch 207 average training loss: 0.3055 average validation loss: 0.8860 \n",
    "# current fold: 1 current epoch: 75 dice_score: 0.8634 acc_metric: 0.8944 accuracy: 0.9014, f1score: 0.9014 epoch 75 average training loss: 0.8292 average validation loss: 0.8694\n",
    "# current fold: 2 current epoch: 482 dice_score: 0.8833 acc_metric: 0.8719 accuracy: 0.8750, f1score: 0.8750 epoch 482 average training loss: 0.1534 average validation loss: 0.8354\n",
    "\n",
    "print('Result on the validation set (3 validationsets from 3 splits)')\n",
    "valDF = pd.DataFrame.from_dict({'ValSplitName':['vsplit0', 'vsplit1', 'vsplit2'],  \"dice_score\": [0.8495, 0.8634, 0.8833],  'acc_metric':[0.8371, 0.8944, 0.8719], \\\n",
    "                                \"accuracy\": [0.8451, 0.9014, 0.8750],  'f1score':[0.8451, 0.9014, 0.8750], 'NanSubjectNos':[0, 0, 0]})\n",
    "display(valDF)\n",
    "valDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75999d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_multilabel_classification\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# X, y = make_multilabel_classification(random_state=0, n_classes=2)\n",
    "# inner_clf = LogisticRegression(solver=\"liblinear\", random_state=0)\n",
    "# clf = MultiOutputClassifier(inner_clf).fit(X, y)\n",
    "# y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)])\n",
    "# roc_auc_score(y, y_score, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd97ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_score(y, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_permute(x):\n",
    "    return F.one_hot(x.squeeze(dim=0).long(), num_classes=3).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d26c720",
   "metadata": {},
   "source": [
    "atensor = torch.tensor([[[0, 2, 0, 0],[0, 0, 0, 2],[0, 2, 0, 0]]])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dede7a40",
   "metadata": {},
   "source": [
    "one_hot_permute(atensor)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1054fb1a",
   "metadata": {},
   "source": [
    "atensor = torch.tensor([[[0, 1, 0, 0],[0, 0, 0, 1],[0, 1, 0, 0]]])\n",
    "one_hot_permute(atensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2218ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc= torch.tensor([0, 1, 0, 0, 2, 3, 10])\n",
    "if torch.any(torch.eq(xc, 11)):\n",
    "    print('Do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc = torch.tensor([0, 100, 500, 10000, 5])\n",
    "torch.argmax(xc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ea5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argsort(xc)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "is_onehot = True\n",
    "img = torch.ones((1, 64, 64, 64))\n",
    "is_onehot2 = img.shape[0] > 1 if is_onehot is None else is_onehot\n",
    "is_onehot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d207f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = np.array([[1.]])\n",
    "dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b9ef8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx = np.stack([dd, dd], axis = 0)\n",
    "dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87e84a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([dx, dx], axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c9f20b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atns1 = torch.tensor([100, 100, 100, 100])\n",
    "atns2 = torch.tensor([5, float('nan'), -10])\n",
    "\n",
    "torch.argmax(atns1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "964af2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_metric# 1.0 , auc# nan , f1# 1.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "y values can not be all 1, skip AUC computation and return `Nan`.\n"
     ]
    }
   ],
   "source": [
    "auc_metric = ROCAUCMetric()\n",
    "#y_pred = torch.tensor([float('NaN'), 0, 1, 0, 1, float('NaN')])\n",
    "y_pred = torch.tensor([float('NaN'), float('NaN'), float('NaN'), float('NaN'), float('NaN'), 1])\n",
    "y = torch.tensor([0, 0, 0, 0, 0, 1])\n",
    "\n",
    "if torch.all(torch.isnan(y_pred))==True:\n",
    "    print('acc_metric#', np.nan, ', auc#', np.nan, ', f1#', np.nan, '\\n')\n",
    "else:\n",
    "    \n",
    "\n",
    "    num_nanvalues = torch.isnan(y_pred).sum().item()\n",
    "    not_nanmask = torch.logical_not(torch.isnan(y_pred))\n",
    "    y = y[not_nanmask]\n",
    "    y_pred = y_pred[not_nanmask]\n",
    "    acc_value = torch.eq(y_pred, y)\n",
    "    acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "   \n",
    "    \n",
    "    \n",
    "    auc_metric(y_pred, y)\n",
    "    auc_result = auc_metric.aggregate()\n",
    "    auc_metric.reset()\n",
    "\n",
    "    accscore = balanced_accuracy_score(y, y_pred)\n",
    "    f1score = f1_score(y, y_pred, average='micro')\n",
    "    print('acc_metric#', acc_metric, ', auc#', auc_result, ', f1#', f1score, '\\n')\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b16d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "13db596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#y_onehot = [post_label(i) for i in torch.unbind(y, dim=0)]\n",
    "#y_pred_act = [post_pred(i) for i in torch.unbind(y_pred, dim=0)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa06419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3c741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
